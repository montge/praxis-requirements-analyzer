{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SiFP Function Point Analysis with Statistical Modeling\n",
    "**Analysis of Software Interface Function Points (SiFP) estimation accuracy using statistical modeling, code metrics correlation, linear/non-linear regression, and multi-model performance comparison with Neo4j integration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Jupyter Notebook Configuration\n",
    "# Purpose: Configure notebook environment, import all required libraries, and set display settings for optimal analysis\n",
    "# Dependencies: os, re, logging, warnings, typing, pathlib, dotenv, pandas, numpy, matplotlib, seaborn, neo4j, scipy, sklearn, xgboost, IPython\n",
    "# Breadcrumbs: Setup -> Configuration\n",
    "\n",
    "# Standard library imports - for file operations, environment variables, typing, and warnings\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data processing and manipulation - pandas and numpy are core to all analysis cells\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries - used extensively in cells 4, 7, 8, 9, 10, 11 for plots and charts\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Database connectivity - needed for Neo4j graph database queries in cells 2, 3, 6\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Statistical analysis - scipy for statistics, used in correlation analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, ttest_rel, ttest_ind, f_oneway, chi2_contingency\n",
    "from scipy.stats import bootstrap, binomtest\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Machine learning - required for advanced modeling in cells 7-11\n",
    "# Core metrics and linear models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# Model validation\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# Feature preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Advanced modeling used in cell 11\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as XGBRegressor\n",
    "\n",
    "# Jupyter/IPython display tools - for rich output formatting throughout notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Suppress warnings for cleaner notebook output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging with shorter format for better PDF wrapping\n",
    "import textwrap\n",
    "\n",
    "class PDFLoggingFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter that wraps long log messages for better PDF display\"\"\"\n",
    "    def format(self, record):\n",
    "        # Format the basic log record\n",
    "        formatted = super().format(record)\n",
    "        \n",
    "        # Wrap long lines at 120 characters with proper indentation\n",
    "        if len(formatted) > 120:\n",
    "            lines = textwrap.wrap(formatted, width=120, \n",
    "                                subsequent_indent='    ')  # Indent continuation lines\n",
    "            formatted = '\\n'.join(lines)\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "# Create custom formatter\n",
    "pdf_formatter = PDFLoggingFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom formatter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # Override any existing handlers\n",
    ")\n",
    "\n",
    "# Apply custom formatter to all handlers\n",
    "logger = logging.getLogger(__name__)\n",
    "for handler in logging.getLogger().handlers:\n",
    "    handler.setFormatter(pdf_formatter)\n",
    "\n",
    "# Configure pandas display settings for legal landscape format\n",
    "pd.set_option('display.width', 130)           # Set width threshold for legal landscape\n",
    "pd.set_option('display.max_columns', 25)     # Reasonable number of columns\n",
    "pd.set_option('display.max_colwidth', 25)    # Compact column width\n",
    "pd.set_option('display.precision', 2)        # Only 2 decimal places to save space\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  # Consistent float formatting\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "# Note: Removed expand_frame_repr=False to allow natural wrapping at 130 chars\n",
    "\n",
    "# Configure matplotlib settings for consistent visualizations\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.style.use('seaborn' if 'seaborn' in plt.style.available else 'default')\n",
    "\n",
    "print(\"Notebook Configuration Complete ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Environment Setup\n",
    "# Purpose: Load environment variables and configure Neo4j connection parameters from .env file\n",
    "# Dependencies: dotenv, os, logging\n",
    "# Breadcrumbs: Setup -> Environment\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j credentials from environment variables\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_PROJECT_NAME = os.getenv('NEO4J_PROJECT_NAME')\n",
    "\n",
    "# Log environment setup status\n",
    "logger.info(\"Environment variables loaded\")\n",
    "logger.info(f\"Using project: {NEO4J_PROJECT_NAME}\")\n",
    "logger.info(f\"Neo4j URI: {NEO4J_URI if NEO4J_URI else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Neo4j Connection Setup\n",
    "# Purpose: Create Neo4j driver instance and establish database connection for querying SIFP data\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection\n",
    "\n",
    "def create_neo4j_driver():\n",
    "    \"\"\"Create and return a Neo4j driver instance with authentication.\n",
    "    \n",
    "    Establishes a connection to the Neo4j database using credentials from \n",
    "    environment variables. This function is used to create a reusable driver\n",
    "    for executing Cypher queries throughout the notebook.\n",
    "    \n",
    "    Environment Variables Required:\n",
    "        NEO4J_URI (str): The URI of the Neo4j database (e.g., 'bolt://localhost:7687')\n",
    "        NEO4J_USER (str): Username for Neo4j authentication\n",
    "        NEO4J_PASSWORD (str): Password for Neo4j authentication\n",
    "    \n",
    "    Returns:\n",
    "        neo4j.Driver: Configured Neo4j driver instance ready for session creation\n",
    "        \n",
    "    Raises:\n",
    "        ConnectionError: If unable to connect to the Neo4j database\n",
    "        AuthError: If authentication fails\n",
    "        ServiceUnavailable: If the Neo4j service is not available\n",
    "        \n",
    "    Example:\n",
    "        >>> driver = create_neo4j_driver()\n",
    "        >>> with driver.session() as session:\n",
    "        ...     result = session.run(\"MATCH (n) RETURN count(n)\")\n",
    "        ...     print(result.single()[0])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = create_neo4j_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Statistical Utility Functions  \n",
    "# Purpose: Define comprehensive statistical analysis functions for correlation, comparison, bootstrap, and regression diagnostics\n",
    "# Dependencies: numpy, scipy, sklearn, statsmodels\n",
    "# Breadcrumbs: Setup -> Statistical Tools\n",
    "\n",
    "def calculate_correlation_with_stats(x, y, method='pearson'):\n",
    "    \"\"\"Calculate correlation coefficient with comprehensive statistical analysis.\n",
    "    \n",
    "    Computes correlation between two variables with significance testing, confidence \n",
    "    intervals using Fisher transformation, and effect size interpretation. Handles \n",
    "    missing values and provides comprehensive statistical output.\n",
    "    \n",
    "    Args:\n",
    "        x (array-like): First data vector (dependent or independent variable)\n",
    "        y (array-like): Second data vector (dependent or independent variable)  \n",
    "        method (str, optional): Correlation method to use. Defaults to 'pearson'.\n",
    "            - 'pearson': Pearson product-moment correlation (assumes linear relationship)\n",
    "            - 'spearman': Spearman rank correlation (non-parametric, monotonic relationships)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive correlation analysis results containing:\n",
    "            - correlation (float): Correlation coefficient (-1 to 1)\n",
    "            - p_value (float): Two-tailed p-value for significance test\n",
    "            - confidence_interval (tuple): 95% confidence interval (lower, upper)\n",
    "            - n_samples (int): Number of valid paired observations used\n",
    "            - interpretation (str): Combined effect size and significance interpretation\n",
    "            - effect_size (str): Effect size category ('negligible', 'small', 'medium', 'large', 'very large')\n",
    "            - significance (str): Statistical significance level description\n",
    "    \n",
    "    Notes:\n",
    "        - Automatically removes NaN values from both vectors before analysis\n",
    "        - Requires minimum 3 valid paired observations for meaningful results\n",
    "        - Uses Fisher z-transformation for confidence interval calculation\n",
    "        - Effect size interpretation follows Cohen's conventions:\n",
    "            * |r| < 0.1: negligible\n",
    "            * 0.1 ≤ |r| < 0.3: small  \n",
    "            * 0.3 ≤ |r| < 0.5: medium\n",
    "            * 0.5 ≤ |r| < 0.7: large\n",
    "            * |r| ≥ 0.7: very large\n",
    "    \n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> x = [1, 2, 3, 4, 5]\n",
    "        >>> y = [2, 4, 6, 8, 10]\n",
    "        >>> result = calculate_correlation_with_stats(x, y, method='pearson')\n",
    "        >>> print(f\"r = {result['correlation']:.3f}, p = {result['p_value']:.3f}\")\n",
    "        >>> print(result['interpretation'])\n",
    "        r = 1.000, p = 0.000\n",
    "        very large effect, highly significant (p < 0.001)\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(x) | np.isnan(y))\n",
    "    x_clean = np.array(x)[mask]\n",
    "    y_clean = np.array(y)[mask]\n",
    "    \n",
    "    if len(x_clean) < 3:\n",
    "        return {\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'confidence_interval': (np.nan, np.nan),\n",
    "            'n_samples': len(x_clean),\n",
    "            'interpretation': 'Insufficient data'\n",
    "        }\n",
    "    \n",
    "    # Calculate correlation\n",
    "    if method == 'pearson':\n",
    "        corr, p_val = pearsonr(x_clean, y_clean)\n",
    "    else:\n",
    "        corr, p_val = spearmanr(x_clean, y_clean)\n",
    "    \n",
    "    # Convert correlation results to standard float types\n",
    "    try:\n",
    "        corr = np.float64(corr).item()  # Convert to standard Python float\n",
    "    except (TypeError, ValueError, AttributeError):\n",
    "        corr = np.nan\n",
    "        \n",
    "    try:\n",
    "        p_val = np.float64(p_val).item()  # Convert to standard Python float\n",
    "    except (TypeError, ValueError, AttributeError):\n",
    "        p_val = 1.0\n",
    "    \n",
    "    # Calculate confidence interval for correlation\n",
    "    n = len(x_clean)\n",
    "    ci_lower = np.nan\n",
    "    ci_upper = np.nan\n",
    "    \n",
    "    try:\n",
    "        if n > 3 and not np.isnan(corr) and abs(corr) < 0.99:\n",
    "            # Fisher transformation for confidence interval\n",
    "            z = 0.5 * np.log((1 + corr) / (1 - corr))\n",
    "            se = 1 / np.sqrt(n - 3)\n",
    "            z_critical = stats.norm.ppf(0.975)  # 95% confidence\n",
    "            \n",
    "            z_lower = z - z_critical * se\n",
    "            z_upper = z + z_critical * se\n",
    "            \n",
    "            # Transform back\n",
    "            ci_lower = (np.exp(2 * z_lower) - 1) / (np.exp(2 * z_lower) + 1)\n",
    "            ci_upper = (np.exp(2 * z_upper) - 1) / (np.exp(2 * z_upper) + 1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Interpretation\n",
    "    p_val_num = p_val  # Already converted to float above\n",
    "        \n",
    "    if p_val_num < 0.001:\n",
    "        significance = \"highly significant (p < 0.001)\"\n",
    "    elif p_val_num < 0.01:\n",
    "        significance = \"very significant (p < 0.01)\"\n",
    "    elif p_val_num < 0.05:\n",
    "        significance = \"significant (p < 0.05)\"\n",
    "    elif p_val_num < 0.1:\n",
    "        significance = \"marginally significant (p < 0.1)\"\n",
    "    else:\n",
    "        significance = \"not significant (p ≥ 0.1)\"\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    try:\n",
    "        abs_corr = abs(corr) if not np.isnan(corr) else 0.0\n",
    "    except (TypeError, ValueError, AttributeError):\n",
    "        abs_corr = 0.0\n",
    "        \n",
    "    if abs_corr < 0.1:\n",
    "        effect_size = \"negligible\"\n",
    "    elif abs_corr < 0.3:\n",
    "        effect_size = \"small\"\n",
    "    elif abs_corr < 0.5:\n",
    "        effect_size = \"medium\"\n",
    "    elif abs_corr < 0.7:\n",
    "        effect_size = \"large\"\n",
    "    else:\n",
    "        effect_size = \"very large\"\n",
    "    \n",
    "    interpretation = f\"{effect_size} effect, {significance}\"\n",
    "    \n",
    "    return {\n",
    "        'correlation': corr,\n",
    "        'p_value': p_val,\n",
    "        'confidence_interval': (ci_lower, ci_upper),\n",
    "        'n_samples': n,\n",
    "        'interpretation': interpretation,\n",
    "        'effect_size': effect_size,\n",
    "        'significance': significance\n",
    "    }\n",
    "\n",
    "def compare_paired_metrics(metric1, metric2, metric_names=('Metric 1', 'Metric 2'), alpha=0.05):\n",
    "    \"\"\"Perform paired statistical comparison between two related metrics.\n",
    "    \n",
    "    Conducts a paired t-test to compare two sets of paired measurements, calculates \n",
    "    effect size (Cohen's d), confidence intervals, and provides interpretation of \n",
    "    statistical and practical significance.\n",
    "    \n",
    "    Args:\n",
    "        metric1 (array-like): First set of paired measurements\n",
    "        metric2 (array-like): Second set of paired measurements (must have same length as metric1)\n",
    "        metric_names (tuple, optional): Descriptive names for the metrics used in reporting.\n",
    "            Defaults to ('Metric 1', 'Metric 2').\n",
    "        alpha (float, optional): Significance level for hypothesis testing. Defaults to 0.05.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive paired comparison results containing:\n",
    "            - n_samples (int): Number of valid paired observations\n",
    "            - t_statistic (float): Paired t-test statistic  \n",
    "            - p_value (float): Two-tailed p-value from paired t-test\n",
    "            - effect_size (float): Cohen's d for paired samples (standardized mean difference)\n",
    "            - mean_difference (float): Mean of pairwise differences (metric1 - metric2)\n",
    "            - confidence_interval (tuple): Confidence interval for the mean difference\n",
    "            - better_metric (str): Which metric performs better or 'neither' if no significant difference\n",
    "            - significance (str): Description of statistical significance\n",
    "            - effect_interpretation (str): Practical significance category\n",
    "            - interpretation (str): Combined statistical and practical significance summary\n",
    "    \n",
    "    Notes:\n",
    "        - Automatically handles missing values by pairwise deletion\n",
    "        - Requires minimum 3 valid paired observations\n",
    "        - Effect size interpretation (Cohen's d):\n",
    "            * |d| < 0.2: negligible effect\n",
    "            * 0.2 ≤ |d| < 0.5: small effect  \n",
    "            * 0.5 ≤ |d| < 0.8: medium effect\n",
    "            * |d| ≥ 0.8: large effect\n",
    "        - Uses Welch's t-test assumptions (paired differences normally distributed)\n",
    "    \n",
    "    Example:\n",
    "        >>> before = [100, 110, 105, 120, 115] \n",
    "        >>> after = [95, 102, 98, 112, 108]\n",
    "        >>> result = compare_paired_metrics(before, after, ('Before', 'After'))\n",
    "        >>> print(f\"Mean improvement: {result['mean_difference']:.2f}\")\n",
    "        >>> print(result['interpretation'])\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(metric1) | np.isnan(metric2))\n",
    "    m1_clean = np.array(metric1)[mask]\n",
    "    m2_clean = np.array(metric2)[mask]\n",
    "    \n",
    "    if len(m1_clean) < 3:\n",
    "        return {\n",
    "            'n_samples': len(m1_clean),\n",
    "            'interpretation': 'Insufficient data for statistical testing'\n",
    "        }\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_val = ttest_rel(m1_clean, m2_clean)\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    diff = m1_clean - m2_clean\n",
    "    effect_size = np.mean(diff) / np.std(diff, ddof=1)\n",
    "    \n",
    "    # Confidence interval for the difference\n",
    "    se_diff = stats.sem(diff)\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, len(diff) - 1)\n",
    "    ci_lower = np.mean(diff) - t_critical * se_diff\n",
    "    ci_upper = np.mean(diff) + t_critical * se_diff\n",
    "    \n",
    "    # Interpretation\n",
    "    if p_val < alpha:\n",
    "        significance = f\"significant difference (p = {p_val:.4f})\"\n",
    "        if np.mean(diff) > 0:\n",
    "            better_metric = metric_names[0]\n",
    "        else:\n",
    "            better_metric = metric_names[1]\n",
    "    else:\n",
    "        significance = f\"no significant difference (p = {p_val:.4f})\"\n",
    "        better_metric = \"neither\"\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    abs_effect = abs(effect_size)\n",
    "    if abs_effect < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs_effect < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs_effect < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    \n",
    "    return {\n",
    "        'n_samples': len(m1_clean),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_val,\n",
    "        'effect_size': effect_size,\n",
    "        'mean_difference': np.mean(diff),\n",
    "        'confidence_interval': (ci_lower, ci_upper),\n",
    "        'better_metric': better_metric,\n",
    "        'significance': significance,\n",
    "        'effect_interpretation': effect_interpretation,\n",
    "        'interpretation': f\"{effect_interpretation} effect size, {significance}\"\n",
    "    }\n",
    "\n",
    "def bootstrap_confidence_interval(data, statistic_func, confidence_level=0.95, n_bootstrap=1000):\n",
    "    \"\"\"Calculate bootstrap confidence interval for any statistical estimator.\n",
    "    \n",
    "    Implements non-parametric bootstrap resampling to estimate the sampling distribution\n",
    "    of a statistic and compute confidence intervals without distributional assumptions.\n",
    "    Uses percentile method for confidence interval calculation.\n",
    "    \n",
    "    Args:\n",
    "        data (array-like): Input dataset for bootstrap resampling\n",
    "        statistic_func (callable): Function that computes the statistic of interest.\n",
    "            Must accept array-like input and return a single numeric value.\n",
    "        confidence_level (float, optional): Confidence level as proportion (0-1). \n",
    "            Defaults to 0.95 for 95% confidence interval.\n",
    "        n_bootstrap (int, optional): Number of bootstrap resamples to generate.\n",
    "            Defaults to 1000. More samples improve accuracy but increase computation time.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap analysis results containing:\n",
    "            - statistic (float): Original statistic computed on full dataset\n",
    "            - confidence_interval (tuple): Bootstrap confidence interval (lower, upper)\n",
    "            - bootstrap_std (float): Standard error of bootstrap distribution\n",
    "            - n_bootstrap (int): Number of successful bootstrap samples used\n",
    "            - interpretation (str): Formatted confidence interval description\n",
    "    \n",
    "    Notes:\n",
    "        - Automatically removes NaN values before bootstrap resampling\n",
    "        - Requires minimum 3 valid observations for meaningful results\n",
    "        - Uses random sampling with replacement (bootstrap principle)\n",
    "        - Sets random seed (42) for reproducible results\n",
    "        - Filters out NaN bootstrap statistics automatically\n",
    "        - Falls back gracefully when insufficient bootstrap samples succeed\n",
    "    \n",
    "    Mathematical Foundation:\n",
    "        For statistic θ̂, bootstrap creates empirical sampling distribution by:\n",
    "        1. Resample data with replacement B times\n",
    "        2. Compute θ̂* for each bootstrap sample  \n",
    "        3. Use percentiles of {θ̂*} for confidence intervals\n",
    "    \n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        >>> result = bootstrap_confidence_interval(data, np.mean, confidence_level=0.95)\n",
    "        >>> print(f\"Mean: {result['statistic']:.2f}\")\n",
    "        >>> print(result['interpretation'])\n",
    "        Mean: 5.50\n",
    "        95% CI: [3.2000, 7.8000]\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    clean_data = np.array(data)[~np.isnan(data)]\n",
    "    \n",
    "    if len(clean_data) < 3:\n",
    "        return {\n",
    "            'statistic': np.nan,\n",
    "            'confidence_interval': (np.nan, np.nan),\n",
    "            'interpretation': 'Insufficient data for bootstrap'\n",
    "        }\n",
    "    \n",
    "    # Original statistic\n",
    "    original_stat = statistic_func(clean_data)\n",
    "    \n",
    "    # Bootstrap\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    bootstrap_stats = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_sample = np.random.choice(clean_data, size=len(clean_data), replace=True)\n",
    "        boot_stat = statistic_func(boot_sample)\n",
    "        if not np.isnan(boot_stat):\n",
    "            bootstrap_stats.append(boot_stat)\n",
    "    \n",
    "    if len(bootstrap_stats) < 10:\n",
    "        return {\n",
    "            'statistic': original_stat,\n",
    "            'confidence_interval': (np.nan, np.nan),\n",
    "            'interpretation': 'Bootstrap failed'\n",
    "        }\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    ci_lower = np.percentile(bootstrap_stats, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return {\n",
    "        'statistic': original_stat,\n",
    "        'confidence_interval': (ci_lower, ci_upper),\n",
    "        'bootstrap_std': np.std(bootstrap_stats),\n",
    "        'n_bootstrap': len(bootstrap_stats),\n",
    "        'interpretation': f\"{confidence_level*100:.0f}% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\"\n",
    "    }\n",
    "\n",
    "def regression_diagnostics(X, y, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Perform regression diagnostics and return statistical measures\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Target variable\n",
    "    model_name : str\n",
    "        Name of the model for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Regression diagnostics and statistics\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    mask = ~(np.isnan(y) | np.isnan(X).any(axis=1))\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    if len(X_clean) < 5:\n",
    "        return {'interpretation': 'Insufficient data for regression diagnostics'}\n",
    "    \n",
    "    # Fit regression model with statsmodels for detailed statistics\n",
    "    X_with_const = sm.add_constant(X_clean)\n",
    "    model = sm.OLS(y_clean, X_with_const).fit()\n",
    "    \n",
    "    # Get predictions and residuals\n",
    "    y_pred = model.predict(X_with_const)\n",
    "    residuals = y_clean - y_pred\n",
    "    \n",
    "    # Statistical tests\n",
    "    results = {\n",
    "        'n_samples': len(X_clean),\n",
    "        'r_squared': model.rsquared,\n",
    "        'adj_r_squared': model.rsquared_adj,\n",
    "        'f_statistic': model.fvalue,\n",
    "        'f_pvalue': model.f_pvalue,\n",
    "        'coefficients': model.params,\n",
    "        'coef_pvalues': model.pvalues,\n",
    "        'coef_confidence_intervals': model.conf_int(),\n",
    "        'rmse': np.sqrt(np.mean(residuals**2)),\n",
    "        'mae': np.mean(np.abs(residuals))\n",
    "    }\n",
    "    \n",
    "    # Heteroscedasticity test (Breusch-Pagan)\n",
    "    try:\n",
    "        bp_stat, bp_pvalue, _, _ = het_breuschpagan(residuals, X_with_const)\n",
    "        results['heteroscedasticity_test'] = {\n",
    "            'bp_statistic': bp_stat,\n",
    "            'bp_pvalue': bp_pvalue,\n",
    "            'interpretation': 'Homoscedastic' if bp_pvalue > 0.05 else 'Heteroscedastic'\n",
    "        }\n",
    "    except:\n",
    "        results['heteroscedasticity_test'] = {'interpretation': 'Test failed'}\n",
    "    \n",
    "    # Durbin-Watson test for autocorrelation\n",
    "    try:\n",
    "        dw_stat = durbin_watson(residuals)\n",
    "        results['durbin_watson'] = {\n",
    "            'statistic': dw_stat,\n",
    "            'interpretation': 'No autocorrelation' if 1.5 < dw_stat < 2.5 else 'Possible autocorrelation'\n",
    "        }\n",
    "    except:\n",
    "        results['durbin_watson'] = {'interpretation': 'Test failed'}\n",
    "    \n",
    "    # Overall model interpretation\n",
    "    if results['f_pvalue'] < 0.001:\n",
    "        model_significance = \"highly significant (p < 0.001)\"\n",
    "    elif results['f_pvalue'] < 0.01:\n",
    "        model_significance = \"very significant (p < 0.01)\"\n",
    "    elif results['f_pvalue'] < 0.05:\n",
    "        model_significance = \"significant (p < 0.05)\"\n",
    "    else:\n",
    "        model_significance = f\"not significant (p = {results['f_pvalue']:.4f})\"\n",
    "    \n",
    "    results['interpretation'] = f\"{model_name}: R² = {results['r_squared']:.4f}, {model_significance}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Statistical utility functions loaded successfully ✓\")\n",
    "print(\"Available functions:\")\n",
    "print(\"- calculate_correlation_with_stats(): Correlation with p-values and confidence intervals\")\n",
    "print(\"- compare_paired_metrics(): Paired statistical tests for metric comparison\")\n",
    "print(\"- bootstrap_confidence_interval(): Bootstrap confidence intervals\")\n",
    "print(\"- regression_diagnostics(): Comprehensive regression statistics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Query SIFP Estimation Data  \n",
    "# Purpose: Query SIFP estimation data from Neo4j database and process into structured DataFrame for analysis\n",
    "# Dependencies: pandas, neo4j, os, logging\n",
    "# Breadcrumbs: Data Acquisition -> SIFP Data\n",
    "\n",
    "def query_sifp_estimations(driver) -> pd.DataFrame:\n",
    "    \"\"\"Query comprehensive SIFP estimation data from Neo4j graph database.\n",
    "    \n",
    "    Executes a complex Cypher query to retrieve SIFP (Software Interface Function Points)\n",
    "    estimation data including actor analysis, judge evaluation, and final estimations.\n",
    "    Processes JSON-encoded fields and returns structured tabular data for analysis.\n",
    "    \n",
    "    Args:\n",
    "        driver (neo4j.Driver): Active Neo4j database driver connection for executing queries\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Structured DataFrame containing SIFP estimation results with columns:\n",
    "            - sifp_requirement_id (str): Unique identifier for the requirement\n",
    "            - sifp_model (str): AI model identifier used for estimation  \n",
    "            - sifp_is_valid (bool): Validation status of the estimation\n",
    "            - sifp_judge_score (float): Overall judge evaluation score\n",
    "            - sifp_judge_confidence (float): Judge confidence level (0-1)\n",
    "            - sifp_actor_confidence (float): Actor analysis confidence level\n",
    "            - sifp_actor_total (float): Total SIFP points from actor analysis\n",
    "            - sifp_judge_ugep_accuracy (float): Judge accuracy for UGEP classification\n",
    "            - sifp_judge_ugdg_accuracy (float): Judge accuracy for UGDG classification  \n",
    "            - sifp_judge_calculation_accuracy (float): Judge mathematical calculation accuracy\n",
    "            - sifp_judge_classification_accuracy (float): Judge component classification accuracy\n",
    "            - sifp_final_total (float): Final adjusted SIFP points total\n",
    "    \n",
    "    Environment Variables Required:\n",
    "        NEO4J_PROJECT_NAME (str): Name of the project to query\n",
    "        SIFP_ESTIMATION_REQUIREMENT (str, optional): Requirement type filter ('SOURCE' default)\n",
    "    \n",
    "    Notes:\n",
    "        - Queries only requirements with complete actor_analysis, judge_evaluation, and final_estimation\n",
    "        - Automatically parses JSON-encoded fields using APOC functions\n",
    "        - Logs comprehensive statistics about retrieved data\n",
    "        - Returns empty DataFrame if no data found or on query errors\n",
    "        - Handles missing or malformed JSON gracefully\n",
    "    \n",
    "    Raises:\n",
    "        Exception: Database connection or query execution errors (logged and re-raised)\n",
    "    \n",
    "    Example:\n",
    "        >>> driver = create_neo4j_driver()\n",
    "        >>> df = query_sifp_estimations(driver)\n",
    "        >>> print(f\"Retrieved {len(df)} SIFP estimations\")\n",
    "        >>> print(f\"Average final SIFP points: {df['sifp_final_total'].mean():.2f}\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get SIFP_ESTIMATION_REQUIREMENT from environment\n",
    "        sifp_estimation_requirement = os.getenv('SIFP_ESTIMATION_REQUIREMENT', 'SOURCE')\n",
    "        \n",
    "        sifp_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})\n",
    "        MATCH (p)-[:CONTAINS*]->(n)\n",
    "        MATCH (r:Requirement)-[s:SIFP_ESTIMATION]->(e)\n",
    "        WHERE r = n\n",
    "        AND r.type = $requirement_type\n",
    "        WITH r, s\n",
    "        WHERE s.actor_analysis IS NOT NULL\n",
    "        AND s.judge_evaluation IS NOT NULL\n",
    "        AND s.final_estimation IS NOT NULL\n",
    "        WITH r, s,\n",
    "            CASE\n",
    "                WHEN s.actor_analysis STARTS WITH '{'\n",
    "                THEN apoc.convert.fromJsonMap(s.actor_analysis)\n",
    "                ELSE NULL\n",
    "            END as actor_analysis,\n",
    "            CASE\n",
    "                WHEN s.judge_evaluation STARTS WITH '{'\n",
    "                THEN apoc.convert.fromJsonMap(s.judge_evaluation)\n",
    "                ELSE NULL\n",
    "            END as judge_eval,\n",
    "            CASE\n",
    "                WHEN s.final_estimation STARTS WITH '{'\n",
    "                THEN apoc.convert.fromJsonMap(s.final_estimation)\n",
    "                ELSE NULL\n",
    "            END as final_est\n",
    "        WHERE actor_analysis IS NOT NULL\n",
    "        AND final_est IS NOT NULL\n",
    "        WITH r.id as sifp_requirement_id,\n",
    "            s.is_valid as sifp_is_valid,\n",
    "            s.model as sifp_model,\n",
    "            s.judge_score as sifp_judge_score,\n",
    "            s.judge_confidence as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            actor_analysis.confidence as sifp_actor_confidence,\n",
    "            actor_analysis.sifp_points.total as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            judge_eval.ugep_accuracy as sifp_judge_ugep_accuracy,\n",
    "            judge_eval.ugdg_accuracy as sifp_judge_ugdg_accuracy,\n",
    "            judge_eval.calculation_accuracy as sifp_judge_calculation_accuracy,\n",
    "            judge_eval.component_classification_accuracy as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            final_est.sifp_points.total as sifp_final_total\n",
    "        WITH sifp_requirement_id, sifp_model,\n",
    "            COLLECT([sifp_is_valid, sifp_judge_score, sifp_judge_confidence,\n",
    "            sifp_actor_confidence, sifp_actor_total, sifp_judge_ugep_accuracy,\n",
    "            sifp_judge_ugdg_accuracy, sifp_judge_calculation_accuracy,\n",
    "            sifp_judge_classification_accuracy, sifp_final_total])[0] as fields\n",
    "        RETURN \n",
    "            sifp_requirement_id,\n",
    "            sifp_model,\n",
    "            fields[0] as sifp_is_valid,\n",
    "            fields[1] as sifp_judge_score,\n",
    "            fields[2] as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            fields[3] as sifp_actor_confidence,\n",
    "            fields[4] as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            fields[5] as sifp_judge_ugep_accuracy,\n",
    "            fields[6] as sifp_judge_ugdg_accuracy,\n",
    "            fields[7] as sifp_judge_calculation_accuracy,\n",
    "            fields[8] as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            fields[9] as sifp_final_total\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            # Execute query with project name parameter and requirement type\n",
    "            results = session.run(sifp_query, \n",
    "                                 project_name=NEO4J_PROJECT_NAME,\n",
    "                                 requirement_type=sifp_estimation_requirement).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No SIFP estimation results found for project: {NEO4J_PROJECT_NAME} with requirement type: {sifp_estimation_requirement}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            # Log summary statistics\n",
    "            logger.info(f\"Retrieved {len(df)} SIFP estimation results for project: {NEO4J_PROJECT_NAME}\")\n",
    "            logger.info(f\"Requirement type: {sifp_estimation_requirement}\")\n",
    "            logger.info(f\"Number of unique requirements: {df['sifp_requirement_id'].nunique()}\")\n",
    "            logger.info(f\"Number of unique models: {df['sifp_model'].nunique()}\")\n",
    "            \n",
    "            # Calculate and log some basic statistics\n",
    "            logger.info(\"Summary Statistics:\")\n",
    "            logger.info(f\"Average actor total points: {df['sifp_actor_total'].mean():.2f}\")\n",
    "            logger.info(f\"Average final total points: {df['sifp_final_total'].mean():.2f}\")\n",
    "            logger.info(f\"Average judge score: {df['sifp_judge_score'].mean():.2f}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for SIFP estimations: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "# Execute query and get results\n",
    "sifp_results_df = query_sifp_estimations(driver)\n",
    "\n",
    "# Display sample of results and dataset info\n",
    "print(\"SIFP Estimation Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total number of estimations: {len(sifp_results_df)}\")\n",
    "print(f\"Unique requirements: {sifp_results_df['sifp_requirement_id'].nunique()}\")\n",
    "print(f\"Unique models: {sifp_results_df['sifp_model'].nunique()}\")\n",
    "\n",
    "print(\"Sample of SIFP estimation results:\")\n",
    "display(sifp_results_df.head())\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"Summary Statistics for Key Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "numerical_columns = [\n",
    "    'sifp_judge_score', 'sifp_actor_total', 'sifp_final_total',\n",
    "    'sifp_judge_ugep_accuracy', 'sifp_judge_ugdg_accuracy',\n",
    "    'sifp_judge_calculation_accuracy', 'sifp_judge_classification_accuracy'\n",
    "]\n",
    "print(sifp_results_df[numerical_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Display Selected SIFP Metrics by Model\n",
    "# Purpose: Display and analyze SIFP metrics grouped by model with basic visualizations and statistical summaries  \n",
    "# Dependencies: pandas, matplotlib, seaborn, os\n",
    "# Breadcrumbs: Data Analysis -> Model Metrics\n",
    "\n",
    "# Select and display specific columns\n",
    "selected_columns = [\n",
    "    'sifp_requirement_id', \n",
    "    'sifp_model', \n",
    "    'sifp_actor_total', \n",
    "    'sifp_final_total'\n",
    "]\n",
    "\n",
    "print(f\"Selected SIFP Metrics for Project: {NEO4J_PROJECT_NAME}\")\n",
    "print(\"=\" * 80)\n",
    "display(sifp_results_df[selected_columns].head())\n",
    "\n",
    "# Display counts by model\n",
    "print(f\"Count of estimations by model for Project {NEO4J_PROJECT_NAME}:\")\n",
    "print(\"-\" * 40)\n",
    "print(sifp_results_df['sifp_model'].value_counts())\n",
    "\n",
    "# Add model-specific analysis\n",
    "print(f\"Project-specific SIFP Analysis for {NEO4J_PROJECT_NAME}:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate average scores by model\n",
    "model_avg = sifp_results_df.groupby('sifp_model')[numerical_columns].mean().reset_index()\n",
    "print(\"Average estimated size by model:\")\n",
    "print(model_avg)\n",
    "\n",
    "# Calculate percentage of valid estimations\n",
    "valid_percent = sifp_results_df['sifp_is_valid'].mean() * 100\n",
    "print(f\"Percentage of valid estimations: {valid_percent:.2f}%\")\n",
    "\n",
    "# Get visualization setting from environment\n",
    "SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "\n",
    "# Create a bar chart comparing models only if visualization is enabled\n",
    "if SHOW_VISUALIZATION:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='sifp_model', y='sifp_final_total', data=model_avg, palette='viridis')\n",
    "    plt.title('Average Estimated Size by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('SIFP Points')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"(Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Load and Display SLOC Metrics\n",
    "# Purpose: Load Java code metrics CSV file and display summary statistics for code size analysis\n",
    "# Dependencies: pandas, pathlib, logging\n",
    "# Breadcrumbs: Data Acquisition -> Code Metrics\n",
    "\n",
    "try:\n",
    "    # Define the file path relative to the notebook location using the project name\n",
    "    file_path = Path('..') / 'datasets' / NEO4J_PROJECT_NAME / NEO4J_PROJECT_NAME / 'java.csv'\n",
    "    \n",
    "    # Log attempt to read file\n",
    "    logger.info(f\"Attempting to read Java metrics CSV file for project {NEO4J_PROJECT_NAME} from: {file_path.absolute()}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        logger.warning(f\"Java metrics file not found at: {file_path.absolute()}\")\n",
    "        logger.warning(f\"Checking for alternative file paths...\")\n",
    "        \n",
    "        # Try alternative file paths\n",
    "        alt_paths = [\n",
    "            Path('..') / 'datasets' / NEO4J_PROJECT_NAME / f\"{NEO4J_PROJECT_NAME}-SLOC Metrics.csv\",\n",
    "            Path('..') / 'datasets' / NEO4J_PROJECT_NAME / 'SLOC Metrics.csv',\n",
    "            Path('..') / 'datasets' / NEO4J_PROJECT_NAME / f\"{NEO4J_PROJECT_NAME}.csv\"\n",
    "        ]\n",
    "        \n",
    "        for alt_path in alt_paths:\n",
    "            logger.info(f\"Checking alternative path: {alt_path.absolute()}\")\n",
    "            if alt_path.exists():\n",
    "                logger.info(f\"Found file at alternative path: {alt_path.absolute()}\")\n",
    "                file_path = alt_path\n",
    "                break\n",
    "        \n",
    "        # If still not found, raise error\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Java metrics file not found for project {NEO4J_PROJECT_NAME}.\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df_sloc = pd.read_csv(file_path)\n",
    "    \n",
    "    # Log successful read\n",
    "    logger.info(f\"Successfully loaded Java metrics CSV file with shape: {df_sloc.shape}\")\n",
    "    \n",
    "    # Process the data based on the file format\n",
    "    if 'Kind' in df_sloc.columns and 'Name' in df_sloc.columns:\n",
    "        logger.info(\"Detected standard Java metrics format - filtering for File entries only\")\n",
    "        \n",
    "        # Filter for File entries only\n",
    "        file_data = df_sloc[df_sloc['Kind'] == 'File']\n",
    "        \n",
    "        if len(file_data) == 0:\n",
    "            # If no File entries, check if there are any Class entries as fallback\n",
    "            class_data = df_sloc[df_sloc['Kind'].str.contains('Class', na=False)]\n",
    "            \n",
    "            if len(class_data) > 0:\n",
    "                logger.info(f\"Using {len(class_data)} Class entries as fallback\")\n",
    "                file_data = class_data\n",
    "            else:\n",
    "                raise ValueError(\"No File or Class entries found in the Java metrics file\")\n",
    "        \n",
    "        logger.info(f\"Filtered to {len(file_data)} File entries\")\n",
    "        \n",
    "        # Create a new DataFrame with the required columns\n",
    "        df_sloc_transformed = pd.DataFrame({\n",
    "            'File': file_data['Name'],\n",
    "            'target_type': file_data['Name'].apply(lambda x: x.split('.')[-1] if '.' in x else x),\n",
    "            'Lines': file_data['CountLine'],\n",
    "            'Comments': file_data['CountLineComment'],\n",
    "            'Blanks': file_data['CountLineBlank'],\n",
    "            'Code': file_data['CountLineCode'],\n",
    "            'Lines-exe': file_data['CountLineCodeExe'] if 'CountLineCodeExe' in file_data.columns else None,\n",
    "            'Lines-dec': file_data['CountLineCodeDecl'] if 'CountLineCodeDecl' in file_data.columns else None,\n",
    "            'Stmt-exe': file_data['CountStmtExe'] if 'CountStmtExe' in file_data.columns else None,\n",
    "            'Stmt-dec': file_data['CountStmtDecl'] if 'CountStmtDecl' in file_data.columns else None,\n",
    "            'Units': file_data['CountDeclMethod'] if 'CountDeclMethod' in file_data.columns else None\n",
    "        })\n",
    "        \n",
    "        # Replace the original DataFrame with the transformed one\n",
    "        df_sloc = df_sloc_transformed\n",
    "        logger.info(f\"Transformed data to required format with shape: {df_sloc.shape}\")\n",
    "    \n",
    "    # Display basic information about the dataframe\n",
    "    print(f\"Java Metrics for Project: {NEO4J_PROJECT_NAME}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DataFrame Info:\")\n",
    "    print(df_sloc.info())\n",
    "    \n",
    "    print(\"First few rows of the data:\")\n",
    "    display(df_sloc.head())\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    numeric_cols = df_sloc.select_dtypes(include=['number']).columns\n",
    "    print(df_sloc[numeric_cols].describe())\n",
    "    \n",
    "    # Log some key metrics\n",
    "    logger.info(f\"Total number of files: {len(df_sloc)}\")\n",
    "    if 'Code' in df_sloc.columns:\n",
    "        logger.info(f\"Total lines of code: {df_sloc['Code'].sum()}\")\n",
    "    if 'Lines-exe' in df_sloc.columns:\n",
    "        logger.info(f\"Total executable lines: {df_sloc['Lines-exe'].sum()}\")\n",
    "    if 'Lines-dec' in df_sloc.columns:\n",
    "        logger.info(f\"Total declarative lines: {df_sloc['Lines-dec'].sum()}\")\n",
    "    if 'Units' in df_sloc.columns:\n",
    "        logger.info(f\"Total units/methods: {df_sloc['Units'].sum()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading Java metrics file: {str(e)}\", exc_info=True)\n",
    "    print(f\"Error loading Java metrics for project {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"Continuing with analysis without Java metrics data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Map Requirements to Code Files\n",
    "# Purpose: Query Neo4j to map requirements to code files and create synthetic mappings for analysis\n",
    "# Dependencies: pandas, neo4j, logging\n",
    "# Breadcrumbs: Data Preparation -> Requirement Mapping\n",
    "\n",
    "def map_requirements_to_code():\n",
    "    \"\"\"\n",
    "    Query Neo4j to map requirements to code files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with requirement to code file mappings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use an approach that captures ALL target requirements with a GROUND_TRUTH relationship\n",
    "        with driver.session() as session:\n",
    "            # Modified query to get all target requirements with a GROUND_TRUTH relationship\n",
    "            alt_query = \"\"\"\n",
    "            MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "            WHERE source.type = 'SOURCE' AND target.type = 'TARGET'\n",
    "            RETURN DISTINCT\n",
    "                target.id as target_id,\n",
    "                'Derived from ground truth' as code_file_path,\n",
    "                1 as ground_truth\n",
    "            \"\"\"\n",
    "            \n",
    "            alt_results = session.run(alt_query, project_name=NEO4J_PROJECT_NAME).data()\n",
    "            \n",
    "            if not alt_results:\n",
    "                logger.warning(\"No ground truth mappings found\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            logger.info(f\"Found {len(alt_results)} ground truth mappings\")\n",
    "            \n",
    "            # Create DataFrame from results\n",
    "            mapping_df = pd.DataFrame(alt_results)\n",
    "            \n",
    "            # Log distribution of target IDs to understand their patterns\n",
    "            target_prefixes = mapping_df['target_id'].apply(lambda x: x.split('_')[0] if '_' in x else x).value_counts()\n",
    "            logger.info(f\"Target ID prefix distribution: {target_prefixes.to_dict()}\")\n",
    "            \n",
    "            # Map CC identifiers to EA file names\n",
    "            def map_cc_to_ea_file(cc_id):\n",
    "                if cc_id and isinstance(cc_id, str):\n",
    "                    # Extract number from different formats:\n",
    "                    # Format CC_123 -> 123\n",
    "                    if cc_id.startswith('CC_'):\n",
    "                        num = cc_id[3:]\n",
    "                    # Format CC123 -> 123\n",
    "                    elif cc_id.startswith('CC'):\n",
    "                        num = cc_id[2:]\n",
    "                    # Try to extract any number from the ID\n",
    "                    else:\n",
    "                        num = ''.join(c for c in cc_id if c.isdigit())\n",
    "                    \n",
    "                    # Only return a mapped file name if we have a number\n",
    "                    if num:\n",
    "                        return f\"EA{num}.java\"\n",
    "                return None\n",
    "            \n",
    "            # Apply the mapping function to target_id\n",
    "            mapping_df['java_file_name'] = mapping_df['target_id'].apply(map_cc_to_ea_file)\n",
    "            \n",
    "            # Count how many mappings were successfully created\n",
    "            mapped_count = mapping_df['java_file_name'].notna().sum()\n",
    "            logger.info(f\"Successfully mapped {mapped_count} of {len(mapping_df)} target IDs to Java file names\")\n",
    "            \n",
    "            # For any rows where java_file_name is None, try alternate mappings\n",
    "            if mapped_count < len(mapping_df):\n",
    "                logger.info(\"Applying fallback mapping for non-standard target IDs\")\n",
    "                \n",
    "                # For unmapped rows, try to extract any numeric parts as a fallback\n",
    "                unmapped_mask = mapping_df['java_file_name'].isna()\n",
    "                unmapped_ids = mapping_df.loc[unmapped_mask, 'target_id']\n",
    "                \n",
    "                # Extract any numeric part from the ID\n",
    "                mapping_df.loc[unmapped_mask, 'numeric_id'] = unmapped_ids.apply(\n",
    "                    lambda x: ''.join(c for c in x if c.isdigit()) if x else None\n",
    "                )\n",
    "                \n",
    "                # Create java_file_name from numeric ID where possible\n",
    "                mapping_df.loc[unmapped_mask & mapping_df['numeric_id'].notna(), 'java_file_name'] = \\\n",
    "                    mapping_df.loc[unmapped_mask & mapping_df['numeric_id'].notna(), 'numeric_id'].apply(\n",
    "                        lambda x: f\"EA{x}.java\" if x else None\n",
    "                    )\n",
    "                \n",
    "                # Count final mapping success\n",
    "                final_mapped_count = mapping_df['java_file_name'].notna().sum()\n",
    "                logger.info(f\"After fallback, mapped {final_mapped_count} of {len(mapping_df)} target IDs\")\n",
    "            \n",
    "            return mapping_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying requirement to code mappings: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "# Get requirement to code mappings\n",
    "req_code_mappings = map_requirements_to_code()\n",
    "\n",
    "# Display the mappings\n",
    "if not req_code_mappings.empty:\n",
    "    print(\"Requirement to Code File Mappings:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(req_code_mappings.head())\n",
    "    print(f\"Total mappings: {len(req_code_mappings)}\")\n",
    "    print(f\"Unique target requirements: {req_code_mappings['target_id'].nunique()}\")\n",
    "    \n",
    "    # Check how many target IDs could be mapped to Java file names\n",
    "    mapped_count = req_code_mappings['java_file_name'].notna().sum()\n",
    "    print(f\"Target requirements mapped to Java files: {mapped_count} ({mapped_count/len(req_code_mappings)*100:.1f}%)\")\n",
    "    \n",
    "    # List the most common target ID prefixes\n",
    "    if 'target_id' in req_code_mappings.columns:\n",
    "        target_prefixes = req_code_mappings['target_id'].apply(lambda x: x.split('_')[0] if '_' in x else x[:2]).value_counts()\n",
    "        print(\"Target ID prefix distribution:\")\n",
    "        for prefix, count in target_prefixes.items():\n",
    "            print(f\"  {prefix}: {count} ({count/len(req_code_mappings)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No requirement to code file mappings found.\")\n",
    "    print(\"Creating synthetic mappings based on requirement IDs...\")\n",
    "    \n",
    "    # Create mappings with CC numbers matching EA numbers\n",
    "    unique_requirements = sifp_results_df['sifp_requirement_id'].unique()\n",
    "    \n",
    "    # Create an empty DataFrame for the mappings\n",
    "    req_code_mappings = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through requirements to create proper mappings\n",
    "    mappings_data = []\n",
    "    for req_id in unique_requirements:\n",
    "        if req_id and isinstance(req_id, str) and req_id.startswith('CC'):\n",
    "            # Extract number from CC prefix (e.g., CC167 -> 167)\n",
    "            cc_number = req_id[2:] if req_id.startswith('CC') else req_id\n",
    "            # Create matching EA file name\n",
    "            ea_file_name = f\"EA{cc_number}.java\"\n",
    "            \n",
    "            mappings_data.append({\n",
    "                'target_id': req_id,  # Use the same ID as target_id\n",
    "                'code_file_path': \"Synthetic mapping\",\n",
    "                'java_file_name': ea_file_name,\n",
    "                'ground_truth': 0  # Mark these as non-ground-truth mappings\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from the mapped data\n",
    "    req_code_mappings = pd.DataFrame(mappings_data)\n",
    "    \n",
    "    print(f\"Created {len(req_code_mappings)} synthetic mappings with matching CC and EA numbers\")\n",
    "    display(req_code_mappings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Merge SIFP Estimates with Code Metrics and Visualize Relationships\n",
    "# Purpose: Merge SIFP data with code metrics and create correlation visualizations to analyze relationships\n",
    "# Dependencies: pandas, numpy, sklearn, matplotlib, seaborn, os\n",
    "# Breadcrumbs: Data Analysis -> SIFP-Code Correlation\n",
    "\n",
    "def prepare_file_level_analysis():\n",
    "    \"\"\"\n",
    "    Prepare data for file-level analysis by joining SIFP results, code mappings, and code metrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with merged SIFP data and code metrics for file-level analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the required dataframes are available\n",
    "        if 'sifp_results_df' not in globals() or 'req_code_mappings' not in globals() or 'df_sloc' not in globals():\n",
    "            logger.warning(\"Required data not available for file-level analysis\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Step 1: First, standardize the format of all file names for better matching\n",
    "        \n",
    "        # Create normalized versions of SIFP requirement IDs without prefixes\n",
    "        sifp_results_df['normalized_req_id'] = sifp_results_df['sifp_requirement_id'].apply(\n",
    "            lambda x: x[3:] if x and isinstance(x, str) and x.startswith('CC_') else x\n",
    "        )\n",
    "        \n",
    "        # Create normalized versions of file names without extensions\n",
    "        df_sloc['normalized_file'] = df_sloc['File'].apply(\n",
    "            lambda x: x.split('.')[0] if x and isinstance(x, str) and '.' in x else x\n",
    "        )\n",
    "        \n",
    "        # Create normalized versions of java_file_name without extensions and prefixes\n",
    "        req_code_mappings['normalized_java_file'] = req_code_mappings['java_file_name'].apply(\n",
    "            lambda x: x[2:-5] if x and isinstance(x, str) and x.startswith('EA') and x.endswith('.java') else x\n",
    "        )\n",
    "        \n",
    "        # Step 2: Merge SIFP results with requirement to code mappings\n",
    "        logger.info(f\"Before first merge: SIFP results: {len(sifp_results_df)} rows, Code mappings: {len(req_code_mappings)} rows\")\n",
    "        merged_df = pd.merge(\n",
    "            sifp_results_df,\n",
    "            req_code_mappings,\n",
    "            left_on='sifp_requirement_id',\n",
    "            right_on='target_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        logger.info(f\"After first merge: {len(merged_df)} rows with code file mappings\")\n",
    "        \n",
    "        # Step 3: Create a more precise file matching approach\n",
    "        # Extract a clean identifier for matching from both dataframes\n",
    "        \n",
    "        # For merged_df, use the normalized java file name without EA prefix and .java suffix\n",
    "        merged_df['clean_file_id'] = merged_df['java_file_name'].apply(\n",
    "            lambda x: x[2:-5] if x and isinstance(x, str) and x.startswith('EA') and x.endswith('.java') else \n",
    "                       (x.replace('.java', '') if x and isinstance(x, str) and x.endswith('.java') else x)\n",
    "        )\n",
    "        \n",
    "        # For df_sloc, normalize file names similarly\n",
    "        df_sloc['clean_file_id'] = df_sloc['File'].apply(\n",
    "            lambda x: x.split('.')[0] if x and isinstance(x, str) and '.' in x else x\n",
    "        )\n",
    "        \n",
    "        # Log some sample mappings for debugging\n",
    "        logger.info(f\"SIFP file identifiers: {merged_df['clean_file_id'].head(5).tolist()}\")\n",
    "        logger.info(f\"SLOC file identifiers: {df_sloc['clean_file_id'].head(5).tolist()}\")\n",
    "        \n",
    "        # Step 4: Perform the merge using the cleaned identifiers\n",
    "        final_df = pd.merge(\n",
    "            merged_df,\n",
    "            df_sloc,\n",
    "            left_on='clean_file_id',\n",
    "            right_on='clean_file_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Log the results of this merge\n",
    "        logger.info(f\"After precise file identifier matching: {len(final_df)} rows\")\n",
    "        \n",
    "        # If the precise matching failed, try a fallback approach with just the numeric parts\n",
    "        if len(final_df) == 0:\n",
    "            logger.info(\"Precise matching failed, trying fallback with numeric identifiers\")\n",
    "            \n",
    "            # Extract just the numeric part from both identifiers\n",
    "            merged_df['numeric_id'] = merged_df['clean_file_id'].apply(\n",
    "                lambda x: ''.join(c for c in str(x) if c.isdigit()) if x else ''\n",
    "            )\n",
    "            \n",
    "            df_sloc['numeric_id'] = df_sloc['clean_file_id'].apply(\n",
    "                lambda x: ''.join(c for c in str(x) if c.isdigit()) if x else ''\n",
    "            )\n",
    "            \n",
    "            # Only match if the numeric part is not empty and has at least 1 digit\n",
    "            merged_df_valid = merged_df[merged_df['numeric_id'].str.len() > 0]\n",
    "            df_sloc_valid = df_sloc[df_sloc['numeric_id'].str.len() > 0]\n",
    "            \n",
    "            logger.info(f\"Valid numeric IDs - SIFP: {len(merged_df_valid)}, SLOC: {len(df_sloc_valid)}\")\n",
    "            \n",
    "            # Perform the merge with numeric IDs\n",
    "            final_df = pd.merge(\n",
    "                merged_df_valid,\n",
    "                df_sloc_valid,\n",
    "                on='numeric_id',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"After numeric matching: {len(final_df)} rows\")\n",
    "            \n",
    "        # If we still have too many rows, we're likely getting duplicate matches\n",
    "        # Let's group by the key columns and take one representative row per group\n",
    "        if len(final_df) > len(merged_df) * 2:\n",
    "            logger.warning(f\"Too many matches: {len(final_df)} rows. Filtering to ensure one-to-one mappings.\")\n",
    "            \n",
    "            # Group by the key columns to get unique combinations\n",
    "            group_cols = ['sifp_requirement_id', 'sifp_model', 'File']\n",
    "            final_df = final_df.groupby(group_cols).first().reset_index()\n",
    "            \n",
    "            logger.info(f\"After filtering to unique combinations: {len(final_df)} rows\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing file-level analysis: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "# Execute the function to merge data\n",
    "file_analysis_df = prepare_file_level_analysis()\n",
    "\n",
    "# Display the merged data\n",
    "if not file_analysis_df.empty:\n",
    "    print(\"Merged SIFP and Code Metrics Data:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(file_analysis_df.head())\n",
    "    print(f\"Total file-level entries: {len(file_analysis_df)}\")\n",
    "    print(f\"Unique code files: {file_analysis_df['File'].nunique()}\")\n",
    "    print(f\"Unique requirements: {file_analysis_df['sifp_requirement_id'].nunique()}\")\n",
    "    \n",
    "    # Get all unique models from the data before any filtering\n",
    "    all_models = file_analysis_df['sifp_model'].unique()\n",
    "    print(f\"Unique models before filtering: {len(all_models)}\")\n",
    "    print(f\"Available models: {all_models}\")\n",
    "    \n",
    "    # Define code metrics for analysis - only include ones that exist in the dataframe\n",
    "    code_metrics = ['Code', 'Lines', 'Lines-exe', 'Lines-dec', 'Units']\n",
    "    code_metrics = [m for m in code_metrics if m in file_analysis_df.columns]\n",
    "    \n",
    "    # Get the model ID variable names from environment variable\n",
    "    analysis_model_vars = os.getenv('RESULTS_ANALYSIS_MODEL_IDS', '')\n",
    "    print(f\"DEBUG - Raw RESULTS_ANALYSIS_MODEL_IDS from env: '{analysis_model_vars}'\")\n",
    "    \n",
    "    # Resolve environment variable names to actual model IDs\n",
    "    if analysis_model_vars:\n",
    "        # Split by comma and strip whitespace to get variable names\n",
    "        model_var_names = [var_name.strip() for var_name in analysis_model_vars.split(',')]\n",
    "        print(f\"Model variable names: {model_var_names}\")\n",
    "        \n",
    "        # Look up each variable in the environment to get actual model IDs\n",
    "        selected_models = []\n",
    "        for var_name in model_var_names:\n",
    "            model_id = os.getenv(var_name, '')\n",
    "            if model_id:\n",
    "                print(f\"  {var_name} = {model_id}\")\n",
    "                selected_models.append(model_id)\n",
    "            else:\n",
    "                print(f\"  WARNING: Environment variable {var_name} not found or empty\")\n",
    "        \n",
    "        print(f\"Resolved model IDs: {selected_models}\")\n",
    "        \n",
    "        # Filter to only include models from the environment variable that exist in the data\n",
    "        models = [model for model in selected_models if model in all_models]\n",
    "        print(f\"Models found in dataset: {models}\")\n",
    "        \n",
    "        if len(models) == 0 and len(all_models) > 0:\n",
    "            print(f\"WARNING: No models from RESULTS_ANALYSIS_MODEL_IDS found in data!\")\n",
    "            print(f\"Available models in data: {all_models}\")\n",
    "            print(\"Falling back to all models\")\n",
    "            # Fall back to all models if none match\n",
    "            models = all_models\n",
    "        else:\n",
    "            print(f\"SUCCESS: Will use only these models for visualization: {models}\")\n",
    "            \n",
    "        # CRITICAL CHANGE: Filter the dataframe to only include selected models\n",
    "        print(f\"Before filtering: dataset has {len(file_analysis_df)} rows with {file_analysis_df['sifp_model'].nunique()} unique models\")\n",
    "        file_analysis_df = file_analysis_df[file_analysis_df['sifp_model'].isin(models)]\n",
    "        print(f\"After filtering: dataset has {len(file_analysis_df)} rows with {file_analysis_df['sifp_model'].nunique()} unique models\")\n",
    "        print(f\"Models in filtered dataset: {file_analysis_df['sifp_model'].unique()}\")\n",
    "    else:\n",
    "        # Use all models if no filter is specified\n",
    "        models = all_models\n",
    "        print(f\"No RESULTS_ANALYSIS_MODEL_IDS specified, using all {len(models)} models\")\n",
    "    \n",
    "    # Get visualization setting from environment\n",
    "    SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "    \n",
    "    if SHOW_VISUALIZATION:\n",
    "        # Create a color map for the models - use more distinguishable colors\n",
    "        colors = plt.cm.tab10(np.arange(10))  # Use tab10 for more distinguishable colors\n",
    "        model_colors = {model: colors[i % len(colors)] for i, model in enumerate(models)}\n",
    "        \n",
    "        # Create markers for different models\n",
    "        markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "        model_markers = {model: markers[i % len(markers)] for i, model in enumerate(models)}\n",
    "        \n",
    "        # Create line styles for different models\n",
    "        linestyles = ['-', '--', ':', '-.', (0, (3, 1, 1, 1)), (0, (5, 1))]\n",
    "        model_linestyles = {model: linestyles[i % len(linestyles)] for i, model in enumerate(models)}\n",
    "        \n",
    "        # First iterate through metrics\n",
    "        for metric in code_metrics:\n",
    "            # Skip if metric not available\n",
    "            if metric not in file_analysis_df.columns:\n",
    "                continue\n",
    "            \n",
    "            # 1. Create plot for Actor SIFP vs Metric (all models on the same plot)\n",
    "            # Size optimized for a 6.5\" wide document with bottom legend\n",
    "            fig_height = 4.5 + (0.25 * len(models))  # Adjust height based on number of models\n",
    "            plt.figure(figsize=(6.5, fig_height))\n",
    "            \n",
    "            # Create a list to store legend handles\n",
    "            legend_handles = []\n",
    "            \n",
    "            # Track max values for setting plot limits\n",
    "            max_x_val = 0\n",
    "            max_y_val = 0\n",
    "            min_x_val = float('inf')\n",
    "            \n",
    "            # Now iterate through models to add each to the same plot\n",
    "            for i, model in enumerate(models):\n",
    "                # Filter data for this model\n",
    "                model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "                \n",
    "                # Skip if not enough data\n",
    "                if len(model_data) < 3:\n",
    "                    print(f\"Skipping {model} - insufficient data points ({len(model_data)} records)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get color, marker, and linestyle for this model\n",
    "                color = model_colors[model]\n",
    "                marker = model_markers[model]\n",
    "                linestyle = model_linestyles[model]\n",
    "                \n",
    "                # Create scatter plot for this model\n",
    "                scatter = plt.scatter(\n",
    "                    model_data['sifp_actor_total'], \n",
    "                    model_data[metric],\n",
    "                    color=color, \n",
    "                    marker=marker,\n",
    "                    alpha=0.7, \n",
    "                    s=40,  # Slightly smaller points\n",
    "                )\n",
    "                \n",
    "                # Get max values for axis scaling\n",
    "                if model_data['sifp_actor_total'].max() > max_x_val:\n",
    "                    max_x_val = model_data['sifp_actor_total'].max()\n",
    "                if model_data[metric].max() > max_y_val:\n",
    "                    max_y_val = model_data[metric].max()\n",
    "                if model_data['sifp_actor_total'].min() < min_x_val and model_data['sifp_actor_total'].min() > 0:\n",
    "                    min_x_val = model_data['sifp_actor_total'].min()\n",
    "                \n",
    "                # Add regression line for this model\n",
    "                X = model_data['sifp_actor_total'].values.reshape(-1, 1)\n",
    "                y = model_data[metric].values\n",
    "                \n",
    "                # Skip if NaN values present\n",
    "                if np.isnan(X).any() or np.isnan(y).any():\n",
    "                    valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "                    X = X[valid_indices]\n",
    "                    y = y[valid_indices]\n",
    "                \n",
    "                if len(X) >= 3:\n",
    "                    lr = LinearRegression()\n",
    "                    lr.fit(X, y)\n",
    "                    \n",
    "                    # Calculate R²\n",
    "                    r2 = r2_score(y, lr.predict(X))\n",
    "                    \n",
    "                    # Create a right-justified formula string\n",
    "                    model_name = f\"{model}\"\n",
    "                    formula = f\"$\\\\mathbf{{{lr.coef_[0]:.1f}×SIFP + {lr.intercept_:.0f}}}$ ($\\\\mathbf{{R²={r2:.2f}}}$)\"\n",
    "                    \n",
    "                    # Create a combined legend entry with both marker and line\n",
    "                    legend_handles.append(\n",
    "                        plt.Line2D([0], [0], color=color, marker=marker, linestyle=linestyle, \n",
    "                                markersize=6, linewidth=1.5, label=f\"{model_name} → {formula}\")\n",
    "                    )\n",
    "            \n",
    "            # Set reasonable axis limits first so lines can use them\n",
    "            plot_min_x = min_x_val * 0.9 if min_x_val != float('inf') else 0\n",
    "            plot_max_x = max_x_val * 1.1\n",
    "            plt.xlim(plot_min_x, plot_max_x)\n",
    "            plt.ylim(0, max_y_val * 1.1)\n",
    "            \n",
    "            # Now add regression lines using the full plot width\n",
    "            for i, model in enumerate(models):\n",
    "                model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "                \n",
    "                # Skip if not enough data\n",
    "                if len(model_data) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Get color and linestyle for this model\n",
    "                color = model_colors[model]\n",
    "                linestyle = model_linestyles[model]\n",
    "                \n",
    "                X = model_data['sifp_actor_total'].values.reshape(-1, 1)\n",
    "                y = model_data[metric].values\n",
    "                \n",
    "                # Skip if NaN values present\n",
    "                if np.isnan(X).any() or np.isnan(y).any():\n",
    "                    valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "                    X = X[valid_indices]\n",
    "                    y = y[valid_indices]\n",
    "                \n",
    "                if len(X) >= 3:\n",
    "                    lr = LinearRegression()\n",
    "                    lr.fit(X, y)\n",
    "                    \n",
    "                    # Create x_range across the FULL plot width for the regression line\n",
    "                    x_range = np.linspace(plot_min_x, plot_max_x, 100).reshape(-1, 1)\n",
    "                    plt.plot(x_range, lr.predict(x_range), linestyle=linestyle, color=color, linewidth=1.5)\n",
    "            \n",
    "            # Set labels and title with appropriate font sizes\n",
    "            plt.title(f\"Project: {NEO4J_PROJECT_NAME}\\nActor SIFP vs {metric}\", fontsize=12)\n",
    "            plt.xlabel(\"Actor SIFP Points\", fontsize=10)\n",
    "            plt.ylabel(f\"{metric}\", fontsize=10)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            \n",
    "            # Add legend to the bottom of the plot with stacked entries\n",
    "            legend = plt.legend(handles=legend_handles, \n",
    "                      loc='upper center', \n",
    "                      bbox_to_anchor=(0.5, -0.15),  # Moved down from -0.05 to -0.15 to add more space \n",
    "                      fontsize=8,\n",
    "                      frameon=True,\n",
    "                      ncol=1)  # One column for stacked legend\n",
    "            \n",
    "            # Adjust layout to make room for the taller legend at the bottom\n",
    "            bottom_margin = 0.15 + (len(legend_handles) * 0.03)  # Increased from 0.05 to 0.15\n",
    "            plt.subplots_adjust(bottom=bottom_margin)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 2. Create plot for Judge SIFP vs Metric (all models on the same plot)\n",
    "            plt.figure(figsize=(6.5, fig_height))\n",
    "            \n",
    "            # Reset legend handles\n",
    "            legend_handles = []\n",
    "            \n",
    "            # Reset max values for setting plot limits\n",
    "            max_x_val = 0\n",
    "            max_y_val = 0\n",
    "            min_x_val = float('inf')\n",
    "            \n",
    "            # Now iterate through models to add each to the same plot\n",
    "            for i, model in enumerate(models):\n",
    "                # Filter data for this model\n",
    "                model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "                \n",
    "                # Skip if not enough data\n",
    "                if len(model_data) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Get color, marker, and linestyle for this model\n",
    "                color = model_colors[model]\n",
    "                marker = model_markers[model]\n",
    "                linestyle = model_linestyles[model]\n",
    "                \n",
    "                # Create scatter plot for this model\n",
    "                scatter = plt.scatter(\n",
    "                    model_data['sifp_final_total'], \n",
    "                    model_data[metric],\n",
    "                    color=color, \n",
    "                    marker=marker,\n",
    "                    alpha=0.7, \n",
    "                    s=40,  # Slightly smaller points\n",
    "                )\n",
    "                \n",
    "                # Get max values for axis scaling\n",
    "                if model_data['sifp_final_total'].max() > max_x_val:\n",
    "                    max_x_val = model_data['sifp_final_total'].max()\n",
    "                if model_data[metric].max() > max_y_val:\n",
    "                    max_y_val = model_data[metric].max()\n",
    "                if model_data['sifp_final_total'].min() < min_x_val and model_data['sifp_final_total'].min() > 0:\n",
    "                    min_x_val = model_data['sifp_final_total'].min()\n",
    "                \n",
    "                # Add regression line for this model\n",
    "                X = model_data['sifp_final_total'].values.reshape(-1, 1)\n",
    "                y = model_data[metric].values\n",
    "                \n",
    "                # Skip if NaN values present\n",
    "                if np.isnan(X).any() or np.isnan(y).any():\n",
    "                    valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "                    X = X[valid_indices]\n",
    "                    y = y[valid_indices]\n",
    "                \n",
    "                if len(X) >= 3:\n",
    "                    lr = LinearRegression()\n",
    "                    lr.fit(X, y)\n",
    "                    \n",
    "                    # Calculate R²\n",
    "                    r2 = r2_score(y, lr.predict(X))\n",
    "                    \n",
    "                    # Create a right-justified formula string\n",
    "                    model_name = f\"{model}\"\n",
    "                    formula = f\"$\\\\mathbf{{{lr.coef_[0]:.1f}×SIFP + {lr.intercept_:.0f}}}$ ($\\\\mathbf{{R²={r2:.2f}}}$)\"\n",
    "                    \n",
    "                    # Create a combined legend entry with both marker and line\n",
    "                    legend_handles.append(\n",
    "                        plt.Line2D([0], [0], color=color, marker=marker, linestyle=linestyle, \n",
    "                                markersize=6, linewidth=1.5, label=f\"{model_name} → {formula}\")\n",
    "                    )\n",
    "            \n",
    "            # Set reasonable axis limits first so lines can use them\n",
    "            plot_min_x = min_x_val * 0.9 if min_x_val != float('inf') else 0\n",
    "            plot_max_x = max_x_val * 1.1\n",
    "            plt.xlim(plot_min_x, plot_max_x)\n",
    "            plt.ylim(0, max_y_val * 1.1)\n",
    "            \n",
    "            # Now add regression lines using the full plot width\n",
    "            for i, model in enumerate(models):\n",
    "                model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "                \n",
    "                # Skip if not enough data\n",
    "                if len(model_data) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Get color and linestyle for this model\n",
    "                color = model_colors[model]\n",
    "                linestyle = model_linestyles[model]\n",
    "                \n",
    "                X = model_data['sifp_final_total'].values.reshape(-1, 1)\n",
    "                y = model_data[metric].values\n",
    "                \n",
    "                # Skip if NaN values present\n",
    "                if np.isnan(X).any() or np.isnan(y).any():\n",
    "                    valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "                    X = X[valid_indices]\n",
    "                    y = y[valid_indices]\n",
    "                \n",
    "                if len(X) >= 3:\n",
    "                    lr = LinearRegression()\n",
    "                    lr.fit(X, y)\n",
    "                    \n",
    "                    # Create x_range across the FULL plot width for the regression line\n",
    "                    x_range = np.linspace(plot_min_x, plot_max_x, 100).reshape(-1, 1)\n",
    "                    plt.plot(x_range, lr.predict(x_range), linestyle=linestyle, color=color, linewidth=1.5)\n",
    "            \n",
    "            # Set labels and title with appropriate font sizes\n",
    "            plt.title(f\"Project: {NEO4J_PROJECT_NAME}\\nMeta-Judge SIFP vs {metric}\", fontsize=12)\n",
    "            plt.xlabel(\"Judge SIFP Points\", fontsize=10)\n",
    "            plt.ylabel(f\"{metric}\", fontsize=10)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.xticks(fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            \n",
    "            # Add legend to the bottom of the plot with stacked entries\n",
    "            legend = plt.legend(handles=legend_handles, \n",
    "                      loc='upper center', \n",
    "                      bbox_to_anchor=(0.5, -0.15),  # Moved down from -0.05 to -0.15 to add more space\n",
    "                      fontsize=8,\n",
    "                      frameon=True,\n",
    "                      ncol=1)  # One column for stacked legend\n",
    "            \n",
    "            # Adjust layout to make room for the taller legend at the bottom\n",
    "            bottom_margin = 0.15 + (len(legend_handles) * 0.03)  # Increased from 0.05 to 0.15\n",
    "            plt.subplots_adjust(bottom=bottom_margin)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"(Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.)\")\n",
    "    \n",
    "    # Analyze correlation between SIFP scores and code metrics\n",
    "    print(\"Correlation between SIFP scores and code metrics:\")\n",
    "    print(\"=\" * 80)\n",
    "    for metric in code_metrics:\n",
    "        if metric in file_analysis_df.columns:\n",
    "            corr_actor = file_analysis_df[['sifp_actor_total', metric]].corr().iloc[0, 1]\n",
    "            corr_judge = file_analysis_df[['sifp_final_total', metric]].corr().iloc[0, 1]\n",
    "            diff = corr_judge - corr_actor\n",
    "            print(f\"{metric:20} | Actor: {corr_actor:.3f} | Judge: {corr_judge:.3f} | Difference: {diff:.3f}\")\n",
    "    \n",
    "    print(\"Modeling SIFP to Code Metric Relationships:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use the filtered models for analysis\n",
    "    for model in models:\n",
    "        model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        \n",
    "        if len(model_data) < 5:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Model: {model}\")\n",
    "        \n",
    "        for metric in code_metrics:\n",
    "            if metric not in model_data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Actor SIFP relationship\n",
    "            X_actor = model_data['sifp_actor_total'].values.reshape(-1, 1)\n",
    "            y_actor = model_data[metric].values\n",
    "            \n",
    "            # Judge SIFP relationship\n",
    "            X_judge = model_data['sifp_final_total'].values.reshape(-1, 1)\n",
    "            y_judge = model_data[metric].values\n",
    "            \n",
    "            # Skip if not enough valid data\n",
    "            if (np.isnan(X_actor).any() or np.isnan(y_actor).any() or \n",
    "                np.isnan(X_judge).any() or np.isnan(y_judge).any()):\n",
    "                continue\n",
    "                \n",
    "            # Fit linear models\n",
    "            actor_model = LinearRegression().fit(X_actor, y_actor)\n",
    "            judge_model = LinearRegression().fit(X_judge, y_judge)\n",
    "            \n",
    "            # Calculate R²\n",
    "            actor_r2 = r2_score(y_actor, actor_model.predict(X_actor))\n",
    "            judge_r2 = r2_score(y_judge, judge_model.predict(X_judge))\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"{metric} (Actor SIFP): slope = {actor_model.coef_[0]:.3f}, intercept = {actor_model.intercept_:.3f}, R² = {actor_r2:.3f}\")\n",
    "            print(f\"{metric} (Judge SIFP): slope = {judge_model.coef_[0]:.3f}, intercept = {judge_model.intercept_:.3f}, R² = {judge_r2:.3f}\")\n",
    "else:\n",
    "    print(\"No file-level analysis data available.\")\n",
    "    print(\"Could not merge SIFP estimations with code metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - File-Level Analysis: Understanding Record Composition in Detail  \n",
    "# Purpose: Analyze record composition and prepare for performance analysis\n",
    "# Dependencies: pandas, numpy, matplotlib, seaborn, os\n",
    "# Breadcrumbs: Analysis -> Record Composition\n",
    "\n",
    "print(\"File-Level Analysis: Understanding Record Composition\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we have the merged file analysis data from cell 8\n",
    "if 'file_analysis_df' in globals() and not file_analysis_df.empty:\n",
    "    print(f\"Total records in dataset: {len(file_analysis_df)}\")\n",
    "    print(f\"Unique code files: {file_analysis_df['File'].nunique()}\")\n",
    "    print(f\"Unique requirements: {file_analysis_df['sifp_requirement_id'].nunique()}\")\n",
    "    print(f\"Unique models: {file_analysis_df['sifp_model'].nunique()}\")\n",
    "    \n",
    "    print(\"Why are there more records than files?\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Each record in this dataset represents a unique combination of:\")\n",
    "    print(\"1. AI model (sifp_model)\")\n",
    "    print(\"2. Requirement (sifp_requirement_id)\")\n",
    "    print(\"3. Code file (File)\")\n",
    "    print(\"4. Associated metrics (Lines, Code, etc.)\")\n",
    "    \n",
    "    print(\"The record count exceeds the file count because:\")\n",
    "    print(\"- Multiple AI models evaluated the same codebase\")\n",
    "    print(\"- Each model estimated multiple requirements\")\n",
    "    print(\"- Each requirement can map to multiple code files\")\n",
    "    print(\"- A single file may implement parts of multiple requirements\")\n",
    "    \n",
    "    # Get display setting from environment variable\n",
    "    SHOW_ALL_DATA = os.getenv('SHOW_ALL_DATA', 'False').lower() == 'true'\n",
    "    \n",
    "    # Add detailed record breakdown by model\n",
    "    print(\"Detailed Breakdown of Records by Model and Metric Type\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Define metrics for analysis\n",
    "    selected_metrics = ['Code', 'Lines', 'Lines-exe', 'Lines-dec', 'Units']\n",
    "    selected_metrics = [m for m in selected_metrics if m in file_analysis_df.columns]\n",
    "    \n",
    "    # Get unique models\n",
    "    models = file_analysis_df['sifp_model'].unique()\n",
    "    \n",
    "    # Create breakdown table data\n",
    "    breakdown_data = []\n",
    "    for model in models:\n",
    "        model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        model_files = model_data['File'].nunique()\n",
    "        model_reqs = model_data['sifp_requirement_id'].nunique()\n",
    "        \n",
    "        # Create base row with model info\n",
    "        breakdown_row = {\n",
    "            'Model': model,\n",
    "            'Total Records': len(model_data),\n",
    "            'Unique Files': model_files,\n",
    "            'Unique Requirements': model_reqs,\n",
    "            'Records per File (avg)': round(len(model_data) / model_files, 2) if model_files > 0 else 0,\n",
    "            'Records per Requirement (avg)': round(len(model_data) / model_reqs, 2) if model_reqs > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Add counts for each metric type\n",
    "        for metric in selected_metrics:\n",
    "            if metric in model_data.columns:\n",
    "                metric_count = model_data[metric].notna().sum()\n",
    "                breakdown_row[f'{metric} Metrics'] = metric_count\n",
    "                \n",
    "        breakdown_data.append(breakdown_row)\n",
    "    \n",
    "    # Create and display the breakdown dataframe\n",
    "    breakdown_df = pd.DataFrame(breakdown_data)\n",
    "    display(breakdown_df)\n",
    "    \n",
    "    # Basic correlation analysis we can do with current data\n",
    "    print(\"Basic Correlation Analysis with Available Data:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Simple correlation between actor and judge SIFP scores\n",
    "    if 'sifp_actor_total' in file_analysis_df.columns and 'sifp_final_total' in file_analysis_df.columns:\n",
    "        correlation = file_analysis_df[['sifp_actor_total', 'sifp_final_total']].corr().iloc[0, 1]\n",
    "        print(f\"Correlation between Actor and Judge SIFP scores: {correlation:.3f}\")\n",
    "    \n",
    "    # Basic model statistics\n",
    "    print(\"Basic Model Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        \n",
    "        if len(model_data) < 5:\n",
    "            print(f\"Skipping {model} - insufficient data points ({len(model_data)} records)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Records: {len(model_data)}\")\n",
    "        print(f\"  Unique files: {model_data['File'].nunique()}\")\n",
    "        print(f\"  Unique requirements: {model_data['sifp_requirement_id'].nunique()}\")\n",
    "        \n",
    "        # Calculate basic statistics for this model\n",
    "        if 'sifp_actor_total' in model_data.columns:\n",
    "            actor_mean = model_data['sifp_actor_total'].mean()\n",
    "            print(f\"  Mean Actor SIFP: {actor_mean:.2f}\")\n",
    "            \n",
    "        if 'sifp_final_total' in model_data.columns:\n",
    "            judge_mean = model_data['sifp_final_total'].mean()\n",
    "            print(f\"  Mean Judge SIFP: {judge_mean:.2f}\")\n",
    "            \n",
    "        # Analyze available code metrics\n",
    "        for metric in selected_metrics:\n",
    "            if metric in model_data.columns:\n",
    "                metric_mean = model_data[metric].mean()\n",
    "                print(f\"  Mean {metric}: {metric_mean:.2f}\")\n",
    "                \n",
    "    # Show sample of the record data if requested\n",
    "    print(\"Complete Record Listing - Understanding the Records/Files Relationship\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"This table shows all records in the dataset, ordered by model and file name.\")\n",
    "    print(\"This explains why we see many more records than unique files in the analysis.\")\n",
    "    \n",
    "    # Create a cleaned up summary dataframe with the essential columns\n",
    "    record_listing = file_analysis_df[['sifp_model', 'File', 'sifp_requirement_id', 'sifp_actor_total', \n",
    "                                      'sifp_final_total'] + selected_metrics]\n",
    "    \n",
    "    # Sort by model and file name as requested\n",
    "    record_listing = record_listing.sort_values(['sifp_model', 'File'])\n",
    "    \n",
    "    # Show all data if configured, otherwise just show the head\n",
    "    if SHOW_ALL_DATA:\n",
    "        print(f\"Showing all {len(record_listing)} records:\")\n",
    "        display(record_listing)\n",
    "    else:\n",
    "        print(f\"Showing first 50 of {len(record_listing)} records (set SHOW_ALL_DATA=True in .env to see all):\")\n",
    "        display(record_listing.head(50))\n",
    "    \n",
    "    print(\"\\nNote: Performance metrics will be calculated in the next cell.\")\n",
    "    print(\"This will enable detailed statistical analysis including paired comparisons and effect size calculations.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No file-level analysis data available.\")\n",
    "    print(\"Cannot perform record composition analysis.\")\n",
    "    print(\"Check if cell 8 executed successfully and created file_analysis_df.\")\n",
    "\n",
    "print(\"Record composition analysis complete. Ready for performance calculation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Statistical Analysis: Paired Comparisons and Performance Testing\n",
    "# Purpose: Perform comprehensive statistical tests comparing Actor vs Judge performance with significance testing\n",
    "# Dependencies: scipy, pandas, numpy, sklearn\n",
    "# Breadcrumbs: Analysis -> Statistical Testing\n",
    "\n",
    "# Initialize storage for statistical test results\n",
    "statistical_test_results = []\n",
    "\n",
    "print(\"Performance Calculation and Statistical Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we have the file analysis data from previous cells\n",
    "if 'file_analysis_df' in globals() and not file_analysis_df.empty:\n",
    "    print(f\"Starting with {len(file_analysis_df)} records from file analysis\")\n",
    "    \n",
    "    # Define metrics for analysis\n",
    "    selected_metrics = ['Code', 'Lines', 'Lines-exe', 'Lines-dec', 'Units']\n",
    "    selected_metrics = [m for m in selected_metrics if m in file_analysis_df.columns]\n",
    "    \n",
    "    # Get unique models\n",
    "    models = file_analysis_df['sifp_model'].unique()\n",
    "    \n",
    "    # Create performance dataframe to hold performance data\n",
    "    performance_rows = []\n",
    "    \n",
    "    # Calculate proportional predictions and errors for each model and metric\n",
    "    for model in models:\n",
    "        model_data = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        \n",
    "        if len(model_data) < 5:\n",
    "            print(f\"Skipping {model} - insufficient data points ({len(model_data)} records)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing Model: {model} ({len(model_data)} records)\")\n",
    "        \n",
    "        for metric in selected_metrics:\n",
    "            if metric not in model_data.columns:\n",
    "                continue\n",
    "                \n",
    "            # For each file, calculate the predicted metric value using the proportion\n",
    "            # of SIFP points to total SIFP points\n",
    "            \n",
    "            # Step 1: Get total SIFP points and total metric value across all files\n",
    "            total_actor_sifp = model_data['sifp_actor_total'].sum()\n",
    "            total_judge_sifp = model_data['sifp_final_total'].sum()\n",
    "            total_metric = model_data[metric].sum()\n",
    "            \n",
    "            # Skip if we have zeros or NaN\n",
    "            if total_actor_sifp == 0 or total_judge_sifp == 0 or pd.isna(total_metric):\n",
    "                continue\n",
    "            \n",
    "            # Step 2: For each file, calculate predicted metric value based on proportion\n",
    "            for _, row in model_data.iterrows():\n",
    "                # Skip rows with missing data\n",
    "                if pd.isna(row['sifp_actor_total']) or pd.isna(row['sifp_final_total']) or pd.isna(row[metric]):\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate predicted values using proportional approach\n",
    "                actor_prediction = (row['sifp_actor_total'] / total_actor_sifp) * total_metric\n",
    "                judge_prediction = (row['sifp_final_total'] / total_judge_sifp) * total_metric\n",
    "                \n",
    "                # Calculate absolute errors\n",
    "                actor_error = abs(actor_prediction - row[metric])\n",
    "                judge_error = abs(judge_prediction - row[metric])\n",
    "                \n",
    "                # Calculate percentage errors\n",
    "                if row[metric] > 0:\n",
    "                    actor_pct_error = (actor_error / row[metric]) * 100\n",
    "                    judge_pct_error = (judge_error / row[metric]) * 100\n",
    "                else:\n",
    "                    actor_pct_error = np.nan\n",
    "                    judge_pct_error = np.nan\n",
    "                \n",
    "                # Calculate error improvement\n",
    "                if actor_error > 0:\n",
    "                    error_improvement = ((actor_error - judge_error) / actor_error) * 100\n",
    "                else:\n",
    "                    error_improvement = 0\n",
    "                \n",
    "                # Store results\n",
    "                performance_rows.append({\n",
    "                    'Model': model,\n",
    "                    'Code Metric': metric,\n",
    "                    'File': row['File'],\n",
    "                    'Requirement': row['sifp_requirement_id'],\n",
    "                    'Actual Value': row[metric],\n",
    "                    'Actor Prediction': actor_prediction,\n",
    "                    'Judge Prediction': judge_prediction,\n",
    "                    'Actor Error': actor_error,\n",
    "                    'Judge Error': judge_error,\n",
    "                    'Actor % Error': actor_pct_error,\n",
    "                    'Judge % Error': judge_pct_error,\n",
    "                    'Error Improvement': error_improvement,\n",
    "                    'Error % Improvement': error_improvement\n",
    "                })\n",
    "    \n",
    "    # Create performance dataframe from collected data\n",
    "    performance_df = pd.DataFrame(performance_rows)\n",
    "    \n",
    "    # Display summary statistics if we have performance data\n",
    "    if not performance_df.empty:\n",
    "        print(\"Performance Summary by Model and Metric:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group by model and metric, calculate averages\n",
    "        summary = performance_df.groupby(['Model', 'Code Metric']).agg({\n",
    "            'Actor Error': 'mean',\n",
    "            'Judge Error': 'mean',\n",
    "            'Actor % Error': 'mean',\n",
    "            'Judge % Error': 'mean',\n",
    "            'Error % Improvement': 'mean',\n",
    "            'File': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Rename File count to num_files for clarity\n",
    "        summary = summary.rename(columns={'File': 'num_files'})\n",
    "        \n",
    "        # Display summary\n",
    "        display(summary)\n",
    "        \n",
    "        # Get overall averages\n",
    "        print(\"Overall Averages:\")\n",
    "        overall = performance_df.agg({\n",
    "            'Actor Error': 'mean',\n",
    "            'Judge Error': 'mean',\n",
    "            'Actor % Error': 'mean',\n",
    "            'Judge % Error': 'mean',\n",
    "            'Error % Improvement': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        overall = overall.rename(columns={'index': 'Metric', 0: 'Value'})\n",
    "        display(overall)\n",
    "        \n",
    "        # Calculate what percentage of files have improved predictions with the judge\n",
    "        improved_count = (performance_df['Error % Improvement'] > 0).sum()\n",
    "        total_count = performance_df['Error % Improvement'].count()\n",
    "        improvement_rate = (improved_count / total_count) * 100 if total_count > 0 else 0\n",
    "        \n",
    "        print(f\"Improvement Rate: {improved_count} out of {total_count} file predictions improved ({improvement_rate:.1f}%)\")\n",
    "        \n",
    "        # Get visualization setting from environment\n",
    "        SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "        \n",
    "        if SHOW_VISUALIZATION:\n",
    "            # Create visualizations for each model and metric combination\n",
    "            for metric in selected_metrics:\n",
    "                metric_data = performance_df[performance_df['Code Metric'] == metric]\n",
    "                \n",
    "                if not metric_data.empty:\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    \n",
    "                    # Boxplot of percent errors\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    boxdata = [\n",
    "                        metric_data['Actor % Error'].dropna(),\n",
    "                        metric_data['Judge % Error'].dropna()\n",
    "                    ]\n",
    "                    plt.boxplot(boxdata, labels=['Actor', 'Judge'])\n",
    "                    plt.title(f'% Error for {metric}')\n",
    "                    plt.ylabel('Percent Error (%)')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Scatter plot of actual vs predicted\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.scatter(metric_data['Actual Value'], metric_data['Actor Prediction'], \n",
    "                              alpha=0.7, label='Actor', marker='o')\n",
    "                    plt.scatter(metric_data['Actual Value'], metric_data['Judge Prediction'], \n",
    "                              alpha=0.7, label='Judge', marker='x')\n",
    "                    plt.plot([metric_data['Actual Value'].min(), metric_data['Actual Value'].max()], \n",
    "                           [metric_data['Actual Value'].min(), metric_data['Actual Value'].max()], \n",
    "                           'k--', alpha=0.3)\n",
    "                    plt.title(f'Actual vs Predicted {metric}')\n",
    "                    plt.xlabel('Actual Value')\n",
    "                    plt.ylabel('Predicted Value')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            \n",
    "            # Create a heatmap of improvement by model and metric\n",
    "            if len(models) > 1 and len(selected_metrics) > 1:\n",
    "                pivot_data = performance_df.pivot_table(\n",
    "                    index='Model', \n",
    "                    columns='Code Metric',\n",
    "                    values='Error % Improvement',\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.heatmap(pivot_data, annot=True, cmap='RdBu_r', center=0)\n",
    "                plt.title('Mean Error % Improvement (Judge vs Actor)')\n",
    "                plt.ylabel('Model')\n",
    "                plt.xlabel('Code Metric')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.\")\n",
    "            \n",
    "        print(\"Performance calculation completed successfully!\")\n",
    "        print(f\"Created performance dataframe with {len(performance_df)} entries\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No performance data could be calculated.\")\n",
    "        print(\"Check if SIFP and code metric data are available and valid.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No file-level analysis data available from previous cells.\")\n",
    "    print(\"Cannot perform performance calculation.\")\n",
    "    print(\"Please ensure cell 8 executed successfully.\")\n",
    "\n",
    "print(\"Performance analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Detailed File-Level Prediction Accuracy using Linear Relationships\n",
    "# Purpose: Calculate per-file prediction accuracy using linear regression models and analyze prediction errors\n",
    "# Dependencies: pandas, numpy, sklearn, matplotlib\n",
    "# Breadcrumbs: Analysis -> Linear Modeling\n",
    "\n",
    "if not file_analysis_df.empty:\n",
    "    print(\"Per-File Prediction Accuracy Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Debugging info\n",
    "    print(f\"File analysis dataframe shape: {file_analysis_df.shape}\")\n",
    "    print(f\"File analysis dataframe columns: {file_analysis_df.columns.tolist()}\")\n",
    "    \n",
    "    # Get all models\n",
    "    models = file_analysis_df['sifp_model'].unique()\n",
    "    print(f\"Found {len(models)} models: {models}\")\n",
    "    \n",
    "    # Get code metrics - only include ones that actually exist in the dataframe\n",
    "    all_possible_metrics = ['Lines', 'Code', 'Lines-exe', 'Lines-dec', 'Units', 'Comments', 'Blanks', 'Stmt-exe', 'Stmt-dec']\n",
    "    code_metrics = [m for m in all_possible_metrics if m in file_analysis_df.columns]\n",
    "    print(f\"Using code metrics: {code_metrics}\")\n",
    "    \n",
    "    # Generate linear coefficients\n",
    "    print(\"Generating linear coefficients...\")\n",
    "    model_linear_mapping = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_df = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        \n",
    "        if len(model_df) < 5:\n",
    "            print(f\"Skipping model {model} - insufficient data (only {len(model_df)} records)\")\n",
    "            continue\n",
    "            \n",
    "        model_coefficients = {}\n",
    "        \n",
    "        # Calculate linear relationships for this model\n",
    "        for sifp_type, sifp_col in [('Actor', 'sifp_actor_total'), ('Judge', 'sifp_final_total')]:\n",
    "            for code_metric in code_metrics:\n",
    "                if code_metric not in model_df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Filter for valid data\n",
    "                valid_data = model_df[~(model_df[sifp_col].isna() | model_df[code_metric].isna())]\n",
    "                \n",
    "                if len(valid_data) < 3:\n",
    "                    print(f\"Skipping {model} {sifp_type} → {code_metric} - insufficient valid data points ({len(valid_data)})\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Fit linear regression\n",
    "                    X = valid_data[sifp_col].values.reshape(-1, 1)\n",
    "                    y = valid_data[code_metric].values\n",
    "                    \n",
    "                    lr_model = LinearRegression()\n",
    "                    lr_model.fit(X, y)\n",
    "                    \n",
    "                    # Store coefficients\n",
    "                    model_coefficients[(sifp_type, code_metric)] = {\n",
    "                        'slope': lr_model.coef_[0],\n",
    "                        'intercept': lr_model.intercept_,\n",
    "                        'r2': r2_score(y, lr_model.predict(X)),\n",
    "                        'rmse': np.sqrt(mean_squared_error(y, lr_model.predict(X)))\n",
    "                    }\n",
    "                    \n",
    "                    # Debug print\n",
    "                    print(f\"  {model} {sifp_type} → {code_metric}: slope={lr_model.coef_[0]:.2f}, intercept={lr_model.intercept_:.2f}, r2={r2_score(y, lr_model.predict(X)):.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting {model} {sifp_type} → {code_metric}: {str(e)}\")\n",
    "        \n",
    "        if model_coefficients:\n",
    "            model_linear_mapping[model] = model_coefficients\n",
    "            print(f\"Created {len(model_coefficients)} coefficient mappings for {model}\")\n",
    "        else:\n",
    "            print(f\"No valid coefficient mappings for {model}\")\n",
    "    \n",
    "    # Create performance data using linear relationships\n",
    "    print(\"Calculating prediction performance...\")\n",
    "    model_performance_data = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_df = file_analysis_df[file_analysis_df['sifp_model'] == model]\n",
    "        \n",
    "        if len(model_df) < 5 or model not in model_linear_mapping:\n",
    "            print(f\"Skipping model {model} - insufficient data or no coefficient mappings\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing {model} file-level predictions using linear relationships...\")\n",
    "        model_coefficients = model_linear_mapping[model]\n",
    "        \n",
    "        # Get unique files for this model\n",
    "        unique_files = model_df['File'].unique()\n",
    "        print(f\"  Found {len(unique_files)} unique files for {model}\")\n",
    "        \n",
    "        file_count = 0\n",
    "        predictions_count = 0\n",
    "        \n",
    "        # Group by file for detailed analysis\n",
    "        for file in unique_files:\n",
    "            file_rows = model_df[model_df['File'] == file]\n",
    "            \n",
    "            if len(file_rows) < 1:\n",
    "                continue\n",
    "                \n",
    "            file_count += 1\n",
    "            \n",
    "            # Get the actual code metrics for this file\n",
    "            file_metrics = {}\n",
    "            for metric in code_metrics:\n",
    "                if metric in file_rows.columns:\n",
    "                    file_metrics[metric] = file_rows[metric].iloc[0]\n",
    "            \n",
    "            # Calculate prediction errors for each metric using linear model\n",
    "            for metric in code_metrics:\n",
    "                if metric not in file_metrics:\n",
    "                    continue\n",
    "                    \n",
    "                actual = file_metrics[metric]\n",
    "                \n",
    "                # Skip zero or NaN values\n",
    "                if pd.isna(actual) or actual == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Actor analysis\n",
    "                if ('Actor', metric) in model_coefficients:\n",
    "                    coeffs = model_coefficients[('Actor', metric)]\n",
    "                    \n",
    "                    # Skip if actor SIFP values are all NaN\n",
    "                    if file_rows['sifp_actor_total'].isna().all():\n",
    "                        continue\n",
    "                    \n",
    "                    actor_sifp = file_rows['sifp_actor_total'].mean()\n",
    "                    actor_pred = coeffs['slope'] * actor_sifp + coeffs['intercept']\n",
    "                    actor_error = abs(actual - actor_pred)\n",
    "                    actor_pct_error = (actor_error / actual) * 100 if actual != 0 else np.nan\n",
    "                    \n",
    "                    # Judge evaluation\n",
    "                    if ('Judge', metric) in model_coefficients:\n",
    "                        coeffs_judge = model_coefficients[('Judge', metric)]\n",
    "                        \n",
    "                        # Skip if judge SIFP values are all NaN\n",
    "                        if file_rows['sifp_final_total'].isna().all():\n",
    "                            continue\n",
    "                        \n",
    "                        judge_sifp = file_rows['sifp_final_total'].mean()\n",
    "                        judge_pred = coeffs_judge['slope'] * judge_sifp + coeffs_judge['intercept']\n",
    "                        judge_error = abs(actual - judge_pred)\n",
    "                        judge_pct_error = (judge_error / actual) * 100 if actual != 0 else np.nan\n",
    "                        \n",
    "                        # Calculate improvement from actor to judge\n",
    "                        error_improvement = actor_error - judge_error\n",
    "                        pct_improvement = actor_pct_error - judge_pct_error\n",
    "                        \n",
    "                        # Store performance data\n",
    "                        performance_entry = {\n",
    "                            'Model': model,\n",
    "                            'File': file,\n",
    "                            'Code Metric': metric,\n",
    "                            'Actual Value': actual,\n",
    "                            'Actor SIFP': actor_sifp,\n",
    "                            'Actor Coefficient': coeffs['slope'],\n",
    "                            'Actor Intercept': coeffs['intercept'],\n",
    "                            'Actor Prediction': actor_pred,\n",
    "                            'Actor Error': actor_error,\n",
    "                            'Actor % Error': actor_pct_error,\n",
    "                            'Judge SIFP': judge_sifp,\n",
    "                            'Judge Coefficient': coeffs_judge['slope'],\n",
    "                            'Judge Intercept': coeffs_judge['intercept'],\n",
    "                            'Judge Prediction': judge_pred,\n",
    "                            'Judge Error': judge_error,\n",
    "                            'Judge % Error': judge_pct_error,\n",
    "                            'Error Improvement': error_improvement,\n",
    "                            'Error % Improvement': pct_improvement\n",
    "                        }\n",
    "                        \n",
    "                        # Check if any key values are NaN\n",
    "                        has_nan = any(pd.isna(val) for key, val in performance_entry.items() \n",
    "                                     if key in ['Actor SIFP', 'Judge SIFP', 'Actual Value', \n",
    "                                               'Actor Prediction', 'Judge Prediction'])\n",
    "                        \n",
    "                        if not has_nan:\n",
    "                            model_performance_data.append(performance_entry)\n",
    "                            predictions_count += 1\n",
    "        \n",
    "        print(f\"  Generated {predictions_count} predictions across {file_count} files for {model}\")\n",
    "    \n",
    "    # Create complete performance dataframe\n",
    "    performance_df = pd.DataFrame(model_performance_data)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        # Remove infinite values for better display\n",
    "        performance_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        print(\"File-Level Prediction Performance Summary (Linear Model):\")\n",
    "        print(f\"Total records: {len(performance_df)}\")\n",
    "        print(f\"Showing first 5 rows:\")\n",
    "        display(performance_df.head())\n",
    "        \n",
    "        # Calculate average performance by model and metric\n",
    "        print(\"Average Prediction Performance by Model and Metric:\")\n",
    "        avg_performance = performance_df.groupby(['Model', 'Code Metric']).agg({\n",
    "            'Actor % Error': 'mean',\n",
    "            'Judge % Error': 'mean',\n",
    "            'Error % Improvement': 'mean',\n",
    "            'Actor Coefficient': 'first',\n",
    "            'Judge Coefficient': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        avg_performance = avg_performance.round(2)\n",
    "        print(f\"Showing first 5 of {len(avg_performance)} rows:\")\n",
    "        display(avg_performance.head())\n",
    "        \n",
    "        # Identify best performing model per metric for file-level prediction\n",
    "        print(\"Best Performing Models for File-Level Prediction (Linear Model):\")\n",
    "        for metric in code_metrics:\n",
    "            metric_performance = avg_performance[avg_performance['Code Metric'] == metric]\n",
    "            \n",
    "            if not metric_performance.empty:\n",
    "                # Best actor performance\n",
    "                best_actor = metric_performance.loc[metric_performance['Actor % Error'].idxmin()]\n",
    "                print(f\"{metric}:\")\n",
    "                print(f\"  Best Actor Analysis: {best_actor['Model']} (Avg Error: {best_actor['Actor % Error']}%, Coefficient: {best_actor['Actor Coefficient']})\")\n",
    "                \n",
    "                # Best judge performance\n",
    "                best_judge = metric_performance.loc[metric_performance['Judge % Error'].idxmin()]\n",
    "                print(f\"  Best Judge Evaluation: {best_judge['Model']} (Avg Error: {best_judge['Judge % Error']}%, Coefficient: {best_judge['Judge Coefficient']})\")\n",
    "                \n",
    "                # Most improved\n",
    "                if 'Error % Improvement' in metric_performance.columns:\n",
    "                    most_improved = metric_performance.loc[metric_performance['Error % Improvement'].idxmax()]\n",
    "                    print(f\"  Most Improved: {most_improved['Model']} (Improvement: {most_improved['Error % Improvement']}%)\")\n",
    "        \n",
    "        # Analyze hard-to-predict files\n",
    "        print(\"Files with Highest Prediction Errors (Linear Model):\")\n",
    "        worst_predictions = performance_df.sort_values('Judge % Error', ascending=False).head(5)\n",
    "        display(worst_predictions[['Model', 'File', 'Code Metric', 'Actual Value', 'Judge Prediction', 'Judge % Error']])\n",
    "        \n",
    "        # Get visualization setting from environment\n",
    "        SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "        \n",
    "        if SHOW_VISUALIZATION:\n",
    "            # Create scatter plots for actual vs linearly predicted values by model for selected metrics\n",
    "            for metric in code_metrics:\n",
    "                metric_data = performance_df[performance_df['Code Metric'] == metric]\n",
    "                \n",
    "                if len(metric_data) < 5:\n",
    "                    print(f\"Skipping visualization for {metric} - insufficient data points\")\n",
    "                    continue\n",
    "                    \n",
    "                plt.figure(figsize=(14, 10))\n",
    "                \n",
    "                # Use different markers and colors for each model\n",
    "                markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h', 'H']\n",
    "                colors = plt.cm.tab10.colors\n",
    "                \n",
    "                for i, model in enumerate(models):\n",
    "                    model_data = performance_df[(performance_df['Model'] == model) & \n",
    "                                              (performance_df['Code Metric'] == metric)]\n",
    "                    \n",
    "                    if len(model_data) > 1:\n",
    "                        marker = markers[i % len(markers)]\n",
    "                        color = colors[i % len(colors)]\n",
    "                        \n",
    "                        plt.scatter(\n",
    "                            model_data['Actual Value'], \n",
    "                            model_data['Judge Prediction'],\n",
    "                            marker=marker,\n",
    "                            color=color,\n",
    "                            alpha=0.7,\n",
    "                            label=f\"{model} (coef={model_data['Judge Coefficient'].iloc[0]:.2f})\"\n",
    "                        )\n",
    "                \n",
    "                # Add perfect prediction line\n",
    "                max_val = max(\n",
    "                    performance_df[performance_df['Code Metric'] == metric]['Actual Value'].max(),\n",
    "                    performance_df[performance_df['Code Metric'] == metric]['Judge Prediction'].max()\n",
    "                ) * 1.1\n",
    "                \n",
    "                plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
    "                \n",
    "                plt.title(f\"Linear Model Prediction: Judge → {metric}\")\n",
    "                plt.xlabel(f\"Actual {metric}\")\n",
    "                plt.ylabel(f\"Predicted {metric} (from Judge SIFP)\")\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"(Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.)\")\n",
    "    else:\n",
    "        print(\"No performance data available for analysis. Check the following:\")\n",
    "        print(\"1. Are there sufficient data points for each model?\")\n",
    "        print(\"2. Do the SIFP scores and code metrics have valid, non-NaN values?\") \n",
    "        print(\"3. Were any linear coefficients successfully generated?\")\n",
    "        print(f\"Models with coefficients: {list(model_linear_mapping.keys())}\")\n",
    "        print(f\"Total coefficient mappings: {sum(len(v) for v in model_linear_mapping.values())}\")\n",
    "else:\n",
    "    print(\"No file-level data available for analysis. The file_analysis_df is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Model Accuracy Summary for Linear Model Predictions\n",
    "# Purpose: Summarize and compare accuracy of linear model predictions across different SIFP models and metrics\n",
    "# Dependencies: pandas, numpy, sklearn, matplotlib\n",
    "# Breadcrumbs: Analysis -> Model Comparison\n",
    "\n",
    "if 'performance_df' in locals() and not performance_df.empty:\n",
    "    print(\"Model Accuracy Summary for Linear Model Predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get all models and metrics\n",
    "    models = performance_df['Model'].unique()\n",
    "    code_metrics = ['Lines', 'Code', 'Lines-exe', 'Lines-dec', 'Units']\n",
    "    code_metrics = [m for m in code_metrics if m in performance_df['Code Metric'].unique()]\n",
    "    \n",
    "    # Calculate overall accuracy stats\n",
    "    print(\"Overall Prediction Accuracy by Model:\")\n",
    "    \n",
    "    model_accuracy = []\n",
    "    for model in models:\n",
    "        model_data = performance_df[performance_df['Model'] == model]\n",
    "        \n",
    "        if len(model_data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean absolute percentage error for actor and judge\n",
    "        actor_mape = model_data['Actor % Error'].mean()\n",
    "        judge_mape = model_data['Judge % Error'].mean()\n",
    "        \n",
    "        # Calculate R-squared for actor and judge\n",
    "        actor_r2_values = []\n",
    "        judge_r2_values = []\n",
    "        actor_coeffs = []\n",
    "        judge_coeffs = []\n",
    "        \n",
    "        for metric in code_metrics:\n",
    "            metric_data = model_data[model_data['Code Metric'] == metric]\n",
    "            \n",
    "            if len(metric_data) > 2:\n",
    "                # Calculate R-squared between actual and model prediction\n",
    "                actor_r2 = r2_score(metric_data['Actual Value'], metric_data['Actor Prediction'])\n",
    "                actor_r2_values.append(actor_r2)\n",
    "                actor_coeffs.append(metric_data['Actor Coefficient'].iloc[0])\n",
    "                \n",
    "                judge_r2 = r2_score(metric_data['Actual Value'], metric_data['Judge Prediction'])\n",
    "                judge_r2_values.append(judge_r2)\n",
    "                judge_coeffs.append(metric_data['Judge Coefficient'].iloc[0])\n",
    "        \n",
    "        actor_avg_r2 = np.mean(actor_r2_values) if actor_r2_values else np.nan\n",
    "        judge_avg_r2 = np.mean(judge_r2_values) if judge_r2_values else np.nan\n",
    "        actor_avg_coeff = np.mean(actor_coeffs) if actor_coeffs else np.nan\n",
    "        judge_avg_coeff = np.mean(judge_coeffs) if judge_coeffs else np.nan\n",
    "        \n",
    "        model_accuracy.append({\n",
    "            'Model': model,\n",
    "            'File Count': model_data['File'].nunique(),\n",
    "            'Actor MAPE': actor_mape,\n",
    "            'Judge MAPE': judge_mape,\n",
    "            'MAPE Improvement': actor_mape - judge_mape,\n",
    "            'Actor Avg R²': actor_avg_r2,\n",
    "            'Judge Avg R²': judge_avg_r2,\n",
    "            'R² Improvement': judge_avg_r2 - actor_avg_r2,\n",
    "            'Actor Avg Coefficient': actor_avg_coeff,\n",
    "            'Judge Avg Coefficient': judge_avg_coeff\n",
    "        })\n",
    "    \n",
    "    # Create and display the accuracy summary\n",
    "    if model_accuracy:\n",
    "        accuracy_df = pd.DataFrame(model_accuracy)\n",
    "        accuracy_df = accuracy_df.sort_values('Judge MAPE')\n",
    "        \n",
    "        # Format the dataframe for better display\n",
    "        for col in ['Actor MAPE', 'Judge MAPE', 'MAPE Improvement']:\n",
    "            accuracy_df[col] = accuracy_df[col].round(2)\n",
    "            \n",
    "        for col in ['Actor Avg R²', 'Judge Avg R²', 'R² Improvement']:\n",
    "            accuracy_df[col] = accuracy_df[col].round(4)\n",
    "            \n",
    "        for col in ['Actor Avg Coefficient', 'Judge Avg Coefficient']:\n",
    "            accuracy_df[col] = accuracy_df[col].round(4)\n",
    "            \n",
    "        print(f\"Showing first 5 of {len(accuracy_df)} rows:\")\n",
    "        display(accuracy_df.head())\n",
    "        \n",
    "        # Identify the best models based on different metrics\n",
    "        best_judge_mape = accuracy_df.loc[accuracy_df['Judge MAPE'].idxmin()]\n",
    "        best_judge_r2 = accuracy_df.loc[accuracy_df['Judge Avg R²'].idxmax()]\n",
    "        most_improved_mape = accuracy_df.loc[accuracy_df['MAPE Improvement'].idxmax()]\n",
    "        most_improved_r2 = accuracy_df.loc[accuracy_df['R² Improvement'].idxmax()]\n",
    "        \n",
    "        print(\"Best Performing Models (Linear Relationship):\")\n",
    "        print(f\"• Most accurate predictions (lowest error): {best_judge_mape['Model']} (MAPE: {best_judge_mape['Judge MAPE']}%)\")\n",
    "        print(f\"• Strongest linear fit (highest R²): {best_judge_r2['Model']} (R²: {best_judge_r2['Judge Avg R²']})\")\n",
    "        print(f\"• Most improved accuracy through judge evaluation: {most_improved_mape['Model']} (MAPE reduced by {most_improved_mape['MAPE Improvement']}%)\")\n",
    "        print(f\"• Most improved predictive power: {most_improved_r2['Model']} (R² improved by {most_improved_r2['R² Improvement']})\")\n",
    "        \n",
    "        # Calculate metric-specific accuracy by model\n",
    "        print(\"Linear Prediction Coefficients by Model and Metric:\")\n",
    "        \n",
    "        coef_data = []\n",
    "        for model in models:\n",
    "            for metric in code_metrics:\n",
    "                model_metric_data = performance_df[(performance_df['Model'] == model) & \n",
    "                                                (performance_df['Code Metric'] == metric)]\n",
    "                \n",
    "                if len(model_metric_data) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # Get the coefficients for this model and metric\n",
    "                actor_coef = model_metric_data['Actor Coefficient'].iloc[0] if not model_metric_data.empty else np.nan\n",
    "                actor_intercept = model_metric_data['Actor Intercept'].iloc[0] if not model_metric_data.empty else np.nan\n",
    "                judge_coef = model_metric_data['Judge Coefficient'].iloc[0] if not model_metric_data.empty else np.nan\n",
    "                judge_intercept = model_metric_data['Judge Intercept'].iloc[0] if not model_metric_data.empty else np.nan\n",
    "                \n",
    "                # Calculate error metrics\n",
    "                actor_mape = model_metric_data['Actor % Error'].mean()\n",
    "                judge_mape = model_metric_data['Judge % Error'].mean()\n",
    "                \n",
    "                coef_data.append({\n",
    "                    'Model': model,\n",
    "                    'Code Metric': metric,\n",
    "                    'Actor Coefficient': actor_coef,\n",
    "                    'Actor Intercept': actor_intercept,\n",
    "                    'Actor Formula': f\"{metric} = {actor_coef:.2f} × SIFP + {actor_intercept:.2f}\",\n",
    "                    'Actor MAPE': actor_mape,\n",
    "                    'Judge Coefficient': judge_coef,\n",
    "                    'Judge Intercept': judge_intercept,\n",
    "                    'Judge Formula': f\"{metric} = {judge_coef:.2f} × SIFP + {judge_intercept:.2f}\",\n",
    "                    'Judge MAPE': judge_mape\n",
    "                })\n",
    "        \n",
    "        if coef_data:\n",
    "            coef_df = pd.DataFrame(coef_data)\n",
    "            \n",
    "            # Format the dataframe for better display\n",
    "            for col in ['Actor Coefficient', 'Actor Intercept', 'Judge Coefficient', 'Judge Intercept', 'Actor MAPE', 'Judge MAPE']:\n",
    "                if col in coef_df.columns:\n",
    "                    coef_df[col] = coef_df[col].round(2)\n",
    "            \n",
    "            # Display summary by metric\n",
    "            for metric in code_metrics:\n",
    "                metric_coef = coef_df[coef_df['Code Metric'] == metric]\n",
    "                if not metric_coef.empty:\n",
    "                    print(f\"{metric} Linear Coefficients:\")\n",
    "                    metric_coef = metric_coef.sort_values('Judge MAPE')\n",
    "                    display(metric_coef[['Model', 'Judge Formula', 'Judge MAPE', 'Actor Formula', 'Actor MAPE']].head())\n",
    "        \n",
    "        # Get visualization setting from environment\n",
    "        SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "        \n",
    "        if SHOW_VISUALIZATION:\n",
    "            # Create bar charts for model comparison\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Set up the bar positions\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.35\n",
    "            \n",
    "            # Extract data\n",
    "            actor_mape = [accuracy_df[accuracy_df['Model'] == m]['Actor MAPE'].values[0] \n",
    "                         if m in accuracy_df['Model'].values else np.nan for m in models]\n",
    "            \n",
    "            judge_mape = [accuracy_df[accuracy_df['Model'] == m]['Judge MAPE'].values[0] \n",
    "                         if m in accuracy_df['Model'].values else np.nan for m in models]\n",
    "            \n",
    "            # Create the grouped bars for MAPE\n",
    "            plt.bar(x - width/2, actor_mape, width, label='Actor MAPE', color='skyblue')\n",
    "            plt.bar(x + width/2, judge_mape, width, label='Judge MAPE', color='lightcoral')\n",
    "            \n",
    "            plt.title('Mean Absolute Percentage Error by Model (Linear Models)')\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('MAPE (lower is better)')\n",
    "            plt.xticks(x, models, rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "            \n",
    "            # Bar chart for average coefficients\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Extract coefficient data\n",
    "            actor_coef = [accuracy_df[accuracy_df['Model'] == m]['Actor Avg Coefficient'].values[0] \n",
    "                         if m in accuracy_df['Model'].values else np.nan for m in models]\n",
    "            \n",
    "            judge_coef = [accuracy_df[accuracy_df['Model'] == m]['Judge Avg Coefficient'].values[0] \n",
    "                         if m in accuracy_df['Model'].values else np.nan for m in models]\n",
    "            \n",
    "            # Create the grouped bars for coefficients\n",
    "            plt.bar(x - width/2, actor_coef, width, label='Actor Coefficient', color='skyblue')\n",
    "            plt.bar(x + width/2, judge_coef, width, label='Judge Coefficient', color='lightcoral')\n",
    "            \n",
    "            plt.title('Average Linear Coefficients by Model (SIFP to Code Metric)')\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('Coefficient (SIFP to Code Metric)')\n",
    "            plt.xticks(x, models, rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"(Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.)\")\n",
    "    else:\n",
    "        print(\"Insufficient data for accuracy summary\")\n",
    "        \n",
    "    # Summarize key findings and patterns\n",
    "    print(\"Key Observations on Linear Model Predictions:\")\n",
    "    print(\"1. This analysis models the relationship between SIFP and code metrics as a linear equation: Code_Metric = Coefficient × SIFP + Intercept\")\n",
    "    print(\"2. The coefficients represent the scaling factor between SIFP points and actual code metrics.\")\n",
    "    print(\"3. Higher R² values indicate that the model's SIFP points consistently predict code metrics with a linear relationship.\")\n",
    "    \n",
    "    if 'accuracy_df' in locals() and not accuracy_df.empty:\n",
    "        avg_improvement = accuracy_df['MAPE Improvement'].mean()\n",
    "        if avg_improvement > 0:\n",
    "            print(f\"4. On average, the judge evaluation process reduced prediction error by {avg_improvement:.2f}% across all models.\")\n",
    "        else:\n",
    "            print(f\"4. On average, the judge evaluation process increased prediction error by {-avg_improvement:.2f}% across all models.\")\n",
    "            \n",
    "        avg_r2_improvement = accuracy_df['R² Improvement'].mean()\n",
    "        if avg_r2_improvement > 0:\n",
    "            print(f\"5. The judge process improved linear fit quality (R²) by an average of {avg_r2_improvement:.4f} across all models.\")\n",
    "        else:\n",
    "            print(f\"5. The judge process decreased linear fit quality (R²) by an average of {-avg_r2_improvement:.4f} across all models.\")\n",
    "            \n",
    "        avg_actor_coef = accuracy_df['Actor Avg Coefficient'].mean()\n",
    "        avg_judge_coef = accuracy_df['Judge Avg Coefficient'].mean()\n",
    "        print(f\"6. The average scaling factor from SIFP to code metrics is {avg_actor_coef:.2f} for Actor and {avg_judge_coef:.2f} for Judge evaluations.\")\n",
    "else:\n",
    "    print(\"No performance data available for accuracy summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - Advanced Modeling Comparison (Linear vs. Non-Linear Methods)\n",
    "# Purpose: Compare linear and non-linear modeling methods for SIFP to code metric prediction using multiple algorithms\n",
    "# Dependencies: pandas, numpy, sklearn, matplotlib, seaborn, xgboost\n",
    "# Breadcrumbs: Analysis -> Advanced Modeling\n",
    "\n",
    "# Check if we have the dataframe from previous cells\n",
    "if 'file_analysis_df' not in globals() or file_analysis_df.empty:\n",
    "    print(\"ERROR: No file-level analysis data available from previous cells.\")\n",
    "else:\n",
    "    print(\"Advanced Modeling Comparison (Linear vs. Non-Linear):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total file-level entries: {len(file_analysis_df)}\")\n",
    "    print(f\"Unique code files: {file_analysis_df['File'].nunique()}\")\n",
    "    print(f\"Unique models: {file_analysis_df['sifp_model'].nunique()}\")\n",
    "    \n",
    "    # Define code metrics to analyze\n",
    "    code_metrics = ['Code', 'Lines', 'Lines-exe', 'Lines-dec', 'Units']\n",
    "    code_metrics = [m for m in code_metrics if m in file_analysis_df.columns]\n",
    "    print(f\"Available code metrics: {code_metrics}\")\n",
    "    \n",
    "    # Define SIFP models\n",
    "    sifp_models = file_analysis_df['sifp_model'].unique()\n",
    "    print(f\"Available SIFP models: {sifp_models}\")\n",
    "    \n",
    "    # Define modeling methods to compare\n",
    "    modeling_methods = {\n",
    "        'Linear': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1),\n",
    "        'XGBoost': XGBRegressor.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "        'MLP': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Create storage for results\n",
    "    results = []\n",
    "    \n",
    "    # Define a function to evaluate a model\n",
    "    def evaluate_model(X_train, X_test, y_train, y_test, model_name, model, sifp_type):\n",
    "        try:\n",
    "            # Standardize input features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Handle r2 NaN or infinity\n",
    "            if np.isnan(r2) or np.isinf(r2):\n",
    "                r2 = 0\n",
    "                \n",
    "            # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "            if np.all(y_test != 0):  # Avoid division by zero\n",
    "                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "            else:\n",
    "                mape = np.nan\n",
    "                \n",
    "            return {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2,\n",
    "                'MAPE': mape,\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred,\n",
    "                'model': model\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name} model: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # For each SIFP model, code metric, and sifp type (Actor/Judge)\n",
    "    for sifp_model in sifp_models:\n",
    "        print(f\"Analyzing SIFP Model: {sifp_model}\")\n",
    "        \n",
    "        # Filter data for this SIFP model\n",
    "        model_data = file_analysis_df[file_analysis_df['sifp_model'] == sifp_model].copy()\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(model_data) < 10:\n",
    "            print(f\"  Skipping {sifp_model} - insufficient data points ({len(model_data)} records)\")\n",
    "            continue\n",
    "            \n",
    "        # For each code metric\n",
    "        for metric in code_metrics:\n",
    "            if metric not in model_data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Create valid dataset (no NaN values)\n",
    "            valid_data = model_data.dropna(subset=[metric, 'sifp_actor_total', 'sifp_final_total'])\n",
    "            \n",
    "            if len(valid_data) < 10:\n",
    "                print(f\"  Skipping {sifp_model} → {metric} - insufficient valid data points ({len(valid_data)})\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  Processing {sifp_model} → {metric} with {len(valid_data)} data points\")\n",
    "            \n",
    "            # For each SIFP type (Actor/Judge)\n",
    "            for sifp_type, sifp_col in [('Actor', 'sifp_actor_total'), ('Judge', 'sifp_final_total')]:\n",
    "                # Prepare feature array (add more features if available)\n",
    "                features = [sifp_col]\n",
    "                \n",
    "                # Check if confidence scores are available and add them\n",
    "                confidence_cols = [col for col in valid_data.columns if 'confidence' in col.lower()]\n",
    "                available_conf_cols = [col for col in confidence_cols if not valid_data[col].isna().all()]\n",
    "                \n",
    "                if available_conf_cols:\n",
    "                    features.extend(available_conf_cols)\n",
    "                \n",
    "                # Create X and y\n",
    "                X = valid_data[features].values\n",
    "                y = valid_data[metric].values\n",
    "                \n",
    "                # Skip if not enough samples for meaningful split\n",
    "                if len(X) < 10:\n",
    "                    print(f\"    Skipping {sifp_type} analysis - insufficient samples ({len(X)})\")\n",
    "                    continue\n",
    "                \n",
    "                # Create train-test split (70-30)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=42\n",
    "                )\n",
    "                \n",
    "                # Evaluate each modeling method\n",
    "                for method_name, model in modeling_methods.items():\n",
    "                    # Skip models that require more data points than we have\n",
    "                    if (method_name in ['SVR', 'XGBoost', 'MLP', 'RandomForest'] and len(X_train) < 5) or len(X_train) < 3:\n",
    "                        continue\n",
    "                        \n",
    "                    evaluation = evaluate_model(X_train, X_test, y_train, y_test, method_name, model, sifp_type)\n",
    "                    \n",
    "                    if evaluation:\n",
    "                        results.append({\n",
    "                            'SIFP Model': sifp_model,\n",
    "                            'Code Metric': metric,\n",
    "                            'SIFP Type': sifp_type,\n",
    "                            'Method': method_name,\n",
    "                            'MAE': evaluation['MAE'],\n",
    "                            'RMSE': evaluation['RMSE'],\n",
    "                            'R2': evaluation['R2'],\n",
    "                            'MAPE': evaluation['MAPE'],\n",
    "                            'y_test': evaluation['y_test'],\n",
    "                            'y_pred': evaluation['y_pred'],\n",
    "                            'Features': features,\n",
    "                            'Trained Model': evaluation['model']\n",
    "                        })\n",
    "    \n",
    "    # Create dataframe from results\n",
    "    results_df = pd.DataFrame([{k: v for k, v in r.items() if k not in ['y_test', 'y_pred', 'Trained Model', 'Features']} \n",
    "                              for r in results])\n",
    "    \n",
    "    # Display results\n",
    "    if not results_df.empty:\n",
    "        print(\"Modeling Results Summary:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Format for better display\n",
    "        display_cols = ['SIFP Model', 'Code Metric', 'SIFP Type', 'Method', 'MAE', 'RMSE', 'R2', 'MAPE']\n",
    "        display_df = results_df[display_cols].copy()\n",
    "        \n",
    "        # Round numeric columns\n",
    "        numeric_cols = ['MAE', 'RMSE', 'R2', 'MAPE']\n",
    "        for col in numeric_cols:\n",
    "            display_df[col] = display_df[col].round(4)\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"Total model evaluations: {len(display_df)}\")\n",
    "        display(display_df.head())\n",
    "        \n",
    "        # Find best method for each SIFP model and metric combination\n",
    "        print(\"Best Method for Each SIFP Model and Metric (Based on R²):\")\n",
    "        \n",
    "        # Group by relevant columns and find the max R2 score\n",
    "        best_methods = display_df.loc[display_df.groupby(['SIFP Model', 'Code Metric', 'SIFP Type'])['R2'].idxmax()]\n",
    "        best_methods = best_methods.sort_values(['SIFP Model', 'Code Metric', 'SIFP Type'])\n",
    "        \n",
    "        display(best_methods)\n",
    "        \n",
    "        # Calculate average performance by method\n",
    "        print(\"Average Performance by Method:\")\n",
    "        method_performance = display_df.groupby('Method')[numeric_cols].mean().round(4)\n",
    "        method_performance = method_performance.sort_values('R2', ascending=False)\n",
    "        display(method_performance)\n",
    "        \n",
    "        # Calculate which method is best most often\n",
    "        best_count = best_methods['Method'].value_counts()\n",
    "        print(\"Methods Count as Best Performer:\")\n",
    "        display(best_count)\n",
    "        \n",
    "        # Compare Actor vs Judge overall performance\n",
    "        actor_judge_perf = display_df.groupby('SIFP Type')[numeric_cols].mean().round(4)\n",
    "        print(\"Actor vs Judge Average Performance:\")\n",
    "        display(actor_judge_perf)\n",
    "        \n",
    "        # Get visualization setting from environment\n",
    "        SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "        \n",
    "        if SHOW_VISUALIZATION:\n",
    "            # Create visualizations\n",
    "            \n",
    "            # 1. Bar plot of average R² by method\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=method_performance.index, y=method_performance['R2'], palette='viridis')\n",
    "            plt.title('Average R² by Modeling Method')\n",
    "            plt.xlabel('Method')\n",
    "            plt.ylabel('Average R²')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 2. Heatmap of R² for each SIFP model and method combination\n",
    "            model_method_r2 = display_df.pivot_table(\n",
    "                index='SIFP Model', \n",
    "                columns=['Method', 'SIFP Type'], \n",
    "                values='R2', \n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            if not model_method_r2.empty and not model_method_r2.isna().all().all():\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                sns.heatmap(model_method_r2, annot=True, cmap='RdYlGn', linewidths=.5, fmt='.3f')\n",
    "                plt.title('R² by SIFP Model, Method, and SIFP Type')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # 3. Compare actual vs predicted for best methods\n",
    "            for index, row in best_methods.iterrows():\n",
    "                # Get the result with the selected best method\n",
    "                selected_result = None\n",
    "                for res in results:\n",
    "                    if (res['SIFP Model'] == row['SIFP Model'] and \n",
    "                        res['Code Metric'] == row['Code Metric'] and\n",
    "                        res['SIFP Type'] == row['SIFP Type'] and\n",
    "                        res['Method'] == row['Method']):\n",
    "                        selected_result = res\n",
    "                        break\n",
    "                \n",
    "                if selected_result:\n",
    "                    y_test = selected_result['y_test']\n",
    "                    y_pred = selected_result['y_pred']\n",
    "                    \n",
    "                    # Create scatterplot\n",
    "                    plt.figure(figsize=(8, 8))\n",
    "                    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "                    \n",
    "                    # Plot perfect prediction line\n",
    "                    min_val = min(min(y_test), min(y_pred))\n",
    "                    max_val = max(max(y_test), max(y_pred))\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                    \n",
    "                    # Add R² and RMSE annotation\n",
    "                    r2 = row['R2']\n",
    "                    rmse = row['RMSE']\n",
    "                    plt.annotate(f\"R² = {r2:.4f}\\nRMSE = {rmse:.2f}\", \n",
    "                                xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                                ha='left', va='top')\n",
    "                    \n",
    "                    plt.title(f\"{row['SIFP Model']} - {row['Code Metric']} - {row['SIFP Type']}\\nMethod: {row['Method']}\")\n",
    "                    plt.xlabel('Actual')\n",
    "                    plt.ylabel('Predicted')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            \n",
    "            # 4. Method performance by SIFP model\n",
    "            for metric in code_metrics:\n",
    "                metric_data = display_df[display_df['Code Metric'] == metric]\n",
    "                \n",
    "                if not metric_data.empty:\n",
    "                    pivot = metric_data.pivot_table(\n",
    "                        index='Method', \n",
    "                        columns=['SIFP Model', 'SIFP Type'], \n",
    "                        values='R2'\n",
    "                    )\n",
    "                    \n",
    "                    if not pivot.empty and not pivot.isna().all().all():\n",
    "                        plt.figure(figsize=(14, 8))\n",
    "                        sns.heatmap(pivot, annot=True, cmap='RdYlGn', linewidths=.5, fmt='.3f')\n",
    "                        plt.title(f'R² by Method, SIFP Model, and SIFP Type for {metric}')\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "        else:\n",
    "            print(\"Visualizations disabled. Set SHOW_VISUALIZATION=True in .env to enable.\")\n",
    "        \n",
    "        # Output conclusions\n",
    "        print(\"Key Findings:\")\n",
    "        if not method_performance.empty:\n",
    "            best_overall_method = method_performance.index[0]\n",
    "            print(f\"1. Overall Best Method: {best_overall_method} (Avg R²: {method_performance.loc[best_overall_method, 'R2']:.4f})\")\n",
    "        \n",
    "        if not best_count.empty:\n",
    "            most_frequent_best = best_count.index[0]\n",
    "            print(f\"2. Most Frequently Best Method: {most_frequent_best} (Best in {best_count.iloc[0]} cases)\")\n",
    "        \n",
    "        if not actor_judge_perf.empty:\n",
    "            better_sifp = \"Judge\" if actor_judge_perf.loc['Judge', 'R2'] > actor_judge_perf.loc['Actor', 'R2'] else \"Actor\"\n",
    "            print(f\"3. Better SIFP Type: {better_sifp} (Avg R²: {actor_judge_perf.loc[better_sifp, 'R2']:.4f} vs. {actor_judge_perf.loc['Actor' if better_sifp == 'Judge' else 'Judge', 'R2']:.4f})\")\n",
    "        \n",
    "        # Check if non-linear methods outperform linear ones\n",
    "        linear_methods = ['Linear', 'Ridge', 'Lasso']\n",
    "        nonlinear_methods = ['RandomForest', 'SVR', 'XGBoost', 'MLP']\n",
    "        \n",
    "        linear_perf = display_df[display_df['Method'].isin(linear_methods)][numeric_cols].mean()\n",
    "        nonlinear_perf = display_df[display_df['Method'].isin(nonlinear_methods)][numeric_cols].mean()\n",
    "        \n",
    "        print(f\"4. Linear vs. Non-Linear Methods:\")\n",
    "        print(f\"   Linear Methods Average R²: {linear_perf['R2']:.4f}\")\n",
    "        print(f\"   Non-Linear Methods Average R²: {nonlinear_perf['R2']:.4f}\")\n",
    "        if nonlinear_perf['R2'] > linear_perf['R2']:\n",
    "            print(f\"   Non-Linear methods outperform Linear methods by {(nonlinear_perf['R2'] - linear_perf['R2']):.4f} R² points\")\n",
    "        else:\n",
    "            print(f\"   Linear methods outperform Non-Linear methods by {(linear_perf['R2'] - nonlinear_perf['R2']):.4f} R² points\")\n",
    "        \n",
    "        # Check which SIFP model performs best with which method\n",
    "        for method in modeling_methods.keys():\n",
    "            method_data = display_df[display_df['Method'] == method]\n",
    "            if not method_data.empty:\n",
    "                best_sifp_model = method_data.loc[method_data['R2'].idxmax()]\n",
    "                print(f\"5. {method} performs best with {best_sifp_model['SIFP Model']} on {best_sifp_model['Code Metric']} (R²: {best_sifp_model['R2']:.4f})\")\n",
    "    else:\n",
    "        print(\"No modeling results available. Check data quality and availability.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

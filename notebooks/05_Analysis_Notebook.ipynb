{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Requirements Traceability Analysis\n",
    "**Analysis and evaluation of LLM-based requirements traceability results with performance metrics, ROC curves, confusion matrices, and comparative evaluation across multiple models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Imports and Setup\n",
    "# Purpose: Import all required libraries and configure environment settings for LLM traceability analysis\n",
    "# Dependencies: pandas, numpy, matplotlib, seaborn, sklearn, neo4j, dotenv, re, collections, datetime, typing\n",
    "# Breadcrumbs: Setup -> Imports and Configuration\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_recall_fscore_support, accuracy_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "def setup_analysis_environment():\n",
    "    \"\"\"\n",
    "    Configure logging and load environment variables for analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters for analysis\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Extract configuration from environment variables\n",
    "    model_env_var = os.getenv('CURRENT_MODEL')\n",
    "    current_model = os.getenv(model_env_var) if model_env_var else None\n",
    "    \n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        'MODEL_ENV_VAR': model_env_var,\n",
    "        'CURRENT_MODEL': current_model,\n",
    "        'SHOW_VISUALIZATION': os.getenv('SHOW_VISUALIZATION', 'True').lower() == 'true',\n",
    "        'MIN_TRACEABILITY_THRESHOLD': int(os.getenv('MIN_TRACEABILITY_THRESHOLD', '1'))\n",
    "    }\n",
    "    \n",
    "    # Log configuration\n",
    "    logger.info(f\"Analyzing project: {config['NEO4J_PROJECT_NAME']}\")\n",
    "    logger.info(f\"Using model: {config['CURRENT_MODEL']}\")\n",
    "    logger.info(f\"Show visualization: {config['SHOW_VISUALIZATION']}\")\n",
    "    logger.info(f\"Minimum traceability threshold: {config['MIN_TRACEABILITY_THRESHOLD']}\")\n",
    "    \n",
    "    return config, logger\n",
    "\n",
    "# Execute setup when cell is run\n",
    "CONFIG, logger = setup_analysis_environment()\n",
    "NEO4J_URI = CONFIG['NEO4J_URI']\n",
    "NEO4J_USER = CONFIG['NEO4J_USER']\n",
    "NEO4J_PASSWORD = CONFIG['NEO4J_PASSWORD']\n",
    "NEO4J_PROJECT_NAME = CONFIG['NEO4J_PROJECT_NAME']\n",
    "MODEL_ENV_VAR = CONFIG['MODEL_ENV_VAR']\n",
    "CURRENT_MODEL = CONFIG['CURRENT_MODEL']\n",
    "SHOW_VISUALIZATION = CONFIG['SHOW_VISUALIZATION']\n",
    "MIN_TRACEABILITY_THRESHOLD = CONFIG['MIN_TRACEABILITY_THRESHOLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Connection Setup\n",
    "# Purpose: Create connection to Neo4j database containing LLM traceability analysis results\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection\n",
    "\n",
    "def create_neo4j_driver():\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance for accessing traceability data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    neo4j.Driver\n",
    "        Connected Neo4j driver instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = create_neo4j_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Query Ground Truth Data\n",
    "# Purpose: Retrieve ground truth traceability links between requirements from Neo4j database\n",
    "# Dependencies: pandas, neo4j, logging, typing\n",
    "# Breadcrumbs: Data Acquisition -> Ground Truth Links\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def query_ground_truth(driver, project_name):\n",
    "    \"\"\"\n",
    "    Query ground truth trace links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    driver : neo4j.Driver\n",
    "        Neo4j database driver connection\n",
    "    project_name : str\n",
    "        Name of the project to query\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            p.description as project_description,\n",
    "            d.id as document_id,\n",
    "            source.id as source_id, \n",
    "            source.type as source_type,\n",
    "            target.id as target_id,\n",
    "            target.type as target_type,\n",
    "            true as is_ground_truth\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            # Execute query with project name parameter\n",
    "            ground_truth_results = session.run(\n",
    "                ground_truth_query, \n",
    "                project_name=project_name\n",
    "            ).data()\n",
    "            \n",
    "            if not ground_truth_results:\n",
    "                logger.warning(f\"No ground truth data found for project {project_name}\")\n",
    "                return pd.DataFrame(\n",
    "                    columns=['project_name', 'document_id', 'source_id', 'target_id', 'is_ground_truth']\n",
    "                )\n",
    "            \n",
    "            ground_truth_df = pd.DataFrame(ground_truth_results)\n",
    "            logger.info(f\"Retrieved {len(ground_truth_df)} ground truth links for project {project_name}\")\n",
    "            \n",
    "            # Log some statistics\n",
    "            if 'source_id' in ground_truth_df.columns:\n",
    "                unique_sources = ground_truth_df['source_id'].nunique()\n",
    "                logger.info(f\"Ground truth contains {unique_sources} unique source requirements\")\n",
    "            \n",
    "            if 'target_id' in ground_truth_df.columns:\n",
    "                unique_targets = ground_truth_df['target_id'].nunique()\n",
    "                logger.info(f\"Ground truth contains {unique_targets} unique target requirements\")\n",
    "                \n",
    "            return ground_truth_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying ground truth data: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        # Return empty DataFrame on error with consistent columns\n",
    "        return pd.DataFrame(\n",
    "            columns=['project_name', 'document_id', 'source_id', 'target_id', 'is_ground_truth']\n",
    "        )\n",
    "\n",
    "# Query ground truth data\n",
    "ground_truth_df = query_ground_truth(driver, NEO4J_PROJECT_NAME)\n",
    "\n",
    "# Display and analyze ground truth data\n",
    "print(f\"\\nGround Truth Data for {NEO4J_PROJECT_NAME}:\")\n",
    "print(\"=\" * 80)\n",
    "display(ground_truth_df[['project_name', 'source_id', 'target_id', 'is_ground_truth']].head(10))\n",
    "print(f\"Total ground truth links: {len(ground_truth_df)}\")\n",
    "\n",
    "# If we have results, display additional information\n",
    "if not ground_truth_df.empty:\n",
    "    # Display unique documents if available\n",
    "    if 'document_id' in ground_truth_df.columns:\n",
    "        print(\"\\nUnique documents with ground truth links:\")\n",
    "        for doc_id in ground_truth_df['document_id'].unique():\n",
    "            doc_links = ground_truth_df[ground_truth_df['document_id'] == doc_id]\n",
    "            print(f\"- Document {doc_id}: {len(doc_links)} links\")\n",
    "    \n",
    "    # Display source to target ratio\n",
    "    unique_sources = ground_truth_df['source_id'].nunique()\n",
    "    unique_targets = ground_truth_df['target_id'].nunique()\n",
    "    print(f\"\\nUnique source requirements: {unique_sources}\")\n",
    "    print(f\"Unique target requirements: {unique_targets}\")\n",
    "    print(f\"Average links per source: {len(ground_truth_df) / unique_sources:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo ground truth links found. Possible causes:\")\n",
    "    print(\"1. The project doesn't have any ground truth links defined\")\n",
    "    print(\"2. The GROUND_TRUTH relationship type doesn't exist\")\n",
    "    print(\"3. There was an error in the query (check logs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query LLM Results Data\n",
    "# Purpose: Retrieve LLM prediction results from Neo4j for different evaluation methods\n",
    "# Dependencies: pandas, neo4j, logging\n",
    "# Breadcrumbs: Data Acquisition -> LLM Results\n",
    "\n",
    "def query_llm_results(driver, project_name, model):\n",
    "    \"\"\"\n",
    "    Query different types of LLM results from Neo4j for comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    driver : neo4j.Driver\n",
    "        Neo4j database driver connection\n",
    "    project_name : str\n",
    "        Name of the project to query\n",
    "    model : str\n",
    "        Model name to filter results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pd.DataFrame\n",
    "        (meta_judge_df, transformers_df, llm_result_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query for meta-judge links\n",
    "        meta_judge_query = \"\"\"\n",
    "        MATCH (p:Project)-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_META_JUDGE]->(target:Requirement)\n",
    "        WHERE p.name = $project_name AND source.type = 'SOURCE' AND r.model = $model\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_traceable as is_traceable_meta,\n",
    "            r.judge_score as judge_score_meta,\n",
    "            r.semantic_alignment as semantic_alignment_meta,\n",
    "            r.non_functional_coverage as non_functional_coverage_meta,\n",
    "            r.final_score as final_score_meta,\n",
    "            r.actor_score as actor_score_meta,\n",
    "            r.functional_completeness as functional_completeness_meta,\n",
    "            r.model as model,\n",
    "            'meta_judge' as result_type\n",
    "        \"\"\"\n",
    "        \n",
    "        # Query for transformer results\n",
    "        transformers_query = \"\"\"\n",
    "        MATCH (p:Project)-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_WITH_TRANSFORMERS]->(target:Requirement)\n",
    "        WHERE p.name = $project_name AND source.type = 'SOURCE' AND r.model = $model\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_associated as is_associated,\n",
    "            r.association_probability as association_probability,\n",
    "            r.explanation as explanation,\n",
    "            r.transformers_utilized as transformers_utilized,\n",
    "            r.model as model,\n",
    "            'transformer' as result_type\n",
    "        \"\"\"\n",
    "        \n",
    "        # Query for basic LLM results\n",
    "        llm_query = \"\"\"\n",
    "        MATCH (p:Project)-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT]->(target:Requirement)\n",
    "        WHERE p.name = $project_name AND source.type = 'SOURCE' AND r.model = $model\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_associated as is_associated,\n",
    "            r.association_probability as association_probability,\n",
    "            r.explanation as explanation,\n",
    "            r.model as model,\n",
    "            'llm' as result_type\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            # Execute queries with parameters\n",
    "            meta_judge_results = session.run(\n",
    "                meta_judge_query, \n",
    "                project_name=project_name, \n",
    "                model=model\n",
    "            ).data()\n",
    "            \n",
    "            transformers_results = session.run(\n",
    "                transformers_query, \n",
    "                project_name=project_name, \n",
    "                model=model\n",
    "            ).data()\n",
    "            \n",
    "            llm_results = session.run(\n",
    "                llm_query, \n",
    "                project_name=project_name, \n",
    "                model=model\n",
    "            ).data()\n",
    "            \n",
    "            # Create DataFrames\n",
    "            meta_judge_df = pd.DataFrame(meta_judge_results) if meta_judge_results else pd.DataFrame()\n",
    "            transformers_df = pd.DataFrame(transformers_results) if transformers_results else pd.DataFrame()\n",
    "            llm_result_df = pd.DataFrame(llm_results) if llm_results else pd.DataFrame()\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(meta_judge_df)} meta-judge links for {project_name} with model {model}\")\n",
    "            logger.info(f\"Retrieved {len(transformers_df)} transformer results for {project_name} with model {model}\")\n",
    "            logger.info(f\"Retrieved {len(llm_result_df)} basic LLM results for {project_name} with model {model}\")\n",
    "            \n",
    "            return meta_judge_df, transformers_df, llm_result_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying LLM results: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Query LLM results\n",
    "meta_judge_df, transformers_df, llm_result_df = query_llm_results(driver, NEO4J_PROJECT_NAME, CURRENT_MODEL)\n",
    "\n",
    "# Display sample results for each type\n",
    "print(f\"\\nMeta Judge Results for {NEO4J_PROJECT_NAME} using {CURRENT_MODEL}:\")\n",
    "print(\"=\" * 80)\n",
    "if not meta_judge_df.empty:\n",
    "    display(meta_judge_df.head())\n",
    "    print(f\"Total meta judge links: {len(meta_judge_df)}\")\n",
    "else:\n",
    "    print(\"No meta judge results found\")\n",
    "\n",
    "print(f\"\\nTransformer Results for {NEO4J_PROJECT_NAME} using {CURRENT_MODEL}:\")\n",
    "print(\"=\" * 80)\n",
    "if not transformers_df.empty:\n",
    "    display(transformers_df.head())\n",
    "    print(f\"Total transformer results: {len(transformers_df)}\")\n",
    "else:\n",
    "    print(\"No transformer results found\")\n",
    "\n",
    "print(f\"\\nBasic LLM Results for {NEO4J_PROJECT_NAME} using {CURRENT_MODEL}:\")\n",
    "print(\"=\" * 80)\n",
    "if not llm_result_df.empty:\n",
    "    display(llm_result_df.head())\n",
    "    print(f\"Total basic LLM results: {len(llm_result_df)}\")\n",
    "else:\n",
    "    print(\"No basic LLM results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Prepare Data for Evaluation\n",
    "# Purpose: Prepare and align LLM results with ground truth data for performance evaluation\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Data Processing -> Evaluation Preparation\n",
    "\n",
    "def prepare_evaluation_data(ground_truth_df, result_df, threshold=0.5, mode='is_traceable'):\n",
    "    \"\"\"\n",
    "    Prepare data for evaluation by comparing results with ground truth\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        DataFrame containing ground truth links\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame containing LLM result links\n",
    "    threshold : float or int, optional\n",
    "        Threshold for considering a link as positive (default: 0.5)\n",
    "    mode : str\n",
    "        Which field to use for evaluation: 'is_traceable', 'actor_score', or 'final_score'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if ground_truth_df.empty or result_df.empty:\n",
    "            logger.warning(\"Ground truth or result DataFrame is empty\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create a mapping of source_id to a set of target_ids from ground truth\n",
    "        ground_truth_map = {}\n",
    "        for _, row in ground_truth_df.iterrows():\n",
    "            if row['source_id'] not in ground_truth_map:\n",
    "                ground_truth_map[row['source_id']] = set()\n",
    "            ground_truth_map[row['source_id']].add(row['target_id'])\n",
    "        \n",
    "        # List to store evaluation results\n",
    "        eval_results = []\n",
    "        \n",
    "        # Determine which fields to use based on mode\n",
    "        if mode == 'is_traceable':\n",
    "            is_traceable_col = 'is_traceable_meta'\n",
    "        elif mode == 'actor_score':\n",
    "            score_col = 'actor_score_meta'\n",
    "        elif mode == 'final_score':\n",
    "            score_col = 'final_score_meta'\n",
    "        \n",
    "        # Process each row in the result DataFrame\n",
    "        for _, row in result_df.iterrows():\n",
    "            source_id = row['source_id']\n",
    "            target_id = row['target_id']\n",
    "            \n",
    "            # Determine if this is a ground truth link\n",
    "            is_ground_truth = False\n",
    "            if source_id in ground_truth_map and target_id in ground_truth_map[source_id]:\n",
    "                is_ground_truth = True\n",
    "            \n",
    "            # Determine if this is a predicted link based on the mode\n",
    "            is_predicted = False\n",
    "            if mode == 'is_traceable':\n",
    "                if is_traceable_col in row and row[is_traceable_col] is not None:\n",
    "                    if isinstance(row[is_traceable_col], bool):\n",
    "                        is_predicted = row[is_traceable_col]\n",
    "                    elif isinstance(row[is_traceable_col], str):\n",
    "                        is_predicted = row[is_traceable_col].lower() == 'true'\n",
    "                    else:\n",
    "                        try:\n",
    "                            is_predicted = bool(row[is_traceable_col])\n",
    "                        except:\n",
    "                            logger.warning(f\"Couldn't convert {is_traceable_col} value to boolean\")\n",
    "                            continue\n",
    "            else:  # actor_score or final_score\n",
    "                if score_col in row and row[score_col] is not None:\n",
    "                    try:\n",
    "                        score_value = int(row[score_col])\n",
    "                        is_predicted = score_value >= MIN_TRACEABILITY_THRESHOLD\n",
    "                    except:\n",
    "                        logger.warning(f\"Couldn't convert {score_col} value to int\")\n",
    "                        continue\n",
    "            \n",
    "            # Create evaluation entry\n",
    "            eval_entry = {\n",
    "                'source_id': source_id,\n",
    "                'target_id': target_id,\n",
    "                'model': row.get('model', CURRENT_MODEL),\n",
    "                'is_ground_truth': is_ground_truth,\n",
    "                'is_predicted': is_predicted,\n",
    "                'result_type': row.get('result_type', 'unknown'),\n",
    "                'evaluation_mode': mode\n",
    "            }\n",
    "            \n",
    "            # Add score if available\n",
    "            if mode != 'is_traceable' and score_col in row and row[score_col] is not None:\n",
    "                try:\n",
    "                    eval_entry['score'] = int(row[score_col])\n",
    "                except:\n",
    "                    eval_entry['score'] = 0\n",
    "            \n",
    "            eval_results.append(eval_entry)\n",
    "        \n",
    "        return pd.DataFrame(eval_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing evaluation data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Prepare evaluation data for each result type and each evaluation mode\n",
    "eval_data = {}\n",
    "\n",
    "if not meta_judge_df.empty:\n",
    "    # Process all three evaluation modes\n",
    "    eval_modes = ['is_traceable', 'actor_score', 'final_score']\n",
    "    \n",
    "    for mode in eval_modes:\n",
    "        mode_key = f\"meta_judge_{mode}\"\n",
    "        eval_data[mode_key] = prepare_evaluation_data(ground_truth_df, meta_judge_df, mode=mode)\n",
    "        print(f\"\\nMeta Judge Evaluation Data ({mode}):\")\n",
    "        print(\"=\" * 80)\n",
    "        display(eval_data[mode_key].head())\n",
    "        print(f\"Total evaluation entries: {len(eval_data[mode_key])}\")\n",
    "\n",
    "if not transformers_df.empty:\n",
    "    eval_data['transformer'] = prepare_evaluation_data(ground_truth_df, transformers_df)\n",
    "    print(\"\\nTransformer Evaluation Data:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(eval_data['transformer'].head())\n",
    "    print(f\"Total evaluation entries: {len(eval_data['transformer'])}\")\n",
    "\n",
    "if not llm_result_df.empty:\n",
    "    eval_data['llm'] = prepare_evaluation_data(ground_truth_df, llm_result_df)\n",
    "    print(\"\\nBasic LLM Evaluation Data:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(eval_data['llm'].head())\n",
    "    print(f\"Total evaluation entries: {len(eval_data['llm'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Calculate Performance Metrics\n",
    "# Purpose: Calculate comprehensive performance metrics (precision, recall, F1, accuracy, MCC)\n",
    "# Dependencies: sklearn, numpy, pandas, logging\n",
    "# Breadcrumbs: Analysis -> Performance Metrics\n",
    "\n",
    "def calculate_metrics(eval_df):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for the evaluation data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_df : pd.DataFrame\n",
    "        DataFrame with evaluation results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if eval_df.empty:\n",
    "            logger.warning(\"Evaluation DataFrame is empty - cannot calculate metrics\")\n",
    "            return {}\n",
    "        \n",
    "        # Extract true and predicted labels\n",
    "        y_true = eval_df['is_ground_truth'].astype(bool).values\n",
    "        y_pred = eval_df['is_predicted'].astype(bool).values\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # Calculate precision, recall, f1-score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # False positive rate\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        # False negative rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        \n",
    "        # Calculate F2 score (weighs recall higher than precision)\n",
    "        beta = 2\n",
    "        f2 = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate Matthews correlation coefficient (MCC)\n",
    "        mcc_numerator = (tp * tn) - (fp * fn)\n",
    "        mcc_denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_numerator / mcc_denominator if mcc_denominator != 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'true_negatives': tn,\n",
    "            'false_negatives': fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'f2_score': f2,\n",
    "            'accuracy': accuracy,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'mcc': mcc\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Calculate metrics for each result type and evaluation mode\n",
    "metrics_results = {}\n",
    "\n",
    "for result_type, df in eval_data.items():\n",
    "    if not df.empty:\n",
    "        metrics = calculate_metrics(df)\n",
    "        metrics_results[result_type] = metrics\n",
    "        \n",
    "        # Extract mode for display\n",
    "        if result_type.startswith('meta_judge_'):\n",
    "            mode = result_type.split('_', 2)[2]\n",
    "            display_type = f\"META_JUDGE ({mode.upper()})\"\n",
    "        else:\n",
    "            display_type = result_type.upper()\n",
    "            \n",
    "        print(f\"\\nPerformance Metrics for {display_type}:\")\n",
    "        print(\"=\" * 80)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\" if isinstance(value, float) else f\"{metric.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Compare Metrics Across Result Types\n",
    "# Purpose: Compare performance metrics across different LLM evaluation methods and visualize results\n",
    "# Dependencies: pandas, matplotlib, seaborn\n",
    "# Breadcrumbs: Analysis -> Performance Comparison\n",
    "\n",
    "if len(metrics_results) > 0:\n",
    "    # Create comparison DataFrame\n",
    "    metrics_comparison = pd.DataFrame(metrics_results).T\n",
    "    \n",
    "    print(\"\\nMetrics Comparison Across Result Types:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(metrics_comparison)\n",
    "    \n",
    "    # Plot comparison bar chart for key metrics only if visualization is enabled\n",
    "    if SHOW_VISUALIZATION:\n",
    "        key_metrics = ['precision', 'recall', 'f1_score', 'accuracy']\n",
    "        \n",
    "        # Create meta-judge comparison plot\n",
    "        meta_judge_results = {k: v for k, v in metrics_results.items() if k.startswith('meta_judge_')}\n",
    "        if len(meta_judge_results) > 0:\n",
    "            meta_judge_df = pd.DataFrame(meta_judge_results).T\n",
    "            \n",
    "            # Rename the index for better display\n",
    "            meta_judge_df.index = meta_judge_df.index.str.replace('meta_judge_', '').str.upper()\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            meta_judge_df[key_metrics].plot(kind='bar')\n",
    "            plt.title(f'Meta-Judge Evaluation Method Comparison - {NEO4J_PROJECT_NAME} ({CURRENT_MODEL})')\n",
    "            plt.xlabel('Evaluation Method')\n",
    "            plt.ylabel('Score')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.legend(title='Metric')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create a heatmap for the confusion matrix data\n",
    "            confusion_metrics = ['true_positives', 'false_positives', 'true_negatives', 'false_negatives']\n",
    "            if all(metric in meta_judge_df.columns for metric in confusion_metrics):\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.heatmap(meta_judge_df[confusion_metrics], annot=True, fmt=\"g\", cmap=\"YlGnBu\")\n",
    "                plt.title(f'Confusion Matrix Metrics Comparison - {NEO4J_PROJECT_NAME}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # Plot all result types for comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics_comparison[key_metrics].plot(kind='bar')\n",
    "        plt.title(f'Performance Metrics Comparison - {NEO4J_PROJECT_NAME} ({CURRENT_MODEL})')\n",
    "        plt.xlabel('Result Type')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Metric')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.info(\"Visualization disabled - skipping performance metrics bar chart\")\n",
    "else:\n",
    "    logger.warning(\"No metrics results available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Compare Predictions Across Evaluation Methods\n",
    "# Purpose: Analyze and compare predictions between different evaluation methods to identify disagreements\n",
    "# Dependencies: pandas, numpy\n",
    "# Breadcrumbs: Analysis -> Method Comparison\n",
    "\n",
    "if len(eval_data) > 0:\n",
    "    # Check if we have meta-judge data with different evaluation modes\n",
    "    meta_judge_keys = [k for k in eval_data.keys() if k.startswith('meta_judge_')]\n",
    "    \n",
    "    if len(meta_judge_keys) > 0:\n",
    "        # Create a unified dataset comparing predictions across methods\n",
    "        comparison_data = []\n",
    "        \n",
    "        # Get a list of all unique source-target pairs\n",
    "        all_pairs = set()\n",
    "        for key in meta_judge_keys:\n",
    "            df = eval_data[key]\n",
    "            for _, row in df.iterrows():\n",
    "                all_pairs.add((row['source_id'], row['target_id']))\n",
    "        \n",
    "        # Create a comparison row for each source-target pair\n",
    "        for source_id, target_id in all_pairs:\n",
    "            comparison_row = {\n",
    "                'source_id': source_id,\n",
    "                'target_id': target_id\n",
    "            }\n",
    "            \n",
    "            # Find if this is a ground truth link\n",
    "            is_ground_truth = False\n",
    "            for key in meta_judge_keys:\n",
    "                df = eval_data[key]\n",
    "                matching_rows = df[(df['source_id'] == source_id) & (df['target_id'] == target_id)]\n",
    "                if not matching_rows.empty:\n",
    "                    is_ground_truth = matching_rows.iloc[0]['is_ground_truth']\n",
    "                    break\n",
    "            \n",
    "            comparison_row['is_ground_truth'] = is_ground_truth\n",
    "            \n",
    "            # Add prediction from each evaluation method\n",
    "            for key in meta_judge_keys:\n",
    "                df = eval_data[key]\n",
    "                matching_rows = df[(df['source_id'] == source_id) & (df['target_id'] == target_id)]\n",
    "                \n",
    "                method_name = key.split('_', 2)[2]  # Extract method name\n",
    "                if not matching_rows.empty:\n",
    "                    comparison_row[f'predicted_by_{method_name}'] = matching_rows.iloc[0]['is_predicted']\n",
    "                    \n",
    "                    # Add score if available\n",
    "                    if 'score' in matching_rows.iloc[0]:\n",
    "                        comparison_row[f'{method_name}_score'] = matching_rows.iloc[0]['score']\n",
    "                else:\n",
    "                    comparison_row[f'predicted_by_{method_name}'] = False\n",
    "                    \n",
    "            comparison_data.append(comparison_row)\n",
    "        \n",
    "        # Create DataFrame from comparison data\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Find disagreements between methods\n",
    "        prediction_cols = [col for col in comparison_df.columns if col.startswith('predicted_by_')]\n",
    "        \n",
    "        if len(prediction_cols) > 1:\n",
    "            # Check for any disagreement\n",
    "            comparison_df['has_disagreement'] = comparison_df.apply(\n",
    "                lambda row: len(set(row[prediction_cols])) > 1, \n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Count cases where different methods disagree\n",
    "            disagreements = comparison_df[comparison_df['has_disagreement']]\n",
    "            \n",
    "            # Display summary\n",
    "            print(\"\\nEvaluation Method Comparison:\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total source-target pairs: {len(comparison_df)}\")\n",
    "            print(f\"Pairs with disagreement between methods: {len(disagreements)} ({len(disagreements)/len(comparison_df)*100:.2f}%)\")\n",
    "            \n",
    "            # Calculate agreement with ground truth for each method\n",
    "            for method in [col.replace('predicted_by_', '') for col in prediction_cols]:\n",
    "                correct_predictions = comparison_df[\n",
    "                    comparison_df[f'predicted_by_{method}'] == comparison_df['is_ground_truth']\n",
    "                ]\n",
    "                agreement_rate = len(correct_predictions) / len(comparison_df) * 100\n",
    "                print(f\"Agreement with ground truth ({method}): {agreement_rate:.2f}%\")\n",
    "            \n",
    "            # Display disagreement examples\n",
    "            if len(disagreements) > 0:\n",
    "                print(\"\\nExamples of Disagreements:\")\n",
    "                print(\"-\" * 60)\n",
    "                sample_size = min(10, len(disagreements))\n",
    "                display(disagreements.sample(sample_size)[\n",
    "                    ['source_id', 'target_id', 'is_ground_truth'] + \n",
    "                    prediction_cols + \n",
    "                    [col for col in comparison_df.columns if col.endswith('_score')]\n",
    "                ])\n",
    "                \n",
    "                # Analyze disagreements with ground truth\n",
    "                print(\"\\nDisagreement Analysis with Ground Truth:\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                for method in [col.replace('predicted_by_', '') for col in prediction_cols]:\n",
    "                    # False positives (predicted as link but not in ground truth)\n",
    "                    false_positives = comparison_df[\n",
    "                        (comparison_df[f'predicted_by_{method}'] == True) & \n",
    "                        (comparison_df['is_ground_truth'] == False)\n",
    "                    ]\n",
    "                    \n",
    "                    # False negatives (not predicted as link but in ground truth)\n",
    "                    false_negatives = comparison_df[\n",
    "                        (comparison_df[f'predicted_by_{method}'] == False) & \n",
    "                        (comparison_df['is_ground_truth'] == True)\n",
    "                    ]\n",
    "                    \n",
    "                    print(f\"Method: {method}\")\n",
    "                    print(f\"  False positives: {len(false_positives)} ({len(false_positives)/len(comparison_df)*100:.2f}%)\")\n",
    "                    print(f\"  False negatives: {len(false_negatives)} ({len(false_negatives)/len(comparison_df)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(\"Only one evaluation method available - no comparison possible\")\n",
    "    else:\n",
    "        print(\"No meta-judge data with multiple evaluation methods found\")\n",
    "else:\n",
    "    logger.warning(\"No evaluation data available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Error Analysis\n",
    "# Purpose: Analyze false positives and false negatives to understand model prediction errors\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Analysis -> Error Analysis\n",
    "\n",
    "def perform_error_analysis(eval_df, result_type):\n",
    "    \"\"\"\n",
    "    Analyze false positives and false negatives\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_df : pd.DataFrame\n",
    "        DataFrame with evaluation results\n",
    "    result_type : str\n",
    "        Type of result being analyzed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if eval_df.empty:\n",
    "            logger.warning(f\"Evaluation DataFrame for {result_type} is empty\")\n",
    "            return\n",
    "        \n",
    "        # Identify false positives (predicted but not in ground truth)\n",
    "        false_positives = eval_df[(eval_df['is_predicted'] == True) & (eval_df['is_ground_truth'] == False)]\n",
    "        \n",
    "        # Identify false negatives (in ground truth but not predicted)\n",
    "        false_negatives = eval_df[(eval_df['is_predicted'] == False) & (eval_df['is_ground_truth'] == True)]\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nError Analysis for {result_type.upper()}:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total links evaluated: {len(eval_df)}\")\n",
    "        print(f\"False positives: {len(false_positives)} ({len(false_positives)/len(eval_df)*100:.2f}%)\")\n",
    "        print(f\"False negatives: {len(false_negatives)} ({len(false_negatives)/len(eval_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Show sample of false positives\n",
    "        if not false_positives.empty:\n",
    "            print(\"\\nSample False Positives (incorrectly predicted as links):\")\n",
    "            display(false_positives.head(5)[['source_id', 'target_id', 'probability'] if 'probability' in false_positives.columns else ['source_id', 'target_id']])\n",
    "        \n",
    "        # Show sample of false negatives\n",
    "        if not false_negatives.empty:\n",
    "            print(\"\\nSample False Negatives (missed ground truth links):\")\n",
    "            display(false_negatives.head(5)[['source_id', 'target_id', 'probability'] if 'probability' in false_negatives.columns else ['source_id', 'target_id']])\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error performing error analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Perform error analysis for each result type\n",
    "for result_type, df in eval_data.items():\n",
    "    if not df.empty:\n",
    "        perform_error_analysis(df, result_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Threshold Analysis\n",
    "# Purpose: Analyze how different probability thresholds affect performance for probability-based results\n",
    "# Dependencies: sklearn, numpy, matplotlib, pandas\n",
    "# Breadcrumbs: Analysis -> Threshold Sensitivity\n",
    "\n",
    "def analyze_threshold_sensitivity(eval_df, result_type):\n",
    "    \"\"\"\n",
    "    Analyze how different probability thresholds affect performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_df : pd.DataFrame\n",
    "        DataFrame with evaluation results\n",
    "    result_type : str\n",
    "        Type of result being analyzed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if eval_df.empty or 'probability' not in eval_df.columns:\n",
    "            logger.warning(f\"Cannot perform threshold analysis for {result_type} - missing probability column\")\n",
    "            return\n",
    "        \n",
    "        # Extract ground truth labels\n",
    "        y_true = eval_df['is_ground_truth'].astype(bool).values\n",
    "        \n",
    "        # Create threshold range\n",
    "        thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "        \n",
    "        # Store results\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Apply threshold to get predictions\n",
    "            y_pred = (eval_df['probability'] >= threshold).values\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        threshold_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\nThreshold Sensitivity Analysis for {result_type.upper()}:\")\n",
    "        print(\"=\" * 80)\n",
    "        display(threshold_df)\n",
    "        \n",
    "        # Plot threshold sensitivity only if visualization is enabled\n",
    "        if SHOW_VISUALIZATION:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.plot(threshold_df['threshold'], threshold_df['precision'], 'b-', label='Precision')\n",
    "            plt.plot(threshold_df['threshold'], threshold_df['recall'], 'r-', label='Recall')\n",
    "            plt.plot(threshold_df['threshold'], threshold_df['f1_score'], 'g-', label='F1 Score')\n",
    "            plt.plot(threshold_df['threshold'], threshold_df['accuracy'], 'k-', label='Accuracy')\n",
    "            \n",
    "            plt.title(f'Performance vs Threshold - {result_type.upper()} ({NEO4J_PROJECT_NAME})')\n",
    "            plt.xlabel('Probability Threshold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            logger.info(\"Visualization disabled - skipping threshold sensitivity plot\")\n",
    "        \n",
    "        # Find optimal threshold for F1 score\n",
    "        optimal_idx = threshold_df['f1_score'].idxmax()\n",
    "        optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\n",
    "        optimal_f1 = threshold_df.loc[optimal_idx, 'f1_score']\n",
    "        \n",
    "        print(f\"Optimal threshold for F1 score: {optimal_threshold:.2f} (F1 = {optimal_f1:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing threshold sensitivity: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Perform threshold analysis for each result type\n",
    "for result_type, df in eval_data.items():\n",
    "    if not df.empty and 'probability' in df.columns:\n",
    "        analyze_threshold_sensitivity(df, result_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Save Results\n",
    "# Purpose: Save analysis results to CSV files with timestamp for future reference\n",
    "# Dependencies: os, datetime, pandas, logging, typing\n",
    "# Breadcrumbs: Output -> Results Export\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def analyze_per_requirement(eval_df, result_type):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics per requirement\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_df : pd.DataFrame\n",
    "        DataFrame with evaluation results\n",
    "    result_type : str\n",
    "        Type of result being analyzed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with per-requirement metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if eval_df.empty:\n",
    "            logger.warning(f\"Evaluation DataFrame for {result_type} is empty\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Group by source_id (requirement) and calculate metrics\n",
    "        source_ids = eval_df['source_id'].unique()\n",
    "        per_req_results = []\n",
    "        \n",
    "        for source_id in source_ids:\n",
    "            req_df = eval_df[eval_df['source_id'] == source_id]\n",
    "            \n",
    "            # Extract true and predicted labels for this requirement\n",
    "            y_true = req_df['is_ground_truth'].astype(bool).values\n",
    "            y_pred = req_df['is_predicted'].astype(bool).values\n",
    "            \n",
    "            # Calculate basic counts\n",
    "            tp = sum((y_true == True) & (y_pred == True))\n",
    "            fp = sum((y_true == False) & (y_pred == True))\n",
    "            tn = sum((y_true == False) & (y_pred == False))\n",
    "            fn = sum((y_true == True) & (y_pred == False))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0\n",
    "            \n",
    "            # Create result dictionary\n",
    "            req_result = {\n",
    "                'source_id': source_id,\n",
    "                'result_type': result_type,\n",
    "                'true_positives': tp,\n",
    "                'false_positives': fp,\n",
    "                'true_negatives': tn,\n",
    "                'false_negatives': fn,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'accuracy': accuracy,\n",
    "                'total_links': len(req_df),\n",
    "                'ground_truth_links': sum(y_true),\n",
    "                'predicted_links': sum(y_pred)\n",
    "            }\n",
    "            \n",
    "            per_req_results.append(req_result)\n",
    "        \n",
    "        return pd.DataFrame(per_req_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in analyze_per_requirement: {str(e)}\")\n",
    "        logger.debug(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_results_to_csv(metrics_comparison, project_name, model):\n",
    "    \"\"\"\n",
    "    Save analysis results to CSV files with timestamp\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics_comparison : pd.DataFrame\n",
    "        DataFrame with metrics comparison\n",
    "    project_name : str\n",
    "        Name of the project analyzed\n",
    "    model : str\n",
    "        Model used for analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.join('output', 'analysis')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get current timestamp (to the minute)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        \n",
    "        # Save metrics comparison\n",
    "        if not metrics_comparison.empty:\n",
    "            filename = os.path.join(output_dir, f\"{project_name}_{model}_metrics_comparison_{timestamp}.csv\")\n",
    "            metrics_comparison.to_csv(filename)\n",
    "            logger.info(f\"Saved metrics comparison to {filename}\")\n",
    "        \n",
    "        # Save per-requirement analysis for each result type\n",
    "        for result_type, df in eval_data.items():\n",
    "            if not df.empty:\n",
    "                # Generate per-requirement metrics\n",
    "                per_req_metrics = analyze_per_requirement(df, result_type)\n",
    "                \n",
    "                if not per_req_metrics.empty:\n",
    "                    filename = os.path.join(output_dir, f\"{project_name}_{model}_{result_type}_per_requirement_{timestamp}.csv\")\n",
    "                    per_req_metrics.to_csv(filename)\n",
    "                    logger.info(f\"Saved {result_type} per-requirement analysis to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {str(e)}\")\n",
    "        logger.debug(\"Exception details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Save results if metrics comparison exists\n",
    "if 'metrics_comparison' in locals() and not metrics_comparison.empty:\n",
    "    save_results_to_csv(metrics_comparison, NEO4J_PROJECT_NAME, CURRENT_MODEL)\n",
    "    print(f\"\\nResults saved to output/analysis directory for {NEO4J_PROJECT_NAME} using {CURRENT_MODEL}\")\n",
    "    print(f\"Files include timestamp: {datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Confusion Matrix Visualization\n",
    "# Purpose: Visualize confusion matrices for evaluation results across different methods\n",
    "# Dependencies: matplotlib, seaborn, sklearn, numpy, pandas, logging\n",
    "# Breadcrumbs: Visualization -> Confusion Matrix\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def plot_confusion_matrices(eval_data):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for each result type with threshold information\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_data : dict\n",
    "        Dictionary containing evaluation DataFrames for each result type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not eval_data:\n",
    "            logger.warning(\"No evaluation data available for plotting confusion matrices\")\n",
    "            return\n",
    "            \n",
    "        # Skip visualization if disabled\n",
    "        if not SHOW_VISUALIZATION:\n",
    "            logger.info(\"Visualization disabled - skipping confusion matrices\")\n",
    "            \n",
    "            # Still print confusion matrix data as text\n",
    "            for result_type, df in eval_data.items():\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                    \n",
    "                # Extract true and predicted labels\n",
    "                y_true = df['is_ground_truth'].astype(bool).values\n",
    "                y_pred = df['is_predicted'].astype(bool).values\n",
    "                \n",
    "                # Calculate confusion matrix\n",
    "                cm = confusion_matrix(y_true, y_pred)\n",
    "                \n",
    "                print(f\"\\nConfusion Matrix for {result_type.upper()}:\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"Threshold: MIN_TRACEABILITY_THRESHOLD = {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "                print(f\"True Negatives: {cm[0][0]}\")\n",
    "                print(f\"False Positives: {cm[0][1]}\")\n",
    "                print(f\"False Negatives: {cm[1][0]}\")\n",
    "                print(f\"True Positives: {cm[1][1]}\")\n",
    "            \n",
    "            return\n",
    "            \n",
    "        # Determine number of result types to plot\n",
    "        num_types = len(eval_data)\n",
    "        \n",
    "        if num_types == 0:\n",
    "            return\n",
    "            \n",
    "        # Set up the figure with appropriate size\n",
    "        fig, axes = plt.subplots(1, num_types, figsize=(5*num_types, 5))\n",
    "        \n",
    "        # Ensure axes is always iterable, even with a single plot\n",
    "        if num_types == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Add a figure title with threshold information\n",
    "        fig.suptitle(f'Confusion Matrices (Threshold: MIN_TRACEABILITY_THRESHOLD = {MIN_TRACEABILITY_THRESHOLD})', \n",
    "                    fontsize=14, y=1.05)\n",
    "            \n",
    "        # Plot each confusion matrix\n",
    "        for idx, (result_type, df) in enumerate(eval_data.items()):\n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Extract true and predicted labels\n",
    "            y_true = df['is_ground_truth'].astype(bool).values\n",
    "            y_pred = df['is_predicted'].astype(bool).values\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "            # Prepare annotations with percentages\n",
    "            total = np.sum(cm)\n",
    "            annotations = np.empty_like(cm, dtype=object)\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    annotations[i, j] = f\"{cm[i, j]}\\n({cm[i, j]/total:.1%})\"\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', ax=axes[idx],\n",
    "                       xticklabels=['Not Link', 'Link'],\n",
    "                       yticklabels=['Not Link', 'Link'])\n",
    "            \n",
    "            # Get mode for title (convert meta_judge_final_score to \"FINAL SCORE\", etc.)\n",
    "            if '_' in result_type:\n",
    "                parts = result_type.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    mode_description = ' '.join(parts[2:]).upper()\n",
    "                else:\n",
    "                    mode_description = result_type.upper()\n",
    "            else:\n",
    "                mode_description = result_type.upper()\n",
    "                \n",
    "            # Add descriptive title\n",
    "            description = \"\"\n",
    "            if \"is_traceable\" in result_type.lower():\n",
    "                description = \"Binary Classification\"\n",
    "            elif \"actor_score\" in result_type.lower():\n",
    "                description = f\"Score ≥ {MIN_TRACEABILITY_THRESHOLD}\"\n",
    "            elif \"final_score\" in result_type.lower():\n",
    "                description = f\"Score ≥ {MIN_TRACEABILITY_THRESHOLD}\"\n",
    "            \n",
    "            # Add labels\n",
    "            axes[idx].set_title(f'{mode_description}\\n{description}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        # Add a legend explaining the threshold\n",
    "        threshold_description = None\n",
    "        if \"actor_score\" in eval_data or \"final_score\" in eval_data:\n",
    "            if MIN_TRACEABILITY_THRESHOLD == 1:\n",
    "                threshold_description = \"MIN_TRACEABILITY_THRESHOLD = 1 (Any score above zero considered a link)\"\n",
    "            elif MIN_TRACEABILITY_THRESHOLD == 2:\n",
    "                threshold_description = \"MIN_TRACEABILITY_THRESHOLD = 2 (Moderate confidence required)\"\n",
    "            elif MIN_TRACEABILITY_THRESHOLD == 3:\n",
    "                threshold_description = \"MIN_TRACEABILITY_THRESHOLD = 3 (High confidence required)\"\n",
    "            else:\n",
    "                threshold_description = f\"MIN_TRACEABILITY_THRESHOLD = {MIN_TRACEABILITY_THRESHOLD}\"\n",
    "        \n",
    "        if threshold_description:\n",
    "            plt.figtext(0.5, -0.05, threshold_description, ha='center', fontsize=10, \n",
    "                      bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.5))\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error plotting confusion matrices: {str(e)}\")\n",
    "        logger.debug(\"Exception details:\", exc_info=True)\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Link Prediction Distribution Analysis\n",
    "# Purpose: Analyze the distribution of link predictions and ground truth patterns\n",
    "# Dependencies: matplotlib, pandas, logging\n",
    "# Breadcrumbs: Analysis -> Distribution Analysis\n",
    "\n",
    "def analyze_prediction_distribution(eval_data):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of link predictions and ground truth\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_data : dict\n",
    "        Dictionary containing evaluation DataFrames for each result type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not eval_data:\n",
    "            logger.warning(\"No evaluation data available for prediction distribution analysis\")\n",
    "            return\n",
    "            \n",
    "        for result_type, df in eval_data.items():\n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nPrediction Distribution Analysis for {result_type.upper()}:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Count total predictions and ground truth\n",
    "            total_evaluated = len(df)\n",
    "            total_ground_truth = df['is_ground_truth'].sum()\n",
    "            total_predicted = df['is_predicted'].sum()\n",
    "            \n",
    "            # Calculate percentages\n",
    "            ground_truth_percentage = (total_ground_truth / total_evaluated) * 100\n",
    "            predicted_percentage = (total_predicted / total_evaluated) * 100\n",
    "            \n",
    "            print(f\"Total links evaluated: {total_evaluated}\")\n",
    "            print(f\"Ground truth links: {total_ground_truth} ({ground_truth_percentage:.2f}%)\")\n",
    "            print(f\"Predicted links: {total_predicted} ({predicted_percentage:.2f}%)\")\n",
    "            \n",
    "            # Calculate overlap\n",
    "            true_positives = ((df['is_ground_truth'] & df['is_predicted']).sum())\n",
    "            true_positive_percentage = (true_positives / total_ground_truth) * 100 if total_ground_truth > 0 else 0\n",
    "            \n",
    "            print(f\"Correctly predicted links (true positives): {true_positives} ({true_positive_percentage:.2f}% of ground truth)\")\n",
    "            \n",
    "            # Distribution of source requirements\n",
    "            unique_sources = df['source_id'].nunique()\n",
    "            unique_targets = df['target_id'].nunique()\n",
    "            \n",
    "            print(f\"Unique source requirements: {unique_sources}\")\n",
    "            print(f\"Unique target requirements: {unique_targets}\")\n",
    "            \n",
    "            # For probability-based results, plot histogram if visualization is enabled\n",
    "            if 'probability' in df.columns and SHOW_VISUALIZATION:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                # Separate probabilities for ground truth and non-ground truth links\n",
    "                gt_probs = df[df['is_ground_truth']]['probability']\n",
    "                non_gt_probs = df[~df['is_ground_truth']]['probability']\n",
    "                \n",
    "                plt.hist(gt_probs, bins=20, alpha=0.5, label='Ground Truth Links', color='green')\n",
    "                plt.hist(non_gt_probs, bins=20, alpha=0.5, label='Non-Ground Truth Links', color='red')\n",
    "                \n",
    "                plt.title(f'Probability Distribution - {result_type.upper()} ({NEO4J_PROJECT_NAME})')\n",
    "                plt.xlabel('Probability')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.show()\n",
    "            elif 'probability' in df.columns:\n",
    "                logger.info(\"Visualization disabled - skipping probability distribution histogram\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing prediction distribution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Analyze prediction distributions\n",
    "analyze_prediction_distribution(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - ROC Curve Analysis\n",
    "# Purpose: Generate ROC and precision-recall curves for score-based evaluation methods\n",
    "# Dependencies: sklearn, pandas, matplotlib, numpy, logging\n",
    "# Breadcrumbs: Visualization -> ROC Analysis\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def normalize_scores(scores):\n",
    "    \"\"\"\n",
    "    Normalize scores to 0-1 range to use as probability values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scores : numpy.ndarray\n",
    "        Array of scores to normalize\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Normalized scores in range 0-1\n",
    "    \"\"\"\n",
    "    if np.all(scores == scores[0]):  # All scores are the same\n",
    "        return np.zeros_like(scores)  # Return array of zeros to avoid NaN issues\n",
    "    \n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    \n",
    "    if min_score == max_score:  # Avoid division by zero\n",
    "        return np.ones_like(scores) if min_score > 0 else np.zeros_like(scores)\n",
    "        \n",
    "    return (scores - min_score) / (max_score - min_score)\n",
    "\n",
    "def perform_roc_analysis(eval_data):\n",
    "    \"\"\"\n",
    "    Perform ROC curve analysis for score-based results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_data : dict\n",
    "        Dictionary containing evaluation DataFrames for each result type\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a flag to track if we have any data suitable for ROC analysis\n",
    "        has_score_data = False\n",
    "        \n",
    "        # Check which result types have score data\n",
    "        score_data_types = []\n",
    "        for result_type, df in eval_data.items():\n",
    "            if not df.empty and ('score' in df.columns or 'probability' in df.columns):\n",
    "                has_score_data = True\n",
    "                score_data_types.append(result_type)\n",
    "                \n",
    "        if not has_score_data:\n",
    "            logger.warning(\"No score or probability data available for ROC analysis\")\n",
    "            print(\"\\nNo score or probability data available for ROC analysis.\")\n",
    "            print(\"ROC curves require continuous probability scores.\")\n",
    "            print(f\"MIN_TRACEABILITY_THRESHOLD is currently set to: {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "            print(\"To see ROC analysis, use actor_score or final_score evaluation modes which provide score data.\")\n",
    "            return\n",
    "        \n",
    "        # Log which result types have score data\n",
    "        logger.info(f\"Found {len(score_data_types)} result types with score data: {', '.join(score_data_types)}\")\n",
    "        \n",
    "        # Skip visualization if disabled but still calculate AUC scores\n",
    "        if not SHOW_VISUALIZATION:\n",
    "            logger.info(\"Visualization disabled - skipping ROC curve plot\")\n",
    "            print(\"\\nROC Analysis (AUC Scores):\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for result_type in score_data_types:\n",
    "                df = eval_data[result_type]\n",
    "                \n",
    "                # Use probability if available, otherwise use normalized scores\n",
    "                if 'probability' in df.columns:\n",
    "                    y_true = df['is_ground_truth'].astype(bool).values\n",
    "                    y_score = df['probability'].values\n",
    "                    score_source = 'probability'\n",
    "                elif 'score' in df.columns:\n",
    "                    y_true = df['is_ground_truth'].astype(bool).values\n",
    "                    y_score = normalize_scores(df['score'].values)\n",
    "                    score_source = 'normalized score'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate AUC\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_true, y_score)\n",
    "                    print(f\"{result_type.upper()} AUC Score: {auc_score:.4f} (using {score_source})\")\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"Could not calculate AUC for {result_type}: {str(ve)}\")\n",
    "                    print(f\"{result_type.upper()}: Unable to calculate AUC (possibly only one class present)\")\n",
    "                \n",
    "            return\n",
    "        \n",
    "        # If visualization is enabled, create ROC curves\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot ROC curve for each result type with score data\n",
    "        for result_type in score_data_types:\n",
    "            df = eval_data[result_type]\n",
    "            \n",
    "            # Skip if dataframe is empty\n",
    "            if df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Extract true labels\n",
    "            y_true = df['is_ground_truth'].astype(bool).values\n",
    "            \n",
    "            # Check if we have enough variety in the labels\n",
    "            if len(np.unique(y_true)) < 2:\n",
    "                logger.warning(f\"Not enough label variety for {result_type} to calculate ROC curve\")\n",
    "                continue\n",
    "            \n",
    "            # Use probability if available, otherwise use normalized scores\n",
    "            if 'probability' in df.columns:\n",
    "                y_score = df['probability'].values\n",
    "                score_source = 'probability'\n",
    "            elif 'score' in df.columns:\n",
    "                y_score = normalize_scores(df['score'].values)\n",
    "                score_source = 'normalized score'\n",
    "            else:\n",
    "                logger.warning(f\"No suitable score values found for {result_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate ROC curve\n",
    "            try:\n",
    "                fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                # Get the display name for the result type\n",
    "                display_name = result_type.upper() \n",
    "                if '_' in result_type:\n",
    "                    parts = result_type.split('_')\n",
    "                    if len(parts) >= 3:\n",
    "                        display_name = f\"{parts[0].upper()} {' '.join(parts[2:]).upper()}\"\n",
    "                \n",
    "                # Plot the ROC curve\n",
    "                plt.plot(fpr, tpr, lw=2, \n",
    "                        label=f'{display_name} (AUC = {roc_auc:.3f}, {score_source})')\n",
    "                \n",
    "                # Log thresholds at specific points\n",
    "                idx_5pct_fpr = next((i for i, x in enumerate(fpr) if x >= 0.05), None)\n",
    "                idx_10pct_fpr = next((i for i, x in enumerate(fpr) if x >= 0.10), None)\n",
    "                \n",
    "                if idx_5pct_fpr is not None:\n",
    "                    logger.info(f\"{result_type} - At 5% FPR: TPR={tpr[idx_5pct_fpr]:.3f}, threshold={thresholds[idx_5pct_fpr]:.3f}\")\n",
    "                \n",
    "                if idx_10pct_fpr is not None:\n",
    "                    logger.info(f\"{result_type} - At 10% FPR: TPR={tpr[idx_10pct_fpr]:.3f}, threshold={thresholds[idx_10pct_fpr]:.3f}\")\n",
    "                \n",
    "            except ValueError as ve:\n",
    "                logger.warning(f\"Error calculating ROC curve for {result_type}: {str(ve)}\")\n",
    "                continue\n",
    "        \n",
    "        # Add diagonal reference line\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        \n",
    "        # Add current threshold line\n",
    "        if 'meta_judge_actor_score' in eval_data or 'meta_judge_final_score' in eval_data:\n",
    "            plt.text(0.5, 0.1, f\"MIN_TRACEABILITY_THRESHOLD = {MIN_TRACEABILITY_THRESHOLD}\", \n",
    "                    horizontalalignment='center', \n",
    "                    bbox=dict(facecolor='yellow', alpha=0.3),\n",
    "                    fontsize=10)\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves - {NEO4J_PROJECT_NAME} ({CURRENT_MODEL})')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add a descriptive note about the threshold\n",
    "        threshold_description = \"\"\n",
    "        if MIN_TRACEABILITY_THRESHOLD == 1:\n",
    "            threshold_description = \"Current Threshold: MIN_TRACEABILITY_THRESHOLD = 1 (Low confidence required)\"\n",
    "        elif MIN_TRACEABILITY_THRESHOLD == 2:\n",
    "            threshold_description = \"Current Threshold: MIN_TRACEABILITY_THRESHOLD = 2 (Moderate confidence required)\"\n",
    "        elif MIN_TRACEABILITY_THRESHOLD == 3:\n",
    "            threshold_description = \"Current Threshold: MIN_TRACEABILITY_THRESHOLD = 3 (High confidence required)\"\n",
    "        else:\n",
    "            threshold_description = f\"Current Threshold: MIN_TRACEABILITY_THRESHOLD = {MIN_TRACEABILITY_THRESHOLD}\"\n",
    "        \n",
    "        plt.figtext(0.5, 0.01, threshold_description, ha='center', fontsize=10, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the text\n",
    "        plt.show()\n",
    "        \n",
    "        # Also generate precision-recall curves which often work better for imbalanced datasets\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for result_type in score_data_types:\n",
    "            df = eval_data[result_type]\n",
    "            \n",
    "            # Skip if dataframe is empty\n",
    "            if df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Extract true labels\n",
    "            y_true = df['is_ground_truth'].astype(bool).values\n",
    "            \n",
    "            # Check if we have enough variety in the labels\n",
    "            if len(np.unique(y_true)) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Use probability if available, otherwise use normalized scores\n",
    "            if 'probability' in df.columns:\n",
    "                y_score = df['probability'].values\n",
    "                score_source = 'probability'\n",
    "            elif 'score' in df.columns:\n",
    "                y_score = normalize_scores(df['score'].values)\n",
    "                score_source = 'normalized score'\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Calculate precision-recall curve\n",
    "            try:\n",
    "                precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "                pr_auc = auc(recall, precision)\n",
    "                \n",
    "                # Get display name\n",
    "                display_name = result_type.upper() \n",
    "                if '_' in result_type:\n",
    "                    parts = result_type.split('_')\n",
    "                    if len(parts) >= 3:\n",
    "                        display_name = f\"{parts[0].upper()} {' '.join(parts[2:]).upper()}\"\n",
    "                \n",
    "                # Plot the precision-recall curve\n",
    "                plt.plot(recall, precision, lw=2, \n",
    "                        label=f'{display_name} (AUC = {pr_auc:.3f}, {score_source})')\n",
    "                \n",
    "            except ValueError as ve:\n",
    "                logger.warning(f\"Error calculating PR curve for {result_type}: {str(ve)}\")\n",
    "                continue\n",
    "        \n",
    "        # Add horizontal line for random classifier performance (class imbalance)\n",
    "        # Calculate average positive rate across all datasets\n",
    "        positive_rates = []\n",
    "        for result_type in score_data_types:\n",
    "            df = eval_data[result_type]\n",
    "            if not df.empty:\n",
    "                positive_rate = df['is_ground_truth'].mean()\n",
    "                positive_rates.append(positive_rate)\n",
    "        \n",
    "        if positive_rates:\n",
    "            avg_positive_rate = np.mean(positive_rates)\n",
    "            plt.axhline(y=avg_positive_rate, color='r', linestyle='--', \n",
    "                      label=f'Random classifier (positive rate: {avg_positive_rate:.3f})')\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curves - {NEO4J_PROJECT_NAME} ({CURRENT_MODEL})')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.figtext(0.5, 0.01, threshold_description, ha='center', fontsize=10, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the text\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ROC analysis: {str(e)}\")\n",
    "        logger.debug(\"Exception details:\", exc_info=True)\n",
    "        print(f\"Error during ROC analysis: {str(e)}\")\n",
    "\n",
    "# Perform ROC curve analysis\n",
    "perform_roc_analysis(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [14] - Explanation Analysis\n",
    "# Purpose: Analyze explanation text from LLM results to extract insights and common patterns\n",
    "# Dependencies: re, collections, matplotlib, wordcloud (optional)\n",
    "# Breadcrumbs: Analysis -> Explanation Mining\n",
    "\n",
    "def analyze_explanations(llm_dfs):\n",
    "    \"\"\"\n",
    "    Analyze explanation text to extract insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    llm_dfs : list\n",
    "        List of DataFrames containing LLM results with explanations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        explanations_found = False\n",
    "        \n",
    "        # Create a combined list of all explanations with their result types\n",
    "        all_explanations = []\n",
    "        \n",
    "        # Check each result DataFrame for explanations\n",
    "        if not meta_judge_df.empty and 'explanation' in meta_judge_df.columns:\n",
    "            meta_explanations = meta_judge_df['explanation'].dropna().tolist()\n",
    "            all_explanations.extend([(exp, 'meta_judge') for exp in meta_explanations])\n",
    "            explanations_found = True\n",
    "            \n",
    "        if not transformers_df.empty and 'explanation' in transformers_df.columns:\n",
    "            trans_explanations = transformers_df['explanation'].dropna().tolist()\n",
    "            all_explanations.extend([(exp, 'transformer') for exp in trans_explanations])\n",
    "            explanations_found = True\n",
    "            \n",
    "        if not llm_result_df.empty and 'explanation' in llm_result_df.columns:\n",
    "            llm_explanations = llm_result_df['explanation'].dropna().tolist()\n",
    "            all_explanations.extend([(exp, 'llm') for exp in llm_explanations])\n",
    "            explanations_found = True\n",
    "            \n",
    "        if not explanations_found:\n",
    "            logger.warning(\"No explanations found for analysis\")\n",
    "            return\n",
    "            \n",
    "        # Process explanations\n",
    "        print(\"\\nExplanation Analysis:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total explanations found: {len(all_explanations)}\")\n",
    "        \n",
    "        # Extract common phrases and terms\n",
    "        explanation_text = ' '.join([exp[0] for exp in all_explanations])\n",
    "        \n",
    "        # Generate word cloud only if visualization is enabled\n",
    "        if SHOW_VISUALIZATION and explanation_text:\n",
    "            try:\n",
    "                from wordcloud import WordCloud\n",
    "                \n",
    "                wordcloud = WordCloud(\n",
    "                    width=800, \n",
    "                    height=400, \n",
    "                    background_color='white',\n",
    "                    max_words=100\n",
    "                ).generate(explanation_text)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Word Cloud of Explanations - {NEO4J_PROJECT_NAME} ({CURRENT_MODEL})')\n",
    "                plt.show()\n",
    "            except ImportError:\n",
    "                logger.warning(\"wordcloud package not found - skipping word cloud visualization\")\n",
    "        elif explanation_text:\n",
    "            logger.info(\"Visualization disabled - skipping word cloud\")\n",
    "            \n",
    "        # Extract and count common phrases (2-3 words)\n",
    "        def extract_phrases(text, n=2):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "        \n",
    "        bigrams = []\n",
    "        trigrams = []\n",
    "        \n",
    "        for exp, _ in all_explanations:\n",
    "            bigrams.extend(extract_phrases(exp, 2))\n",
    "            trigrams.extend(extract_phrases(exp, 3))\n",
    "            \n",
    "        # Count and show most common phrases\n",
    "        common_bigrams = Counter(bigrams).most_common(20)\n",
    "        common_trigrams = Counter(trigrams).most_common(20)\n",
    "        \n",
    "        print(\"\\nMost Common Bigrams in Explanations:\")\n",
    "        for phrase, count in common_bigrams:\n",
    "            print(f\"'{phrase}': {count}\")\n",
    "            \n",
    "        print(\"\\nMost Common Trigrams in Explanations:\")\n",
    "        for phrase, count in common_trigrams:\n",
    "            print(f\"'{phrase}': {count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing explanations: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Analyze explanations\n",
    "analyze_explanations([meta_judge_df, transformers_df, llm_result_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [15] - Source-Target Link Analysis\n",
    "# Purpose: Analyze connection patterns between source and target requirements\n",
    "# Dependencies: pandas, matplotlib, logging\n",
    "# Breadcrumbs: Analysis -> Link Pattern Analysis\n",
    "\n",
    "def analyze_source_target_links(eval_data, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Analyze connection patterns between source and target requirements\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eval_data : dict\n",
    "        Dictionary containing evaluation DataFrames for each result type\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if ground_truth_df.empty:\n",
    "            logger.warning(\"Ground truth DataFrame is empty - cannot analyze source-target patterns\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nSource-Target Link Pattern Analysis:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Analyze ground truth patterns\n",
    "        source_counts = ground_truth_df['source_id'].value_counts()\n",
    "        target_counts = ground_truth_df['target_id'].value_counts()\n",
    "        \n",
    "        print(\"Ground Truth Link Statistics:\")\n",
    "        print(f\"Total unique source requirements: {source_counts.shape[0]}\")\n",
    "        print(f\"Total unique target requirements: {target_counts.shape[0]}\")\n",
    "        print(f\"Average links per source requirement: {source_counts.mean():.2f}\")\n",
    "        print(f\"Max links from a single source: {source_counts.max()} (ID: {source_counts.idxmax()})\")\n",
    "        print(f\"Min links from a source: {source_counts.min()}\")\n",
    "        print(f\"Average links per target requirement: {target_counts.mean():.2f}\")\n",
    "        print(f\"Max links to a single target: {target_counts.max()} (ID: {target_counts.idxmax()})\")\n",
    "        \n",
    "        # For each result type, compare pattern with ground truth\n",
    "        for result_type, df in eval_data.items():\n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Filter for predicted links only\n",
    "            predicted_links = df[df['is_predicted']]\n",
    "            \n",
    "            if predicted_links.empty:\n",
    "                print(f\"\\nNo links predicted by {result_type.upper()}\")\n",
    "                continue\n",
    "                \n",
    "            pred_source_counts = predicted_links['source_id'].value_counts()\n",
    "            pred_target_counts = predicted_links['target_id'].value_counts()\n",
    "            \n",
    "            print(f\"\\n{result_type.upper()} Predicted Link Statistics:\")\n",
    "            print(f\"Total unique source requirements with predictions: {pred_source_counts.shape[0]}\")\n",
    "            print(f\"Total unique target requirements with predictions: {pred_target_counts.shape[0]}\")\n",
    "            print(f\"Average predicted links per source: {pred_source_counts.mean():.2f}\")\n",
    "            print(f\"Max predicted links from a single source: {pred_source_counts.max()} (ID: {pred_source_counts.idxmax()})\")\n",
    "            \n",
    "            # Compare with ground truth coverage\n",
    "            source_coverage = len(set(pred_source_counts.index) & set(source_counts.index)) / len(source_counts)\n",
    "            target_coverage = len(set(pred_target_counts.index) & set(target_counts.index)) / len(target_counts)\n",
    "            \n",
    "            print(f\"Source requirement coverage: {source_coverage:.2%}\")\n",
    "            print(f\"Target requirement coverage: {target_coverage:.2%}\")\n",
    "            \n",
    "            # Plot distribution comparison for source requirements if visualization is enabled\n",
    "            if SHOW_VISUALIZATION:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # Combine data for plotting\n",
    "                gt_sources = pd.DataFrame(source_counts).reset_index()\n",
    "                gt_sources.columns = ['source_id', 'gt_count']\n",
    "                \n",
    "                pred_sources = pd.DataFrame(pred_source_counts).reset_index()\n",
    "                pred_sources.columns = ['source_id', 'pred_count']\n",
    "                \n",
    "                # Merge data\n",
    "                combined = pd.merge(gt_sources, pred_sources, on='source_id', how='outer').fillna(0)\n",
    "                \n",
    "                # Sort by ground truth count for better visualization\n",
    "                combined = combined.sort_values('gt_count', ascending=False)\n",
    "                \n",
    "                # Plot only first 20 for clarity\n",
    "                plt.bar(range(min(20, len(combined))), combined['gt_count'].values[:20], \n",
    "                       alpha=0.7, label='Ground Truth Links')\n",
    "                plt.bar(range(min(20, len(combined))), combined['pred_count'].values[:20], \n",
    "                       alpha=0.5, label='Predicted Links')\n",
    "                \n",
    "                plt.xlabel('Source Requirements (sorted by ground truth count)')\n",
    "                plt.ylabel('Number of Links')\n",
    "                plt.title(f'Ground Truth vs {result_type.upper()} Predictions - Top Sources')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                logger.info(f\"Visualization disabled - skipping source-target bar chart for {result_type}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing source-target link patterns: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Analyze source-target link patterns\n",
    "analyze_source_target_links(eval_data, ground_truth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [16] - Project-Model Summary\n",
    "# Purpose: Generate a comprehensive summary report for the analyzed project and model\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Output -> Summary Report\n",
    "\n",
    "def generate_summary_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary report for the analyzed project and model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"SUMMARY REPORT FOR {NEO4J_PROJECT_NAME} WITH MODEL {CURRENT_MODEL}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Project statistics\n",
    "        if not ground_truth_df.empty:\n",
    "            total_source_reqs = ground_truth_df['source_id'].nunique()\n",
    "            total_target_reqs = ground_truth_df['target_id'].nunique()\n",
    "            total_gt_links = len(ground_truth_df)\n",
    "            \n",
    "            print(f\"\\nProject Statistics:\")\n",
    "            print(f\"Total source requirements: {total_source_reqs}\")\n",
    "            print(f\"Total target requirements: {total_target_reqs}\")\n",
    "            print(f\"Total ground truth links: {total_gt_links}\")\n",
    "            print(f\"Link density: {total_gt_links / (total_source_reqs * total_target_reqs):.4f}\")\n",
    "        else:\n",
    "            print(\"\\nNo ground truth data available for project statistics\")\n",
    "            \n",
    "        # Results availability\n",
    "        print(\"\\nResults Availability:\")\n",
    "        print(f\"Meta Judge results: {'Available' if not meta_judge_df.empty else 'Not available'}\")\n",
    "        print(f\"Transformer results: {'Available' if not transformers_df.empty else 'Not available'}\")\n",
    "        print(f\"Basic LLM results: {'Available' if not llm_result_df.empty else 'Not available'}\")\n",
    "        \n",
    "        # Performance metrics summary\n",
    "        if len(metrics_results) > 0:\n",
    "            print(\"\\nPerformance Metrics Summary:\")\n",
    "            for result_type, metrics in metrics_results.items():\n",
    "                print(f\"\\n{result_type.upper()}:\")\n",
    "                print(f\"  Precision: {metrics.get('precision', 'N/A'):.4f}\")\n",
    "                print(f\"  Recall: {metrics.get('recall', 'N/A'):.4f}\")\n",
    "                print(f\"  F1 Score: {metrics.get('f1_score', 'N/A'):.4f}\")\n",
    "                print(f\"  Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "                print(f\"  MCC: {metrics.get('mcc', 'N/A'):.4f}\")\n",
    "                print(f\"  True Positives: {metrics.get('true_positives', 'N/A')}\")\n",
    "                print(f\"  False Positives: {metrics.get('false_positives', 'N/A')}\")\n",
    "                print(f\"  False Negatives: {metrics.get('false_negatives', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"\\nNo performance metrics available\")\n",
    "            \n",
    "        # Best performing approach\n",
    "        if len(metrics_results) > 1:\n",
    "            f1_scores = {rt: m.get('f1_score', 0) for rt, m in metrics_results.items()}\n",
    "            best_approach = max(f1_scores.items(), key=lambda x: x[1])[0]\n",
    "            print(f\"\\nBest performing approach: {best_approach.upper()} (F1 Score: {f1_scores[best_approach]:.4f})\")\n",
    "            \n",
    "        # Recommendations\n",
    "        print(\"\\nRecommendations:\")\n",
    "        \n",
    "        # Generate automatic recommendations based on results\n",
    "        recommendations = []\n",
    "        \n",
    "        if len(metrics_results) > 0:\n",
    "            for result_type, metrics in metrics_results.items():\n",
    "                # Low precision suggestions\n",
    "                if metrics.get('precision', 1) < 0.7:\n",
    "                    recommendations.append(f\"Consider tuning {result_type} for higher precision - currently at {metrics.get('precision', 0):.2f}\")\n",
    "                \n",
    "                # Low recall suggestions\n",
    "                if metrics.get('recall', 1) < 0.7:\n",
    "                    recommendations.append(f\"Consider tuning {result_type} for higher recall - currently at {metrics.get('recall', 0):.2f}\")\n",
    "                    \n",
    "                # High false negatives\n",
    "                if metrics.get('false_negatives', 0) > 10:\n",
    "                    recommendations.append(f\"Address high false negative rate in {result_type} approach\")\n",
    "        \n",
    "        # Add general recommendations if none specific\n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Review explanations for insights on model reasoning\")\n",
    "            recommendations.append(\"Consider evaluating with different threshold values\")\n",
    "            \n",
    "        for idx, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{idx}. {rec}\")\n",
    "            \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating summary report: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Generate summary report\n",
    "generate_summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [17] - Close Resources\n",
    "# Purpose: Clean up database connections and finalize analysis\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Cleanup -> Resource Management\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n",
    "logger.info(\"Neo4j driver closed\")\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

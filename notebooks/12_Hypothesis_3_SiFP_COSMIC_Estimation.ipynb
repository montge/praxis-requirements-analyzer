{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 3: Hallucination Reduction and Time-to-Market Impact\n",
    "**Implementing hallucination-reducing techniques in LLMs significantly improves (>30%) time to market in new product development.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Enhanced Setup and Imports with Advanced Statistical Testing (FIXED) + Model Filtering\n",
    "# Purpose: Import all required libraries and configure environment settings for SiFP COSMIC Estimation Analysis with advanced statistical methods\n",
    "# Dependencies: pandas, numpy, matplotlib, seaborn, scipy, neo4j, scikit-learn, dotenv, pymc, arviz, bayesian-testing\n",
    "# Breadcrumbs: Setup -> Environment Configuration -> Analysis Preparation -> Advanced Statistical Methods\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Core statistical and machine learning imports\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Database connections\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Check scipy version for permutation_test availability\n",
    "import scipy\n",
    "scipy_version = [int(x) for x in scipy.__version__.split('.')]\n",
    "SCIPY_HAS_PERMUTATION_TEST = (scipy_version[0] > 1) or (scipy_version[0] == 1 and scipy_version[1] >= 7)\n",
    "\n",
    "print(f\"SciPy version: {scipy.__version__}\")\n",
    "print(f\"Permutation test available: {SCIPY_HAS_PERMUTATION_TEST}\")\n",
    "\n",
    "# Advanced statistical testing imports with better error handling\n",
    "try:\n",
    "    # Basic scipy stats (should always be available)\n",
    "    from scipy.stats import (\n",
    "        mannwhitneyu, wilcoxon, kruskal, friedmanchisquare, \n",
    "        chi2_contingency, fisher_exact, pearsonr, spearmanr, \n",
    "        kendalltau, norm, t as t_dist\n",
    "    )\n",
    "    \n",
    "    # Try to import permutation_test and bootstrap (scipy 1.7.0+)\n",
    "    if SCIPY_HAS_PERMUTATION_TEST:\n",
    "        from scipy.stats import permutation_test, bootstrap\n",
    "        PERMUTATION_TEST_AVAILABLE = True\n",
    "        BOOTSTRAP_AVAILABLE = True\n",
    "        print(\"✓ SciPy permutation_test and bootstrap imported successfully\")\n",
    "    else:\n",
    "        PERMUTATION_TEST_AVAILABLE = False\n",
    "        BOOTSTRAP_AVAILABLE = False\n",
    "        print(\"⚠ SciPy permutation_test not available (requires SciPy >= 1.7.0)\")\n",
    "    \n",
    "    # Bayesian analysis imports\n",
    "    try:\n",
    "        import pymc as pm\n",
    "        import arviz as az\n",
    "        BAYESIAN_AVAILABLE = True\n",
    "        print(\"✓ PyMC and ArviZ imported successfully\")\n",
    "    except ImportError:\n",
    "        BAYESIAN_AVAILABLE = False\n",
    "        print(\"⚠ PyMC/ArviZ not available\")\n",
    "    \n",
    "    # Bayesian hypothesis testing\n",
    "    try:\n",
    "        from bayesian_testing.experiments import BinaryDataTest, NormalDataTest\n",
    "        BAYESIAN_TESTING_AVAILABLE = True\n",
    "        print(\"✓ Bayesian-testing imported successfully\")\n",
    "    except ImportError:\n",
    "        BAYESIAN_TESTING_AVAILABLE = False\n",
    "        print(\"⚠ bayesian-testing not available\")\n",
    "    \n",
    "    # Additional statistical utilities (CORRECTED FUNCTION NAMES)\n",
    "    try:\n",
    "        from statsmodels.stats.contingency_tables import mcnemar\n",
    "        from statsmodels.stats.power import ttest_power, tt_ind_solve_power\n",
    "        from statsmodels.stats.proportion import proportion_confint, proportions_ztest\n",
    "        from statsmodels.stats.descriptivestats import describe\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        STATSMODELS_AVAILABLE = True\n",
    "        print(\"✓ Statsmodels imported successfully\")\n",
    "    except ImportError:\n",
    "        STATSMODELS_AVAILABLE = False\n",
    "        print(\"⚠ Statsmodels not available\")\n",
    "    \n",
    "    ADVANCED_STATS_AVAILABLE = True\n",
    "    print(\"✓ Basic advanced statistical libraries loaded successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Warning: Some statistical libraries not available: {e}\")\n",
    "    ADVANCED_STATS_AVAILABLE = False\n",
    "    PERMUTATION_TEST_AVAILABLE = False\n",
    "    BOOTSTRAP_AVAILABLE = False\n",
    "    BAYESIAN_AVAILABLE = False\n",
    "    BAYESIAN_TESTING_AVAILABLE = False\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "\n",
    "def custom_permutation_test(sample1, sample2, statistic_func, n_resamples=10000, alternative='two-sided', random_state=None):\n",
    "    \"\"\"\n",
    "    Custom implementation of permutation test for older SciPy versions\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Calculate observed statistic\n",
    "    observed_stat = statistic_func(sample1, sample2)\n",
    "    \n",
    "    # Combine samples for permutation\n",
    "    combined = np.concatenate([sample1, sample2])\n",
    "    n1 = len(sample1)\n",
    "    \n",
    "    # Generate permutation distribution\n",
    "    perm_stats = []\n",
    "    for _ in range(n_resamples):\n",
    "        perm_combined = np.random.permutation(combined)\n",
    "        perm_sample1 = perm_combined[:n1]\n",
    "        perm_sample2 = perm_combined[n1:]\n",
    "        perm_stat = statistic_func(perm_sample1, perm_sample2)\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    \n",
    "    # Calculate p-value based on alternative hypothesis\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = np.mean(np.abs(perm_stats) >= np.abs(observed_stat))\n",
    "    elif alternative == 'greater':\n",
    "        p_value = np.mean(perm_stats >= observed_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = np.mean(perm_stats <= observed_stat)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "    \n",
    "    # Return result object similar to scipy's permutation_test\n",
    "    class PermutationTestResult:\n",
    "        def __init__(self, statistic, pvalue, null_distribution):\n",
    "            self.statistic = statistic\n",
    "            self.pvalue = pvalue\n",
    "            self.null_distribution = null_distribution\n",
    "    \n",
    "    return PermutationTestResult(observed_stat, p_value, perm_stats)\n",
    "\n",
    "def custom_bootstrap_ci(data, statistic_func, n_resamples=10000, confidence_level=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Custom implementation of bootstrap confidence intervals\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_stats = []\n",
    "    for _ in range(n_resamples):\n",
    "        bootstrap_sample = np.random.choice(data, len(data), replace=True)\n",
    "        bootstrap_stat = statistic_func(bootstrap_sample)\n",
    "        bootstrap_stats.append(bootstrap_stat)\n",
    "    \n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_stats, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_stats, upper_percentile)\n",
    "    \n",
    "    # Return result object similar to scipy's bootstrap\n",
    "    class BootstrapResult:\n",
    "        def __init__(self, confidence_interval, bootstrap_distribution):\n",
    "            self.confidence_interval = (ci_lower, ci_upper)\n",
    "            self.bootstrap_distribution = bootstrap_distribution\n",
    "    \n",
    "    return BootstrapResult((ci_lower, ci_upper), bootstrap_stats)\n",
    "\n",
    "def setup_analysis_environment():\n",
    "    \"\"\"\n",
    "    Configure analysis environment with display options and styling\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters for the analysis\n",
    "    \"\"\"\n",
    "    # Suppress warnings for cleaner output\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Configure matplotlib and seaborn styling\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # Configure pandas display options for better readability\n",
    "    # Configure pandas display settings for legal landscape format\n",
    "    pd.set_option('display.width', 130)           # Set width threshold for legal landscape\n",
    "    pd.set_option('display.max_columns', 25)     # Reasonable number of columns\n",
    "    pd.set_option('display.max_colwidth', 25)    # Compact column width\n",
    "    pd.set_option('display.precision', 2)        # Only 2 decimal places to save space\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)  # Consistent float formatting\n",
    "    # Note: Removed expand_frame_repr=False to allow natural wrapping at 130 chars\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Parse target model IDs from environment\n",
    "    target_model_ids = []\n",
    "    results_analysis_models = os.getenv('RESULTS_ANALYSIS_MODEL_IDS', '').strip()\n",
    "    \n",
    "    if results_analysis_models:\n",
    "        # Parse the comma-separated list of model variable names\n",
    "        model_var_names = [name.strip() for name in results_analysis_models.split(',')]\n",
    "        \n",
    "        for var_name in model_var_names:\n",
    "            model_id = os.getenv(var_name, '').strip()\n",
    "            if model_id:\n",
    "                target_model_ids.append(model_id)\n",
    "                print(f\"✓ Added target model: {var_name} = {model_id}\")\n",
    "            else:\n",
    "                print(f\"⚠ Warning: {var_name} not found in environment variables\")\n",
    "    \n",
    "    if not target_model_ids:\n",
    "        print(\"⚠ Warning: No target model IDs found. Will analyze all models.\")\n",
    "        print(\"   Please check RESULTS_ANALYSIS_MODEL_IDS in .env file\")\n",
    "    \n",
    "    # Configuration parameters\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        'TARGET_MODEL_IDS': target_model_ids,  # NEW: List of specific model IDs to analyze\n",
    "        'CONVERSION_FACTOR': 0.957,  # SiFP = 0.957 × UFP (Desharnais)\n",
    "        'COST_PER_HOUR': 100,  # Industry standard for cost impact calculations\n",
    "        \n",
    "        # Statistical testing configuration\n",
    "        'ALPHA_LEVEL': 0.05,  # Significance level\n",
    "        'BOOTSTRAP_SAMPLES': 10000,  # Number of bootstrap samples\n",
    "        'PERMUTATION_SAMPLES': 10000,  # Number of permutation samples\n",
    "        'BAYESIAN_SAMPLES': 2000,  # Number of MCMC samples\n",
    "        'BAYESIAN_CHAINS': 4,  # Number of MCMC chains\n",
    "        'IMPROVEMENT_THRESHOLD': 0.30,  # 30% improvement threshold\n",
    "        'POWER_TARGET': 0.80,  # Target statistical power\n",
    "        \n",
    "        # Advanced testing flags\n",
    "        'ADVANCED_STATS_AVAILABLE': ADVANCED_STATS_AVAILABLE,\n",
    "        'SCIPY_VERSION': scipy.__version__\n",
    "    }\n",
    "    \n",
    "    print(\"✓ Analysis environment configured successfully\")\n",
    "    print(f\"✓ Project: {config['NEO4J_PROJECT_NAME']}\")\n",
    "    print(f\"✓ Target models for analysis: {len(target_model_ids)}\")\n",
    "    if target_model_ids:\n",
    "        for i, model_id in enumerate(target_model_ids, 1):\n",
    "            print(f\"   {i}. {model_id}\")\n",
    "    print(f\"✓ Advanced statistical methods: {'Available' if ADVANCED_STATS_AVAILABLE else 'Limited'}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "def initialize_statistical_methods():\n",
    "    \"\"\"\n",
    "    Initialize and test statistical methods availability with fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        dict: Available statistical methods configuration\n",
    "    \"\"\"\n",
    "    methods_config = {\n",
    "        'permutation_tests': False,\n",
    "        'bootstrap_tests': False,\n",
    "        'bayesian_analysis': False,\n",
    "        'power_analysis': False\n",
    "    }\n",
    "    \n",
    "    # Test permutation functionality\n",
    "    try:\n",
    "        test_data1 = np.random.normal(0, 1, 10)\n",
    "        test_data2 = np.random.normal(0, 1, 10)\n",
    "        \n",
    "        def test_statistic(x, y):\n",
    "            return np.mean(x) - np.mean(y)\n",
    "        \n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            # Use scipy's permutation_test\n",
    "            perm_result = permutation_test((test_data1, test_data2), test_statistic, \n",
    "                                         n_resamples=100, random_state=42)\n",
    "            methods_config['permutation_tests'] = True\n",
    "            print(\"✓ SciPy permutation_test available and working\")\n",
    "        else:\n",
    "            # Use custom implementation\n",
    "            perm_result = custom_permutation_test(test_data1, test_data2, test_statistic, \n",
    "                                                n_resamples=100, random_state=42)\n",
    "            methods_config['permutation_tests'] = True\n",
    "            print(\"✓ Custom permutation_test fallback available and working\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Permutation test functionality issue: {e}\")\n",
    "        methods_config['permutation_tests'] = False\n",
    "    \n",
    "    # Test bootstrap functionality\n",
    "    try:\n",
    "        if BOOTSTRAP_AVAILABLE:\n",
    "            # Use scipy's bootstrap\n",
    "            bootstrap_result = bootstrap((test_data1,), np.mean, n_resamples=100, \n",
    "                                       random_state=42)\n",
    "            methods_config['bootstrap_tests'] = True\n",
    "            print(\"✓ SciPy bootstrap available and working\")\n",
    "        else:\n",
    "            # Use custom implementation\n",
    "            bootstrap_result = custom_bootstrap_ci(test_data1, np.mean, n_resamples=100, \n",
    "                                                 random_state=42)\n",
    "            methods_config['bootstrap_tests'] = True\n",
    "            print(\"✓ Custom bootstrap fallback available and working\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Bootstrap functionality issue: {e}\")\n",
    "        methods_config['bootstrap_tests'] = False\n",
    "    \n",
    "    # Test Bayesian functionality\n",
    "    if BAYESIAN_AVAILABLE:\n",
    "        try:\n",
    "            with pm.Model() as test_model:\n",
    "                mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "            methods_config['bayesian_analysis'] = True\n",
    "            print(\"✓ Bayesian analysis available and working\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Bayesian analysis issue: {e}\")\n",
    "            methods_config['bayesian_analysis'] = False\n",
    "    \n",
    "    # Test power analysis (CORRECTED FUNCTION CALL)\n",
    "    if STATSMODELS_AVAILABLE:\n",
    "        try:\n",
    "            power_result = tt_ind_solve_power(effect_size=0.5, nobs1=20, alpha=0.05)\n",
    "            methods_config['power_analysis'] = True\n",
    "            print(\"✓ Power analysis available and working\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Power analysis issue: {e}\")\n",
    "            methods_config['power_analysis'] = False\n",
    "    \n",
    "    return methods_config\n",
    "\n",
    "def setup_bayesian_environment():\n",
    "    \"\"\"\n",
    "    Configure PyMC and ArviZ for Bayesian analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian analysis configuration\n",
    "    \"\"\"\n",
    "    if not BAYESIAN_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Configure ArviZ styling\n",
    "        az.style.use('arviz-darkgrid')\n",
    "        \n",
    "        # Set up PyMC configuration (version-aware)\n",
    "        try:\n",
    "            # For PyMC3 compatibility\n",
    "            pm.set_tt_config('floatX', 'float64')\n",
    "        except AttributeError:\n",
    "            # PyMC v4+ doesn't have set_tt_config - configuration is handled differently\n",
    "            # This is normal and expected for newer PyMC versions\n",
    "            pass\n",
    "        \n",
    "        bayesian_config = {\n",
    "            'target_accept': 0.9,\n",
    "            'chains': 4,\n",
    "            'draws': 2000,\n",
    "            'tune': 1000,\n",
    "            'cores': min(4, os.cpu_count() or 1),\n",
    "            'return_inferencedata': True,\n",
    "            'random_seed': 42\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Bayesian analysis environment configured\")\n",
    "        return bayesian_config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Bayesian environment setup issue: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Execute setup when cell runs\n",
    "CONFIG = setup_analysis_environment()\n",
    "STATISTICAL_CONFIG = initialize_statistical_methods()\n",
    "BAYESIAN_CONFIG = setup_bayesian_environment()\n",
    "\n",
    "# Make custom functions available globally if needed\n",
    "if not PERMUTATION_TEST_AVAILABLE:\n",
    "    permutation_test = custom_permutation_test\n",
    "    print(\"✓ Custom permutation_test function registered globally\")\n",
    "\n",
    "if not BOOTSTRAP_AVAILABLE:\n",
    "    bootstrap = custom_bootstrap_ci\n",
    "    print(\"✓ Custom bootstrap function registered globally\")\n",
    "\n",
    "# Display configuration summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL ANALYSIS CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SciPy Version: {scipy.__version__}\")\n",
    "print(f\"Available Methods:\")\n",
    "print(f\"  ✓ Basic statistics and visualization\")\n",
    "print(f\"  ✓ Standard hypothesis testing (t-tests, Mann-Whitney)\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('permutation_tests') else '✗'} Permutation tests {'(custom fallback)' if not PERMUTATION_TEST_AVAILABLE and STATISTICAL_CONFIG.get('permutation_tests') else '(scipy native)' if STATISTICAL_CONFIG.get('permutation_tests') else ''}\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('bootstrap_tests') else '✗'} Advanced bootstrapping {'(custom fallback)' if not BOOTSTRAP_AVAILABLE and STATISTICAL_CONFIG.get('bootstrap_tests') else '(scipy native)' if STATISTICAL_CONFIG.get('bootstrap_tests') else ''}\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('bayesian_analysis') else '✗'} Bayesian hypothesis testing\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('power_analysis') else '✗'} Power analysis\")\n",
    "\n",
    "print(f\"\\nConfiguration Parameters:\")\n",
    "print(f\"  Alpha level: {CONFIG['ALPHA_LEVEL']}\")\n",
    "print(f\"  Improvement threshold: {CONFIG['IMPROVEMENT_THRESHOLD']:.0%}\")\n",
    "print(f\"  Bootstrap samples: {CONFIG['BOOTSTRAP_SAMPLES']:,}\")\n",
    "print(f\"  Permutation samples: {CONFIG['PERMUTATION_SAMPLES']:,}\")\n",
    "if BAYESIAN_CONFIG:\n",
    "    print(f\"  MCMC samples: {CONFIG['BAYESIAN_SAMPLES']:,}\")\n",
    "    print(f\"  MCMC chains: {CONFIG['BAYESIAN_CHAINS']}\")\n",
    "\n",
    "print(f\"\\nModel Filtering Configuration:\")\n",
    "if CONFIG['TARGET_MODEL_IDS']:\n",
    "    print(f\"  Analysis scope: FILTERED to {len(CONFIG['TARGET_MODEL_IDS'])} specific models\")\n",
    "    for i, model_id in enumerate(CONFIG['TARGET_MODEL_IDS'], 1):\n",
    "        print(f\"    {i}. {model_id}\")\n",
    "else:\n",
    "    print(f\"  Analysis scope: ALL MODELS (no filtering applied)\")\n",
    "\n",
    "if not PERMUTATION_TEST_AVAILABLE:\n",
    "    print(f\"\\n⚠ NOTE: Using custom permutation test implementation\")\n",
    "    print(f\"   To use native SciPy implementation: pip install 'scipy>=1.7.0'\")\n",
    "\n",
    "if not BOOTSTRAP_AVAILABLE:\n",
    "    print(f\"\\n⚠ NOTE: Using custom bootstrap implementation\")\n",
    "    print(f\"   To use native SciPy implementation: pip install 'scipy>=1.7.0'\")\n",
    "\n",
    "print(f\"\\n✓ Enhanced statistical analysis environment ready with model filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Load and Process Java Code Metrics\n",
    "# Purpose: Load actual implementation metrics from iTrust java.csv for baseline establishment\n",
    "# Dependencies: pandas, configured environment (Cell 0)\n",
    "# Breadcrumbs: Setup -> Code Metrics Loading -> Baseline Data Preparation\n",
    "\n",
    "def load_code_metrics():\n",
    "    \"\"\"\n",
    "    Load and process Java code metrics from iTrust dataset\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed code metrics with derived calculations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load java.csv from iTrust dataset\n",
    "        java_df = pd.read_csv('../datasets/iTrust/iTrust/java.csv')\n",
    "        print(f\"✓ Loaded java.csv: {java_df.shape[0]} code entities\")\n",
    "\n",
    "        # Filter to only File entries (excluding methods, functions, etc.)\n",
    "        file_df = java_df[java_df['Kind'] == 'File'].copy()\n",
    "        print(f\"  Files: {len(file_df)}\")\n",
    "\n",
    "        # Select relevant metrics for SiFP analysis\n",
    "        metrics_columns = [\n",
    "            'Name', 'CountLine', 'CountLineCode', 'CountLineComment',\n",
    "            'CountDeclClass', 'CountDeclMethod', 'CountDeclMethodAll',\n",
    "            'CountDeclExecutableUnit', 'Cyclomatic', 'MaxCyclomatic'\n",
    "        ]\n",
    "\n",
    "        # Create analysis-ready dataframe\n",
    "        code_metrics_df = file_df[metrics_columns].copy()\n",
    "\n",
    "        # Calculate derived metrics that correlate with function points\n",
    "        code_metrics_df['TotalUnits'] = (\n",
    "            code_metrics_df['CountDeclClass'] + \n",
    "            code_metrics_df['CountDeclMethod']\n",
    "        )\n",
    "\n",
    "        return code_metrics_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading code metrics: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load and process the code metrics\n",
    "code_metrics_df = load_code_metrics()\n",
    "\n",
    "# Display sample data for verification\n",
    "print(\"\\nSample code metrics:\")\n",
    "print(code_metrics_df.head())\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\nCode Metrics Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  Total files: {len(code_metrics_df)}\")\n",
    "print(f\"  Average lines of code: {code_metrics_df['CountLineCode'].mean():.0f}\")\n",
    "print(f\"  Average methods per file: {code_metrics_df['CountDeclMethod'].mean():.1f}\")\n",
    "print(f\"  Average cyclomatic complexity: {code_metrics_df['Cyclomatic'].mean():.1f}\")\n",
    "print(f\"  Total LOC in codebase: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "\n",
    "# Store key metrics for later analysis\n",
    "code_summary = {\n",
    "    'total_files': len(code_metrics_df),\n",
    "    'total_lines': code_metrics_df['CountLineCode'].sum(),\n",
    "    'total_classes': code_metrics_df['CountDeclClass'].sum(),\n",
    "    'total_methods': code_metrics_df['CountDeclMethod'].sum(),\n",
    "    'avg_complexity': code_metrics_df['Cyclomatic'].mean(),\n",
    "    'total_units': code_metrics_df['TotalUnits'].sum()\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Code metrics loaded and processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Connect to Neo4j and Retrieve LLM SiFP Estimates (UPDATED WITH MODEL FILTERING)\n",
    "# Purpose: Retrieve LLM-generated SiFP estimates for requirements with established ground truth links, filtered by target models\n",
    "# Dependencies: Neo4j connection, json processing, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Code Metrics -> Neo4j Data Retrieval -> LLM Estimates Analysis\n",
    "\n",
    "def retrieve_llm_estimates():\n",
    "    \"\"\"\n",
    "    Connect to Neo4j and retrieve LLM SiFP estimates for ground truth requirements, filtered by target models\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm_estimates_df, ground_truth_requirements, model_success_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Establish Neo4j connection using configuration\n",
    "        driver = GraphDatabase.driver(\n",
    "            CONFIG['NEO4J_URI'], \n",
    "            auth=(CONFIG['NEO4J_USER'], CONFIG['NEO4J_PASSWORD'])\n",
    "        )\n",
    "        print(f\"✓ Connected to Neo4j for project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "\n",
    "        with driver.session() as session:\n",
    "            # First, identify TARGET requirements with ground truth\n",
    "            gt_query = \"\"\"\n",
    "                MATCH (r1:Requirement {project: $project_name, type: 'TARGET'})\n",
    "                WHERE EXISTS((r1)-[:GROUND_TRUTH]-()) OR EXISTS(()-[:GROUND_TRUTH]-(r1))\n",
    "                RETURN DISTINCT r1.id as requirement_id\n",
    "            \"\"\"\n",
    "            \n",
    "            gt_result = session.run(gt_query, project_name=CONFIG['NEO4J_PROJECT_NAME'])\n",
    "            ground_truth_requirements = [record['requirement_id'] for record in gt_result]\n",
    "            \n",
    "            print(f\"✓ Found {len(ground_truth_requirements)} TARGET requirements with ground truth\")\n",
    "\n",
    "            # Query for LLM estimates on ground truth requirements with model filtering\n",
    "            if CONFIG['TARGET_MODEL_IDS']:\n",
    "                # Build model filtering clause\n",
    "                model_filter = \"AND se.model IN $target_models\"\n",
    "                print(f\"✓ Applying model filter for {len(CONFIG['TARGET_MODEL_IDS'])} specific models:\")\n",
    "                for i, model_id in enumerate(CONFIG['TARGET_MODEL_IDS'], 1):\n",
    "                    print(f\"   {i}. {model_id}\")\n",
    "            else:\n",
    "                model_filter = \"\"\n",
    "                print(f\"✓ No model filtering applied - analyzing all models\")\n",
    "            \n",
    "            estimation_query = f\"\"\"\n",
    "                MATCH (r1:Requirement {{project: $project_name, type: 'TARGET'}})-[se:SIFP_ESTIMATION]->(r2:Requirement)\n",
    "                WHERE r1.id IN $ground_truth_reqs\n",
    "                  AND se.final_estimation IS NOT NULL\n",
    "                  AND se.is_valid = true\n",
    "                  {model_filter}\n",
    "                RETURN DISTINCT r1.id as requirement_id,\n",
    "                       r1.content as requirement_content,\n",
    "                       se.model as model,\n",
    "                       se.actor_analysis as actor_json,\n",
    "                       se.final_estimation as final_json,\n",
    "                       se.judge_evaluation as judge_eval_json,\n",
    "                       se.confidence as confidence,\n",
    "                       se.judge_confidence as judge_confidence,\n",
    "                       se.judge_score as judge_score\n",
    "                ORDER BY r1.id, se.model\n",
    "            \"\"\"\n",
    "            \n",
    "            # Prepare query parameters\n",
    "            query_params = {\n",
    "                'project_name': CONFIG['NEO4J_PROJECT_NAME'], \n",
    "                'ground_truth_reqs': ground_truth_requirements\n",
    "            }\n",
    "            \n",
    "            # Add model filtering parameter if applicable\n",
    "            if CONFIG['TARGET_MODEL_IDS']:\n",
    "                query_params['target_models'] = CONFIG['TARGET_MODEL_IDS']\n",
    "            \n",
    "            result = session.run(estimation_query, **query_params)\n",
    "            \n",
    "            # Process and structure the results\n",
    "            records = []\n",
    "            for record in result:\n",
    "                try:\n",
    "                    # Parse JSON data from Neo4j\n",
    "                    actor_data = json.loads(record['actor_json']) if record['actor_json'] else {}\n",
    "                    final_data = json.loads(record['final_json']) if record['final_json'] else {}\n",
    "                    \n",
    "                    # Extract UGEP and UGDG counts\n",
    "                    actor_ugep = len(actor_data.get('ugeps', []))\n",
    "                    actor_ugdg = len(actor_data.get('ugdgs', []))\n",
    "                    final_ugep = len(final_data.get('ugeps', []))\n",
    "                    final_ugdg = len(final_data.get('ugdgs', []))\n",
    "                    \n",
    "                    # Calculate SiFP using standard formula: SiFP = 4.6 × UGEP + 7.0 × UGDG\n",
    "                    actor_sifp = 4.6 * actor_ugep + 7 * actor_ugdg\n",
    "                    final_sifp = 4.6 * final_ugep + 7 * final_ugdg\n",
    "                    \n",
    "                    records.append({\n",
    "                        'requirement_id': record['requirement_id'],\n",
    "                        'requirement_content': record['requirement_content'][:100] + '...',\n",
    "                        'model': record['model'],\n",
    "                        'actor_ugep': actor_ugep,\n",
    "                        'actor_ugdg': actor_ugdg,\n",
    "                        'actor_sifp': actor_sifp,\n",
    "                        'final_ugep': final_ugep,\n",
    "                        'final_ugdg': final_ugdg,\n",
    "                        'final_sifp': final_sifp,\n",
    "                        'judge_score': record['judge_score'],\n",
    "                        'confidence': record['confidence']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing record for {record.get('requirement_id', 'unknown')}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Create estimates DataFrame\n",
    "        llm_estimates_df = pd.DataFrame(records)\n",
    "        \n",
    "        if not llm_estimates_df.empty:\n",
    "            print(f\"\\n✓ Retrieved {len(llm_estimates_df)} LLM estimates for ground truth requirements\")\n",
    "            \n",
    "            # Verify model filtering worked correctly\n",
    "            unique_models = sorted(llm_estimates_df['model'].unique())\n",
    "            print(f\"\\nModels found in results: {len(unique_models)}\")\n",
    "            for i, model in enumerate(unique_models, 1):\n",
    "                print(f\"  {i}. {model}\")\n",
    "            \n",
    "            # Check if any target models are missing\n",
    "            if CONFIG['TARGET_MODEL_IDS']:\n",
    "                missing_models = set(CONFIG['TARGET_MODEL_IDS']) - set(unique_models)\n",
    "                if missing_models:\n",
    "                    print(f\"\\n⚠ Warning: Target models not found in results:\")\n",
    "                    for model in missing_models:\n",
    "                        print(f\"     • {model}\")\n",
    "                else:\n",
    "                    print(f\"\\n✓ All target models found in results\")\n",
    "            \n",
    "            # Calculate model success rates\n",
    "            print(f\"\\nModel Success Rates (Ground Truth Requirements):\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            model_success = []\n",
    "            for model in sorted(llm_estimates_df['model'].unique()):\n",
    "                model_estimates = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "                successful_reqs = model_estimates['requirement_id'].nunique()\n",
    "                success_rate = successful_reqs / len(ground_truth_requirements) * 100\n",
    "                \n",
    "                model_success.append({\n",
    "                    'model': model,\n",
    "                    'successful_estimates': successful_reqs,\n",
    "                    'total_ground_truth': len(ground_truth_requirements),\n",
    "                    'success_rate': success_rate\n",
    "                })\n",
    "                \n",
    "                print(f\"  {model}: {successful_reqs}/{len(ground_truth_requirements)} ({success_rate:.1f}%)\")\n",
    "            \n",
    "            model_success_df = pd.DataFrame(model_success)\n",
    "            \n",
    "            # Display sample estimates for verification\n",
    "            print(f\"\\nSample LLM estimates (filtered results):\")\n",
    "            display_cols = ['requirement_id', 'model', 'final_sifp', 'judge_score']\n",
    "            print(llm_estimates_df[display_cols].head(10))\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(f\"\\nFiltered Analysis Summary:\")\n",
    "            print(f\"  Total estimates: {len(llm_estimates_df)}\")\n",
    "            print(f\"  Unique requirements: {llm_estimates_df['requirement_id'].nunique()}\")\n",
    "            print(f\"  Unique models: {len(unique_models)}\")\n",
    "            print(f\"  Average SiFP per estimate: {llm_estimates_df['final_sifp'].mean():.1f}\")\n",
    "            print(f\"  Average judge score: {llm_estimates_df['judge_score'].mean():.2f}\")\n",
    "            \n",
    "            return llm_estimates_df, ground_truth_requirements, model_success_df\n",
    "            \n",
    "        else:\n",
    "            print(\"Warning: No LLM estimates found for ground truth requirements with specified models!\")\n",
    "            if CONFIG['TARGET_MODEL_IDS']:\n",
    "                print(\"This could be because:\")\n",
    "                print(\"  1. The specified models haven't analyzed these requirements\")\n",
    "                print(\"  2. The model IDs in .env don't match the exact model names in Neo4j\")\n",
    "                print(\"  3. The requirements don't have valid estimations from these models\")\n",
    "                print(f\"\\nTarget models specified: {CONFIG['TARGET_MODEL_IDS']}\")\n",
    "            return pd.DataFrame(), ground_truth_requirements, pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving LLM estimates: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        driver.close()\n",
    "        print(\"✓ Neo4j connection closed\")\n",
    "\n",
    "# Execute the retrieval process\n",
    "llm_estimates_df, ground_truth_requirements, model_success_df = retrieve_llm_estimates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Establish Requirements-to-Code Mapping and Feature Analysis\n",
    "# Purpose: Create mapping between requirements and actual code files for validation baseline\n",
    "# Dependencies: llm_estimates_df from Cell 2, feature extraction logic\n",
    "# Breadcrumbs: Setup -> Data Retrieval -> Requirements Mapping -> Feature Analysis\n",
    "\n",
    "def analyze_requirement_features():\n",
    "    \"\"\"\n",
    "    Analyze requirements by extracting feature identifiers and establishing mappings\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (feature_requirements_df, feature_mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_feature_from_requirement(req_id):\n",
    "        \"\"\"\n",
    "        Extract feature/module name from requirement ID using common patterns\n",
    "        \n",
    "        Args:\n",
    "            req_id (str): Requirement identifier\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted feature name\n",
    "        \"\"\"\n",
    "        # Handle common requirement ID patterns\n",
    "        if 'UC' in req_id:\n",
    "            # Use case format: UC1.1 -> UC1\n",
    "            return req_id.split('.')[0]\n",
    "        elif '-' in req_id:\n",
    "            # Functional requirement format: FR-AUTH-001 -> AUTH\n",
    "            parts = req_id.split('-')\n",
    "            if len(parts) >= 2:\n",
    "                return parts[1]\n",
    "        return req_id  # Return original if no pattern matches\n",
    "\n",
    "    # Check if we have LLM estimates to analyze\n",
    "    if not llm_estimates_df.empty:\n",
    "        print(\"Analyzing requirement features and groupings...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract features from requirement IDs\n",
    "        llm_estimates_df['feature'] = llm_estimates_df['requirement_id'].apply(extract_feature_from_requirement)\n",
    "        \n",
    "        # Group requirements by feature for analysis\n",
    "        feature_requirements = llm_estimates_df.groupby('feature').agg({\n",
    "            'requirement_id': 'nunique',  # Count unique requirements\n",
    "            'final_sifp': ['mean', 'sum', 'std'],\n",
    "            'model': 'nunique'  # Count how many models estimated this feature\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names for better readability\n",
    "        feature_requirements.columns = [\n",
    "            'unique_requirements', 'avg_sifp', 'total_sifp', 'std_sifp', 'models_count'\n",
    "        ]\n",
    "        \n",
    "        # Sort by total SiFP for better insights\n",
    "        feature_requirements = feature_requirements.sort_values('total_sifp', ascending=False)\n",
    "        \n",
    "        print(\"Requirements grouped by feature:\")\n",
    "        print(feature_requirements)\n",
    "        \n",
    "        # Calculate feature statistics\n",
    "        print(f\"\\nFeature Analysis Summary:\")\n",
    "        print(f\"  Total features identified: {len(feature_requirements)}\")\n",
    "        print(f\"  Average requirements per feature: {feature_requirements['unique_requirements'].mean():.1f}\")\n",
    "        print(f\"  Average SiFP per feature: {feature_requirements['avg_sifp'].mean():.1f}\")\n",
    "        print(f\"  Most complex feature: {feature_requirements.index[0]} ({feature_requirements['total_sifp'].max():.1f} SiFP)\")\n",
    "        \n",
    "        # Create feature mapping for traceability\n",
    "        feature_mapping = {}\n",
    "        for feature in feature_requirements.index:\n",
    "            feature_reqs = llm_estimates_df[llm_estimates_df['feature'] == feature]['requirement_id'].unique()\n",
    "            feature_mapping[feature] = {\n",
    "                'requirements': list(feature_reqs),\n",
    "                'count': len(feature_reqs),\n",
    "                'estimated_loc': feature_requirements.loc[feature, 'total_sifp'] * CONFIG.get('avg_loc_per_sifp', 100)\n",
    "            }\n",
    "        \n",
    "        return feature_requirements, feature_mapping\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: No LLM estimates available for feature analysis\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "# Execute feature analysis\n",
    "if not llm_estimates_df.empty:\n",
    "    feature_requirements_df, feature_mapping = analyze_requirement_features()\n",
    "    \n",
    "    # Display insights about the mapping approach\n",
    "    print(f\"\\nRequirement-to-Code Mapping Approach:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"• Using aggregate analysis based on requirement feature groupings\")\n",
    "    print(\"• Features extracted from requirement IDs using pattern matching\")\n",
    "    print(\"• In production, explicit traceability links would provide direct mapping\")\n",
    "    print(\"• Current approach enables statistical validation at feature level\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping feature analysis - no LLM estimates available\")\n",
    "    feature_requirements_df = pd.DataFrame()\n",
    "    feature_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Calculate Normalized Metrics and Establish Conversion Baselines\n",
    "# Purpose: Establish normalized relationships between SiFP and code metrics using UFP→SiFP conversion\n",
    "# Dependencies: code_summary from Cell 1, llm_estimates_df from Cell 2, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Data Collection -> Mapping -> Baseline Establishment\n",
    "\n",
    "def calculate_normalized_metrics():\n",
    "    \"\"\"\n",
    "    Calculate normalized metrics and establish baseline relationships between SiFP and code metrics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm_analysis_df, baseline_metrics, industry_metrics)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Code Base Summary (Full Codebase):\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in code_summary.items():\n",
    "        print(f\"  {key}: {value:.0f}\")\n",
    "\n",
    "    # Calculate normalized code metrics\n",
    "    print(\"\\nNormalized Code Metrics (Full Codebase):\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Key normalized metrics\n",
    "    loc_per_file = code_summary['total_lines'] / code_summary['total_files']\n",
    "    methods_per_kloc = (code_summary['total_methods'] / code_summary['total_lines']) * 1000\n",
    "    classes_per_kloc = (code_summary['total_classes'] / code_summary['total_lines']) * 1000\n",
    "\n",
    "    print(f\"  Lines of code per file: {loc_per_file:.1f}\")\n",
    "    print(f\"  Methods per KLOC: {methods_per_kloc:.1f}\")\n",
    "    print(f\"  Classes per KLOC: {classes_per_kloc:.1f}\")\n",
    "\n",
    "    # Establish industry baselines\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE CALCULATION APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Industry standards from research\n",
    "    INDUSTRY_LOC_PER_UFP = 100  # Typical for Java\n",
    "    INDUSTRY_LOC_PER_SIFP = INDUSTRY_LOC_PER_UFP / CONFIG['CONVERSION_FACTOR']  # Adjust for conversion\n",
    "\n",
    "    industry_metrics = {\n",
    "        'LOC_PER_UFP': INDUSTRY_LOC_PER_UFP,\n",
    "        'LOC_PER_SIFP': INDUSTRY_LOC_PER_SIFP,\n",
    "        'SIFP_PER_KLOC': 1000/INDUSTRY_LOC_PER_SIFP\n",
    "    }\n",
    "\n",
    "    print(f\"\\nIndustry Baseline (Research-based):\")\n",
    "    print(f\"  Typical LOC per UFP (Java): {INDUSTRY_LOC_PER_UFP}\")\n",
    "    print(f\"  Implied LOC per SiFP: {INDUSTRY_LOC_PER_SIFP:.1f}\")\n",
    "    print(f\"  SiFP per KLOC: {industry_metrics['SIFP_PER_KLOC']:.2f}\")\n",
    "\n",
    "    # Analyze LLM estimates if available\n",
    "    if not llm_estimates_df.empty and len(ground_truth_requirements) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LLM SIFP ANALYSIS (Scaled to Estimated Requirements)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get unique requirements that were successfully estimated\n",
    "        estimated_requirements = llm_estimates_df['requirement_id'].unique()\n",
    "        estimation_coverage = len(estimated_requirements) / len(ground_truth_requirements)\n",
    "        \n",
    "        print(f\"\\nEstimation Coverage:\")\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "        print(f\"  Requirements with estimates: {len(estimated_requirements)} ({estimation_coverage:.1%})\")\n",
    "        \n",
    "        # Scale code metrics based on estimation coverage\n",
    "        scaled_code_metrics = {\n",
    "            'lines': code_summary['total_lines'] * estimation_coverage,\n",
    "            'classes': code_summary['total_classes'] * estimation_coverage,\n",
    "            'methods': code_summary['total_methods'] * estimation_coverage\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nScaled Code Metrics (for estimated requirements only):\")\n",
    "        print(f\"  Estimated lines of code: {scaled_code_metrics['lines']:.0f}\")\n",
    "        print(f\"  Estimated classes: {scaled_code_metrics['classes']:.0f}\")\n",
    "        print(f\"  Estimated methods: {scaled_code_metrics['methods']:.0f}\")\n",
    "        \n",
    "        # Calculate metrics for each LLM model\n",
    "        llm_analysis = []\n",
    "        \n",
    "        for model in sorted(llm_estimates_df['model'].unique()):\n",
    "            model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "            \n",
    "            # Calculate model totals\n",
    "            total_sifp = model_data['final_sifp'].sum()\n",
    "            total_ugep = model_data['final_ugep'].sum()\n",
    "            total_ugdg = model_data['final_ugdg'].sum()\n",
    "            successful_reqs = model_data['requirement_id'].nunique()\n",
    "            \n",
    "            # Calculate normalized metrics based on MODEL-SPECIFIC coverage\n",
    "            model_coverage = successful_reqs / len(ground_truth_requirements)\n",
    "            model_estimated_loc = code_summary['total_lines'] * model_coverage\n",
    "            \n",
    "            # Key normalized metrics\n",
    "            sifp_per_kloc = (total_sifp / model_estimated_loc) * 1000 if model_estimated_loc > 0 else 0\n",
    "            loc_per_sifp = model_estimated_loc / total_sifp if total_sifp > 0 else 0\n",
    "            sifp_per_req = total_sifp / successful_reqs if successful_reqs > 0 else 0\n",
    "            \n",
    "            # Calculate equivalent UFP for comparison\n",
    "            equivalent_ufp = total_sifp / CONFIG['CONVERSION_FACTOR']\n",
    "            \n",
    "            llm_analysis.append({\n",
    "                'model': model,\n",
    "                'successful_reqs': successful_reqs,\n",
    "                'coverage': model_coverage,\n",
    "                'total_sifp': total_sifp,\n",
    "                'equivalent_ufp': equivalent_ufp,\n",
    "                'total_ugep': total_ugep,\n",
    "                'total_ugdg': total_ugdg,\n",
    "                'estimated_loc': model_estimated_loc,\n",
    "                'sifp_per_kloc': sifp_per_kloc,\n",
    "                'loc_per_sifp': loc_per_sifp,\n",
    "                'sifp_per_req': sifp_per_req\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  Successfully estimated: {successful_reqs}/{len(ground_truth_requirements)} requirements ({model_coverage:.1%})\")\n",
    "            print(f\"  Total SiFP: {total_sifp:.1f} (equivalent to {equivalent_ufp:.1f} UFP)\")\n",
    "            print(f\"  Estimated LOC coverage: {model_estimated_loc:.0f} lines\")\n",
    "            print(f\"  SiFP per KLOC: {sifp_per_kloc:.2f}\")\n",
    "            print(f\"  LOC per SiFP point: {loc_per_sifp:.1f}\")\n",
    "            print(f\"  Deviation from industry baseline: {(loc_per_sifp - INDUSTRY_LOC_PER_SIFP)/INDUSTRY_LOC_PER_SIFP*100:+.1f}%\")\n",
    "        \n",
    "        # Create analysis DataFrame\n",
    "        llm_analysis_df = pd.DataFrame(llm_analysis)\n",
    "        \n",
    "        # Calculate project-specific baseline (weighted average of LLM estimates)\n",
    "        if not llm_analysis_df.empty:\n",
    "            weighted_loc_per_sifp = np.average(llm_analysis_df['loc_per_sifp'], \n",
    "                                              weights=llm_analysis_df['coverage'])\n",
    "            \n",
    "            baseline_metrics = {\n",
    "                'project_loc_per_sifp': weighted_loc_per_sifp,\n",
    "                'industry_loc_per_sifp': INDUSTRY_LOC_PER_SIFP,\n",
    "                'difference_pct': (weighted_loc_per_sifp - INDUSTRY_LOC_PER_SIFP)/INDUSTRY_LOC_PER_SIFP*100,\n",
    "                'estimated_requirements': estimated_requirements,\n",
    "                'scaled_code_metrics': scaled_code_metrics\n",
    "            }\n",
    "            \n",
    "            print(\"\\n\\nBASELINE COMPARISON:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"  Industry baseline LOC/SiFP: {INDUSTRY_LOC_PER_SIFP:.1f}\")\n",
    "            print(f\"  Project baseline LOC/SiFP (weighted avg): {weighted_loc_per_sifp:.1f}\")\n",
    "            print(f\"  Difference: {baseline_metrics['difference_pct']:+.1f}%\")\n",
    "            \n",
    "            return llm_analysis_df, baseline_metrics, industry_metrics\n",
    "        \n",
    "    # Return empty results if no LLM data\n",
    "    return pd.DataFrame(), {}, industry_metrics\n",
    "\n",
    "# Execute the normalized metrics calculation\n",
    "llm_analysis_df, baseline_metrics, industry_metrics = calculate_normalized_metrics()\n",
    "\n",
    "print(f\"\\n✓ Normalized metrics calculated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Detailed Normalized Performance Analysis\n",
    "# Purpose: Analyze model accuracy in normalized units (per SiFP point) with quality metrics\n",
    "# Dependencies: llm_analysis_df and baseline_metrics from Cell 4, llm_estimates_df from Cell 2  \n",
    "# Breadcrumbs: Setup -> Data Collection -> Baseline Establishment -> Performance Analysis\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Analyze detailed performance metrics for each LLM model\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance analysis with accuracy rankings\n",
    "    \"\"\"\n",
    "    \n",
    "    if llm_estimates_df.empty or llm_analysis_df.empty:\n",
    "        print(\"Warning: No LLM data available for performance analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Normalized Model Performance Analysis (Per SiFP Point)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model_performance = []\n",
    "    \n",
    "    # Use the project baseline from calculated metrics\n",
    "    baseline_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', \n",
    "                                                industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "    \n",
    "    print(f\"Using baseline: {baseline_loc_per_sifp:.1f} LOC per SiFP\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in llm_analysis_df.iterrows():\n",
    "        model = row['model']\n",
    "        \n",
    "        # Calculate normalized accuracy metrics\n",
    "        loc_per_sifp_error = row['loc_per_sifp'] - baseline_loc_per_sifp\n",
    "        loc_per_sifp_error_pct = (loc_per_sifp_error / baseline_loc_per_sifp) * 100 if baseline_loc_per_sifp > 0 else 0\n",
    "        \n",
    "        # Get quality metrics from original LLM data\n",
    "        model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "        avg_confidence = model_data['confidence'].mean() if 'confidence' in model_data.columns else 0\n",
    "        avg_judge_score = model_data['judge_score'].mean() if 'judge_score' in model_data.columns else 0\n",
    "        std_sifp = model_data['final_sifp'].std() if 'final_sifp' in model_data.columns else 0\n",
    "        \n",
    "        # Calculate success rate\n",
    "        success_rate = row['successful_reqs'] / len(ground_truth_requirements) if len(ground_truth_requirements) > 0 else 0\n",
    "        \n",
    "        model_performance.append({\n",
    "            'Model': model,\n",
    "            'Success_Rate': success_rate,\n",
    "            'SiFP_per_KLOC': row['sifp_per_kloc'],\n",
    "            'LOC_per_SiFP': row['loc_per_sifp'],\n",
    "            'LOC_per_SiFP_Error': loc_per_sifp_error,\n",
    "            'Error_Pct': abs(loc_per_sifp_error_pct),\n",
    "            'Avg_SiFP_per_Req': row['sifp_per_req'],\n",
    "            'Std_SiFP': std_sifp,\n",
    "            'Avg_Confidence': avg_confidence,\n",
    "            'Avg_Judge_Score': avg_judge_score\n",
    "        })\n",
    "        \n",
    "        # Display individual model analysis\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  SiFP per KLOC: {row['sifp_per_kloc']:.2f}\")\n",
    "        print(f\"  LOC per SiFP point: {row['loc_per_sifp']:.1f}\")\n",
    "        print(f\"  Error vs baseline: {loc_per_sifp_error:+.1f} LOC/SiFP ({loc_per_sifp_error_pct:+.1f}%)\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.2%}\")\n",
    "        print(f\"  Average judge score: {avg_judge_score:.2f}/5\")\n",
    "        print(f\"  SiFP variability (std): {std_sifp:.2f}\")\n",
    "    \n",
    "    # Create performance DataFrame\n",
    "    performance_df = pd.DataFrame(model_performance)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        # Rank models by normalized accuracy (lower error is better)\n",
    "        performance_df['Accuracy_Rank'] = performance_df['Error_Pct'].rank()\n",
    "        \n",
    "        print(\"\\n\\nNormalized Performance Summary:\")\n",
    "        print(\"=\" * 60)\n",
    "        summary_cols = ['Model', 'Success_Rate', 'LOC_per_SiFP', 'Error_Pct', 'Accuracy_Rank']\n",
    "        print(performance_df[summary_cols].round(3).to_string(index=False))\n",
    "        \n",
    "        # Additional insights\n",
    "        best_accuracy = performance_df.loc[performance_df['Error_Pct'].idxmin()]\n",
    "        best_coverage = performance_df.loc[performance_df['Success_Rate'].idxmax()]\n",
    "        \n",
    "        print(f\"\\nKey Insights:\")\n",
    "        print(f\"  Most accurate model: {best_accuracy['Model']} ({best_accuracy['Error_Pct']:.1f}% error)\")\n",
    "        print(f\"  Best coverage model: {best_coverage['Model']} ({best_coverage['Success_Rate']:.1%} success rate)\")\n",
    "        print(f\"  Average error across all models: {performance_df['Error_Pct'].mean():.1f}%\")\n",
    "        \n",
    "    return performance_df\n",
    "\n",
    "# Execute performance analysis\n",
    "performance_df = analyze_model_performance()\n",
    "\n",
    "print(f\"\\n✓ Performance analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Load Desharnais Dataset and Establish UFP→SiFP→Effort Relationships  \n",
    "# Purpose: Load industry benchmark dataset and establish the complete conversion chain for effort estimation\n",
    "# Dependencies: sklearn LinearRegression, CONFIG from Cell 0, pandas processing\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Industry Benchmarks -> Effort Conversion Chain\n",
    "\n",
    "def load_and_analyze_desharnais():\n",
    "    \"\"\"\n",
    "    Load Desharnais dataset and establish UFP→SiFP→Effort conversion relationships\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (desharnais_df, effort_metrics, effort_model)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load industry benchmark dataset\n",
    "        desharnais_df = pd.read_csv('../datasets/CostEstimation/Desharnais.csv')\n",
    "        print(f\"✓ Loaded Desharnais dataset: {desharnais_df.shape[0]} projects\")\n",
    "\n",
    "        # Identify column names (handle variations in dataset)\n",
    "        ufp_column = 'PointsNonAdjust' if 'PointsNonAdjust' in desharnais_df.columns else 'UFP'\n",
    "        effort_column = 'Effort' if 'Effort' in desharnais_df.columns else 'effort'\n",
    "\n",
    "        print(f\"Using columns: UFP='{ufp_column}', Effort='{effort_column}'\")\n",
    "        \n",
    "        # Apply UFP to SiFP conversion using research-validated factor\n",
    "        print(f\"\\nApplying UFP→SiFP conversion factor: {CONFIG['CONVERSION_FACTOR']}\")\n",
    "        desharnais_df['SiFP_converted'] = desharnais_df[ufp_column] * CONFIG['CONVERSION_FACTOR']\n",
    "\n",
    "        # Calculate effort per SiFP metrics\n",
    "        print(\"\\nDesharnais Normalized Metrics:\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Calculate hours per SiFP point for each project\n",
    "        desharnais_df['hours_per_sifp'] = desharnais_df[effort_column] / desharnais_df['SiFP_converted']\n",
    "\n",
    "        # Calculate summary statistics\n",
    "        effort_metrics = {\n",
    "            'avg_hours_per_sifp': desharnais_df['hours_per_sifp'].mean(),\n",
    "            'median_hours_per_sifp': desharnais_df['hours_per_sifp'].median(),\n",
    "            'std_hours_per_sifp': desharnais_df['hours_per_sifp'].std(),\n",
    "            'min_hours_per_sifp': desharnais_df['hours_per_sifp'].min(),\n",
    "            'max_hours_per_sifp': desharnais_df['hours_per_sifp'].max()\n",
    "        }\n",
    "\n",
    "        print(f\"  Average hours per SiFP: {effort_metrics['avg_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Median hours per SiFP: {effort_metrics['median_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Std dev hours per SiFP: {effort_metrics['std_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Range: {effort_metrics['min_hours_per_sifp']:.2f} - {effort_metrics['max_hours_per_sifp']:.2f}\")\n",
    "\n",
    "        # Build linear effort prediction model\n",
    "        print(f\"\\nBuilding Linear Effort Model:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Prepare data for sklearn\n",
    "        X = desharnais_df[['SiFP_converted']].values.astype(np.float64)\n",
    "        y = desharnais_df[effort_column].values.astype(np.float64)\n",
    "\n",
    "        # Fit linear regression model\n",
    "        effort_model = LinearRegression()\n",
    "        effort_model.fit(X, y)\n",
    "\n",
    "        # Extract model coefficients\n",
    "        linear_hours_per_sifp = float(effort_model.coef_[0])\n",
    "        intercept = float(effort_model.intercept_)\n",
    "        \n",
    "        # Calculate model performance\n",
    "        y_pred = effort_model.predict(X)\n",
    "        r2 = float(r2_score(y, y_pred))\n",
    "\n",
    "        print(f\"  Hours per SiFP (coefficient): {linear_hours_per_sifp:.2f}\")\n",
    "        print(f\"  Base hours (intercept): {intercept:.2f}\")\n",
    "        print(f\"  R² score: {r2:.3f}\")\n",
    "        \n",
    "        # Add model metrics to effort_metrics\n",
    "        effort_metrics.update({\n",
    "            'linear_hours_per_sifp': linear_hours_per_sifp,\n",
    "            'intercept': intercept,\n",
    "            'r2_score': r2\n",
    "        })\n",
    "\n",
    "        # Analyze SiFP distribution in industry data\n",
    "        print(f\"\\nSiFP Distribution in Desharnais Dataset:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Mean SiFP per project: {desharnais_df['SiFP_converted'].mean():.1f}\")\n",
    "        print(f\"  Median SiFP per project: {desharnais_df['SiFP_converted'].median():.1f}\")\n",
    "        print(f\"  Range: {desharnais_df['SiFP_converted'].min():.1f} - {desharnais_df['SiFP_converted'].max():.1f}\")\n",
    "        print(f\"  Total projects: {len(desharnais_df)}\")\n",
    "\n",
    "        return desharnais_df, effort_metrics, effort_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Desharnais dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute Desharnais analysis\n",
    "desharnais_df, effort_metrics, effort_model = load_and_analyze_desharnais()\n",
    "\n",
    "print(f\"\\n✓ Desharnais dataset analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Normalized Effort Impact Analysis with Complete Conversion Chain\n",
    "# Purpose: Analyze effort impact using UFP→SiFP→Effort conversion chain and calculate cost implications  \n",
    "# Dependencies: performance_df from Cell 5, effort_metrics from Cell 6, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Industry Benchmarks -> Effort Impact Analysis\n",
    "\n",
    "def analyze_effort_impact():\n",
    "    \"\"\"\n",
    "    Analyze effort impact using the complete UFP→SiFP→Effort conversion chain\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Effort impact analysis with cost implications\n",
    "    \"\"\"\n",
    "    \n",
    "    if performance_df.empty or not effort_metrics:\n",
    "        print(\"Warning: Missing required data for effort impact analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Normalized Effort Impact Analysis (Per SiFP Point)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display conversion chain information\n",
    "    print(f\"\\nConversion Chain:\")\n",
    "    print(f\"  UFP → SiFP: Factor = {CONFIG['CONVERSION_FACTOR']} (SiFP = {CONFIG['CONVERSION_FACTOR']} × UFP)\")\n",
    "    print(f\"  SiFP → Effort: {effort_metrics['avg_hours_per_sifp']:.2f} hours/SiFP (from Desharnais)\")\n",
    "    \n",
    "    if baseline_metrics:\n",
    "        print(f\"  SiFP → LOC: {baseline_metrics['project_loc_per_sifp']:.1f} LOC/SiFP (project baseline)\")\n",
    "    \n",
    "    effort_impact = []\n",
    "    \n",
    "    for _, row in performance_df.iterrows():\n",
    "        model = row['Model']\n",
    "        \n",
    "        # Get model's LOC per SiFP\n",
    "        model_loc_per_sifp = row['LOC_per_SiFP']\n",
    "        baseline_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', \n",
    "                                                   industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "        \n",
    "        # Calculate SiFP estimation accuracy\n",
    "        # If model estimates fewer LOC per SiFP, it's overestimating SiFP count\n",
    "        sifp_estimation_factor = baseline_loc_per_sifp / model_loc_per_sifp if model_loc_per_sifp > 0 else 1\n",
    "        \n",
    "        # Calculate effort impact using Desharnais baseline\n",
    "        baseline_hours_per_sifp = effort_metrics['avg_hours_per_sifp']\n",
    "        \n",
    "        # The effective hours per estimated SiFP\n",
    "        effective_hours_per_estimated_sifp = baseline_hours_per_sifp / sifp_estimation_factor\n",
    "        \n",
    "        # Calculate percentage error in effort estimation\n",
    "        effort_error_pct = (sifp_estimation_factor - 1) * 100\n",
    "        \n",
    "        # Get total SiFP estimated by this model\n",
    "        if not llm_analysis_df.empty:\n",
    "            model_row = llm_analysis_df[llm_analysis_df['model'] == model]\n",
    "            if not model_row.empty:\n",
    "                model_total_sifp = model_row['total_sifp'].values[0]\n",
    "                actual_sifp = model_total_sifp / sifp_estimation_factor\n",
    "                \n",
    "                # Calculate total effort impact\n",
    "                estimated_total_effort = model_total_sifp * baseline_hours_per_sifp\n",
    "                actual_total_effort = actual_sifp * baseline_hours_per_sifp\n",
    "                total_effort_error = estimated_total_effort - actual_total_effort\n",
    "                \n",
    "                # Calculate cost impact using standard rate\n",
    "                total_cost_impact = total_effort_error * CONFIG['COST_PER_HOUR']\n",
    "            else:\n",
    "                model_total_sifp = actual_sifp = total_effort_error = total_cost_impact = 0\n",
    "        else:\n",
    "            model_total_sifp = actual_sifp = total_effort_error = total_cost_impact = 0\n",
    "        \n",
    "        effort_impact.append({\n",
    "            'Model': model,\n",
    "            'LOC_per_SiFP': model_loc_per_sifp,\n",
    "            'SiFP_Estimation_Factor': sifp_estimation_factor,\n",
    "            'Desharnais_Hours_per_SiFP': baseline_hours_per_sifp,\n",
    "            'Effective_Hours_per_Est_SiFP': effective_hours_per_estimated_sifp,\n",
    "            'Effort_Error_Pct': effort_error_pct,\n",
    "            'Model_Total_SiFP': model_total_sifp,\n",
    "            'Actual_SiFP': actual_sifp,\n",
    "            'Total_Effort_Error_Hours': total_effort_error,\n",
    "            'Total_Cost_Impact_USD': total_cost_impact\n",
    "        })\n",
    "        \n",
    "        # Display model-specific analysis\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  LOC per SiFP: {model_loc_per_sifp:.1f} (baseline: {baseline_loc_per_sifp:.1f})\")\n",
    "        print(f\"  SiFP estimation factor: {sifp_estimation_factor:.2f}x\")\n",
    "        print(f\"  Interpretation: Model {'overestimates' if sifp_estimation_factor > 1 else 'underestimates'} SiFP count\")\n",
    "        print(f\"  Desharnais baseline: {baseline_hours_per_sifp:.2f} hours per actual SiFP\")\n",
    "        print(f\"  Effective hours per estimated SiFP: {effective_hours_per_estimated_sifp:.2f}\")\n",
    "        print(f\"  Effort estimation error: {effort_error_pct:+.1f}%\")\n",
    "        if model_total_sifp > 0:\n",
    "            print(f\"  Total SiFP estimated: {model_total_sifp:.0f}\")\n",
    "            print(f\"  Actual SiFP (implied): {actual_sifp:.0f}\")\n",
    "            print(f\"  Total effort error: {total_effort_error:+.0f} hours (${total_cost_impact:+,.0f})\")\n",
    "    \n",
    "    effort_impact_df = pd.DataFrame(effort_impact)\n",
    "    \n",
    "    if not effort_impact_df.empty:\n",
    "        # Summary statistics\n",
    "        print(\"\\n\\nEffort Impact Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"  Desharnais hours per SiFP: {effort_metrics['avg_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Average SiFP estimation factor: {effort_impact_df['SiFP_Estimation_Factor'].mean():.2f}x\")\n",
    "        print(f\"  Average effort error: {effort_impact_df['Effort_Error_Pct'].mean():+.1f}%\")\n",
    "        print(f\"  Total cost impact range: ${effort_impact_df['Total_Cost_Impact_USD'].min():,.0f} to ${effort_impact_df['Total_Cost_Impact_USD'].max():,.0f}\")\n",
    "        \n",
    "        if effort_impact_df['Effort_Error_Pct'].abs().size > 0:\n",
    "            best_model = effort_impact_df.loc[effort_impact_df['Effort_Error_Pct'].abs().idxmin(), 'Model']\n",
    "            print(f\"  Most accurate effort model: {best_model}\")\n",
    "    \n",
    "    return effort_impact_df\n",
    "\n",
    "# Execute effort impact analysis\n",
    "effort_impact_df = analyze_effort_impact()\n",
    "\n",
    "print(f\"\\n✓ Effort impact analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Comprehensive Visualization of Normalized Results and Distributions\n",
    "# Purpose: Create comprehensive visualizations of model performance, accuracy distributions, and cost impacts\n",
    "# Dependencies: performance_df from Cell 5, effort_impact_df from Cell 7, matplotlib/seaborn from Cell 0\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Effort Impact -> Comprehensive Visualization\n",
    "\n",
    "if 'performance_df' in globals() and 'effort_impact_df' in globals() and not performance_df.empty and not effort_impact_df.empty:\n",
    "    \n",
    "    # Get baseline values from previous calculations\n",
    "    avg_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10)\n",
    "    project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "    \n",
    "    # Create a larger figure with more subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Define grid for subplots\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    models = performance_df['Model'].values\n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    # 1. LOC per SiFP Point Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    bars = ax1.bar(x, performance_df['LOC_per_SiFP'], alpha=0.7, color='skyblue')\n",
    "    ax1.axhline(y=avg_loc_per_sifp, color='red', linestyle='--', \n",
    "                label=f'Baseline ({avg_loc_per_sifp:.1f} LOC/SiFP)')\n",
    "    \n",
    "    # Color bars based on performance\n",
    "    for i, bar in enumerate(bars):\n",
    "        if performance_df.iloc[i]['LOC_per_SiFP'] < avg_loc_per_sifp * 0.8:\n",
    "            bar.set_color('green')\n",
    "        elif performance_df.iloc[i]['LOC_per_SiFP'] > avg_loc_per_sifp * 1.2:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Lines of Code per SiFP Point')\n",
    "    ax1.set_title('Code Density per SiFP Point by Model')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add success rate and error annotations\n",
    "    for i, (model, success_rate, error) in enumerate(zip(models, performance_df['Success_Rate'], performance_df['Error_Pct'])):\n",
    "        ax1.text(i, performance_df.iloc[i]['LOC_per_SiFP'] + 1, \n",
    "                f'{success_rate:.0%}\\n±{error:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 2. Model Performance Comparison Table\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Create performance summary table\n",
    "    table_data = []\n",
    "    for _, row in performance_df.iterrows():\n",
    "        model_name = row['Model'].split('/')[-1][:20]\n",
    "        table_data.append([\n",
    "            model_name,\n",
    "            f\"{row['Success_Rate']:.0%}\",\n",
    "            f\"{row['LOC_per_SiFP']:.1f}\",\n",
    "            f\"{row['Error_Pct']:.0f}%\"\n",
    "        ])\n",
    "    \n",
    "    table = ax2.table(cellText=table_data,\n",
    "                     colLabels=['Model', 'Success', 'LOC/SiFP', 'Error'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    ax2.set_title('Model Performance Summary', pad=20)\n",
    "    \n",
    "    # 3. Effort per SiFP Point\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    bars = ax3.bar(x, effort_impact_df['Effective_Hours_per_Est_SiFP'], \n",
    "            color=['green' if x < desharnais_hours_per_sifp else 'orange' \n",
    "                   for x in effort_impact_df['Effective_Hours_per_Est_SiFP']], alpha=0.7)\n",
    "    ax3.axhline(y=desharnais_hours_per_sifp, color='red', linestyle='--', \n",
    "                label=f'Desharnais Baseline ({desharnais_hours_per_sifp:.1f} hrs/SiFP)')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Effective Hours per Estimated SiFP')\n",
    "    ax3.set_title('Effort Estimation per SiFP Point')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. SiFP per KLOC\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.bar(x, performance_df['SiFP_per_KLOC'], alpha=0.7, color='coral')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('SiFP per KLOC')\n",
    "    ax4.set_title('Function Point Density (SiFP per 1000 LOC)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Total Cost Impact\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    bars = ax5.bar(x, effort_impact_df['Total_Cost_Impact_USD'], \n",
    "                   color=['darkgreen' if x < 0 else 'darkred' \n",
    "                          for x in effort_impact_df['Total_Cost_Impact_USD']], alpha=0.7)\n",
    "    ax5.set_xlabel('Model')\n",
    "    ax5.set_ylabel('Total Cost Impact ($)')\n",
    "    ax5.set_title('Total Cost Impact')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax5.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(effort_impact_df['Total_Cost_Impact_USD']):\n",
    "        ax5.text(i, v + (1000 if v > 0 else -1000), f'${v:,.0f}', \n",
    "                ha='center', va='bottom' if v > 0 else 'top', fontsize=8)\n",
    "    \n",
    "    # 6-10. Histograms for each model showing requirement-level accuracy\n",
    "    histogram_count = 0\n",
    "    max_histograms = 6  # Limit to 6 histograms to fit in remaining subplot space\n",
    "    \n",
    "    for idx, model in enumerate(models[:max_histograms]):\n",
    "        row_idx = 2 + histogram_count // 3\n",
    "        col_idx = histogram_count % 3\n",
    "        \n",
    "        if row_idx >= 4:  # Don't exceed our grid\n",
    "            break\n",
    "            \n",
    "        ax = fig.add_subplot(gs[row_idx, col_idx])\n",
    "        \n",
    "        model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            model_coverage = model_data['requirement_id'].nunique() / len(ground_truth_requirements)\n",
    "            \n",
    "            # Calculate LOC per SiFP for each requirement\n",
    "            req_loc_per_sifp = []\n",
    "            for _, req in model_data.iterrows():\n",
    "                if req['final_sifp'] > 0:\n",
    "                    est_loc_per_req = (code_summary['total_lines'] * model_coverage) / model_data['requirement_id'].nunique()\n",
    "                    loc_per_sifp = est_loc_per_req / req['final_sifp']\n",
    "                    req_loc_per_sifp.append(loc_per_sifp)\n",
    "            \n",
    "            if req_loc_per_sifp:\n",
    "                # Create histogram\n",
    "                n, bins, patches = ax.hist(req_loc_per_sifp, bins=min(15, len(req_loc_per_sifp)), \n",
    "                                         alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                \n",
    "                # Color code bins\n",
    "                for i, patch in enumerate(patches):\n",
    "                    if i < len(bins) - 1:  # bins has one more element than patches\n",
    "                        if bins[i] < avg_loc_per_sifp * 0.8:\n",
    "                            patch.set_facecolor('green')\n",
    "                        elif bins[i] > avg_loc_per_sifp * 1.2:\n",
    "                            patch.set_facecolor('red')\n",
    "                \n",
    "                # Add baseline line\n",
    "                ax.axvline(x=avg_loc_per_sifp, color='red', linestyle='--', linewidth=2, \n",
    "                          label=f'Baseline: {avg_loc_per_sifp:.1f}')\n",
    "                ax.axvline(x=np.mean(req_loc_per_sifp), color='blue', linestyle='-', linewidth=2,\n",
    "                          label=f'Model mean: {np.mean(req_loc_per_sifp):.1f}')\n",
    "                \n",
    "                ax.set_xlabel('LOC per SiFP')\n",
    "                ax.set_ylabel('# Requirements')\n",
    "                ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nAccuracy Distribution')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics text\n",
    "                ax.text(0.95, 0.95, f'n={len(req_loc_per_sifp)}\\nσ={np.std(req_loc_per_sifp):.1f}',\n",
    "                       transform=ax.transAxes, ha='right', va='top', fontsize=8,\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No valid data', transform=ax.transAxes, ha='center', va='center')\n",
    "                ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nNo Data')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No model data', transform=ax.transAxes, ha='center', va='center')\n",
    "            ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nNo Data')\n",
    "        \n",
    "        histogram_count += 1\n",
    "    \n",
    "    plt.suptitle(f'Comprehensive SiFP Analysis - {project_name}\\n'\n",
    "                 f'All Models Performance and Accuracy Distribution', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Comprehensive visualization completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create comprehensive visualization - required data not available\")\n",
    "    print(\"Available variables:\")\n",
    "    if 'performance_df' in globals():\n",
    "        print(f\"  - performance_df: {len(performance_df) if not performance_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - performance_df: not defined\")\n",
    "    \n",
    "    if 'effort_impact_df' in globals():\n",
    "        print(f\"  - effort_impact_df: {len(effort_impact_df) if not effort_impact_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - effort_impact_df: not defined\")\n",
    "    \n",
    "    if 'baseline_metrics' in globals():\n",
    "        print(\"  - baseline_metrics: available\")\n",
    "    else:\n",
    "        print(\"  - baseline_metrics: not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Executive Summary with Complete UFP→SiFP→LOC→Effort Analysis  \n",
    "# Purpose: Generate comprehensive executive summary with complete conversion chain analysis and business insights\n",
    "# Dependencies: All previous analysis results, CONFIG settings, comprehensive metrics from entire workflow\n",
    "# Breadcrumbs: Setup -> Analysis -> Recommendations -> Executive Summary & Business Impact Report\n",
    "\n",
    "print(\"EXECUTIVE SUMMARY - NORMALIZED SIFP ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get project name safely\n",
    "project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project') if 'CONFIG' in globals() else 'Unknown Project'\n",
    "print(f\"Project: {project_name}\")\n",
    "print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if 'performance_df' in globals() and 'effort_impact_df' in globals() and not performance_df.empty and not effort_impact_df.empty:\n",
    "    print(f\"\\nData Summary:\")\n",
    "    \n",
    "    if 'code_metrics_df' in globals():\n",
    "        print(f\"  Total code files in project: {len(code_metrics_df)}\")\n",
    "        print(f\"  Total lines of code in project: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "    else:\n",
    "        print(\"  Code metrics: Not available\")\n",
    "    \n",
    "    if 'ground_truth_requirements' in globals():\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "    else:\n",
    "        print(\"  Ground truth requirements: Not available\")\n",
    "    \n",
    "    if 'llm_estimates_df' in globals() and not llm_estimates_df.empty:\n",
    "        estimated_requirements = llm_estimates_df['requirement_id'].unique()\n",
    "        if 'ground_truth_requirements' in globals():\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)} ({len(estimated_requirements)/len(ground_truth_requirements):.1%})\")\n",
    "        else:\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)}\")\n",
    "    \n",
    "    # Get baseline values safely\n",
    "    conversion_factor = CONFIG.get('CONVERSION_FACTOR', 0.957) if 'CONFIG' in globals() else 0.957\n",
    "    \n",
    "    # Get industry and project baselines\n",
    "    industry_loc_per_sifp = industry_metrics.get('LOC_PER_SIFP', 100) if 'industry_metrics' in globals() else 100\n",
    "    project_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_loc_per_sifp) if 'baseline_metrics' in globals() else industry_loc_per_sifp\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10) if 'effort_metrics' in globals() else 10\n",
    "    \n",
    "    print(f\"\\nConversion Factors and Baselines:\")\n",
    "    print(f\"  UFP → SiFP: {conversion_factor} (from Desharnais research)\")\n",
    "    print(f\"  SiFP → Effort: {desharnais_hours_per_sifp:.2f} hours/SiFP (Desharnais dataset)\")\n",
    "    print(f\"  SiFP → LOC: {project_loc_per_sifp:.1f} LOC/SiFP (project weighted average)\")\n",
    "    print(f\"  Industry baseline: {industry_loc_per_sifp:.1f} LOC/SiFP\")\n",
    "    print(f\"  Project vs Industry: {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}%\")\n",
    "    \n",
    "    # Detailed performance for each model\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED MODEL PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, row in performance_df.iterrows():\n",
    "        model = row['Model']\n",
    "        effort_row = effort_impact_df[effort_impact_df['Model'] == model]\n",
    "        \n",
    "        if not effort_row.empty:\n",
    "            effort_row = effort_row.iloc[0]\n",
    "            \n",
    "            if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "                llm_row = llm_analysis_df[llm_analysis_df['model'] == model]\n",
    "                if not llm_row.empty:\n",
    "                    llm_row = llm_row.iloc[0]\n",
    "                else:\n",
    "                    llm_row = None\n",
    "            else:\n",
    "                llm_row = None\n",
    "            \n",
    "            print(f\"\\n{idx+1}. {model}\")\n",
    "            print(\"-\" * len(f\"{idx+1}. {model}\"))\n",
    "            \n",
    "            print(f\"\\n  Estimation Coverage:\")\n",
    "            print(f\"    - Success rate: {row['Success_Rate']:.1%}\")\n",
    "            if llm_row is not None:\n",
    "                print(f\"    - Requirements estimated: {llm_row['successful_reqs']}/{len(ground_truth_requirements) if 'ground_truth_requirements' in globals() else 'unknown'}\")\n",
    "            \n",
    "            print(f\"\\n  SiFP Estimates:\")\n",
    "            if llm_row is not None:\n",
    "                print(f\"    - Total SiFP: {llm_row['total_sifp']:.0f}\")\n",
    "                print(f\"    - Equivalent UFP: {llm_row['equivalent_ufp']:.0f}\")\n",
    "                print(f\"    - Average SiFP per requirement: {row['Avg_SiFP_per_Req']:.1f}\")\n",
    "            else:\n",
    "                print(f\"    - Average SiFP per requirement: {row.get('Avg_SiFP_per_Req', 'N/A')}\")\n",
    "            \n",
    "            print(f\"\\n  Accuracy Metrics:\")\n",
    "            print(f\"    - LOC per SiFP: {row['LOC_per_SiFP']:.1f} (baseline: {project_loc_per_sifp:.1f})\")\n",
    "            print(f\"    - Error: {row.get('LOC_per_SiFP_Error', 'N/A'):+.1f} LOC/SiFP ({row['Error_Pct']:+.1f}%)\")\n",
    "            print(f\"    - SiFP estimation factor: {effort_row['SiFP_Estimation_Factor']:.2f}x\")\n",
    "            \n",
    "            print(f\"\\n  Effort Impact:\")\n",
    "            print(f\"    - Desharnais baseline: {desharnais_hours_per_sifp:.2f} hours/SiFP\")\n",
    "            print(f\"    - Effort estimation error: {effort_row['Effort_Error_Pct']:+.1f}%\")\n",
    "            print(f\"    - Total effort error: {effort_row['Total_Effort_Error_Hours']:+.0f} hours\")\n",
    "            print(f\"    - Cost impact: ${effort_row['Total_Cost_Impact_USD']:+,.0f}\")\n",
    "            \n",
    "            print(f\"\\n  Quality Indicators:\")\n",
    "            print(f\"    - Average confidence: {row.get('Avg_Confidence', 0):.1%}\")\n",
    "            print(f\"    - Average judge score: {row.get('Avg_Judge_Score', 0):.2f}/5\")\n",
    "    \n",
    "    # Analysis of the conversion chain\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"CONVERSION CHAIN ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nFor a typical requirement in this project:\")\n",
    "    if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "        avg_sifp_per_req = llm_analysis_df['sifp_per_req'].mean()\n",
    "        print(f\"  Average SiFP per requirement: {avg_sifp_per_req:.1f}\")\n",
    "        print(f\"  Equivalent UFP: {avg_sifp_per_req / conversion_factor:.1f}\")\n",
    "        print(f\"  Expected LOC: {avg_sifp_per_req * project_loc_per_sifp:.0f}\")\n",
    "        print(f\"  Expected effort: {avg_sifp_per_req * desharnais_hours_per_sifp:.0f} hours\")\n",
    "    else:\n",
    "        print(\"  Analysis not available - LLM analysis data missing\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        best_accuracy = performance_df.loc[performance_df['Error_Pct'].idxmin()]['Model']\n",
    "        print(f\"\\n1. For most accurate code size estimation: {best_accuracy}\")\n",
    "    else:\n",
    "        print(\"\\n1. For most accurate code size estimation: Data not available\")\n",
    "    \n",
    "    if not effort_impact_df.empty:\n",
    "        best_effort = effort_impact_df.loc[effort_impact_df['Effort_Error_Pct'].abs().idxmin()]['Model']\n",
    "        print(f\"2. For most accurate effort estimation: {best_effort}\")\n",
    "    else:\n",
    "        print(\"2. For most accurate effort estimation: Data not available\")\n",
    "    \n",
    "    print(f\"3. Use Desharnais baseline of {desharnais_hours_per_sifp:.1f} hours per SiFP for effort planning\")\n",
    "    print(f\"4. Apply UFP conversion factor of {conversion_factor} when comparing to UFP-based estimates\")\n",
    "    print(f\"5. Consider that this project has {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}% different LOC/SiFP than industry average\")\n",
    "    \n",
    "    # Save results with all conversion factors\n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Create comprehensive summary\n",
    "        conversion_summary = pd.DataFrame({\n",
    "            'Metric': ['UFP→SiFP Factor', 'Industry LOC/SiFP', 'Project LOC/SiFP', 'Desharnais Hours/SiFP'],\n",
    "            'Value': [conversion_factor, industry_loc_per_sifp, project_loc_per_sifp, desharnais_hours_per_sifp]\n",
    "        })\n",
    "        conversion_summary.to_csv(f'results/conversion_factors_{project_name}.csv', index=False)\n",
    "        \n",
    "        # Save all other results\n",
    "        performance_df.to_csv(f'results/normalized_performance_{project_name}.csv', index=False)\n",
    "        effort_impact_df.to_csv(f'results/normalized_effort_impact_{project_name}.csv', index=False)\n",
    "        \n",
    "        if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "            llm_analysis_df.to_csv(f'results/normalized_llm_analysis_{project_name}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Results saved to results/ directory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not save results - {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Missing required data for executive summary\")\n",
    "    print(\"Available data:\")\n",
    "    \n",
    "    if 'performance_df' in globals():\n",
    "        print(f\"  - performance_df: {len(performance_df) if not performance_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - performance_df: not available\")\n",
    "    \n",
    "    if 'effort_impact_df' in globals():\n",
    "        print(f\"  - effort_impact_df: {len(effort_impact_df) if not effort_impact_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - effort_impact_df: not available\")\n",
    "    \n",
    "    if 'llm_analysis_df' in globals():\n",
    "        print(f\"  - llm_analysis_df: {len(llm_analysis_df) if not llm_analysis_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - llm_analysis_df: not available\")\n",
    "\n",
    "print(f\"\\n✓ Executive summary completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Statistical Hypothesis Testing for Estimation Accuracy vs Ground Truth (CORRECTED FP/FN CLASSIFICATION)\n",
    "# Purpose: Test whether hallucination-reducing techniques improve estimation accuracy by reducing FP/FN rates\n",
    "# Dependencies: performance_df, llm_estimates_df, code_summary from previous cells\n",
    "# Breadcrumbs: Setup -> Analysis -> Executive Summary -> Statistical Accuracy Testing vs Ground Truth\n",
    "\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, fisher_exact\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def perform_accuracy_hypothesis_testing():\n",
    "    \"\"\"\n",
    "    Perform statistical hypothesis testing comparing estimation accuracy against ground truth\n",
    "    Tests whether hallucination-reducing techniques improve accuracy by reducing FP/FN rates\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistical test results for accuracy vs ground truth\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"STATISTICAL HYPOTHESIS TESTING - ESTIMATION ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques significantly improve\")\n",
    "    print(\"estimation accuracy by >30% through reduced False Positives and False Negatives\")\n",
    "    print(\"\\nOPERATIONAL DEFINITIONS:\")\n",
    "    print(\"- Ground Truth: Actual iTrust codebase metrics scaled to estimated requirements\")\n",
    "    print(\"- True Positive (TP): Estimate within ±20% of ground truth\")\n",
    "    print(\"- False Positive (FP): Overestimate >20% (too few LOC per SiFP = more SiFP estimated)\")\n",
    "    print(\"- False Negative (FN): Underestimate >20% (too many LOC per SiFP = fewer SiFP estimated)\")\n",
    "    print(\"- Hallucination-reducing: Models with judge scores above median\")\n",
    "    \n",
    "    if llm_estimates_df.empty or not baseline_metrics:\n",
    "        print(\"\\nWarning: Insufficient data for accuracy testing\")\n",
    "        return {}\n",
    "    \n",
    "    # Establish ground truth baseline from actual codebase\n",
    "    ground_truth_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', \n",
    "                                                   industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "    \n",
    "    print(f\"\\nGROUND TRUTH BASELINE:\")\n",
    "    print(f\"  Actual codebase LOC per SiFP: {ground_truth_loc_per_sifp:.1f}\")\n",
    "    print(f\"  Acceptable accuracy range: ±20% ({ground_truth_loc_per_sifp*0.8:.1f} - {ground_truth_loc_per_sifp*1.2:.1f})\")\n",
    "    \n",
    "    # Calculate accuracy classifications for each estimate\n",
    "    accuracy_results = []\n",
    "    \n",
    "    for _, row in llm_estimates_df.iterrows():\n",
    "        requirement_id = row['requirement_id']\n",
    "        model = row['model']\n",
    "        final_sifp = row['final_sifp']\n",
    "        judge_score = row.get('judge_score', 0)\n",
    "        \n",
    "        # Calculate estimated LOC per SiFP for this requirement\n",
    "        # Assume uniform distribution of LOC across estimated requirements\n",
    "        if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "            model_data = llm_analysis_df[llm_analysis_df['model'] == model]\n",
    "            if not model_data.empty:\n",
    "                model_loc_per_sifp = model_data['loc_per_sifp'].iloc[0]\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # CORRECTED: Classify estimate accuracy with proper FP/FN definitions\n",
    "        accuracy_ratio = model_loc_per_sifp / ground_truth_loc_per_sifp\n",
    "        \n",
    "        if 0.8 <= accuracy_ratio <= 1.2:\n",
    "            classification = 'TP'  # True Positive - within ±20%\n",
    "        elif accuracy_ratio > 1.2:\n",
    "            # Model thinks each SiFP requires MORE LOC than reality\n",
    "            # For fixed LOC, this means FEWER SiFP estimated = UNDERESTIMATE\n",
    "            classification = 'FN'  # False Negative - underestimate (fewer SiFP than should be)\n",
    "        else:\n",
    "            # Model thinks each SiFP requires LESS LOC than reality  \n",
    "            # For fixed LOC, this means MORE SiFP estimated = OVERESTIMATE\n",
    "            classification = 'FP'  # False Positive - overestimate (more SiFP than should be)\n",
    "        \n",
    "        # Determine if model uses hallucination-reducing techniques\n",
    "        median_judge_score = llm_estimates_df['judge_score'].median()\n",
    "        hallucination_reducing = judge_score > median_judge_score\n",
    "        \n",
    "        accuracy_results.append({\n",
    "            'requirement_id': requirement_id,\n",
    "            'model': model,\n",
    "            'final_sifp': final_sifp,\n",
    "            'judge_score': judge_score,\n",
    "            'model_loc_per_sifp': model_loc_per_sifp,\n",
    "            'accuracy_ratio': accuracy_ratio,\n",
    "            'classification': classification,\n",
    "            'hallucination_reducing': hallucination_reducing,\n",
    "            'accuracy_error_pct': abs((accuracy_ratio - 1.0) * 100)\n",
    "        })\n",
    "    \n",
    "    if not accuracy_results:\n",
    "        print(\"Warning: No accuracy results to analyze\")\n",
    "        return {}\n",
    "    \n",
    "    accuracy_df = pd.DataFrame(accuracy_results)\n",
    "    \n",
    "    # Group by hallucination-reducing techniques\n",
    "    hr_group = accuracy_df[accuracy_df['hallucination_reducing'] == True]\n",
    "    standard_group = accuracy_df[accuracy_df['hallucination_reducing'] == False]\n",
    "    \n",
    "    print(f\"\\nACCURACY CLASSIFICATION RESULTS:\")\n",
    "    print(f\"  Total estimates analyzed: {len(accuracy_df)}\")\n",
    "    print(f\"  Hallucination-reducing estimates: {len(hr_group)}\")\n",
    "    print(f\"  Standard estimates: {len(standard_group)}\")\n",
    "    print(f\"  Median judge score threshold: {median_judge_score:.2f}\")\n",
    "    \n",
    "    if len(hr_group) == 0 or len(standard_group) == 0:\n",
    "        print(\"Warning: Insufficient samples in both groups for comparison\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate accuracy metrics for each group\n",
    "    def calculate_accuracy_metrics(group_df, group_name):\n",
    "        \"\"\"Calculate TP, FP, FN rates and overall accuracy for a group\"\"\"\n",
    "        total = len(group_df)\n",
    "        tp_count = len(group_df[group_df['classification'] == 'TP'])\n",
    "        fp_count = len(group_df[group_df['classification'] == 'FP'])\n",
    "        fn_count = len(group_df[group_df['classification'] == 'FN'])\n",
    "        \n",
    "        tp_rate = tp_count / total\n",
    "        fp_rate = fp_count / total\n",
    "        fn_rate = fn_count / total\n",
    "        accuracy = tp_rate  # Accuracy = TP / (TP + FP + FN)\n",
    "        \n",
    "        avg_error = group_df['accuracy_error_pct'].mean()\n",
    "        std_error = group_df['accuracy_error_pct'].std()\n",
    "        \n",
    "        print(f\"\\n{group_name} Group (n={total}):\")\n",
    "        print(f\"  True Positives (±20%): {tp_count} ({tp_rate:.2%})\")\n",
    "        print(f\"  False Positives (overestimates): {fp_count} ({fp_rate:.2%})\")\n",
    "        print(f\"  False Negatives (underestimates): {fn_count} ({fn_rate:.2%})\")\n",
    "        print(f\"  Overall Accuracy: {accuracy:.2%}\")\n",
    "        print(f\"  Average Error: {avg_error:.1f}% ± {std_error:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'tp_count': tp_count, 'fp_count': fp_count, 'fn_count': fn_count,\n",
    "            'tp_rate': tp_rate, 'fp_rate': fp_rate, 'fn_rate': fn_rate,\n",
    "            'accuracy': accuracy, 'avg_error': avg_error, 'std_error': std_error,\n",
    "            'group_data': group_df\n",
    "        }\n",
    "    \n",
    "    # Calculate metrics for both groups\n",
    "    hr_metrics = calculate_accuracy_metrics(hr_group, \"Hallucination-Reducing\")\n",
    "    standard_metrics = calculate_accuracy_metrics(standard_group, \"Standard\")\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    accuracy_improvement = ((hr_metrics['accuracy'] - standard_metrics['accuracy']) / \n",
    "                           standard_metrics['accuracy']) * 100\n",
    "    fp_reduction = ((standard_metrics['fp_rate'] - hr_metrics['fp_rate']) / \n",
    "                   standard_metrics['fp_rate']) * 100 if standard_metrics['fp_rate'] > 0 else 0\n",
    "    fn_reduction = ((standard_metrics['fn_rate'] - hr_metrics['fn_rate']) / \n",
    "                   standard_metrics['fn_rate']) * 100 if standard_metrics['fn_rate'] > 0 else 0\n",
    "    error_reduction = ((standard_metrics['avg_error'] - hr_metrics['avg_error']) / \n",
    "                      standard_metrics['avg_error']) * 100\n",
    "    \n",
    "    print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "    print(f\"  Accuracy improvement: {accuracy_improvement:+.1f}%\")\n",
    "    print(f\"  False Positive reduction (fewer overestimates): {fp_reduction:+.1f}%\")\n",
    "    print(f\"  False Negative reduction (fewer underestimates): {fn_reduction:+.1f}%\")\n",
    "    print(f\"  Average error reduction: {error_reduction:+.1f}%\")\n",
    "    print(f\"  Meets >30% threshold: {'YES' if max(accuracy_improvement, fp_reduction, fn_reduction, error_reduction) > 30 else 'NO'}\")\n",
    "    \n",
    "    # Business impact interpretation\n",
    "    print(f\"\\nBUSINESS IMPACT INTERPRETATION:\")\n",
    "    print(f\"  FP reduction (overestimate reduction): Reduces resource waste, over-scoping\")\n",
    "    print(f\"  FN reduction (underestimate reduction): Reduces project overruns, missed deadlines\")\n",
    "    print(f\"  Combined effect: More accurate project planning and resource allocation\")\n",
    "    \n",
    "    statistical_results = {}\n",
    "    \n",
    "    # 1. Chi-square test for accuracy classification differences\n",
    "    print(f\"\\nSTATISTICAL TESTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create contingency table for chi-square test\n",
    "        contingency_table = pd.crosstab(accuracy_df['hallucination_reducing'], \n",
    "                                      accuracy_df['classification'])\n",
    "        print(f\"\\nContingency Table:\")\n",
    "        print(contingency_table)\n",
    "        \n",
    "        chi2_stat, chi2_p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        print(f\"\\n1. Chi-Square Test (Classification Independence):\")\n",
    "        print(f\"   H₀: Classification is independent of hallucination-reducing techniques\")\n",
    "        print(f\"   H₁: Classification depends on hallucination-reducing techniques\")\n",
    "        print(f\"   χ² statistic: {chi2_stat:.3f}\")\n",
    "        print(f\"   p-value: {chi2_p:.6f}\")\n",
    "        print(f\"   Significant: {'YES' if chi2_p < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        \n",
    "        statistical_results['chi_square'] = {\n",
    "            'statistic': chi2_stat,\n",
    "            'p_value': chi2_p,\n",
    "            'significant': chi2_p < CONFIG['ALPHA_LEVEL'],\n",
    "            'contingency_table': contingency_table\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in chi-square test: {e}\")\n",
    "        statistical_results['chi_square'] = {}\n",
    "    \n",
    "    # 2. Two-proportion z-test for accuracy rates\n",
    "    try:\n",
    "        from statsmodels.stats.proportion import proportions_ztest\n",
    "        \n",
    "        counts = np.array([hr_metrics['tp_count'], standard_metrics['tp_count']])\n",
    "        nobs = np.array([hr_metrics['total'], standard_metrics['total']])\n",
    "        \n",
    "        z_stat, z_p = proportions_ztest(counts, nobs)\n",
    "        \n",
    "        print(f\"\\n2. Two-Proportion Z-Test (Accuracy Rates):\")\n",
    "        print(f\"   H₀: No difference in accuracy rates between groups\")\n",
    "        print(f\"   H₁: Hallucination-reducing group has higher accuracy\")\n",
    "        print(f\"   z-statistic: {z_stat:.3f}\")\n",
    "        print(f\"   p-value: {z_p:.6f}\")\n",
    "        print(f\"   Significant: {'YES' if z_p < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        \n",
    "        statistical_results['proportion_test'] = {\n",
    "            'statistic': z_stat,\n",
    "            'p_value': z_p,\n",
    "            'significant': z_p < CONFIG['ALPHA_LEVEL']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in proportion test: {e}\")\n",
    "        statistical_results['proportion_test'] = {}\n",
    "    \n",
    "    # 3. Mann-Whitney U test for error distributions\n",
    "    try:\n",
    "        hr_errors = hr_group['accuracy_error_pct'].values\n",
    "        standard_errors = standard_group['accuracy_error_pct'].values\n",
    "        \n",
    "        u_stat, u_p = mannwhitneyu(hr_errors, standard_errors, alternative='less')  # HR should have lower errors\n",
    "        \n",
    "        print(f\"\\n3. Mann-Whitney U Test (Error Distributions):\")\n",
    "        print(f\"   H₀: No difference in error distributions\")\n",
    "        print(f\"   H₁: Hallucination-reducing group has lower errors\")\n",
    "        print(f\"   U-statistic: {u_stat:.3f}\")\n",
    "        print(f\"   p-value: {u_p:.6f}\")\n",
    "        print(f\"   Significant: {'YES' if u_p < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        \n",
    "        # Calculate effect size (rank-biserial correlation)\n",
    "        n1, n2 = len(hr_errors), len(standard_errors)\n",
    "        effect_size = 1 - (2 * u_stat) / (n1 * n2)\n",
    "        \n",
    "        print(f\"   Effect size (rank-biserial): {effect_size:.3f}\")\n",
    "        \n",
    "        statistical_results['mann_whitney'] = {\n",
    "            'statistic': u_stat,\n",
    "            'p_value': u_p,\n",
    "            'significant': u_p < CONFIG['ALPHA_LEVEL'],\n",
    "            'effect_size': effect_size\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Mann-Whitney test: {e}\")\n",
    "        statistical_results['mann_whitney'] = {}\n",
    "    \n",
    "    # 4. Fisher's Exact Test for small samples (if applicable)\n",
    "    if len(hr_group) < 30 or len(standard_group) < 30:\n",
    "        try:\n",
    "            # Create 2x2 table for accurate vs inaccurate estimates\n",
    "            hr_accurate = hr_metrics['tp_count']\n",
    "            hr_inaccurate = hr_metrics['fp_count'] + hr_metrics['fn_count']\n",
    "            standard_accurate = standard_metrics['tp_count']\n",
    "            standard_inaccurate = standard_metrics['fp_count'] + standard_metrics['fn_count']\n",
    "            \n",
    "            table_2x2 = [[hr_accurate, hr_inaccurate], \n",
    "                        [standard_accurate, standard_inaccurate]]\n",
    "            \n",
    "            odds_ratio, fisher_p = fisher_exact(table_2x2)\n",
    "            \n",
    "            print(f\"\\n4. Fisher's Exact Test (Small Sample Correction):\")\n",
    "            print(f\"   2x2 Table: HR [{hr_accurate}, {hr_inaccurate}] vs Standard [{standard_accurate}, {standard_inaccurate}]\")\n",
    "            print(f\"   Odds ratio: {odds_ratio:.3f}\")\n",
    "            print(f\"   p-value: {fisher_p:.6f}\")\n",
    "            print(f\"   Significant: {'YES' if fisher_p < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "            \n",
    "            statistical_results['fisher_exact'] = {\n",
    "                'odds_ratio': odds_ratio,\n",
    "                'p_value': fisher_p,\n",
    "                'significant': fisher_p < CONFIG['ALPHA_LEVEL']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Fisher's exact test: {e}\")\n",
    "            statistical_results['fisher_exact'] = {}\n",
    "    \n",
    "    # Overall hypothesis testing conclusion\n",
    "    print(f\"\\nHYPOTHESIS TESTING CONCLUSION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    significant_tests = sum(1 for test in statistical_results.values() \n",
    "                          if test.get('significant', False))\n",
    "    total_tests = len([test for test in statistical_results.values() if test])\n",
    "    \n",
    "    meets_threshold = max(accuracy_improvement, fp_reduction, fn_reduction, error_reduction) > 30\n",
    "    \n",
    "    print(f\"\\n✓ Magnitude Test: {'PASS' if meets_threshold else 'FAIL'}\")\n",
    "    print(f\"  - Required: >30% improvement in any metric\")\n",
    "    print(f\"  - Best observed: {max(accuracy_improvement, fp_reduction, fn_reduction, error_reduction):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n✓ Statistical Significance: {'PASS' if significant_tests > 0 else 'FAIL'}\")\n",
    "    print(f\"  - Significant tests: {significant_tests}/{total_tests}\")\n",
    "    print(f\"  - Alpha level: {CONFIG['ALPHA_LEVEL']}\")\n",
    "    \n",
    "    hypothesis_supported = meets_threshold and significant_tests > 0\n",
    "    \n",
    "    print(f\"\\n🎯 FINAL VERDICT: {'HYPOTHESIS SUPPORTED' if hypothesis_supported else 'HYPOTHESIS NOT SUPPORTED'}\")\n",
    "    \n",
    "    if not hypothesis_supported:\n",
    "        print(f\"\\nRECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
    "        print(f\"  1. Collect more estimation samples (current: {len(accuracy_df)})\")\n",
    "        print(f\"  2. Refine ground truth establishment methodology\")\n",
    "        print(f\"  3. Adjust accuracy thresholds based on industry standards\")\n",
    "        print(f\"  4. Implement longitudinal study design\")\n",
    "        print(f\"  5. Add direct time-to-market measurements\")\n",
    "    \n",
    "    print(f\"\\nCORRECTED CLASSIFICATION VERIFICATION:\")\n",
    "    print(f\"  ✓ FP (False Positive) = Overestimate = More SiFP than reality\")\n",
    "    print(f\"  ✓ FN (False Negative) = Underestimate = Fewer SiFP than reality\") \n",
    "    print(f\"  ✓ Business Impact: FP → resource waste, FN → project overruns\")\n",
    "    \n",
    "    # Store comprehensive results\n",
    "    accuracy_analysis_results = {\n",
    "        'accuracy_df': accuracy_df,\n",
    "        'hr_metrics': hr_metrics,\n",
    "        'standard_metrics': standard_metrics,\n",
    "        'improvements': {\n",
    "            'accuracy': accuracy_improvement,\n",
    "            'fp_reduction': fp_reduction,\n",
    "            'fn_reduction': fn_reduction,\n",
    "            'error_reduction': error_reduction\n",
    "        },\n",
    "        'statistical_tests': statistical_results,\n",
    "        'hypothesis_supported': hypothesis_supported,\n",
    "        'meets_threshold': meets_threshold,\n",
    "        'ground_truth_baseline': ground_truth_loc_per_sifp\n",
    "    }\n",
    "    \n",
    "    return accuracy_analysis_results\n",
    "\n",
    "# Execute accuracy-based hypothesis testing\n",
    "if 'llm_estimates_df' in globals() and not llm_estimates_df.empty:\n",
    "    accuracy_test_results = perform_accuracy_hypothesis_testing()\n",
    "    print(f\"\\n✓ Accuracy-based hypothesis testing completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform accuracy-based hypothesis testing - LLM estimates not available\")\n",
    "    accuracy_test_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Advanced Permutation Testing for Estimation Accuracy vs Ground Truth\n",
    "# Purpose: Implement robust permutation tests for estimation accuracy without distributional assumptions\n",
    "# Dependencies: accuracy_test_results from Cell 11, STATISTICAL_CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Analysis -> Accuracy Testing -> Advanced Permutation Testing for Accuracy\n",
    "\n",
    "def perform_accuracy_permutation_testing():\n",
    "    \"\"\"\n",
    "    Perform comprehensive permutation testing for estimation accuracy vs ground truth\n",
    "    Tests whether hallucination-reducing techniques improve accuracy through exact p-values\n",
    "    \n",
    "    Returns:\n",
    "        dict: Permutation test results for accuracy improvements\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ADVANCED PERMUTATION TESTING - ESTIMATION ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve estimation accuracy by >30%\")\n",
    "    print(\"METHOD: Distribution-free permutation tests with exact p-values on accuracy metrics\")\n",
    "    \n",
    "    if not accuracy_test_results or 'accuracy_df' not in accuracy_test_results:\n",
    "        print(\"\\nWarning: Accuracy test results not available from Cell 11\")\n",
    "        return {}\n",
    "    \n",
    "    accuracy_df = accuracy_test_results['accuracy_df']\n",
    "    hr_metrics = accuracy_test_results['hr_metrics']\n",
    "    standard_metrics = accuracy_test_results['standard_metrics']\n",
    "    \n",
    "    print(f\"\\nACCURACY DATA SUMMARY:\")\n",
    "    print(f\"  Total estimates: {len(accuracy_df)}\")\n",
    "    print(f\"  Hallucination-reducing estimates: {len(hr_metrics['group_data'])}\")\n",
    "    print(f\"  Standard estimates: {len(standard_metrics['group_data'])}\")\n",
    "    print(f\"  Ground truth baseline: {accuracy_test_results['ground_truth_baseline']:.1f} LOC/SiFP\")\n",
    "    \n",
    "    if len(hr_metrics['group_data']) == 0 or len(standard_metrics['group_data']) == 0:\n",
    "        print(\"Warning: Insufficient samples for permutation testing\")\n",
    "        return {}\n",
    "    \n",
    "    permutation_results = {}\n",
    "    n_resamples = CONFIG.get('PERMUTATION_SAMPLES', 10000)\n",
    "    \n",
    "    print(f\"\\nPERMUTATION TEST CONFIGURATION:\")\n",
    "    print(f\"  Number of resamples: {n_resamples:,}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    print(f\"  Test type: One-sided (greater for improvements)\")\n",
    "    \n",
    "    # Extract data for permutation tests\n",
    "    hr_data = hr_metrics['group_data']\n",
    "    standard_data = standard_metrics['group_data']\n",
    "    \n",
    "    # 1. Permutation test for overall accuracy improvement\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"1. PERMUTATION TEST: OVERALL ACCURACY IMPROVEMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in accuracy rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has higher accuracy rate\")\n",
    "    \n",
    "    def accuracy_difference_statistic(hr_group, standard_group):\n",
    "        \"\"\"Test statistic: difference in accuracy rates (HR - Standard)\"\"\"\n",
    "        hr_accuracy = np.mean(hr_group['classification'] == 'TP')\n",
    "        standard_accuracy = np.mean(standard_group['classification'] == 'TP')\n",
    "        return hr_accuracy - standard_accuracy\n",
    "    \n",
    "    try:\n",
    "        # Observed test statistic\n",
    "        observed_accuracy_diff = accuracy_difference_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Permutation test\n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            accuracy_perm_result = permutation_test(\n",
    "                (hr_data, standard_data),\n",
    "                lambda x, y: accuracy_difference_statistic(x, y),\n",
    "                n_resamples=n_resamples,\n",
    "                alternative='greater',\n",
    "                random_state=42\n",
    "            )\n",
    "            accuracy_p_value = accuracy_perm_result.pvalue\n",
    "            null_distribution = accuracy_perm_result.null_distribution\n",
    "        else:\n",
    "            # Custom permutation test\n",
    "            accuracy_p_value, null_distribution = custom_permutation_test_accuracy(\n",
    "                hr_data, standard_data, accuracy_difference_statistic, n_resamples\n",
    "            )\n",
    "        \n",
    "        # Calculate improvement percentage\n",
    "        accuracy_improvement_pct = (observed_accuracy_diff / standard_metrics['accuracy']) * 100\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed accuracy difference: {observed_accuracy_diff:.3f}\")\n",
    "        print(f\"  Improvement percentage: {accuracy_improvement_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {accuracy_p_value:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if accuracy_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if accuracy_improvement_pct > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['accuracy_improvement'] = {\n",
    "            'observed_statistic': observed_accuracy_diff,\n",
    "            'improvement_pct': accuracy_improvement_pct,\n",
    "            'p_value': accuracy_p_value,\n",
    "            'significant': accuracy_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': accuracy_improvement_pct > 30,\n",
    "            'null_distribution': null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in accuracy improvement permutation test: {e}\")\n",
    "        permutation_results['accuracy_improvement'] = {}\n",
    "    \n",
    "    # 2. Permutation test for False Positive reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"2. PERMUTATION TEST: FALSE POSITIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in False Positive rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower False Positive rate\")\n",
    "    \n",
    "    def fp_reduction_statistic(hr_group, standard_group):\n",
    "        \"\"\"Test statistic: reduction in FP rate (Standard - HR)\"\"\"\n",
    "        hr_fp_rate = np.mean(hr_group['classification'] == 'FP')\n",
    "        standard_fp_rate = np.mean(standard_group['classification'] == 'FP')\n",
    "        return standard_fp_rate - hr_fp_rate  # Positive means HR has lower FP rate\n",
    "    \n",
    "    try:\n",
    "        observed_fp_reduction = fp_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Permutation test\n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            fp_perm_result = permutation_test(\n",
    "                (hr_data, standard_data),\n",
    "                lambda x, y: fp_reduction_statistic(x, y),\n",
    "                n_resamples=n_resamples,\n",
    "                alternative='greater',\n",
    "                random_state=42\n",
    "            )\n",
    "            fp_p_value = fp_perm_result.pvalue\n",
    "            fp_null_distribution = fp_perm_result.null_distribution\n",
    "        else:\n",
    "            fp_p_value, fp_null_distribution = custom_permutation_test_accuracy(\n",
    "                hr_data, standard_data, fp_reduction_statistic, n_resamples\n",
    "            )\n",
    "        \n",
    "        # Calculate reduction percentage\n",
    "        fp_reduction_pct = (observed_fp_reduction / standard_metrics['fp_rate']) * 100 if standard_metrics['fp_rate'] > 0 else 0\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed FP rate reduction: {observed_fp_reduction:.3f}\")\n",
    "        print(f\"  FP reduction percentage: {fp_reduction_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {fp_p_value:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if fp_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if fp_reduction_pct > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['fp_reduction'] = {\n",
    "            'observed_statistic': observed_fp_reduction,\n",
    "            'reduction_pct': fp_reduction_pct,\n",
    "            'p_value': fp_p_value,\n",
    "            'significant': fp_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': fp_reduction_pct > 30,\n",
    "            'null_distribution': fp_null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in FP reduction permutation test: {e}\")\n",
    "        permutation_results['fp_reduction'] = {}\n",
    "    \n",
    "    # 3. Permutation test for False Negative reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"3. PERMUTATION TEST: FALSE NEGATIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in False Negative rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower False Negative rate\")\n",
    "    \n",
    "    def fn_reduction_statistic(hr_group, standard_group):\n",
    "        \"\"\"Test statistic: reduction in FN rate (Standard - HR)\"\"\"\n",
    "        hr_fn_rate = np.mean(hr_group['classification'] == 'FN')\n",
    "        standard_fn_rate = np.mean(standard_group['classification'] == 'FN')\n",
    "        return standard_fn_rate - hr_fn_rate  # Positive means HR has lower FN rate\n",
    "    \n",
    "    try:\n",
    "        observed_fn_reduction = fn_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Permutation test\n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            fn_perm_result = permutation_test(\n",
    "                (hr_data, standard_data),\n",
    "                lambda x, y: fn_reduction_statistic(x, y),\n",
    "                n_resamples=n_resamples,\n",
    "                alternative='greater',\n",
    "                random_state=42\n",
    "            )\n",
    "            fn_p_value = fn_perm_result.pvalue\n",
    "            fn_null_distribution = fn_perm_result.null_distribution\n",
    "        else:\n",
    "            fn_p_value, fn_null_distribution = custom_permutation_test_accuracy(\n",
    "                hr_data, standard_data, fn_reduction_statistic, n_resamples\n",
    "            )\n",
    "        \n",
    "        # Calculate reduction percentage\n",
    "        fn_reduction_pct = (observed_fn_reduction / standard_metrics['fn_rate']) * 100 if standard_metrics['fn_rate'] > 0 else 0\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed FN rate reduction: {observed_fn_reduction:.3f}\")\n",
    "        print(f\"  FN reduction percentage: {fn_reduction_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {fn_p_value:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if fn_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if fn_reduction_pct > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['fn_reduction'] = {\n",
    "            'observed_statistic': observed_fn_reduction,\n",
    "            'reduction_pct': fn_reduction_pct,\n",
    "            'p_value': fn_p_value,\n",
    "            'significant': fn_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': fn_reduction_pct > 30,\n",
    "            'null_distribution': fn_null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in FN reduction permutation test: {e}\")\n",
    "        permutation_results['fn_reduction'] = {}\n",
    "    \n",
    "    # 4. Permutation test for overall error reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"4. PERMUTATION TEST: OVERALL ERROR REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in average estimation errors between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower average errors\")\n",
    "    \n",
    "    def error_reduction_statistic(hr_group, standard_group):\n",
    "        \"\"\"Test statistic: reduction in average error (Standard - HR)\"\"\"\n",
    "        hr_avg_error = hr_group['accuracy_error_pct'].mean()\n",
    "        standard_avg_error = standard_group['accuracy_error_pct'].mean()\n",
    "        return standard_avg_error - hr_avg_error  # Positive means HR has lower errors\n",
    "    \n",
    "    try:\n",
    "        observed_error_reduction = error_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Permutation test\n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            error_perm_result = permutation_test(\n",
    "                (hr_data, standard_data),\n",
    "                lambda x, y: error_reduction_statistic(x, y),\n",
    "                n_resamples=n_resamples,\n",
    "                alternative='greater',\n",
    "                random_state=42\n",
    "            )\n",
    "            error_p_value = error_perm_result.pvalue\n",
    "            error_null_distribution = error_perm_result.null_distribution\n",
    "        else:\n",
    "            error_p_value, error_null_distribution = custom_permutation_test_accuracy(\n",
    "                hr_data, standard_data, error_reduction_statistic, n_resamples\n",
    "            )\n",
    "        \n",
    "        # Calculate reduction percentage\n",
    "        error_reduction_pct = (observed_error_reduction / standard_metrics['avg_error']) * 100\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed error reduction: {observed_error_reduction:.2f} percentage points\")\n",
    "        print(f\"  Error reduction percentage: {error_reduction_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {error_p_value:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if error_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if error_reduction_pct > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['error_reduction'] = {\n",
    "            'observed_statistic': observed_error_reduction,\n",
    "            'reduction_pct': error_reduction_pct,\n",
    "            'p_value': error_p_value,\n",
    "            'significant': error_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': error_reduction_pct > 30,\n",
    "            'null_distribution': error_null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in error reduction permutation test: {e}\")\n",
    "        permutation_results['error_reduction'] = {}\n",
    "    \n",
    "    # 5. Multiple comparisons adjustment\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"5. MULTIPLE COMPARISONS ADJUSTMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        \n",
    "        # Collect all p-values for adjustment\n",
    "        p_values = []\n",
    "        test_names = []\n",
    "        \n",
    "        for test_name, results in permutation_results.items():\n",
    "            if results and 'p_value' in results:\n",
    "                p_values.append(results['p_value'])\n",
    "                test_names.append(test_name.replace('_', ' ').title())\n",
    "        \n",
    "        if p_values:\n",
    "            # Apply Bonferroni correction\n",
    "            bonferroni_rejected, bonferroni_pvals, _, _ = multipletests(p_values, method='bonferroni')\n",
    "            \n",
    "            # Apply Benjamini-Hochberg (FDR) correction\n",
    "            fdr_rejected, fdr_pvals, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "            \n",
    "            print(f\"Multiple Comparisons Results:\")\n",
    "            print(f\"{'Test':<20} {'Raw p-value':<12} {'Bonferroni':<12} {'FDR (B-H)':<12} {'Significant'}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i, (test_name, raw_p) in enumerate(zip(test_names, p_values)):\n",
    "                bonf_sig = '✓' if bonferroni_rejected[i] else '✗'\n",
    "                fdr_sig = '✓' if fdr_rejected[i] else '✗'\n",
    "                print(f\"{test_name:<20} {raw_p:<12.6f} {bonferroni_pvals[i]:<12.6f} {fdr_pvals[i]:<12.6f} {bonf_sig}/{fdr_sig}\")\n",
    "            \n",
    "            permutation_results['multiple_comparisons'] = {\n",
    "                'raw_p_values': p_values,\n",
    "                'test_names': test_names,\n",
    "                'bonferroni_p_values': bonferroni_pvals.tolist(),\n",
    "                'bonferroni_rejected': bonferroni_rejected.tolist(),\n",
    "                'fdr_p_values': fdr_pvals.tolist(),\n",
    "                'fdr_rejected': fdr_rejected.tolist()\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiple comparisons adjustment: {e}\")\n",
    "        permutation_results['multiple_comparisons'] = {}\n",
    "    \n",
    "    # Summary of permutation testing results\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"PERMUTATION TESTING SUMMARY - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    significant_tests = 0\n",
    "    total_tests = 0\n",
    "    meets_threshold_tests = 0\n",
    "    \n",
    "    for test_name, results in permutation_results.items():\n",
    "        if test_name == 'multiple_comparisons':\n",
    "            continue\n",
    "        if results and 'significant' in results:\n",
    "            total_tests += 1\n",
    "            if results['significant']:\n",
    "                significant_tests += 1\n",
    "            if results.get('meets_threshold', False):\n",
    "                meets_threshold_tests += 1\n",
    "    \n",
    "    print(f\"\\nOverall Results:\")\n",
    "    print(f\"  Total permutation tests: {total_tests}\")\n",
    "    print(f\"  Statistically significant: {significant_tests}/{total_tests}\")\n",
    "    print(f\"  Meet >30% threshold: {meets_threshold_tests}/{total_tests}\")\n",
    "    \n",
    "    # Calculate best improvements\n",
    "    best_improvement = 0\n",
    "    best_metric = \"None\"\n",
    "    for test_name, results in permutation_results.items():\n",
    "        if test_name == 'multiple_comparisons':\n",
    "            continue\n",
    "        if results and 'improvement_pct' in results:\n",
    "            improvement = results['improvement_pct']\n",
    "        elif results and 'reduction_pct' in results:\n",
    "            improvement = results['reduction_pct']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if improvement > best_improvement:\n",
    "            best_improvement = improvement\n",
    "            best_metric = test_name.replace('_', ' ').title()\n",
    "    \n",
    "    print(f\"  Best improvement: {best_improvement:.1f}% in {best_metric}\")\n",
    "    \n",
    "    # Final verdict for permutation testing\n",
    "    any_significant = significant_tests > 0\n",
    "    any_threshold = meets_threshold_tests > 0\n",
    "    strong_evidence = significant_tests >= total_tests * 0.5 and meets_threshold_tests > 0\n",
    "    \n",
    "    print(f\"\\n🎯 PERMUTATION TESTING VERDICT:\")\n",
    "    print(f\"  Statistical Evidence: {'Found' if any_significant else 'Not Found'}\")\n",
    "    print(f\"  Practical Significance: {'Found' if any_threshold else 'Not Found'}\")\n",
    "    print(f\"  Overall Support: {'STRONG' if strong_evidence else 'MODERATE' if any_significant or any_threshold else 'WEAK'}\")\n",
    "    \n",
    "    if strong_evidence:\n",
    "        print(f\"\\n✅ CONCLUSION: Permutation tests provide strong evidence that\")\n",
    "        print(f\"   hallucination-reducing techniques improve estimation accuracy\")\n",
    "        print(f\"   through reduced false positives and false negatives.\")\n",
    "    elif any_significant or any_threshold:\n",
    "        print(f\"\\n⚠️  CONCLUSION: Permutation tests provide moderate evidence\")\n",
    "        print(f\"   for improved estimation accuracy, but results are mixed.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ CONCLUSION: Permutation tests do not provide sufficient\")\n",
    "        print(f\"   evidence for improved estimation accuracy.\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF ACCURACY-BASED PERMUTATION TESTING:\")\n",
    "    print(f\"  ✓ Compares against ground truth rather than between models\")\n",
    "    print(f\"  ✓ Directly tests False Positive and False Negative reduction\")\n",
    "    print(f\"  ✓ No distributional assumptions required\")\n",
    "    print(f\"  ✓ Exact p-values for small samples\")\n",
    "    print(f\"  ✓ Multiple comparison corrections applied\")\n",
    "    print(f\"  ✓ Robust to outliers and non-normal distributions\")\n",
    "    \n",
    "    return permutation_results\n",
    "\n",
    "def custom_permutation_test_accuracy(hr_data, standard_data, statistic_func, n_resamples):\n",
    "    \"\"\"\n",
    "    Custom permutation test implementation for accuracy analysis\n",
    "    \"\"\"\n",
    "    # Combine the data\n",
    "    combined_data = pd.concat([hr_data, standard_data], ignore_index=True)\n",
    "    n_hr = len(hr_data)\n",
    "    \n",
    "    # Calculate observed statistic\n",
    "    observed_stat = statistic_func(hr_data, standard_data)\n",
    "    \n",
    "    # Generate permutation distribution\n",
    "    perm_stats = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for _ in range(n_resamples):\n",
    "        # Randomly permute the combined data\n",
    "        perm_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "        perm_hr = perm_data.iloc[:n_hr]\n",
    "        perm_standard = perm_data.iloc[n_hr:]\n",
    "        \n",
    "        perm_stat = statistic_func(perm_hr, perm_standard)\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    \n",
    "    # Calculate p-value (one-sided test - greater)\n",
    "    p_value = np.mean(perm_stats >= observed_stat)\n",
    "    \n",
    "    return p_value, perm_stats\n",
    "\n",
    "# Execute accuracy-based permutation testing\n",
    "if ('accuracy_test_results' in globals() and accuracy_test_results and \n",
    "    STATISTICAL_CONFIG.get('permutation_tests', False)):\n",
    "    accuracy_permutation_results = perform_accuracy_permutation_testing()\n",
    "    print(f\"\\n✓ Accuracy-based permutation testing completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform accuracy-based permutation testing:\")\n",
    "    if 'accuracy_test_results' not in globals() or not accuracy_test_results:\n",
    "        print(\"  - Accuracy test results not available (run Cell 11 first)\")\n",
    "    if not STATISTICAL_CONFIG.get('permutation_tests', False):\n",
    "        print(\"  - Permutation testing functionality not available\")\n",
    "    \n",
    "    accuracy_permutation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - Bootstrap Hypothesis Testing for Estimation Accuracy vs Ground Truth\n",
    "# Purpose: Bootstrap-based hypothesis testing for >30% accuracy improvement with confidence intervals\n",
    "# Dependencies: accuracy_test_results from Cell 11, bootstrap functionality from Cell 0\n",
    "# Breadcrumbs: Setup -> Analysis -> Accuracy Testing -> Bootstrap Hypothesis Testing for Accuracy\n",
    "\n",
    "def perform_bootstrap_accuracy_testing():\n",
    "    \"\"\"\n",
    "    Perform comprehensive bootstrap hypothesis testing for estimation accuracy vs ground truth\n",
    "    Tests whether hallucination-reducing techniques improve accuracy by >30% through bootstrap resampling\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap test results with confidence intervals and bias corrections\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"BOOTSTRAP HYPOTHESIS TESTING - ESTIMATION ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve estimation accuracy by >30%\")\n",
    "    print(\"METHOD: Bootstrap resampling with bias-corrected confidence intervals on accuracy metrics\")\n",
    "    \n",
    "    if not accuracy_test_results or 'accuracy_df' not in accuracy_test_results:\n",
    "        print(\"\\nWarning: Accuracy test results not available from Cell 11\")\n",
    "        return {}\n",
    "    \n",
    "    accuracy_df = accuracy_test_results['accuracy_df']\n",
    "    hr_metrics = accuracy_test_results['hr_metrics']\n",
    "    standard_metrics = accuracy_test_results['standard_metrics']\n",
    "    \n",
    "    # Extract accuracy data for bootstrap testing\n",
    "    hr_data = hr_metrics['group_data']\n",
    "    standard_data = standard_metrics['group_data']\n",
    "    \n",
    "    print(f\"\\nACCURACY DATA SUMMARY:\")\n",
    "    print(f\"  Total estimates: {len(accuracy_df)}\")\n",
    "    print(f\"  Hallucination-reducing estimates: {len(hr_data)}\")\n",
    "    print(f\"  Standard estimates: {len(standard_data)}\")\n",
    "    print(f\"  Ground truth baseline: {accuracy_test_results['ground_truth_baseline']:.1f} LOC/SiFP\")\n",
    "    \n",
    "    if len(hr_data) == 0 or len(standard_data) == 0:\n",
    "        print(\"Warning: Cannot perform bootstrap tests - insufficient group sizes\")\n",
    "        return {}\n",
    "    \n",
    "    bootstrap_results = {}\n",
    "    n_bootstrap = CONFIG.get('BOOTSTRAP_SAMPLES', 10000)\n",
    "    \n",
    "    print(f\"\\nBOOTSTRAP TEST CONFIGURATION:\")\n",
    "    print(f\"  Number of bootstrap samples: {n_bootstrap:,}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    print(f\"  Confidence level: 95%\")\n",
    "    print(f\"  Bias correction: BCa (Bias-Corrected and accelerated)\")\n",
    "    \n",
    "    # 1. Bootstrap test for overall accuracy improvement\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"1. BOOTSTRAP TEST: OVERALL ACCURACY IMPROVEMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in accuracy rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has higher accuracy rate (>30% improvement)\")\n",
    "    \n",
    "    try:\n",
    "        def accuracy_improvement_statistic(hr_sample, standard_sample):\n",
    "            \"\"\"Calculate accuracy improvement percentage: (HR - Standard) / Standard * 100\"\"\"\n",
    "            hr_accuracy = np.mean(hr_sample['classification'] == 'TP')\n",
    "            standard_accuracy = np.mean(standard_sample['classification'] == 'TP')\n",
    "            if standard_accuracy == 0:\n",
    "                return 0\n",
    "            return ((hr_accuracy - standard_accuracy) / standard_accuracy) * 100\n",
    "        \n",
    "        # Bootstrap for accuracy improvement\n",
    "        def bootstrap_accuracy_improvement(n_samples=n_bootstrap):\n",
    "            improvements = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                # Resample with replacement from each group\n",
    "                hr_sample = hr_data.sample(len(hr_data), replace=True)\n",
    "                standard_sample = standard_data.sample(len(standard_data), replace=True)\n",
    "                \n",
    "                improvement = accuracy_improvement_statistic(hr_sample, standard_sample)\n",
    "                improvements.append(improvement)\n",
    "            \n",
    "            return np.array(improvements)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        accuracy_bootstrap_dist = bootstrap_accuracy_improvement()\n",
    "        observed_accuracy_improvement = accuracy_improvement_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        accuracy_ci_lower = np.percentile(accuracy_bootstrap_dist, 2.5)\n",
    "        accuracy_ci_upper = np.percentile(accuracy_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis test: H₀: improvement ≤ 0\n",
    "        null_violations = np.sum(accuracy_bootstrap_dist <= 0)\n",
    "        accuracy_bootstrap_p_value = null_violations / len(accuracy_bootstrap_dist)\n",
    "        \n",
    "        # Test for >30% improvement specifically\n",
    "        improvement_30_violations = np.sum(accuracy_bootstrap_dist < 30)\n",
    "        accuracy_30_p_value = improvement_30_violations / len(accuracy_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed accuracy improvement: {observed_accuracy_improvement:.1f}%\")\n",
    "        print(f\"  Bootstrap mean improvement: {np.mean(accuracy_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std improvement: {np.std(accuracy_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{accuracy_ci_lower:.1f}%, {accuracy_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(improvement ≤ 0): {accuracy_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(improvement < 30%): {accuracy_30_p_value:.4f}\")\n",
    "        print(f\"  Significant improvement: {'YES' if accuracy_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_accuracy_improvement > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - accuracy_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['accuracy_improvement'] = {\n",
    "            'observed_improvement': observed_accuracy_improvement,\n",
    "            'bootstrap_mean': np.mean(accuracy_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(accuracy_bootstrap_dist),\n",
    "            'confidence_interval': (accuracy_ci_lower, accuracy_ci_upper),\n",
    "            'p_value_improvement': accuracy_bootstrap_p_value,\n",
    "            'p_value_30_percent': accuracy_30_p_value,\n",
    "            'significant': accuracy_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_accuracy_improvement > 30,\n",
    "            'threshold_confidence': 1 - accuracy_30_p_value,\n",
    "            'bootstrap_distribution': accuracy_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap accuracy improvement test: {e}\")\n",
    "        bootstrap_results['accuracy_improvement'] = {}\n",
    "    \n",
    "    # 2. Bootstrap test for False Positive reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"2. BOOTSTRAP TEST: FALSE POSITIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in False Positive rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower FP rate (>30% reduction)\")\n",
    "    \n",
    "    try:\n",
    "        def fp_reduction_statistic(hr_sample, standard_sample):\n",
    "            \"\"\"Calculate FP reduction percentage: (Standard - HR) / Standard * 100\"\"\"\n",
    "            hr_fp_rate = np.mean(hr_sample['classification'] == 'FP')\n",
    "            standard_fp_rate = np.mean(standard_sample['classification'] == 'FP')\n",
    "            if standard_fp_rate == 0:\n",
    "                return 0\n",
    "            return ((standard_fp_rate - hr_fp_rate) / standard_fp_rate) * 100\n",
    "        \n",
    "        # Bootstrap for FP reduction\n",
    "        def bootstrap_fp_reduction(n_samples=n_bootstrap):\n",
    "            reductions = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                hr_sample = hr_data.sample(len(hr_data), replace=True)\n",
    "                standard_sample = standard_data.sample(len(standard_data), replace=True)\n",
    "                \n",
    "                reduction = fp_reduction_statistic(hr_sample, standard_sample)\n",
    "                reductions.append(reduction)\n",
    "            \n",
    "            return np.array(reductions)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        fp_bootstrap_dist = bootstrap_fp_reduction()\n",
    "        observed_fp_reduction = fp_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        fp_ci_lower = np.percentile(fp_bootstrap_dist, 2.5)\n",
    "        fp_ci_upper = np.percentile(fp_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis tests\n",
    "        null_violations = np.sum(fp_bootstrap_dist <= 0)\n",
    "        fp_bootstrap_p_value = null_violations / len(fp_bootstrap_dist)\n",
    "        \n",
    "        reduction_30_violations = np.sum(fp_bootstrap_dist < 30)\n",
    "        fp_30_p_value = reduction_30_violations / len(fp_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed FP reduction: {observed_fp_reduction:.1f}%\")\n",
    "        print(f\"  Bootstrap mean reduction: {np.mean(fp_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std reduction: {np.std(fp_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{fp_ci_lower:.1f}%, {fp_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(reduction ≤ 0): {fp_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(reduction < 30%): {fp_30_p_value:.4f}\")\n",
    "        print(f\"  Significant reduction: {'YES' if fp_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_fp_reduction > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - fp_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['fp_reduction'] = {\n",
    "            'observed_reduction': observed_fp_reduction,\n",
    "            'bootstrap_mean': np.mean(fp_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(fp_bootstrap_dist),\n",
    "            'confidence_interval': (fp_ci_lower, fp_ci_upper),\n",
    "            'p_value_reduction': fp_bootstrap_p_value,\n",
    "            'p_value_30_percent': fp_30_p_value,\n",
    "            'significant': fp_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_fp_reduction > 30,\n",
    "            'threshold_confidence': 1 - fp_30_p_value,\n",
    "            'bootstrap_distribution': fp_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap FP reduction test: {e}\")\n",
    "        bootstrap_results['fp_reduction'] = {}\n",
    "    \n",
    "    # 3. Bootstrap test for False Negative reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"3. BOOTSTRAP TEST: FALSE NEGATIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in False Negative rates between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower FN rate (>30% reduction)\")\n",
    "    \n",
    "    try:\n",
    "        def fn_reduction_statistic(hr_sample, standard_sample):\n",
    "            \"\"\"Calculate FN reduction percentage: (Standard - HR) / Standard * 100\"\"\"\n",
    "            hr_fn_rate = np.mean(hr_sample['classification'] == 'FN')\n",
    "            standard_fn_rate = np.mean(standard_sample['classification'] == 'FN')\n",
    "            if standard_fn_rate == 0:\n",
    "                return 0\n",
    "            return ((standard_fn_rate - hr_fn_rate) / standard_fn_rate) * 100\n",
    "        \n",
    "        # Bootstrap for FN reduction\n",
    "        def bootstrap_fn_reduction(n_samples=n_bootstrap):\n",
    "            reductions = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                hr_sample = hr_data.sample(len(hr_data), replace=True)\n",
    "                standard_sample = standard_data.sample(len(standard_data), replace=True)\n",
    "                \n",
    "                reduction = fn_reduction_statistic(hr_sample, standard_sample)\n",
    "                reductions.append(reduction)\n",
    "            \n",
    "            return np.array(reductions)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        fn_bootstrap_dist = bootstrap_fn_reduction()\n",
    "        observed_fn_reduction = fn_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        fn_ci_lower = np.percentile(fn_bootstrap_dist, 2.5)\n",
    "        fn_ci_upper = np.percentile(fn_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis tests\n",
    "        null_violations = np.sum(fn_bootstrap_dist <= 0)\n",
    "        fn_bootstrap_p_value = null_violations / len(fn_bootstrap_dist)\n",
    "        \n",
    "        reduction_30_violations = np.sum(fn_bootstrap_dist < 30)\n",
    "        fn_30_p_value = reduction_30_violations / len(fn_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed FN reduction: {observed_fn_reduction:.1f}%\")\n",
    "        print(f\"  Bootstrap mean reduction: {np.mean(fn_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std reduction: {np.std(fn_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{fn_ci_lower:.1f}%, {fn_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(reduction ≤ 0): {fn_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(reduction < 30%): {fn_30_p_value:.4f}\")\n",
    "        print(f\"  Significant reduction: {'YES' if fn_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_fn_reduction > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - fn_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['fn_reduction'] = {\n",
    "            'observed_reduction': observed_fn_reduction,\n",
    "            'bootstrap_mean': np.mean(fn_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(fn_bootstrap_dist),\n",
    "            'confidence_interval': (fn_ci_lower, fn_ci_upper),\n",
    "            'p_value_reduction': fn_bootstrap_p_value,\n",
    "            'p_value_30_percent': fn_30_p_value,\n",
    "            'significant': fn_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_fn_reduction > 30,\n",
    "            'threshold_confidence': 1 - fn_30_p_value,\n",
    "            'bootstrap_distribution': fn_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap FN reduction test: {e}\")\n",
    "        bootstrap_results['fn_reduction'] = {}\n",
    "    \n",
    "    # 4. Bootstrap test for overall error reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"4. BOOTSTRAP TEST: OVERALL ERROR REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"H₀: No difference in average estimation errors between groups\")\n",
    "    print(f\"H₁: Hallucination-reducing group has lower average errors (>30% reduction)\")\n",
    "    \n",
    "    try:\n",
    "        def error_reduction_statistic(hr_sample, standard_sample):\n",
    "            \"\"\"Calculate error reduction percentage: (Standard - HR) / Standard * 100\"\"\"\n",
    "            hr_avg_error = hr_sample['accuracy_error_pct'].mean()\n",
    "            standard_avg_error = standard_sample['accuracy_error_pct'].mean()\n",
    "            if standard_avg_error == 0:\n",
    "                return 0\n",
    "            return ((standard_avg_error - hr_avg_error) / standard_avg_error) * 100\n",
    "        \n",
    "        # Bootstrap for error reduction\n",
    "        def bootstrap_error_reduction(n_samples=n_bootstrap):\n",
    "            reductions = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                hr_sample = hr_data.sample(len(hr_data), replace=True)\n",
    "                standard_sample = standard_data.sample(len(standard_data), replace=True)\n",
    "                \n",
    "                reduction = error_reduction_statistic(hr_sample, standard_sample)\n",
    "                reductions.append(reduction)\n",
    "            \n",
    "            return np.array(reductions)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        error_bootstrap_dist = bootstrap_error_reduction()\n",
    "        observed_error_reduction = error_reduction_statistic(hr_data, standard_data)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        error_ci_lower = np.percentile(error_bootstrap_dist, 2.5)\n",
    "        error_ci_upper = np.percentile(error_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis tests\n",
    "        null_violations = np.sum(error_bootstrap_dist <= 0)\n",
    "        error_bootstrap_p_value = null_violations / len(error_bootstrap_dist)\n",
    "        \n",
    "        reduction_30_violations = np.sum(error_bootstrap_dist < 30)\n",
    "        error_30_p_value = reduction_30_violations / len(error_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed error reduction: {observed_error_reduction:.1f}%\")\n",
    "        print(f\"  Bootstrap mean reduction: {np.mean(error_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std reduction: {np.std(error_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{error_ci_lower:.1f}%, {error_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(reduction ≤ 0): {error_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(reduction < 30%): {error_30_p_value:.4f}\")\n",
    "        print(f\"  Significant reduction: {'YES' if error_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_error_reduction > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - error_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['error_reduction'] = {\n",
    "            'observed_reduction': observed_error_reduction,\n",
    "            'bootstrap_mean': np.mean(error_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(error_bootstrap_dist),\n",
    "            'confidence_interval': (error_ci_lower, error_ci_upper),\n",
    "            'p_value_reduction': error_bootstrap_p_value,\n",
    "            'p_value_30_percent': error_30_p_value,\n",
    "            'significant': error_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_error_reduction > 30,\n",
    "            'threshold_confidence': 1 - error_30_p_value,\n",
    "            'bootstrap_distribution': error_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap error reduction test: {e}\")\n",
    "        bootstrap_results['error_reduction'] = {}\n",
    "    \n",
    "    # 5. Bias-Corrected and Accelerated (BCa) Bootstrap Intervals\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"5. BIAS-CORRECTED AND ACCELERATED (BCa) INTERVALS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        def calculate_bca_interval_accuracy(hr_data, standard_data, statistic_func, alpha=0.05, n_bootstrap=n_bootstrap):\n",
    "            \"\"\"\n",
    "            Calculate BCa (Bias-Corrected and accelerated) bootstrap confidence interval for accuracy metrics\n",
    "            \"\"\"\n",
    "            # Original statistic\n",
    "            original_stat = statistic_func(hr_data, standard_data)\n",
    "            \n",
    "            # Bootstrap statistics\n",
    "            bootstrap_stats = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_bootstrap):\n",
    "                hr_sample = hr_data.sample(len(hr_data), replace=True)\n",
    "                standard_sample = standard_data.sample(len(standard_data), replace=True)\n",
    "                bootstrap_stats.append(statistic_func(hr_sample, standard_sample))\n",
    "            \n",
    "            bootstrap_stats = np.array(bootstrap_stats)\n",
    "            \n",
    "            # Bias correction\n",
    "            num_less = np.sum(bootstrap_stats < original_stat)\n",
    "            bias_correction = stats.norm.ppf(num_less / n_bootstrap)\n",
    "            \n",
    "            # Acceleration (jackknife)\n",
    "            n_hr, n_std = len(hr_data), len(standard_data)\n",
    "            jackknife_stats = []\n",
    "            \n",
    "            # Jackknife for hr_data\n",
    "            for i in range(min(n_hr, 20)):  # Limit jackknife for computational efficiency\n",
    "                jack_hr = hr_data.drop(hr_data.index[i])\n",
    "                jackknife_stats.append(statistic_func(jack_hr, standard_data))\n",
    "            \n",
    "            # Jackknife for standard_data\n",
    "            for i in range(min(n_std, 20)):\n",
    "                jack_std = standard_data.drop(standard_data.index[i])\n",
    "                jackknife_stats.append(statistic_func(hr_data, jack_std))\n",
    "            \n",
    "            jackknife_stats = np.array(jackknife_stats)\n",
    "            jackknife_mean = np.mean(jackknife_stats)\n",
    "            \n",
    "            # Acceleration parameter\n",
    "            numerator = np.sum((jackknife_mean - jackknife_stats)**3)\n",
    "            denominator = 6 * (np.sum((jackknife_mean - jackknife_stats)**2))**1.5\n",
    "            acceleration = numerator / denominator if denominator != 0 else 0\n",
    "            \n",
    "            # BCa percentiles\n",
    "            z_alpha_2 = stats.norm.ppf(alpha/2)\n",
    "            z_1_alpha_2 = stats.norm.ppf(1 - alpha/2)\n",
    "            \n",
    "            alpha_1 = stats.norm.cdf(bias_correction + (bias_correction + z_alpha_2)/(1 - acceleration*(bias_correction + z_alpha_2)))\n",
    "            alpha_2 = stats.norm.cdf(bias_correction + (bias_correction + z_1_alpha_2)/(1 - acceleration*(bias_correction + z_1_alpha_2)))\n",
    "            \n",
    "            # Ensure percentiles are within valid range\n",
    "            alpha_1 = max(0, min(1, alpha_1))\n",
    "            alpha_2 = max(0, min(1, alpha_2))\n",
    "            \n",
    "            bca_lower = np.percentile(bootstrap_stats, alpha_1 * 100)\n",
    "            bca_upper = np.percentile(bootstrap_stats, alpha_2 * 100)\n",
    "            \n",
    "            return bca_lower, bca_upper, bias_correction, acceleration\n",
    "        \n",
    "        # Calculate BCa intervals for key metrics\n",
    "        if 'accuracy_improvement' in bootstrap_results and bootstrap_results['accuracy_improvement']:\n",
    "            acc_bca_lower, acc_bca_upper, acc_bias, acc_accel = calculate_bca_interval_accuracy(\n",
    "                hr_data, standard_data, accuracy_improvement_statistic\n",
    "            )\n",
    "            \n",
    "            print(f\"BCa Confidence Intervals (95%):\")\n",
    "            print(f\"  Accuracy Improvement:\")\n",
    "            print(f\"    Standard CI: [{bootstrap_results['accuracy_improvement']['confidence_interval'][0]:.1f}%, {bootstrap_results['accuracy_improvement']['confidence_interval'][1]:.1f}%]\")\n",
    "            print(f\"    BCa CI: [{acc_bca_lower:.1f}%, {acc_bca_upper:.1f}%]\")\n",
    "            print(f\"    Bias correction: {acc_bias:.3f}\")\n",
    "            print(f\"    Acceleration: {acc_accel:.3f}\")\n",
    "            \n",
    "            bootstrap_results['bca_intervals'] = {\n",
    "                'accuracy_improvement_bca': (acc_bca_lower, acc_bca_upper),\n",
    "                'accuracy_bias_correction': acc_bias,\n",
    "                'accuracy_acceleration': acc_accel\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BCa intervals: {e}\")\n",
    "        bootstrap_results['bca_intervals'] = {}\n",
    "    \n",
    "    # 6. Bootstrap Hypothesis Testing Summary\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BOOTSTRAP HYPOTHESIS TESTING SUMMARY - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    significant_tests = 0\n",
    "    total_tests = 0\n",
    "    meets_threshold_tests = 0\n",
    "    high_confidence_tests = 0\n",
    "    \n",
    "    for test_name, results in bootstrap_results.items():\n",
    "        if test_name in ['bca_intervals']:\n",
    "            continue\n",
    "        if results and 'significant' in results:\n",
    "            total_tests += 1\n",
    "            if results['significant']:\n",
    "                significant_tests += 1\n",
    "            if results.get('meets_threshold', False):\n",
    "                meets_threshold_tests += 1\n",
    "            if results.get('threshold_confidence', 0) > 0.8:\n",
    "                high_confidence_tests += 1\n",
    "    \n",
    "    print(f\"\\nOverall Results:\")\n",
    "    print(f\"  Total bootstrap tests: {total_tests}\")\n",
    "    print(f\"  Statistically significant: {significant_tests}/{total_tests}\")\n",
    "    print(f\"  Meet >30% threshold: {meets_threshold_tests}/{total_tests}\")\n",
    "    print(f\"  High confidence (>80%) for 30% threshold: {high_confidence_tests}/{total_tests}\")\n",
    "    \n",
    "    # Final verdict for bootstrap testing\n",
    "    any_significant = significant_tests > 0\n",
    "    any_threshold = meets_threshold_tests > 0\n",
    "    high_confidence = high_confidence_tests > 0\n",
    "    \n",
    "    print(f\"\\n🎯 BOOTSTRAP TESTING VERDICT:\")\n",
    "    print(f\"  Statistical Evidence: {'Found' if any_significant else 'Not Found'}\")\n",
    "    print(f\"  Practical Significance: {'Found' if any_threshold else 'Not Found'}\")\n",
    "    print(f\"  High Confidence in 30% Threshold: {'YES' if high_confidence else 'NO'}\")\n",
    "    print(f\"  Overall Support: {'STRONG' if any_significant and any_threshold and high_confidence else 'MODERATE' if any_significant or any_threshold else 'WEAK'}\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF ACCURACY-BASED BOOTSTRAP TESTING:\")\n",
    "    print(f\"  ✓ Tests accuracy improvements against ground truth\")\n",
    "    print(f\"  ✓ Provides confidence intervals for complex accuracy statistics\")\n",
    "    print(f\"  ✓ Accounts for bias in small samples (BCa intervals)\")\n",
    "    print(f\"  ✓ Direct probability statements about thresholds\")\n",
    "    print(f\"  ✓ Robust to non-normal distributions\")\n",
    "    print(f\"  ✓ Intuitive interpretation of results\")\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "# Execute accuracy-based bootstrap hypothesis testing\n",
    "if ('accuracy_test_results' in globals() and accuracy_test_results and \n",
    "    STATISTICAL_CONFIG.get('bootstrap_tests', False)):\n",
    "    accuracy_bootstrap_results = perform_bootstrap_accuracy_testing()\n",
    "    print(f\"\\n✓ Accuracy-based bootstrap testing completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform accuracy-based bootstrap testing:\")\n",
    "    if 'accuracy_test_results' not in globals() or not accuracy_test_results:\n",
    "        print(\"  - Accuracy test results not available (run Cell 11 first)\")\n",
    "    if not STATISTICAL_CONFIG.get('bootstrap_tests', False):\n",
    "        print(\"  - Bootstrap testing functionality not available\")\n",
    "    \n",
    "    accuracy_bootstrap_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [14] - Bayesian Analysis for Estimation Accuracy vs Ground Truth\n",
    "# Purpose: Bayesian hypothesis testing with posterior probability statements for accuracy improvements\n",
    "# Dependencies: accuracy_test_results from Cell 11, pymc, arviz from Cell 0, BAYESIAN_CONFIG\n",
    "# Breadcrumbs: Setup -> Analysis -> Accuracy Testing -> Bayesian Analysis for Accuracy\n",
    "\n",
    "def perform_bayesian_accuracy_analysis():\n",
    "    \"\"\"\n",
    "    Perform comprehensive Bayesian analysis for estimation accuracy vs ground truth\n",
    "    Tests whether hallucination-reducing techniques improve accuracy by >30% through Bayesian inference\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian analysis results with posterior probabilities and credible intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"BAYESIAN ANALYSIS - ESTIMATION ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve estimation accuracy by >30%\")\n",
    "    print(\"METHOD: Bayesian inference with posterior probability statements on accuracy metrics\")\n",
    "    \n",
    "    if not accuracy_test_results or not STATISTICAL_CONFIG.get('bayesian_analysis', False):\n",
    "        print(\"\\nWarning: Insufficient data or Bayesian functionality not available\")\n",
    "        return {}\n",
    "    \n",
    "    accuracy_df = accuracy_test_results['accuracy_df']\n",
    "    hr_metrics = accuracy_test_results['hr_metrics']\n",
    "    standard_metrics = accuracy_test_results['standard_metrics']\n",
    "    \n",
    "    # Extract accuracy data for Bayesian analysis\n",
    "    hr_data = hr_metrics['group_data']\n",
    "    standard_data = standard_metrics['group_data']\n",
    "    \n",
    "    print(f\"\\nACCURACY DATA SUMMARY:\")\n",
    "    print(f\"  Total estimates: {len(accuracy_df)}\")\n",
    "    print(f\"  Hallucination-reducing estimates: {len(hr_data)}\")\n",
    "    print(f\"  Standard estimates: {len(standard_data)}\")\n",
    "    print(f\"  Ground truth baseline: {accuracy_test_results['ground_truth_baseline']:.1f} LOC/SiFP\")\n",
    "    \n",
    "    if len(hr_data) == 0 or len(standard_data) == 0:\n",
    "        print(\"Warning: Cannot perform Bayesian analysis - insufficient group sizes\")\n",
    "        return {}\n",
    "    \n",
    "    bayesian_results = {}\n",
    "    \n",
    "    print(f\"\\nBAYESIAN ANALYSIS CONFIGURATION:\")\n",
    "    print(f\"  MCMC samples: {CONFIG.get('BAYESIAN_SAMPLES', 2000):,}\")\n",
    "    print(f\"  MCMC chains: {CONFIG.get('BAYESIAN_CHAINS', 4)}\")\n",
    "    print(f\"  Improvement threshold: {CONFIG.get('IMPROVEMENT_THRESHOLD', 0.30):.0%}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    \n",
    "    # 1. Bayesian analysis for overall accuracy improvement\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"1. BAYESIAN ANALYSIS: OVERALL ACCURACY IMPROVEMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Bayesian model for accuracy improvement...\")\n",
    "        \n",
    "        # Prepare accuracy data\n",
    "        hr_accuracy_counts = hr_data['classification'].value_counts()\n",
    "        standard_accuracy_counts = standard_data['classification'].value_counts()\n",
    "        \n",
    "        hr_tp_count = hr_accuracy_counts.get('TP', 0)\n",
    "        hr_total = len(hr_data)\n",
    "        standard_tp_count = standard_accuracy_counts.get('TP', 0)\n",
    "        standard_total = len(standard_data)\n",
    "        \n",
    "        with pm.Model() as accuracy_model:\n",
    "            # Priors for accuracy rates (Beta distribution for proportions)\n",
    "            p_hr = pm.Beta('p_hr', alpha=1, beta=1)  # Uniform prior\n",
    "            p_standard = pm.Beta('p_standard', alpha=1, beta=1)\n",
    "            \n",
    "            # Likelihood for observed accuracy counts\n",
    "            hr_successes = pm.Binomial('hr_successes', n=hr_total, p=p_hr, observed=hr_tp_count)\n",
    "            standard_successes = pm.Binomial('standard_successes', n=standard_total, p=p_standard, observed=standard_tp_count)\n",
    "            \n",
    "            # Derived quantities of interest\n",
    "            accuracy_difference = pm.Deterministic('accuracy_difference', p_hr - p_standard)\n",
    "            accuracy_improvement_pct = pm.Deterministic('accuracy_improvement_pct', \n",
    "                                                       100 * accuracy_difference / p_standard)\n",
    "            \n",
    "            # Probability of >30% improvement\n",
    "            improvement_30_indicator = pm.Deterministic('improvement_30_plus', \n",
    "                                                      pm.math.switch(accuracy_improvement_pct > 30, 1, 0))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace_accuracy = pm.sample(\n",
    "                draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=42,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        accuracy_improvement_samples = trace_accuracy.posterior['accuracy_improvement_pct'].values.flatten()\n",
    "        accuracy_difference_samples = trace_accuracy.posterior['accuracy_difference'].values.flatten()\n",
    "        \n",
    "        # Calculate posterior statistics\n",
    "        accuracy_posterior_mean = np.mean(accuracy_improvement_samples)\n",
    "        accuracy_posterior_std = np.std(accuracy_improvement_samples)\n",
    "        accuracy_credible_interval = np.percentile(accuracy_improvement_samples, [2.5, 97.5])\n",
    "        \n",
    "        # Probability of >30% improvement\n",
    "        prob_30_plus = np.mean(accuracy_improvement_samples > 30)\n",
    "        prob_any_improvement = np.mean(accuracy_improvement_samples > 0)\n",
    "        \n",
    "        # ROPE analysis (Region of Practical Equivalence: -5% to +5%)\n",
    "        rope_lower, rope_upper = -5, 5\n",
    "        prob_in_rope = np.mean((accuracy_improvement_samples >= rope_lower) & \n",
    "                              (accuracy_improvement_samples <= rope_upper))\n",
    "        prob_above_rope = np.mean(accuracy_improvement_samples > rope_upper)\n",
    "        \n",
    "        print(f\"\\nBayesian Results for Accuracy Improvement:\")\n",
    "        print(f\"  Observed HR accuracy: {hr_tp_count}/{hr_total} ({hr_tp_count/hr_total:.2%})\")\n",
    "        print(f\"  Observed Standard accuracy: {standard_tp_count}/{standard_total} ({standard_tp_count/standard_total:.2%})\")\n",
    "        print(f\"  Posterior mean improvement: {accuracy_posterior_mean:.1f}%\")\n",
    "        print(f\"  Posterior std improvement: {accuracy_posterior_std:.1f}%\")\n",
    "        print(f\"  95% Credible interval: [{accuracy_credible_interval[0]:.1f}%, {accuracy_credible_interval[1]:.1f}%]\")\n",
    "        print(f\"  P(improvement > 0%): {prob_any_improvement:.3f}\")\n",
    "        print(f\"  P(improvement > 30%): {prob_30_plus:.3f}\")\n",
    "        print(f\"  P(improvement in ROPE [-5%, +5%]): {prob_in_rope:.3f}\")\n",
    "        print(f\"  P(improvement > ROPE): {prob_above_rope:.3f}\")\n",
    "        \n",
    "        bayesian_results['accuracy_improvement'] = {\n",
    "            'posterior_mean': accuracy_posterior_mean,\n",
    "            'posterior_std': accuracy_posterior_std,\n",
    "            'credible_interval': accuracy_credible_interval,\n",
    "            'prob_any_improvement': prob_any_improvement,\n",
    "            'prob_30_plus': prob_30_plus,\n",
    "            'prob_in_rope': prob_in_rope,\n",
    "            'prob_above_rope': prob_above_rope,\n",
    "            'trace': trace_accuracy,\n",
    "            'posterior_samples': accuracy_improvement_samples\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian accuracy improvement analysis: {e}\")\n",
    "        bayesian_results['accuracy_improvement'] = {}\n",
    "    \n",
    "    # 2. Bayesian analysis for False Positive reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"2. BAYESIAN ANALYSIS: FALSE POSITIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Bayesian model for False Positive reduction...\")\n",
    "        \n",
    "        # Prepare FP data\n",
    "        hr_fp_count = hr_accuracy_counts.get('FP', 0)\n",
    "        standard_fp_count = standard_accuracy_counts.get('FP', 0)\n",
    "        \n",
    "        with pm.Model() as fp_model:\n",
    "            # Priors for FP rates\n",
    "            p_hr_fp = pm.Beta('p_hr_fp', alpha=1, beta=1)\n",
    "            p_standard_fp = pm.Beta('p_standard_fp', alpha=1, beta=1)\n",
    "            \n",
    "            # Likelihood for observed FP counts\n",
    "            hr_fp_obs = pm.Binomial('hr_fp_obs', n=hr_total, p=p_hr_fp, observed=hr_fp_count)\n",
    "            standard_fp_obs = pm.Binomial('standard_fp_obs', n=standard_total, p=p_standard_fp, observed=standard_fp_count)\n",
    "            \n",
    "            # Derived quantities\n",
    "            fp_reduction = pm.Deterministic('fp_reduction', p_standard_fp - p_hr_fp)\n",
    "            fp_reduction_pct = pm.Deterministic('fp_reduction_pct', \n",
    "                                              100 * fp_reduction / p_standard_fp)\n",
    "            \n",
    "            # Probability of >30% FP reduction\n",
    "            fp_30_indicator = pm.Deterministic('fp_30_plus', \n",
    "                                             pm.math.switch(fp_reduction_pct > 30, 1, 0))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace_fp = pm.sample(\n",
    "                draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=42,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        fp_reduction_samples = trace_fp.posterior['fp_reduction_pct'].values.flatten()\n",
    "        \n",
    "        # Calculate posterior statistics\n",
    "        fp_posterior_mean = np.mean(fp_reduction_samples)\n",
    "        fp_posterior_std = np.std(fp_reduction_samples)\n",
    "        fp_credible_interval = np.percentile(fp_reduction_samples, [2.5, 97.5])\n",
    "        \n",
    "        # Probability calculations\n",
    "        prob_fp_30_plus = np.mean(fp_reduction_samples > 30)\n",
    "        prob_fp_any_reduction = np.mean(fp_reduction_samples > 0)\n",
    "        \n",
    "        print(f\"\\nBayesian Results for False Positive Reduction:\")\n",
    "        print(f\"  Observed HR FP rate: {hr_fp_count}/{hr_total} ({hr_fp_count/hr_total:.2%})\")\n",
    "        print(f\"  Observed Standard FP rate: {standard_fp_count}/{standard_total} ({standard_fp_count/standard_total:.2%})\")\n",
    "        print(f\"  Posterior mean reduction: {fp_posterior_mean:.1f}%\")\n",
    "        print(f\"  Posterior std reduction: {fp_posterior_std:.1f}%\")\n",
    "        print(f\"  95% Credible interval: [{fp_credible_interval[0]:.1f}%, {fp_credible_interval[1]:.1f}%]\")\n",
    "        print(f\"  P(reduction > 0%): {prob_fp_any_reduction:.3f}\")\n",
    "        print(f\"  P(reduction > 30%): {prob_fp_30_plus:.3f}\")\n",
    "        \n",
    "        bayesian_results['fp_reduction'] = {\n",
    "            'posterior_mean': fp_posterior_mean,\n",
    "            'posterior_std': fp_posterior_std,\n",
    "            'credible_interval': fp_credible_interval,\n",
    "            'prob_any_reduction': prob_fp_any_reduction,\n",
    "            'prob_30_plus': prob_fp_30_plus,\n",
    "            'trace': trace_fp,\n",
    "            'posterior_samples': fp_reduction_samples\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian FP reduction analysis: {e}\")\n",
    "        bayesian_results['fp_reduction'] = {}\n",
    "    \n",
    "    # 3. Bayesian analysis for False Negative reduction\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"3. BAYESIAN ANALYSIS: FALSE NEGATIVE REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Bayesian model for False Negative reduction...\")\n",
    "        \n",
    "        # Prepare FN data\n",
    "        hr_fn_count = hr_accuracy_counts.get('FN', 0)\n",
    "        standard_fn_count = standard_accuracy_counts.get('FN', 0)\n",
    "        \n",
    "        with pm.Model() as fn_model:\n",
    "            # Priors for FN rates\n",
    "            p_hr_fn = pm.Beta('p_hr_fn', alpha=1, beta=1)\n",
    "            p_standard_fn = pm.Beta('p_standard_fn', alpha=1, beta=1)\n",
    "            \n",
    "            # Likelihood for observed FN counts\n",
    "            hr_fn_obs = pm.Binomial('hr_fn_obs', n=hr_total, p=p_hr_fn, observed=hr_fn_count)\n",
    "            standard_fn_obs = pm.Binomial('standard_fn_obs', n=standard_total, p=p_standard_fn, observed=standard_fn_count)\n",
    "            \n",
    "            # Derived quantities\n",
    "            fn_reduction = pm.Deterministic('fn_reduction', p_standard_fn - p_hr_fn)\n",
    "            fn_reduction_pct = pm.Deterministic('fn_reduction_pct', \n",
    "                                              100 * fn_reduction / p_standard_fn)\n",
    "            \n",
    "            # Probability of >30% FN reduction\n",
    "            fn_30_indicator = pm.Deterministic('fn_30_plus', \n",
    "                                             pm.math.switch(fn_reduction_pct > 30, 1, 0))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace_fn = pm.sample(\n",
    "                draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=42,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        fn_reduction_samples = trace_fn.posterior['fn_reduction_pct'].values.flatten()\n",
    "        \n",
    "        # Calculate posterior statistics\n",
    "        fn_posterior_mean = np.mean(fn_reduction_samples)\n",
    "        fn_posterior_std = np.std(fn_reduction_samples)\n",
    "        fn_credible_interval = np.percentile(fn_reduction_samples, [2.5, 97.5])\n",
    "        \n",
    "        # Probability calculations\n",
    "        prob_fn_30_plus = np.mean(fn_reduction_samples > 30)\n",
    "        prob_fn_any_reduction = np.mean(fn_reduction_samples > 0)\n",
    "        \n",
    "        print(f\"\\nBayesian Results for False Negative Reduction:\")\n",
    "        print(f\"  Observed HR FN rate: {hr_fn_count}/{hr_total} ({hr_fn_count/hr_total:.2%})\")\n",
    "        print(f\"  Observed Standard FN rate: {standard_fn_count}/{standard_total} ({standard_fn_count/standard_total:.2%})\")\n",
    "        print(f\"  Posterior mean reduction: {fn_posterior_mean:.1f}%\")\n",
    "        print(f\"  Posterior std reduction: {fn_posterior_std:.1f}%\")\n",
    "        print(f\"  95% Credible interval: [{fn_credible_interval[0]:.1f}%, {fn_credible_interval[1]:.1f}%]\")\n",
    "        print(f\"  P(reduction > 0%): {prob_fn_any_reduction:.3f}\")\n",
    "        print(f\"  P(reduction > 30%): {prob_fn_30_plus:.3f}\")\n",
    "        \n",
    "        bayesian_results['fn_reduction'] = {\n",
    "            'posterior_mean': fn_posterior_mean,\n",
    "            'posterior_std': fn_posterior_std,\n",
    "            'credible_interval': fn_credible_interval,\n",
    "            'prob_any_reduction': prob_fn_any_reduction,\n",
    "            'prob_30_plus': prob_fn_30_plus,\n",
    "            'trace': trace_fn,\n",
    "            'posterior_samples': fn_reduction_samples\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian FN reduction analysis: {e}\")\n",
    "        bayesian_results['fn_reduction'] = {}\n",
    "    \n",
    "    # 4. Bayesian Model Comparison using WAIC\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"4. BAYESIAN MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        print(\"Comparing models with and without hallucination-reducing effect...\")\n",
    "        \n",
    "        # Model with treatment effect (already computed above)\n",
    "        if 'accuracy_improvement' in bayesian_results and bayesian_results['accuracy_improvement']:\n",
    "            trace_with_effect = bayesian_results['accuracy_improvement']['trace']\n",
    "            \n",
    "            # Null model (no treatment effect - pooled accuracy)\n",
    "            with pm.Model() as null_model:\n",
    "                # Single accuracy rate for all estimates\n",
    "                p_pooled = pm.Beta('p_pooled', alpha=1, beta=1)\n",
    "                \n",
    "                # All observations from same distribution\n",
    "                total_tp = hr_tp_count + standard_tp_count\n",
    "                total_estimates = hr_total + standard_total\n",
    "                pooled_obs = pm.Binomial('pooled_obs', n=total_estimates, p=p_pooled, observed=total_tp)\n",
    "                \n",
    "                trace_null = pm.sample(\n",
    "                    draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                    chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                    tune=1000,\n",
    "                    random_seed=42,\n",
    "                    return_inferencedata=True\n",
    "                )\n",
    "            \n",
    "            # Calculate WAIC for model comparison\n",
    "            waic_with_effect = az.waic(trace_with_effect)\n",
    "            waic_null = az.waic(trace_null)\n",
    "            \n",
    "            # Bayes Factor approximation using WAIC\n",
    "            delta_waic = waic_with_effect.waic - waic_null.waic\n",
    "            \n",
    "            print(f\"Model Comparison Results:\")\n",
    "            print(f\"  Treatment effect model WAIC: {waic_with_effect.waic:.2f} ± {waic_with_effect.waic_se:.2f}\")\n",
    "            print(f\"  Null model WAIC: {waic_null.waic:.2f} ± {waic_null.waic_se:.2f}\")\n",
    "            print(f\"  ΔWAIC (effect - null): {delta_waic:.2f}\")\n",
    "            \n",
    "            if delta_waic < -2:\n",
    "                model_preference = \"Strong evidence for treatment effect\"\n",
    "            elif delta_waic < 0:\n",
    "                model_preference = \"Moderate evidence for treatment effect\"\n",
    "            elif delta_waic < 2:\n",
    "                model_preference = \"Weak evidence either way\"\n",
    "            else:\n",
    "                model_preference = \"Evidence against treatment effect\"\n",
    "            \n",
    "            print(f\"  Model preference: {model_preference}\")\n",
    "            \n",
    "            bayesian_results['model_comparison'] = {\n",
    "                'waic_with_effect': waic_with_effect.waic,\n",
    "                'waic_null': waic_null.waic,\n",
    "                'delta_waic': delta_waic,\n",
    "                'preference': model_preference\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian model comparison: {e}\")\n",
    "        bayesian_results['model_comparison'] = {}\n",
    "    \n",
    "    # 5. Posterior Predictive Checks\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"5. POSTERIOR PREDICTIVE CHECKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        if 'accuracy_improvement' in bayesian_results and bayesian_results['accuracy_improvement']:\n",
    "            trace_accuracy = bayesian_results['accuracy_improvement']['trace']\n",
    "            \n",
    "            # Generate posterior predictive samples\n",
    "            with accuracy_model:\n",
    "                ppc = pm.sample_posterior_predictive(trace_accuracy, random_seed=42)\n",
    "            \n",
    "            # Compare observed vs predicted accuracy rates\n",
    "            obs_hr_rate = hr_tp_count / hr_total\n",
    "            obs_standard_rate = standard_tp_count / standard_total\n",
    "            obs_difference = obs_hr_rate - obs_standard_rate\n",
    "            \n",
    "            pred_hr_rates = ppc.posterior_predictive['hr_successes'].values.flatten() / hr_total\n",
    "            pred_standard_rates = ppc.posterior_predictive['standard_successes'].values.flatten() / standard_total\n",
    "            pred_differences = pred_hr_rates - pred_standard_rates\n",
    "            \n",
    "            # Calculate Bayesian p-values\n",
    "            p_value_difference = np.mean(pred_differences >= obs_difference)\n",
    "            \n",
    "            print(f\"Posterior Predictive Check Results:\")\n",
    "            print(f\"  Observed HR accuracy: {obs_hr_rate:.2%}\")\n",
    "            print(f\"  Predicted HR accuracy: {np.mean(pred_hr_rates):.2%} ± {np.std(pred_hr_rates):.2%}\")\n",
    "            print(f\"  Observed Standard accuracy: {obs_standard_rate:.2%}\")\n",
    "            print(f\"  Predicted Standard accuracy: {np.mean(pred_standard_rates):.2%} ± {np.std(pred_standard_rates):.2%}\")\n",
    "            print(f\"  Bayesian p-value (difference): {p_value_difference:.3f}\")\n",
    "            \n",
    "            bayesian_results['posterior_predictive'] = {\n",
    "                'obs_hr_rate': obs_hr_rate,\n",
    "                'obs_standard_rate': obs_standard_rate,\n",
    "                'pred_hr_rates': pred_hr_rates,\n",
    "                'pred_standard_rates': pred_standard_rates,\n",
    "                'p_value_difference': p_value_difference\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in posterior predictive checks: {e}\")\n",
    "        bayesian_results['posterior_predictive'] = {}\n",
    "    \n",
    "    # 6. Bayesian Decision Analysis\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"6. BAYESIAN DECISION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        if ('accuracy_improvement' in bayesian_results and bayesian_results['accuracy_improvement'] and\n",
    "            'fp_reduction' in bayesian_results and bayesian_results['fp_reduction'] and\n",
    "            'fn_reduction' in bayesian_results and bayesian_results['fn_reduction']):\n",
    "            \n",
    "            accuracy_samples = bayesian_results['accuracy_improvement']['posterior_samples']\n",
    "            fp_samples = bayesian_results['fp_reduction']['posterior_samples']\n",
    "            fn_samples = bayesian_results['fn_reduction']['posterior_samples']\n",
    "            \n",
    "            # Combined probability of achieving >30% in any metric\n",
    "            prob_any_30 = np.mean((accuracy_samples > 30) | (fp_samples > 30) | (fn_samples > 30))\n",
    "            \n",
    "            # Combined probability of achieving >30% in all metrics\n",
    "            prob_all_30 = np.mean((accuracy_samples > 30) & (fp_samples > 30) & (fn_samples > 30))\n",
    "            \n",
    "            # Expected values\n",
    "            expected_accuracy_improvement = np.mean(accuracy_samples)\n",
    "            expected_fp_reduction = np.mean(fp_samples)\n",
    "            expected_fn_reduction = np.mean(fn_samples)\n",
    "            \n",
    "            # Risk assessment (probability of negative outcomes)\n",
    "            prob_accuracy_worse = np.mean(accuracy_samples < -10)  # >10% worse accuracy\n",
    "            prob_fp_worse = np.mean(fp_samples < -10)  # >10% worse FP rate\n",
    "            prob_fn_worse = np.mean(fn_samples < -10)  # >10% worse FN rate\n",
    "            \n",
    "            print(f\"Bayesian Decision Analysis:\")\n",
    "            print(f\"  P(>30% improvement in any metric): {prob_any_30:.3f}\")\n",
    "            print(f\"  P(>30% improvement in all metrics): {prob_all_30:.3f}\")\n",
    "            print(f\"  Expected accuracy improvement: {expected_accuracy_improvement:.1f}%\")\n",
    "            print(f\"  Expected FP reduction: {expected_fp_reduction:.1f}%\")\n",
    "            print(f\"  Expected FN reduction: {expected_fn_reduction:.1f}%\")\n",
    "            print(f\"  P(>10% worse accuracy): {prob_accuracy_worse:.3f}\")\n",
    "            print(f\"  P(>10% worse FP rate): {prob_fp_worse:.3f}\")\n",
    "            print(f\"  P(>10% worse FN rate): {prob_fn_worse:.3f}\")\n",
    "            \n",
    "            # Decision recommendation\n",
    "            if prob_any_30 > 0.8:\n",
    "                decision = \"STRONG RECOMMENDATION: Adopt hallucination-reducing techniques\"\n",
    "            elif prob_any_30 > 0.6:\n",
    "                decision = \"MODERATE RECOMMENDATION: Consider adoption with monitoring\"\n",
    "            elif prob_any_30 > 0.4:\n",
    "                decision = \"WEAK RECOMMENDATION: Proceed with caution\"\n",
    "            else:\n",
    "                decision = \"NOT RECOMMENDED: Insufficient evidence of benefit\"\n",
    "            \n",
    "            print(f\"\\nDecision Recommendation: {decision}\")\n",
    "            \n",
    "            bayesian_results['decision_analysis'] = {\n",
    "                'prob_any_30': prob_any_30,\n",
    "                'prob_all_30': prob_all_30,\n",
    "                'expected_accuracy_improvement': expected_accuracy_improvement,\n",
    "                'expected_fp_reduction': expected_fp_reduction,\n",
    "                'expected_fn_reduction': expected_fn_reduction,\n",
    "                'prob_accuracy_worse': prob_accuracy_worse,\n",
    "                'prob_fp_worse': prob_fp_worse,\n",
    "                'prob_fn_worse': prob_fn_worse,\n",
    "                'recommendation': decision\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian decision analysis: {e}\")\n",
    "        bayesian_results['decision_analysis'] = {}\n",
    "    \n",
    "    # 7. Bayesian Analysis Summary\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"BAYESIAN ANALYSIS SUMMARY - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nKey Posterior Probabilities:\")\n",
    "    if 'accuracy_improvement' in bayesian_results and bayesian_results['accuracy_improvement']:\n",
    "        print(f\"  P(Accuracy improvement > 30%): {bayesian_results['accuracy_improvement']['prob_30_plus']:.3f}\")\n",
    "    if 'fp_reduction' in bayesian_results and bayesian_results['fp_reduction']:\n",
    "        print(f\"  P(FP reduction > 30%): {bayesian_results['fp_reduction']['prob_30_plus']:.3f}\")\n",
    "    if 'fn_reduction' in bayesian_results and bayesian_results['fn_reduction']:\n",
    "        print(f\"  P(FN reduction > 30%): {bayesian_results['fn_reduction']['prob_30_plus']:.3f}\")\n",
    "    if 'decision_analysis' in bayesian_results and bayesian_results['decision_analysis']:\n",
    "        print(f\"  P(Any metric > 30%): {bayesian_results['decision_analysis']['prob_any_30']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nModel Evidence:\")\n",
    "    if 'model_comparison' in bayesian_results and bayesian_results['model_comparison']:\n",
    "        print(f\"  {bayesian_results['model_comparison']['preference']}\")\n",
    "        print(f\"  ΔWAIC: {bayesian_results['model_comparison']['delta_waic']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF BAYESIAN ACCURACY ANALYSIS:\")\n",
    "    print(f\"  ✓ Direct probability statements about accuracy improvements\")\n",
    "    print(f\"  ✓ Credible intervals for TP/FP/FN rates\")\n",
    "    print(f\"  ✓ Incorporates uncertainty in ground truth comparisons\")\n",
    "    print(f\"  ✓ Model comparison for treatment vs null hypotheses\")\n",
    "    print(f\"  ✓ Decision-theoretic framework for accuracy-based recommendations\")\n",
    "    print(f\"  ✓ Handles small sample sizes with proper uncertainty quantification\")\n",
    "    \n",
    "    return bayesian_results\n",
    "\n",
    "# Execute Bayesian accuracy analysis\n",
    "if ('accuracy_test_results' in globals() and accuracy_test_results and \n",
    "    STATISTICAL_CONFIG.get('bayesian_analysis', False)):\n",
    "    bayesian_accuracy_results = perform_bayesian_accuracy_analysis()\n",
    "    print(f\"\\n✓ Bayesian accuracy analysis completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform Bayesian accuracy analysis:\")\n",
    "    if 'accuracy_test_results' not in globals() or not accuracy_test_results:\n",
    "        print(\"  - Accuracy test results not available (run Cell 11 first)\")\n",
    "    if not STATISTICAL_CONFIG.get('bayesian_analysis', False):\n",
    "        print(\"  - Bayesian analysis functionality not available\")\n",
    "        print(\"  - Install with: pip install 'praxis-requirements-analyzer[dev]'\")\n",
    "    \n",
    "    bayesian_accuracy_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [15] - Enhanced Statistical Validation Visualizations with Advanced Methods (UPDATED FOR ACCURACY)\n",
    "# Purpose: Create comprehensive visualizations for all statistical tests including permutation, bootstrap, and Bayesian results\n",
    "# Dependencies: accuracy_test_results, accuracy_permutation_results, accuracy_bootstrap_results, bayesian_accuracy_results from previous cells\n",
    "# Breadcrumbs: Setup -> Analysis -> Advanced Statistical Testing -> Enhanced Validation Visualizations for Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "import numpy as np\n",
    "\n",
    "def safe_format_value(value, format_spec, fallback='N/A'):\n",
    "    \"\"\"\n",
    "    Safely format a value with the given format specification\n",
    "    \n",
    "    Args:\n",
    "        value: The value to format\n",
    "        format_spec: Format specification (e.g., '.4f', '.1f')\n",
    "        fallback: Value to return if formatting fails\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted value or fallback\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if value is None or value == 'N/A' or (isinstance(value, str) and value.lower() == 'n/a'):\n",
    "            return fallback\n",
    "        # Try to convert to float and format\n",
    "        float_value = float(value)\n",
    "        return f\"{float_value:{format_spec}}\"\n",
    "    except (ValueError, TypeError):\n",
    "        return fallback\n",
    "\n",
    "def create_enhanced_accuracy_visualizations():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations to support all statistical hypothesis testing methods for accuracy analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    if not accuracy_test_results or 'accuracy_df' not in accuracy_test_results:\n",
    "        print(\"No accuracy data available for statistical visualizations\")\n",
    "        return\n",
    "    \n",
    "    accuracy_df = accuracy_test_results['accuracy_df']\n",
    "    hr_metrics = accuracy_test_results['hr_metrics']\n",
    "    standard_metrics = accuracy_test_results['standard_metrics']\n",
    "    \n",
    "    # Set up the enhanced figure with more subplots\n",
    "    fig = plt.figure(figsize=(24, 20), constrained_layout=True)\n",
    "    fig.suptitle('Enhanced Statistical Validation of Hallucination-Reducing Techniques\\n' +\n",
    "                 'Hypothesis: >30% Improvement in Estimation Accuracy vs Ground Truth\\n' +\n",
    "                 'Methods: Classical, Permutation, Bootstrap, and Bayesian Analysis', fontsize=18)\n",
    "    \n",
    "    # Create a more complex grid layout\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Extract data for visualizations\n",
    "    hr_data = hr_metrics['group_data']\n",
    "    standard_data = standard_metrics['group_data']\n",
    "    \n",
    "    # 1. Enhanced accuracy classification comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    # Create accuracy classification data for plotting\n",
    "    hr_classifications = hr_data['classification'].value_counts()\n",
    "    standard_classifications = standard_data['classification'].value_counts()\n",
    "    \n",
    "    # Ensure all categories are present\n",
    "    categories = ['TP', 'FP', 'FN']\n",
    "    hr_counts = [hr_classifications.get(cat, 0) for cat in categories]\n",
    "    standard_counts = [standard_classifications.get(cat, 0) for cat in categories]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, standard_counts, width, label='Standard', alpha=0.7, color='lightcoral')\n",
    "    bars2 = ax1.bar(x + width/2, hr_counts, width, label='Hallucination-Reducing', alpha=0.7, color='lightgreen')\n",
    "    \n",
    "    ax1.set_xlabel('Classification')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Accuracy Classifications: HR vs Standard\\nTP=Correct, FP=Overestimate, FN=Underestimate')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, (std_count, hr_count) in enumerate(zip(standard_counts, hr_counts)):\n",
    "        std_total = sum(standard_counts)\n",
    "        hr_total = sum(hr_counts)\n",
    "        if std_total > 0:\n",
    "            ax1.text(i - width/2, std_count + 0.1, f'{std_count/std_total:.1%}', \n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "        if hr_total > 0:\n",
    "            ax1.text(i + width/2, hr_count + 0.1, f'{hr_count/hr_total:.1%}', \n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Add comprehensive significance indicators\n",
    "    test_results_text = \"\"\n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        if 'chi_square' in accuracy_test_results['statistical_tests']:\n",
    "            p_val = accuracy_test_results['statistical_tests']['chi_square'].get('p_value', 'N/A')\n",
    "            test_results_text += f\"χ²-test: p={safe_format_value(p_val, '.4f')}\\n\"\n",
    "    if 'accuracy_permutation_results' in globals() and 'accuracy_improvement' in accuracy_permutation_results:\n",
    "        perm_p = accuracy_permutation_results['accuracy_improvement'].get('p_value', 'N/A')\n",
    "        test_results_text += f\"Permutation: p={safe_format_value(perm_p, '.4f')}\\n\"\n",
    "    if 'accuracy_bootstrap_results' in globals() and 'accuracy_improvement' in accuracy_bootstrap_results:\n",
    "        boot_p = accuracy_bootstrap_results['accuracy_improvement'].get('p_value_30_percent', 'N/A')\n",
    "        test_results_text += f\"Bootstrap: p={safe_format_value(boot_p, '.4f')}\\n\"\n",
    "    if 'bayesian_accuracy_results' in globals() and 'accuracy_improvement' in bayesian_accuracy_results:\n",
    "        bayes_prob = bayesian_accuracy_results['accuracy_improvement'].get('prob_30_plus', 'N/A')\n",
    "        test_results_text += f\"Bayesian P(>30%): {safe_format_value(bayes_prob, '.3f')}\"\n",
    "    \n",
    "    ax1.text(0.02, 0.98, test_results_text, transform=ax1.transAxes, \n",
    "             verticalalignment='top', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Permutation Test Results for Accuracy\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    if 'accuracy_permutation_results' in globals() and 'accuracy_improvement' in accuracy_permutation_results:\n",
    "        perm_data = accuracy_permutation_results['accuracy_improvement']\n",
    "        if 'null_distribution' in perm_data:\n",
    "            null_dist = perm_data['null_distribution']\n",
    "            observed_stat = perm_data['observed_statistic']\n",
    "            \n",
    "            ax2.hist(null_dist, bins=50, alpha=0.7, color='lightblue', density=True, \n",
    "                    label='Null Distribution')\n",
    "            ax2.axvline(observed_stat, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Observed: {safe_format_value(observed_stat, \".3f\")}')\n",
    "            ax2.set_xlabel('Accuracy Difference (HR - Standard)')\n",
    "            ax2.set_ylabel('Density')\n",
    "            ax2.set_title('Permutation Test: Accuracy Improvement\\nNull Distribution vs Observed')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add p-value annotation\n",
    "            p_val = perm_data.get('p_value', 'N/A')\n",
    "            formatted_p = safe_format_value(p_val, '.4f')\n",
    "            ax2.text(0.95, 0.95, f'p = {formatted_p}', transform=ax2.transAxes, \n",
    "                    ha='right', va='top', fontsize=12,\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow' if (p_val != 'N/A' and float(p_val) < 0.05) else 'white'))\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Permutation Test\\nNot Available', ha='center', va='center', \n",
    "                transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('Permutation Test Results')\n",
    "    \n",
    "    # 3. Bootstrap Confidence Intervals for Accuracy Metrics\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    if 'accuracy_bootstrap_results' in globals():\n",
    "        metrics = []\n",
    "        improvements = []\n",
    "        ci_lowers = []\n",
    "        ci_uppers = []\n",
    "        \n",
    "        test_names = ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction']\n",
    "        display_names = ['Accuracy\\nImprovement', 'FP\\nReduction', 'FN\\nReduction', 'Error\\nReduction']\n",
    "        \n",
    "        for test_name, display_name in zip(test_names, display_names):\n",
    "            if test_name in accuracy_bootstrap_results and accuracy_bootstrap_results[test_name]:\n",
    "                metrics.append(display_name)\n",
    "                if 'observed_improvement' in accuracy_bootstrap_results[test_name]:\n",
    "                    improvements.append(accuracy_bootstrap_results[test_name]['observed_improvement'])\n",
    "                elif 'observed_reduction' in accuracy_bootstrap_results[test_name]:\n",
    "                    improvements.append(accuracy_bootstrap_results[test_name]['observed_reduction'])\n",
    "                else:\n",
    "                    improvements.append(0)\n",
    "                \n",
    "                ci = accuracy_bootstrap_results[test_name].get('confidence_interval', (0, 0))\n",
    "                ci_lowers.append(ci[0])\n",
    "                ci_uppers.append(ci[1])\n",
    "        \n",
    "        if metrics:\n",
    "            x_pos = np.arange(len(metrics))\n",
    "            bars = ax3.bar(x_pos, improvements, alpha=0.7, \n",
    "                          color=['green' if imp > 30 else 'orange' if imp > 0 else 'red' \n",
    "                                for imp in improvements])\n",
    "            \n",
    "            # Add error bars for confidence intervals\n",
    "            errors = [[imp - ci_low for imp, ci_low in zip(improvements, ci_lowers)],\n",
    "                     [ci_up - imp for imp, ci_up in zip(improvements, ci_uppers)]]\n",
    "            ax3.errorbar(x_pos, improvements, yerr=errors, fmt='none', \n",
    "                        color='black', capsize=5, capthick=2)\n",
    "            \n",
    "            ax3.axhline(y=30, color='red', linestyle='--', linewidth=2, label='30% Threshold')\n",
    "            ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "            ax3.set_xticks(x_pos)\n",
    "            ax3.set_xticklabels(metrics)\n",
    "            ax3.set_ylabel('Improvement (%)')\n",
    "            ax3.set_title('Bootstrap Confidence Intervals\\nfor Accuracy Improvement Metrics')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Bootstrap Results\\nNot Available', ha='center', va='center',\n",
    "                    transform=ax3.transAxes, fontsize=14)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Bootstrap Results\\nNot Available', ha='center', va='center',\n",
    "                transform=ax3.transAxes, fontsize=14)\n",
    "        ax3.set_title('Bootstrap Confidence Intervals')\n",
    "    \n",
    "    # 4. Bayesian Posterior Distributions\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    if 'bayesian_accuracy_results' in globals():\n",
    "        colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "        test_names = ['accuracy_improvement', 'fp_reduction', 'fn_reduction']\n",
    "        display_names = ['Accuracy', 'FP Reduction', 'FN Reduction']\n",
    "        \n",
    "        for i, (test_name, display_name, color) in enumerate(zip(test_names, display_names, colors)):\n",
    "            if test_name in bayesian_accuracy_results and 'posterior_samples' in bayesian_accuracy_results[test_name]:\n",
    "                samples = bayesian_accuracy_results[test_name]['posterior_samples']\n",
    "                ax4.hist(samples, bins=30, alpha=0.6, color=color, density=True,\n",
    "                        label=display_name)\n",
    "        \n",
    "        ax4.axvline(30, color='red', linestyle='--', linewidth=2, label='30% Threshold')\n",
    "        ax4.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "        ax4.set_xlabel('Improvement (%)')\n",
    "        ax4.set_ylabel('Posterior Density')\n",
    "        ax4.set_title('Bayesian Posterior Distributions\\nAccuracy Improvements')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add probability annotations\n",
    "        if 'accuracy_improvement' in bayesian_accuracy_results:\n",
    "            prob_30 = bayesian_accuracy_results['accuracy_improvement'].get('prob_30_plus', 0)\n",
    "            ax4.text(0.02, 0.95, f'P(Accuracy >30%): {safe_format_value(prob_30, \".3f\")}', \n",
    "                    transform=ax4.transAxes, va='top', fontsize=10,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Bayesian Results\\nNot Available', ha='center', va='center',\n",
    "                transform=ax4.transAxes, fontsize=14)\n",
    "        ax4.set_title('Bayesian Posterior Distributions')\n",
    "    \n",
    "    # 5. Accuracy Error Distribution Comparison\n",
    "    ax5 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    # Plot error distributions for both groups\n",
    "    hr_errors = hr_data['accuracy_error_pct'].values\n",
    "    standard_errors = standard_data['accuracy_error_pct'].values\n",
    "    \n",
    "    ax5.hist(standard_errors, bins=20, alpha=0.6, color='lightcoral', label='Standard', density=True)\n",
    "    ax5.hist(hr_errors, bins=20, alpha=0.6, color='lightgreen', label='Hallucination-Reducing', density=True)\n",
    "    \n",
    "    # Add vertical lines for means\n",
    "    ax5.axvline(np.mean(standard_errors), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Standard Mean: {np.mean(standard_errors):.1f}%')\n",
    "    ax5.axvline(np.mean(hr_errors), color='green', linestyle='--', linewidth=2,\n",
    "               label=f'HR Mean: {np.mean(hr_errors):.1f}%')\n",
    "    \n",
    "    # Enhanced correlation and statistical info\n",
    "    improvement = ((np.mean(standard_errors) - np.mean(hr_errors)) / np.mean(standard_errors)) * 100\n",
    "    stats_text = f'Error Reduction: {improvement:.1f}%\\n'\n",
    "    \n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        if 'mann_whitney' in accuracy_test_results['statistical_tests']:\n",
    "            u_stat = accuracy_test_results['statistical_tests']['mann_whitney'].get('statistic', 'N/A')\n",
    "            u_p = accuracy_test_results['statistical_tests']['mann_whitney'].get('p_value', 'N/A')\n",
    "            stats_text += f\"Mann-Whitney U: {safe_format_value(u_stat, '.1f')}\\n\"\n",
    "            stats_text += f\"p-value: {safe_format_value(u_p, '.4f')}\"\n",
    "    \n",
    "    ax5.text(0.02, 0.98, stats_text, transform=ax5.transAxes, \n",
    "             verticalalignment='top', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax5.set_xlabel('Accuracy Error (%)')\n",
    "    ax5.set_ylabel('Density')\n",
    "    ax5.set_title('Error Distribution Comparison\\nHallucination-Reducing vs Standard Techniques')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Statistical Methods Comparison Table\n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create comprehensive comparison table for accuracy analysis\n",
    "    methods = ['Classical Tests', 'Permutation Test', 'Bootstrap Test', 'Bayesian Analysis']\n",
    "    metrics = ['p-value/Probability', 'Accuracy Improvement', 'Confidence Interval', 'Interpretation']\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    # Classical tests\n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        chi_sq = accuracy_test_results['statistical_tests'].get('chi_square', {})\n",
    "        classical_row = [\n",
    "            safe_format_value(chi_sq.get('p_value'), '.4f'),\n",
    "            f\"{accuracy_test_results['improvements']['accuracy']:+.1f}%\" if 'improvements' in accuracy_test_results else 'N/A',\n",
    "            'Classical CI' if chi_sq else 'N/A',\n",
    "            'Significant' if chi_sq.get('significant', False) else 'Not Significant'\n",
    "        ]\n",
    "    else:\n",
    "        classical_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(classical_row)\n",
    "    \n",
    "    # Permutation test\n",
    "    if 'accuracy_permutation_results' in globals() and 'accuracy_improvement' in accuracy_permutation_results:\n",
    "        perm_result = accuracy_permutation_results['accuracy_improvement']\n",
    "        perm_row = [\n",
    "            safe_format_value(perm_result.get('p_value'), '.4f'),\n",
    "            f\"{safe_format_value(perm_result.get('improvement_pct'), '.1f')}%\" if perm_result.get('improvement_pct') != 'N/A' else 'N/A',\n",
    "            'Exact CI Available',\n",
    "            'Significant' if perm_result.get('significant', False) else 'Not Significant'\n",
    "        ]\n",
    "    else:\n",
    "        perm_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(perm_row)\n",
    "    \n",
    "    # Bootstrap test\n",
    "    if 'accuracy_bootstrap_results' in globals() and 'accuracy_improvement' in accuracy_bootstrap_results:\n",
    "        boot_result = accuracy_bootstrap_results['accuracy_improvement']\n",
    "        boot_row = [\n",
    "            safe_format_value(boot_result.get('p_value_30_percent'), '.4f'),\n",
    "            f\"{safe_format_value(boot_result.get('observed_improvement'), '.1f')}%\" if boot_result.get('observed_improvement') != 'N/A' else 'N/A',\n",
    "            f\"[{safe_format_value(boot_result.get('confidence_interval', [0,0])[0], '.1f')}, {safe_format_value(boot_result.get('confidence_interval', [0,0])[1], '.1f')}]\" if 'confidence_interval' in boot_result else 'N/A',\n",
    "            'Meets Threshold' if boot_result.get('meets_threshold', False) else 'Below Threshold'\n",
    "        ]\n",
    "    else:\n",
    "        boot_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(boot_row)\n",
    "    \n",
    "    # Bayesian analysis\n",
    "    if 'bayesian_accuracy_results' in globals() and 'accuracy_improvement' in bayesian_accuracy_results:\n",
    "        bayes_result = bayesian_accuracy_results['accuracy_improvement']\n",
    "        bayes_row = [\n",
    "            safe_format_value(bayes_result.get('prob_30_plus'), '.3f'),\n",
    "            f\"{safe_format_value(bayes_result.get('posterior_mean'), '.1f')}%\" if bayes_result.get('posterior_mean') != 'N/A' else 'N/A',\n",
    "            f\"[{safe_format_value(bayes_result.get('credible_interval', [0,0])[0], '.1f')}, {safe_format_value(bayes_result.get('credible_interval', [0,0])[1], '.1f')}]\" if 'credible_interval' in bayes_result else 'N/A',\n",
    "            'Strong Evidence' if bayes_result.get('prob_30_plus', 0) > 0.8 else 'Moderate Evidence' if bayes_result.get('prob_30_plus', 0) > 0.6 else 'Weak Evidence'\n",
    "        ]\n",
    "    else:\n",
    "        bayes_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(bayes_row)\n",
    "    \n",
    "    table = ax6.table(cellText=table_data,\n",
    "                     rowLabels=methods,\n",
    "                     colLabels=metrics,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Color code the table based on results\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(metrics)):\n",
    "            cell = table[(i+1, j)]\n",
    "            if 'Significant' in table_data[i][j] or 'Strong Evidence' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightgreen')\n",
    "            elif 'Not Significant' in table_data[i][j] or 'Weak Evidence' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightcoral')\n",
    "            elif 'Moderate' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightyellow')\n",
    "    \n",
    "    ax6.set_title('Statistical Methods Comparison Summary\\nAccuracy vs Ground Truth Analysis', pad=20, fontsize=14)\n",
    "    \n",
    "    # 7-10. Detailed Bootstrap and Bayesian Distributions\n",
    "    methods_available = 0\n",
    "    \n",
    "    # Bootstrap distributions for accuracy metrics\n",
    "    if 'accuracy_bootstrap_results' in globals():\n",
    "        test_names = ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction']\n",
    "        labels = ['Accuracy Improvement', 'FP Reduction', 'FN Reduction', 'Error Reduction']\n",
    "        \n",
    "        for idx, (test_name, label) in enumerate(zip(test_names, labels)):\n",
    "            if test_name in accuracy_bootstrap_results and 'bootstrap_distribution' in accuracy_bootstrap_results[test_name] and methods_available < 4:\n",
    "                ax = fig.add_subplot(gs[2, methods_available])\n",
    "                \n",
    "                boot_dist = accuracy_bootstrap_results[test_name]['bootstrap_distribution']\n",
    "                if 'observed_improvement' in accuracy_bootstrap_results[test_name]:\n",
    "                    observed = accuracy_bootstrap_results[test_name]['observed_improvement']\n",
    "                else:\n",
    "                    observed = accuracy_bootstrap_results[test_name].get('observed_reduction', 0)\n",
    "                ci = accuracy_bootstrap_results[test_name]['confidence_interval']\n",
    "                \n",
    "                ax.hist(boot_dist, bins=40, alpha=0.7, color='steelblue', density=True)\n",
    "                ax.axvline(observed, color='red', linestyle='-', linewidth=2, \n",
    "                          label=f'Observed: {safe_format_value(observed, \".1f\")}%')\n",
    "                ax.axvline(ci[0], color='orange', linestyle='--', alpha=0.7)\n",
    "                ax.axvline(ci[1], color='orange', linestyle='--', alpha=0.7, \n",
    "                          label=f'95% CI: [{safe_format_value(ci[0], \".1f\")}, {safe_format_value(ci[1], \".1f\")}]')\n",
    "                ax.axvline(30, color='green', linestyle=':', linewidth=2, label='30% Threshold')\n",
    "                \n",
    "                ax.set_xlabel('Improvement (%)')\n",
    "                ax.set_ylabel('Bootstrap Density')\n",
    "                ax.set_title(f'Bootstrap Distribution\\n{label}')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                methods_available += 1\n",
    "    \n",
    "    # Bayesian posterior distributions (detailed)\n",
    "    if 'bayesian_accuracy_results' in globals() and methods_available < 4:\n",
    "        test_names = ['accuracy_improvement', 'fp_reduction', 'fn_reduction']\n",
    "        labels = ['Accuracy Improvement', 'FP Reduction', 'FN Reduction']\n",
    "        \n",
    "        for idx, (test_name, label) in enumerate(zip(test_names, labels)):\n",
    "            if (test_name in bayesian_accuracy_results and 'posterior_samples' in bayesian_accuracy_results[test_name] \n",
    "                and methods_available < 4):\n",
    "                ax = fig.add_subplot(gs[2, methods_available])\n",
    "                \n",
    "                samples = bayesian_accuracy_results[test_name]['posterior_samples']\n",
    "                ci = bayesian_accuracy_results[test_name]['credible_interval']\n",
    "                prob_30 = bayesian_accuracy_results[test_name]['prob_30_plus']\n",
    "                \n",
    "                ax.hist(samples, bins=40, alpha=0.7, color='lightcoral', density=True)\n",
    "                ax.axvline(ci[0], color='blue', linestyle='--', alpha=0.7)\n",
    "                ax.axvline(ci[1], color='blue', linestyle='--', alpha=0.7, \n",
    "                          label=f'95% Credible: [{safe_format_value(ci[0], \".1f\")}, {safe_format_value(ci[1], \".1f\")}]')\n",
    "                ax.axvline(30, color='green', linestyle=':', linewidth=2, label='30% Threshold')\n",
    "                \n",
    "                # Shade area above 30%\n",
    "                x_range = np.linspace(samples.min(), samples.max(), 1000)\n",
    "                density = stats.gaussian_kde(samples)(x_range)\n",
    "                ax.fill_between(x_range, 0, density, where=(x_range >= 30), \n",
    "                               alpha=0.3, color='green', label=f'P(>30%) = {safe_format_value(prob_30, \".3f\")}')\n",
    "                \n",
    "                ax.set_xlabel('Improvement (%)')\n",
    "                ax.set_ylabel('Posterior Density')\n",
    "                ax.set_title(f'Bayesian Posterior\\n{label}')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                methods_available += 1\n",
    "    \n",
    "    # 11. Overall Statistical Evidence Summary\n",
    "    ax11 = fig.add_subplot(gs[3, :])\n",
    "    ax11.axis('off')\n",
    "    \n",
    "    # Create evidence summary for accuracy analysis\n",
    "    evidence_summary = \"COMPREHENSIVE STATISTICAL EVIDENCE SUMMARY - ACCURACY vs GROUND TRUTH\\n\" + \"=\"*90 + \"\\n\\n\"\n",
    "    \n",
    "    # Classical statistics\n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        classical_tests = accuracy_test_results['statistical_tests']\n",
    "        evidence_summary += f\"📊 CLASSICAL STATISTICS: \"\n",
    "        significant_classical = sum(1 for test in classical_tests.values() if test.get('significant', False))\n",
    "        total_classical = len([test for test in classical_tests.values() if test])\n",
    "        evidence_summary += f\"{significant_classical}/{total_classical} SIGNIFICANT\\n\"\n",
    "        \n",
    "        if 'chi_square' in classical_tests:\n",
    "            evidence_summary += f\"   χ² test p-value: {safe_format_value(classical_tests['chi_square'].get('p_value'), '.4f')}\\n\"\n",
    "        if 'mann_whitney' in classical_tests:\n",
    "            evidence_summary += f\"   Mann-Whitney U p-value: {safe_format_value(classical_tests['mann_whitney'].get('p_value'), '.4f')}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Permutation tests\n",
    "    if 'accuracy_permutation_results' in globals():\n",
    "        perm_sig_count = sum(1 for test in ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction'] \n",
    "                           if test in accuracy_permutation_results and accuracy_permutation_results[test].get('significant', False))\n",
    "        perm_total = len([test for test in ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction'] \n",
    "                         if test in accuracy_permutation_results])\n",
    "        evidence_summary += f\"🔄 PERMUTATION TESTS: {perm_sig_count}/{perm_total} SIGNIFICANT\\n\"\n",
    "        if 'accuracy_improvement' in accuracy_permutation_results:\n",
    "            evidence_summary += f\"   Accuracy improvement p-value: {safe_format_value(accuracy_permutation_results['accuracy_improvement'].get('p_value'), '.4f')}\\n\"\n",
    "        if 'fp_reduction' in accuracy_permutation_results:\n",
    "            evidence_summary += f\"   FP reduction p-value: {safe_format_value(accuracy_permutation_results['fp_reduction'].get('p_value'), '.4f')}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Bootstrap results\n",
    "    if 'accuracy_bootstrap_results' in globals():\n",
    "        boot_threshold_count = sum(1 for test in ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction'] \n",
    "                                 if test in accuracy_bootstrap_results and accuracy_bootstrap_results[test].get('meets_threshold', False))\n",
    "        boot_total = len([test for test in ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction'] \n",
    "                         if test in accuracy_bootstrap_results])\n",
    "        evidence_summary += f\"🔀 BOOTSTRAP ANALYSIS: {boot_threshold_count}/{boot_total} MEET >30% THRESHOLD\\n\"\n",
    "        if 'accuracy_improvement' in accuracy_bootstrap_results:\n",
    "            conf = accuracy_bootstrap_results['accuracy_improvement'].get('threshold_confidence', 0)\n",
    "            evidence_summary += f\"   Accuracy improvement confidence for 30%: {safe_format_value(conf, '.3f')}\\n\"\n",
    "        if 'fp_reduction' in accuracy_bootstrap_results:\n",
    "            conf = accuracy_bootstrap_results['fp_reduction'].get('threshold_confidence', 0)\n",
    "            evidence_summary += f\"   FP reduction confidence for 30%: {safe_format_value(conf, '.3f')}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Bayesian results\n",
    "    if 'bayesian_accuracy_results' in globals():\n",
    "        evidence_summary += f\"🎯 BAYESIAN ANALYSIS: POSTERIOR PROBABILITIES\\n\"\n",
    "        if 'accuracy_improvement' in bayesian_accuracy_results:\n",
    "            prob = bayesian_accuracy_results['accuracy_improvement'].get('prob_30_plus', 0)\n",
    "            evidence_summary += f\"   P(Accuracy improvement > 30%): {safe_format_value(prob, '.3f')}\\n\"\n",
    "        if 'fp_reduction' in bayesian_accuracy_results:\n",
    "            prob = bayesian_accuracy_results['fp_reduction'].get('prob_30_plus', 0)\n",
    "            evidence_summary += f\"   P(FP reduction > 30%): {safe_format_value(prob, '.3f')}\\n\"\n",
    "        if 'fn_reduction' in bayesian_accuracy_results:\n",
    "            prob = bayesian_accuracy_results['fn_reduction'].get('prob_30_plus', 0)\n",
    "            evidence_summary += f\"   P(FN reduction > 30%): {safe_format_value(prob, '.3f')}\\n\"\n",
    "        if 'decision_analysis' in bayesian_accuracy_results:\n",
    "            prob_any = bayesian_accuracy_results['decision_analysis'].get('prob_any_30', 0)\n",
    "            evidence_summary += f\"   P(Any metric > 30%): {safe_format_value(prob_any, '.3f')}\\n\"\n",
    "            recommendation = bayesian_accuracy_results['decision_analysis'].get('recommendation', 'N/A')\n",
    "            evidence_summary += f\"   Decision: {recommendation}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\" + \"=\"*90 + \"\\n\"\n",
    "    evidence_summary += \"🎯 OVERALL CONCLUSION: \"\n",
    "    \n",
    "    # Determine overall conclusion based on accuracy analysis\n",
    "    significant_methods = 0\n",
    "    total_methods = 0\n",
    "    \n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        total_methods += 1\n",
    "        if any(test.get('significant', False) for test in accuracy_test_results['statistical_tests'].values()):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'accuracy_permutation_results' in globals():\n",
    "        total_methods += 1\n",
    "        if any(test.get('significant', False) for test in accuracy_permutation_results.values() if isinstance(test, dict)):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'accuracy_bootstrap_results' in globals():\n",
    "        total_methods += 1\n",
    "        if any(test.get('meets_threshold', False) for test in accuracy_bootstrap_results.values() if isinstance(test, dict)):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'bayesian_accuracy_results' in globals():\n",
    "        total_methods += 1\n",
    "        if any(test.get('prob_30_plus', 0) > 0.8 for test in bayesian_accuracy_results.values() if isinstance(test, dict) and 'prob_30_plus' in test):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if significant_methods >= total_methods * 0.75:\n",
    "        conclusion = \"STRONG EVIDENCE for >30% accuracy improvement hypothesis\"\n",
    "    elif significant_methods >= total_methods * 0.5:\n",
    "        conclusion = \"MODERATE EVIDENCE for >30% accuracy improvement hypothesis\"\n",
    "    else:\n",
    "        conclusion = \"WEAK EVIDENCE for >30% accuracy improvement hypothesis\"\n",
    "    \n",
    "    evidence_summary += conclusion\n",
    "    \n",
    "    ax11.text(0.05, 0.95, evidence_summary, transform=ax11.transAxes, \n",
    "              fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Enhanced accuracy-based statistical visualizations completed successfully\")\n",
    "    print(f\"✓ Integrated results from {total_methods} statistical methods\")\n",
    "    print(f\"✓ {significant_methods}/{total_methods} methods show positive evidence for accuracy improvement\")\n",
    "\n",
    "# Execute enhanced statistical visualizations for accuracy analysis\n",
    "if 'accuracy_test_results' in globals() and accuracy_test_results:\n",
    "    create_enhanced_accuracy_visualizations()\n",
    "else:\n",
    "    print(\"Cannot create enhanced statistical visualizations - accuracy test results not available\")\n",
    "    print(\"Please run Cell 11 first to generate accuracy test results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [16] - Enhanced Final Executive Summary with Advanced Statistical Analysis\n",
    "# Purpose: Generate comprehensive executive summary integrating all statistical methods and business insights\n",
    "# Dependencies: accuracy_test_results, accuracy_permutation_results, accuracy_bootstrap_results, bayesian_accuracy_results\n",
    "# Breadcrumbs: Setup -> Analysis -> Advanced Statistical Testing -> Enhanced Final Executive Summary & Business Impact Report\n",
    "\n",
    "print(\"ENHANCED EXECUTIVE SUMMARY - COMPREHENSIVE ACCURACY-BASED STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get project name safely\n",
    "project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project') if 'CONFIG' in globals() else 'Unknown Project'\n",
    "print(f\"Project: {project_name}\")\n",
    "print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Analysis Framework: Ground Truth Accuracy Analysis with Classical, Permutation, Bootstrap, and Bayesian Methods\")\n",
    "\n",
    "if 'accuracy_test_results' in globals() and accuracy_test_results:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    accuracy_df = accuracy_test_results['accuracy_df']\n",
    "    hr_metrics = accuracy_test_results['hr_metrics']\n",
    "    standard_metrics = accuracy_test_results['standard_metrics']\n",
    "    ground_truth_baseline = accuracy_test_results['ground_truth_baseline']\n",
    "    \n",
    "    print(f\"  Ground truth baseline: {ground_truth_baseline:.1f} LOC/SiFP (iTrust codebase)\")\n",
    "    print(f\"  Total estimates analyzed: {len(accuracy_df)}\")\n",
    "    print(f\"  Hallucination-reducing estimates: {len(hr_metrics['group_data'])}\")\n",
    "    print(f\"  Standard estimates: {len(standard_metrics['group_data'])}\")\n",
    "    print(f\"  Accuracy tolerance: ±20% of ground truth\")\n",
    "    \n",
    "    if 'code_metrics_df' in globals():\n",
    "        print(f\"  Total code files in project: {len(code_metrics_df)}\")\n",
    "        print(f\"  Total lines of code in project: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "    \n",
    "    if 'ground_truth_requirements' in globals():\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "    \n",
    "    # Get baseline values safely\n",
    "    conversion_factor = CONFIG.get('CONVERSION_FACTOR', 0.957) if 'CONFIG' in globals() else 0.957\n",
    "    industry_loc_per_sifp = industry_metrics.get('LOC_PER_SIFP', 100) if 'industry_metrics' in globals() else 100\n",
    "    project_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_loc_per_sifp) if 'baseline_metrics' in globals() else industry_loc_per_sifp\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10) if 'effort_metrics' in globals() else 10\n",
    "    \n",
    "    print(f\"\\nConversion Factors and Baselines:\")\n",
    "    print(f\"  UFP → SiFP: {conversion_factor} (from Desharnais research)\")\n",
    "    print(f\"  SiFP → Effort: {desharnais_hours_per_sifp:.2f} hours/SiFP (Desharnais dataset)\")\n",
    "    print(f\"  SiFP → LOC: {project_loc_per_sifp:.1f} LOC/SiFP (project weighted average)\")\n",
    "    print(f\"  Industry baseline: {industry_loc_per_sifp:.1f} LOC/SiFP\")\n",
    "    print(f\"  Project vs Industry: {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}%\")\n",
    "    \n",
    "    # COMPREHENSIVE STATISTICAL HYPOTHESIS TESTING SUMMARY\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE STATISTICAL HYPOTHESIS TESTING - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nHypothesis: Hallucination-reducing techniques improve estimation accuracy by >30%\")\n",
    "    print(f\"Operational Definition: Multi-stage refinement (actor→judge→meta-judge)\")\n",
    "    print(f\"Ground Truth: iTrust codebase metrics scaled to estimated requirements\")\n",
    "    print(f\"Statistical Significance Threshold: α = 0.05\")\n",
    "    print(f\"Practical Significance Threshold: >30% improvement\")\n",
    "    \n",
    "    # Collect results from all statistical methods\n",
    "    statistical_evidence = {\n",
    "        'classical': {},\n",
    "        'permutation': {},\n",
    "        'bootstrap': {},\n",
    "        'bayesian': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Classical Statistics\n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(\"1. CLASSICAL STATISTICAL ANALYSIS - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'accuracy_test_results' in globals() and 'statistical_tests' in accuracy_test_results:\n",
    "        statistical_tests = accuracy_test_results['statistical_tests']\n",
    "        improvements = accuracy_test_results['improvements']\n",
    "        \n",
    "        print(f\"  Method: Chi-square, Fisher's exact, Mann-Whitney U tests\")\n",
    "        \n",
    "        if 'chi_square' in statistical_tests:\n",
    "            chi_result = statistical_tests['chi_square']\n",
    "            print(f\"  Chi-square test (independence): χ² = {chi_result.get('statistic', 'N/A'):.3f}, p = {chi_result.get('p_value', 'N/A'):.6f}\")\n",
    "            print(f\"  Statistically significant: {'YES' if chi_result.get('significant', False) else 'NO'}\")\n",
    "            \n",
    "            statistical_evidence['classical'] = {\n",
    "                'significant': chi_result.get('significant', False),\n",
    "                'p_value': chi_result.get('p_value', 'N/A'),\n",
    "                'method': 'Chi-square test'\n",
    "            }\n",
    "        \n",
    "        if 'fisher_exact' in statistical_tests:\n",
    "            fisher_result = statistical_tests['fisher_exact']\n",
    "            print(f\"  Fisher's exact test: OR = {fisher_result.get('odds_ratio', 'N/A'):.3f}, p = {fisher_result.get('p_value', 'N/A'):.6f}\")\n",
    "        \n",
    "        if 'mann_whitney' in statistical_tests:\n",
    "            mw_result = statistical_tests['mann_whitney']\n",
    "            print(f\"  Mann-Whitney U test: U = {mw_result.get('statistic', 'N/A'):.3f}, p = {mw_result.get('p_value', 'N/A'):.6f}\")\n",
    "            print(f\"  Effect size: {mw_result.get('effect_size', 'N/A'):.3f}\")\n",
    "        \n",
    "        print(f\"\\n  Accuracy Improvements:\")\n",
    "        print(f\"    Overall accuracy: {improvements['accuracy']:+.1f}%\")\n",
    "        print(f\"    False Positive reduction: {improvements['fp_reduction']:+.1f}%\")\n",
    "        print(f\"    False Negative reduction: {improvements['fn_reduction']:+.1f}%\")\n",
    "        print(f\"    Average error reduction: {improvements['error_reduction']:+.1f}%\")\n",
    "        \n",
    "        best_improvement = max(improvements['accuracy'], improvements['fp_reduction'], \n",
    "                             improvements['fn_reduction'], improvements['error_reduction'])\n",
    "        print(f\"    Best improvement: {best_improvement:.1f}%\")\n",
    "        print(f\"    Meets >30% threshold: {'YES' if best_improvement > 30 else 'NO'}\")\n",
    "        \n",
    "        statistical_evidence['classical']['best_improvement'] = best_improvement\n",
    "        \n",
    "    else:\n",
    "        print(f\"  Classical statistical testing: NOT COMPLETED\")\n",
    "        print(f\"  Reason: Accuracy test results not available\")\n",
    "    \n",
    "    # 2. Permutation Tests\n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(\"2. PERMUTATION TEST ANALYSIS - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'accuracy_permutation_results' in globals() and accuracy_permutation_results:\n",
    "        perm_tests = ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction']\n",
    "        significant_perm_tests = 0\n",
    "        total_perm_tests = 0\n",
    "        threshold_perm_tests = 0\n",
    "        \n",
    "        for test_name in perm_tests:\n",
    "            if test_name in accuracy_permutation_results and accuracy_permutation_results[test_name]:\n",
    "                total_perm_tests += 1\n",
    "                result = accuracy_permutation_results[test_name]\n",
    "                test_label = test_name.replace('_', ' ').title()\n",
    "                \n",
    "                print(f\"  {test_label}:\")\n",
    "                print(f\"    p-value: {result.get('p_value', 'N/A'):.6f}\")\n",
    "                if 'improvement_pct' in result:\n",
    "                    print(f\"    Improvement: {result.get('improvement_pct', 'N/A'):.1f}%\")\n",
    "                elif 'reduction_pct' in result:\n",
    "                    print(f\"    Reduction: {result.get('reduction_pct', 'N/A'):.1f}%\")\n",
    "                print(f\"    Significant: {'YES' if result.get('significant', False) else 'NO'}\")\n",
    "                print(f\"    Meets >30% threshold: {'YES' if result.get('meets_threshold', False) else 'NO'}\")\n",
    "                \n",
    "                if result.get('significant', False):\n",
    "                    significant_perm_tests += 1\n",
    "                if result.get('meets_threshold', False):\n",
    "                    threshold_perm_tests += 1\n",
    "        \n",
    "        print(f\"\\n  Overall Permutation Results: {significant_perm_tests}/{total_perm_tests} tests significant\")\n",
    "        print(f\"  Tests meeting >30% threshold: {threshold_perm_tests}/{total_perm_tests}\")\n",
    "        print(f\"  Method advantages: Distribution-free, exact p-values, robust to outliers\")\n",
    "        \n",
    "        statistical_evidence['permutation'] = {\n",
    "            'significant_tests': significant_perm_tests,\n",
    "            'threshold_tests': threshold_perm_tests,\n",
    "            'total_tests': total_perm_tests,\n",
    "            'any_significant': significant_perm_tests > 0,\n",
    "            'any_threshold': threshold_perm_tests > 0\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Permutation testing: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: Advanced statistical libraries not installed or analysis not run\")\n",
    "    \n",
    "    # 3. Bootstrap Analysis\n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(\"3. BOOTSTRAP HYPOTHESIS TESTING - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'accuracy_bootstrap_results' in globals() and accuracy_bootstrap_results:\n",
    "        boot_tests = ['accuracy_improvement', 'fp_reduction', 'fn_reduction', 'error_reduction']\n",
    "        threshold_boot_tests = 0\n",
    "        high_confidence_boot_tests = 0\n",
    "        total_boot_tests = 0\n",
    "        \n",
    "        for test_name in boot_tests:\n",
    "            if test_name in accuracy_bootstrap_results and accuracy_bootstrap_results[test_name]:\n",
    "                total_boot_tests += 1\n",
    "                result = accuracy_bootstrap_results[test_name]\n",
    "                test_label = test_name.replace('_', ' ').title()\n",
    "                \n",
    "                print(f\"  {test_label}:\")\n",
    "                if 'observed_improvement' in result:\n",
    "                    print(f\"    Observed improvement: {result.get('observed_improvement', 'N/A'):.1f}%\")\n",
    "                elif 'observed_reduction' in result:\n",
    "                    print(f\"    Observed reduction: {result.get('observed_reduction', 'N/A'):.1f}%\")\n",
    "                \n",
    "                ci = result.get('confidence_interval', [0, 0])\n",
    "                print(f\"    95% Bootstrap CI: [{ci[0]:.1f}, {ci[1]:.1f}]%\")\n",
    "                print(f\"    P(>30% threshold): {result.get('threshold_confidence', 'N/A'):.3f}\")\n",
    "                print(f\"    Meets >30% threshold: {'YES' if result.get('meets_threshold', False) else 'NO'}\")\n",
    "                \n",
    "                if result.get('meets_threshold', False):\n",
    "                    threshold_boot_tests += 1\n",
    "                if result.get('threshold_confidence', 0) > 0.8:\n",
    "                    high_confidence_boot_tests += 1\n",
    "        \n",
    "        print(f\"\\n  Overall Bootstrap Results: {threshold_boot_tests}/{total_boot_tests} tests meet >30% threshold\")\n",
    "        print(f\"  High confidence (>80%) tests: {high_confidence_boot_tests}/{total_boot_tests}\")\n",
    "        print(f\"  Method advantages: Direct threshold testing, bias-corrected intervals\")\n",
    "        \n",
    "        statistical_evidence['bootstrap'] = {\n",
    "            'threshold_tests': threshold_boot_tests,\n",
    "            'high_confidence_tests': high_confidence_boot_tests,\n",
    "            'total_tests': total_boot_tests,\n",
    "            'any_threshold': threshold_boot_tests > 0,\n",
    "            'strong_evidence': high_confidence_boot_tests > 0\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Bootstrap testing: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: Advanced statistical libraries not installed or analysis not run\")\n",
    "    \n",
    "    # 4. Bayesian Analysis\n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(\"4. BAYESIAN HYPOTHESIS TESTING - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if 'bayesian_accuracy_results' in globals() and bayesian_accuracy_results:\n",
    "        print(f\"  Method: Bayesian inference with posterior probability statements\")\n",
    "        \n",
    "        if 'accuracy_improvement' in bayesian_accuracy_results and bayesian_accuracy_results['accuracy_improvement']:\n",
    "            accuracy_result = bayesian_accuracy_results['accuracy_improvement']\n",
    "            print(f\"  Accuracy Improvement Analysis:\")\n",
    "            print(f\"    Posterior mean improvement: {accuracy_result.get('posterior_mean', 'N/A'):.1f}%\")\n",
    "            ci = accuracy_result.get('credible_interval', [0, 0])\n",
    "            print(f\"    95% Credible interval: [{ci[0]:.1f}, {ci[1]:.1f}]%\")\n",
    "            print(f\"    P(improvement > 30%): {accuracy_result.get('prob_30_plus', 'N/A'):.3f}\")\n",
    "            print(f\"    P(any improvement): {accuracy_result.get('prob_any_improvement', 'N/A'):.3f}\")\n",
    "        \n",
    "        if 'fp_reduction' in bayesian_accuracy_results and bayesian_accuracy_results['fp_reduction']:\n",
    "            fp_result = bayesian_accuracy_results['fp_reduction']\n",
    "            print(f\"  False Positive Reduction Analysis:\")\n",
    "            print(f\"    P(FP reduction > 30%): {fp_result.get('prob_30_plus', 'N/A'):.3f}\")\n",
    "            print(f\"    P(any FP reduction): {fp_result.get('prob_any_reduction', 'N/A'):.3f}\")\n",
    "        \n",
    "        if 'fn_reduction' in bayesian_accuracy_results and bayesian_accuracy_results['fn_reduction']:\n",
    "            fn_result = bayesian_accuracy_results['fn_reduction']\n",
    "            print(f\"  False Negative Reduction Analysis:\")\n",
    "            print(f\"    P(FN reduction > 30%): {fn_result.get('prob_30_plus', 'N/A'):.3f}\")\n",
    "            print(f\"    P(any FN reduction): {fn_result.get('prob_any_reduction', 'N/A'):.3f}\")\n",
    "        \n",
    "        if 'decision_analysis' in bayesian_accuracy_results and bayesian_accuracy_results['decision_analysis']:\n",
    "            decision_result = bayesian_accuracy_results['decision_analysis']\n",
    "            print(f\"  Combined Decision Analysis:\")\n",
    "            print(f\"    P(any metric > 30%): {decision_result.get('prob_any_30', 'N/A'):.3f}\")\n",
    "            print(f\"    P(all metrics > 30%): {decision_result.get('prob_all_30', 'N/A'):.3f}\")\n",
    "            print(f\"    Recommendation: {decision_result.get('recommendation', 'N/A')}\")\n",
    "        \n",
    "        if 'model_comparison' in bayesian_accuracy_results and bayesian_accuracy_results['model_comparison']:\n",
    "            model_result = bayesian_accuracy_results['model_comparison']\n",
    "            print(f\"  Model Comparison:\")\n",
    "            print(f\"    Evidence: {model_result.get('preference', 'N/A')}\")\n",
    "            print(f\"    ΔWAIC: {model_result.get('delta_waic', 'N/A'):.2f}\")\n",
    "        \n",
    "        print(f\"  Method advantages: Direct probability statements, credible intervals, decision framework\")\n",
    "        \n",
    "        # Determine Bayesian evidence strength\n",
    "        bayes_strong = False\n",
    "        if ('accuracy_improvement' in bayesian_accuracy_results and \n",
    "            bayesian_accuracy_results['accuracy_improvement'].get('prob_30_plus', 0) > 0.8):\n",
    "            bayes_strong = True\n",
    "        if ('decision_analysis' in bayesian_accuracy_results and \n",
    "            bayesian_accuracy_results['decision_analysis'].get('prob_any_30', 0) > 0.8):\n",
    "            bayes_strong = True\n",
    "            \n",
    "        statistical_evidence['bayesian'] = {\n",
    "            'strong_evidence': bayes_strong,\n",
    "            'available': True,\n",
    "            'decision_recommendation': bayesian_accuracy_results.get('decision_analysis', {}).get('recommendation', 'N/A')\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Bayesian analysis: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: PyMC/ArviZ not installed or analysis not run\")\n",
    "        statistical_evidence['bayesian'] = {'available': False}\n",
    "    \n",
    "    # INTEGRATED STATISTICAL CONCLUSION\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"INTEGRATED STATISTICAL CONCLUSION - ACCURACY vs GROUND TRUTH\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count evidence across methods\n",
    "    evidence_count = 0\n",
    "    total_methods = 0\n",
    "    evidence_details = []\n",
    "    \n",
    "    # Classical evidence\n",
    "    if statistical_evidence['classical']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['classical'].get('significant', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Classical tests: SIGNIFICANT\")\n",
    "        else:\n",
    "            evidence_details.append(\"Classical tests: Not significant\")\n",
    "    \n",
    "    # Permutation evidence\n",
    "    if statistical_evidence['permutation']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['permutation'].get('any_significant', False) or statistical_evidence['permutation'].get('any_threshold', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Permutation tests: SIGNIFICANT/THRESHOLD\")\n",
    "        else:\n",
    "            evidence_details.append(\"Permutation tests: Not significant\")\n",
    "    \n",
    "    # Bootstrap evidence\n",
    "    if statistical_evidence['bootstrap']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['bootstrap'].get('any_threshold', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Bootstrap tests: MEETS THRESHOLD\")\n",
    "        else:\n",
    "            evidence_details.append(\"Bootstrap tests: Below threshold\")\n",
    "    \n",
    "    # Bayesian evidence\n",
    "    if statistical_evidence['bayesian']['available']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['bayesian'].get('strong_evidence', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Bayesian analysis: STRONG EVIDENCE\")\n",
    "        else:\n",
    "            evidence_details.append(\"Bayesian analysis: Weak evidence\")\n",
    "    \n",
    "    print(f\"\\nEvidence Summary:\")\n",
    "    for detail in evidence_details:\n",
    "        print(f\"  • {detail}\")\n",
    "    \n",
    "    print(f\"\\nStatistical Methods Agreement: {evidence_count}/{total_methods} methods show positive evidence\")\n",
    "    \n",
    "    # Overall statistical conclusion\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        statistical_conclusion = \"STRONG STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🟢\"\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        statistical_conclusion = \"MODERATE STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🟡\"\n",
    "    else:\n",
    "        statistical_conclusion = \"WEAK STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🔴\"\n",
    "    \n",
    "    print(f\"\\n{conclusion_color} OVERALL STATISTICAL CONCLUSION: {statistical_conclusion}\")\n",
    "    print(f\"   for >30% improvement in estimation accuracy through hallucination-reducing techniques\")\n",
    "    \n",
    "    # ACCURACY ANALYSIS AND BUSINESS IMPACT\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"ACCURACY ANALYSIS AND BUSINESS RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Accuracy classification analysis\n",
    "    print(f\"\\n1. ACCURACY CLASSIFICATION ANALYSIS:\")\n",
    "    print(f\"   Hallucination-Reducing Group:\")\n",
    "    print(f\"     • True Positives (accurate): {hr_metrics['tp_count']}/{hr_metrics['total']} ({hr_metrics['tp_rate']:.2%})\")\n",
    "    print(f\"     • False Positives (overestimates): {hr_metrics['fp_count']}/{hr_metrics['total']} ({hr_metrics['fp_rate']:.2%})\")\n",
    "    print(f\"     • False Negatives (underestimates): {hr_metrics['fn_count']}/{hr_metrics['total']} ({hr_metrics['fn_rate']:.2%})\")\n",
    "    print(f\"     • Overall Accuracy: {hr_metrics['accuracy']:.2%}\")\n",
    "    \n",
    "    print(f\"\\n   Standard Group:\")\n",
    "    print(f\"     • True Positives (accurate): {standard_metrics['tp_count']}/{standard_metrics['total']} ({standard_metrics['tp_rate']:.2%})\")\n",
    "    print(f\"     • False Positives (overestimates): {standard_metrics['fp_count']}/{standard_metrics['total']} ({standard_metrics['fp_rate']:.2%})\")\n",
    "    print(f\"     • False Negatives (underestimates): {standard_metrics['fn_count']}/{standard_metrics['total']} ({standard_metrics['fn_rate']:.2%})\")\n",
    "    print(f\"     • Overall Accuracy: {standard_metrics['accuracy']:.2%}\")\n",
    "    \n",
    "    # Business impact from accuracy improvements\n",
    "    if 'accuracy_test_results' in globals() and 'improvements' in accuracy_test_results:\n",
    "        improvements = accuracy_test_results['improvements']\n",
    "        print(f\"\\n2. BUSINESS IMPACT OF ACCURACY IMPROVEMENTS:\")\n",
    "        \n",
    "        # Calculate potential cost savings from improved accuracy\n",
    "        if 'effort_impact_df' in globals() and not effort_impact_df.empty:\n",
    "            avg_cost_impact = effort_impact_df['Total_Cost_Impact_USD'].mean()\n",
    "            print(f\"   Average cost impact per model: ${avg_cost_impact:+,.0f}\")\n",
    "            \n",
    "            # Estimate savings from accuracy improvement\n",
    "            accuracy_improvement_factor = improvements['accuracy'] / 100\n",
    "            estimated_savings = abs(avg_cost_impact) * accuracy_improvement_factor\n",
    "            print(f\"   Estimated savings from {improvements['accuracy']:.1f}% accuracy improvement: ${estimated_savings:,.0f}\")\n",
    "        \n",
    "        print(f\"   False Positive reduction: {improvements['fp_reduction']:.1f}%\")\n",
    "        print(f\"     • Business benefit: Reduced over-scoping and resource waste\")\n",
    "        print(f\"   False Negative reduction: {improvements['fn_reduction']:.1f}%\")\n",
    "        print(f\"     • Business benefit: Reduced under-scoping and deadline pressure\")\n",
    "        print(f\"   Overall error reduction: {improvements['error_reduction']:.1f}%\")\n",
    "        print(f\"     • Business benefit: More predictable project planning\")\n",
    "    \n",
    "    # Risk assessment based on accuracy\n",
    "    print(f\"\\n3. RISK ASSESSMENT:\")\n",
    "    overall_accuracy = (hr_metrics['accuracy'] + standard_metrics['accuracy']) / 2\n",
    "    if overall_accuracy > 0.8:\n",
    "        print(f\"   ✅ LOW RISK: High overall accuracy ({overall_accuracy:.1%})\")\n",
    "    elif overall_accuracy > 0.6:\n",
    "        print(f\"   ⚠️  MODERATE RISK: Moderate accuracy ({overall_accuracy:.1%})\")\n",
    "    else:\n",
    "        print(f\"   🚨 HIGH RISK: Low accuracy ({overall_accuracy:.1%}) requires attention\")\n",
    "    \n",
    "    fp_rate = (hr_metrics['fp_rate'] + standard_metrics['fp_rate']) / 2\n",
    "    if fp_rate < 0.2:\n",
    "        print(f\"   ✅ LOW OVERESTIMATION RISK: FP rate {fp_rate:.1%}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  OVERESTIMATION RISK: High FP rate {fp_rate:.1%}\")\n",
    "    \n",
    "    fn_rate = (hr_metrics['fn_rate'] + standard_metrics['fn_rate']) / 2\n",
    "    if fn_rate < 0.2:\n",
    "        print(f\"   ✅ LOW UNDERESTIMATION RISK: FN rate {fn_rate:.1%}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  UNDERESTIMATION RISK: High FN rate {fn_rate:.1%}\")\n",
    "    \n",
    "    # IMPLEMENTATION RECOMMENDATIONS\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPLEMENTATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n1. STATISTICAL EVIDENCE BASED:\")\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        print(f\"   🚀 STRONG RECOMMENDATION: Implement hallucination-reducing techniques\")\n",
    "        print(f\"      • Multiple statistical methods converge on positive evidence\")\n",
    "        print(f\"      • Significant accuracy improvements demonstrated against ground truth\")\n",
    "        print(f\"      • Risk of Type I error minimized through diverse analytical approaches\")\n",
    "        print(f\"      • Expected accuracy improvement supported by robust statistical framework\")\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        print(f\"   📋 MODERATE RECOMMENDATION: Pilot implementation with careful monitoring\")\n",
    "        print(f\"      • Mixed statistical evidence suggests cautious adoption\")\n",
    "        print(f\"      • Implement in controlled environment with accuracy measurement systems\")\n",
    "        print(f\"      • Establish clear success metrics based on TP/FP/FN rates\")\n",
    "        print(f\"      • Monitor against ground truth continuously\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  WEAK RECOMMENDATION: Additional research needed before implementation\")\n",
    "        print(f\"      • Insufficient statistical evidence for confident recommendation\")\n",
    "        print(f\"      • Consider larger sample sizes or refined accuracy measurement\")\n",
    "        print(f\"      • Focus on improving ground truth establishment and validation\")\n",
    "    \n",
    "    print(f\"\\n2. ACCURACY-BASED IMPLEMENTATION:\")\n",
    "    print(f\"   • Multi-stage refinement: actor → judge → meta-judge architecture\")\n",
    "    print(f\"   • Ground truth validation systems for continuous accuracy monitoring\")\n",
    "    print(f\"   • TP/FP/FN classification frameworks for estimation quality assessment\")\n",
    "    print(f\"   • Automated accuracy feedback loops for model improvement\")\n",
    "    print(f\"   • Integration with existing project management and estimation processes\")\n",
    "    \n",
    "    print(f\"\\n3. ORGANIZATIONAL READINESS:\")\n",
    "    print(f\"   • Training on accuracy-based estimation methodologies\")\n",
    "    print(f\"   • Establishment of ground truth baselines for different project types\")\n",
    "    print(f\"   • Change management for adoption of accuracy-validated AI estimation\")\n",
    "    print(f\"   • Quality assurance protocols based on TP/FP/FN metrics\")\n",
    "    print(f\"   • Continuous monitoring and improvement processes\")\n",
    "    \n",
    "    # LIMITATIONS AND FUTURE RESEARCH\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"LIMITATIONS AND FUTURE RESEARCH DIRECTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nStudy Limitations:\")\n",
    "    print(f\"  • Sample size constraints limit statistical power\")\n",
    "    print(f\"  • Single project ground truth (iTrust) may not generalize\")\n",
    "    print(f\"  • Accuracy tolerance of ±20% is somewhat arbitrary\")\n",
    "    print(f\"  • Cross-sectional design rather than longitudinal study\")\n",
    "    print(f\"  • Limited direct measurement of time-to-market impact\")\n",
    "    print(f\"  • Proxy measures for 'hallucination-reducing techniques'\")\n",
    "    \n",
    "    print(f\"\\nFuture Research Priorities:\")\n",
    "    print(f\"  1. Multi-project validation with diverse ground truth baselines\")\n",
    "    print(f\"  2. Longitudinal studies tracking accuracy improvements over time\")\n",
    "    print(f\"  3. Direct measurement of business impact metrics (time-to-market, cost)\")\n",
    "    print(f\"  4. Refinement of accuracy classification thresholds\")\n",
    "    print(f\"  5. Development of automated ground truth establishment methods\")\n",
    "    print(f\"  6. Comparative analysis with other estimation accuracy frameworks\")\n",
    "    print(f\"  7. Integration of accuracy metrics with project success indicators\")\n",
    "    \n",
    "    # FINAL EXECUTIVE DECISION FRAMEWORK\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTIVE DECISION FRAMEWORK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 KEY FINDINGS:\")\n",
    "    print(f\"   • Statistical Analysis: {statistical_conclusion}\")\n",
    "    print(f\"   • Methods Agreement: {evidence_count}/{total_methods} approaches show positive results\")\n",
    "    print(f\"   • Accuracy Improvement: {improvements['accuracy']:+.1f}% overall accuracy\")\n",
    "    print(f\"   • False Positive Reduction: {improvements['fp_reduction']:+.1f}%\")\n",
    "    print(f\"   • False Negative Reduction: {improvements['fn_reduction']:+.1f}%\")\n",
    "    print(f\"   • Ground Truth Validation: Against iTrust codebase metrics\")\n",
    "    \n",
    "    print(f\"\\n📊 DECISION MATRIX:\")\n",
    "    print(f\"   Statistical Evidence: {'Strong' if evidence_count >= total_methods * 0.75 else 'Moderate' if evidence_count >= total_methods * 0.5 else 'Weak'}\")\n",
    "    print(f\"   Accuracy Improvement: {'High' if overall_accuracy > 0.8 else 'Moderate' if overall_accuracy > 0.6 else 'Low'}\")\n",
    "    print(f\"   Implementation Risk: {'Low' if fp_rate < 0.2 and fn_rate < 0.2 else 'Moderate'}\")\n",
    "    print(f\"   Ground Truth Validation: {'Established' if ground_truth_baseline else 'Limited'}\")\n",
    "    print(f\"   Technical Readiness: {'High' if len(accuracy_df) > 50 else 'Moderate'}\")\n",
    "    \n",
    "    print(f\"\\n🎯 EXECUTIVE RECOMMENDATION:\")\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        print(f\"   PROCEED WITH IMPLEMENTATION\")\n",
    "        print(f\"   • Strong statistical evidence supports adoption\")\n",
    "        print(f\"   • Accuracy improvements demonstrated against ground truth\")\n",
    "        print(f\"   • Establish continuous accuracy monitoring systems\")\n",
    "        print(f\"   • Implement with robust TP/FP/FN tracking\")\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        print(f\"   PROCEED WITH PILOT PROGRAM\")\n",
    "        print(f\"   • Moderate evidence supports cautious adoption\")\n",
    "        print(f\"   • Implement with comprehensive accuracy measurement\")\n",
    "        print(f\"   • Establish clear go/no-go criteria based on accuracy metrics\")\n",
    "        print(f\"   • Focus on ground truth validation and continuous improvement\")\n",
    "    else:\n",
    "        print(f\"   DEFER IMPLEMENTATION - CONDUCT ADDITIONAL RESEARCH\")\n",
    "        print(f\"   • Insufficient evidence for confident business decision\")\n",
    "        print(f\"   • Invest in larger-scale accuracy validation studies\")\n",
    "        print(f\"   • Improve ground truth establishment methodologies\")\n",
    "        print(f\"   • Focus on accuracy measurement system development\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Create comprehensive executive summary\n",
    "        final_summary = {\n",
    "            'project': project_name,\n",
    "            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "            'hypothesis': 'Hallucination-reducing techniques improve estimation accuracy by >30%',\n",
    "            'ground_truth_baseline': ground_truth_baseline,\n",
    "            'statistical_methods_used': total_methods,\n",
    "            'methods_with_positive_evidence': evidence_count,\n",
    "            'overall_statistical_conclusion': statistical_conclusion,\n",
    "            'evidence_strength_ratio': f\"{evidence_count}/{total_methods}\",\n",
    "            'accuracy_improvements': {\n",
    "                'overall_accuracy': improvements['accuracy'],\n",
    "                'fp_reduction': improvements['fp_reduction'],\n",
    "                'fn_reduction': improvements['fn_reduction'],\n",
    "                'error_reduction': improvements['error_reduction']\n",
    "            },\n",
    "            'accuracy_metrics': {\n",
    "                'hr_accuracy': hr_metrics['accuracy'],\n",
    "                'standard_accuracy': standard_metrics['accuracy'],\n",
    "                'hr_fp_rate': hr_metrics['fp_rate'],\n",
    "                'hr_fn_rate': hr_metrics['fn_rate'],\n",
    "                'standard_fp_rate': standard_metrics['fp_rate'],\n",
    "                'standard_fn_rate': standard_metrics['fn_rate']\n",
    "            },\n",
    "            'executive_recommendation': 'PROCEED WITH IMPLEMENTATION' if evidence_count >= total_methods * 0.75 else 'PROCEED WITH PILOT PROGRAM' if evidence_count >= total_methods * 0.5 else 'DEFER IMPLEMENTATION',\n",
    "            'statistical_evidence_details': evidence_details,\n",
    "            'analysis_timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        # Save detailed results\n",
    "        import json\n",
    "        with open(f'results/enhanced_accuracy_executive_summary_{project_name}_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_summary, f, indent=2, default=str)\n",
    "        \n",
    "        # Save statistical evidence summary\n",
    "        with open(f'results/accuracy_statistical_evidence_{project_name}_{timestamp}.json', 'w') as f:\n",
    "            json.dump(statistical_evidence, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n✅ COMPREHENSIVE ACCURACY ANALYSIS RESULTS SAVED\")\n",
    "        print(f\"   Location: results/enhanced_accuracy_executive_summary_{project_name}_{timestamp}.json\")\n",
    "        print(f\"   Statistical Evidence: results/accuracy_statistical_evidence_{project_name}_{timestamp}.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️  Warning: Could not save results - {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ ANALYSIS INCOMPLETE\")\n",
    "    print(\"Missing required data components:\")\n",
    "    print(\"  - accuracy_test_results: Ground truth accuracy analysis from Cell 11\")\n",
    "    print(\"  - accuracy_permutation_results: Permutation tests from Cell 12 (optional)\")\n",
    "    print(\"  - accuracy_bootstrap_results: Bootstrap tests from Cell 13 (optional)\")\n",
    "    print(\"  - bayesian_accuracy_results: Bayesian analysis from Cell 14 (optional)\")\n",
    "    print(\"\\nPlease ensure Cell 11 has completed successfully to generate accuracy test results.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ENHANCED ACCURACY-BASED STATISTICAL ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Framework: Ground Truth Accuracy → Classical → Permutation → Bootstrap → Bayesian → Integration\")\n",
    "print(\"Thank you for using the Enhanced SiFP COSMIC Estimation Accuracy Analysis Framework!\")\n",
    "print(\"For questions or support, refer to the comprehensive documentation and statistical methodologies.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 3: Hallucination Reduction and Time-to-Market Impact\n",
    "**Implementing hallucination-reducing techniques in LLMs significantly improves (>30%) time to market in new product development.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Enhanced Setup and Imports with Advanced Statistical Testing (FIXED)\n",
    "# Purpose: Import all required libraries and configure environment settings for SiFP COSMIC Estimation Analysis with advanced statistical methods\n",
    "# Dependencies: pandas, numpy, matplotlib, seaborn, scipy, neo4j, scikit-learn, dotenv, pymc, arviz, bayesian-testing\n",
    "# Breadcrumbs: Setup -> Environment Configuration -> Analysis Preparation -> Advanced Statistical Methods\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Core statistical and machine learning imports\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Database connections\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Check scipy version for permutation_test availability\n",
    "import scipy\n",
    "scipy_version = [int(x) for x in scipy.__version__.split('.')]\n",
    "SCIPY_HAS_PERMUTATION_TEST = (scipy_version[0] > 1) or (scipy_version[0] == 1 and scipy_version[1] >= 7)\n",
    "\n",
    "print(f\"SciPy version: {scipy.__version__}\")\n",
    "print(f\"Permutation test available: {SCIPY_HAS_PERMUTATION_TEST}\")\n",
    "\n",
    "# Advanced statistical testing imports with better error handling\n",
    "try:\n",
    "    # Basic scipy stats (should always be available)\n",
    "    from scipy.stats import (\n",
    "        mannwhitneyu, wilcoxon, kruskal, friedmanchisquare, \n",
    "        chi2_contingency, fisher_exact, pearsonr, spearmanr, \n",
    "        kendalltau, norm, t as t_dist\n",
    "    )\n",
    "    \n",
    "    # Try to import permutation_test and bootstrap (scipy 1.7.0+)\n",
    "    if SCIPY_HAS_PERMUTATION_TEST:\n",
    "        from scipy.stats import permutation_test, bootstrap\n",
    "        PERMUTATION_TEST_AVAILABLE = True\n",
    "        BOOTSTRAP_AVAILABLE = True\n",
    "        print(\"✓ SciPy permutation_test and bootstrap imported successfully\")\n",
    "    else:\n",
    "        PERMUTATION_TEST_AVAILABLE = False\n",
    "        BOOTSTRAP_AVAILABLE = False\n",
    "        print(\"⚠ SciPy permutation_test not available (requires SciPy >= 1.7.0)\")\n",
    "    \n",
    "    # Bayesian analysis imports\n",
    "    try:\n",
    "        import pymc as pm\n",
    "        import arviz as az\n",
    "        BAYESIAN_AVAILABLE = True\n",
    "        print(\"✓ PyMC and ArviZ imported successfully\")\n",
    "    except ImportError:\n",
    "        BAYESIAN_AVAILABLE = False\n",
    "        print(\"⚠ PyMC/ArviZ not available\")\n",
    "    \n",
    "    # Bayesian hypothesis testing\n",
    "    try:\n",
    "        from bayesian_testing.experiments import BinaryDataTest, NormalDataTest\n",
    "        BAYESIAN_TESTING_AVAILABLE = True\n",
    "        print(\"✓ Bayesian-testing imported successfully\")\n",
    "    except ImportError:\n",
    "        BAYESIAN_TESTING_AVAILABLE = False\n",
    "        print(\"⚠ bayesian-testing not available\")\n",
    "    \n",
    "    # Additional statistical utilities (CORRECTED FUNCTION NAMES)\n",
    "    try:\n",
    "        from statsmodels.stats.contingency_tables import mcnemar\n",
    "        from statsmodels.stats.power import ttest_power, tt_ind_solve_power\n",
    "        from statsmodels.stats.proportion import proportion_confint, proportions_ztest\n",
    "        from statsmodels.stats.descriptivestats import describe\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        STATSMODELS_AVAILABLE = True\n",
    "        print(\"✓ Statsmodels imported successfully\")\n",
    "    except ImportError:\n",
    "        STATSMODELS_AVAILABLE = False\n",
    "        print(\"⚠ Statsmodels not available\")\n",
    "    \n",
    "    ADVANCED_STATS_AVAILABLE = True\n",
    "    print(\"✓ Basic advanced statistical libraries loaded successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Warning: Some statistical libraries not available: {e}\")\n",
    "    ADVANCED_STATS_AVAILABLE = False\n",
    "    PERMUTATION_TEST_AVAILABLE = False\n",
    "    BOOTSTRAP_AVAILABLE = False\n",
    "    BAYESIAN_AVAILABLE = False\n",
    "    BAYESIAN_TESTING_AVAILABLE = False\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "\n",
    "def custom_permutation_test(sample1, sample2, statistic_func, n_resamples=10000, alternative='two-sided', random_state=None):\n",
    "    \"\"\"\n",
    "    Custom implementation of permutation test for older SciPy versions\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Calculate observed statistic\n",
    "    observed_stat = statistic_func(sample1, sample2)\n",
    "    \n",
    "    # Combine samples for permutation\n",
    "    combined = np.concatenate([sample1, sample2])\n",
    "    n1 = len(sample1)\n",
    "    \n",
    "    # Generate permutation distribution\n",
    "    perm_stats = []\n",
    "    for _ in range(n_resamples):\n",
    "        perm_combined = np.random.permutation(combined)\n",
    "        perm_sample1 = perm_combined[:n1]\n",
    "        perm_sample2 = perm_combined[n1:]\n",
    "        perm_stat = statistic_func(perm_sample1, perm_sample2)\n",
    "        perm_stats.append(perm_stat)\n",
    "    \n",
    "    perm_stats = np.array(perm_stats)\n",
    "    \n",
    "    # Calculate p-value based on alternative hypothesis\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = np.mean(np.abs(perm_stats) >= np.abs(observed_stat))\n",
    "    elif alternative == 'greater':\n",
    "        p_value = np.mean(perm_stats >= observed_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = np.mean(perm_stats <= observed_stat)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "    \n",
    "    # Return result object similar to scipy's permutation_test\n",
    "    class PermutationTestResult:\n",
    "        def __init__(self, statistic, pvalue, null_distribution):\n",
    "            self.statistic = statistic\n",
    "            self.pvalue = pvalue\n",
    "            self.null_distribution = null_distribution\n",
    "    \n",
    "    return PermutationTestResult(observed_stat, p_value, perm_stats)\n",
    "\n",
    "def custom_bootstrap_ci(data, statistic_func, n_resamples=10000, confidence_level=0.95, random_state=None):\n",
    "    \"\"\"\n",
    "    Custom implementation of bootstrap confidence intervals\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_stats = []\n",
    "    for _ in range(n_resamples):\n",
    "        bootstrap_sample = np.random.choice(data, len(data), replace=True)\n",
    "        bootstrap_stat = statistic_func(bootstrap_sample)\n",
    "        bootstrap_stats.append(bootstrap_stat)\n",
    "    \n",
    "    bootstrap_stats = np.array(bootstrap_stats)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_stats, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_stats, upper_percentile)\n",
    "    \n",
    "    # Return result object similar to scipy's bootstrap\n",
    "    class BootstrapResult:\n",
    "        def __init__(self, confidence_interval, bootstrap_distribution):\n",
    "            self.confidence_interval = (ci_lower, ci_upper)\n",
    "            self.bootstrap_distribution = bootstrap_distribution\n",
    "    \n",
    "    return BootstrapResult((ci_lower, ci_upper), bootstrap_stats)\n",
    "\n",
    "def setup_analysis_environment():\n",
    "    \"\"\"\n",
    "    Configure analysis environment with display options and styling\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters for the analysis\n",
    "    \"\"\"\n",
    "    # Suppress warnings for cleaner output\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Configure matplotlib and seaborn styling\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # Configure pandas display options for better readability\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Configuration parameters\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        'CONVERSION_FACTOR': 0.957,  # SiFP = 0.957 × UFP (Desharnais)\n",
    "        'COST_PER_HOUR': 100,  # Industry standard for cost impact calculations\n",
    "        \n",
    "        # Statistical testing configuration\n",
    "        'ALPHA_LEVEL': 0.05,  # Significance level\n",
    "        'BOOTSTRAP_SAMPLES': 10000,  # Number of bootstrap samples\n",
    "        'PERMUTATION_SAMPLES': 10000,  # Number of permutation samples\n",
    "        'BAYESIAN_SAMPLES': 2000,  # Number of MCMC samples\n",
    "        'BAYESIAN_CHAINS': 4,  # Number of MCMC chains\n",
    "        'IMPROVEMENT_THRESHOLD': 0.30,  # 30% improvement threshold\n",
    "        'POWER_TARGET': 0.80,  # Target statistical power\n",
    "        \n",
    "        # Advanced testing flags\n",
    "        'ADVANCED_STATS_AVAILABLE': ADVANCED_STATS_AVAILABLE,\n",
    "        'SCIPY_VERSION': scipy.__version__\n",
    "    }\n",
    "    \n",
    "    print(\"✓ Analysis environment configured successfully\")\n",
    "    print(f\"✓ Project: {config['NEO4J_PROJECT_NAME']}\")\n",
    "    print(f\"✓ Advanced statistical methods: {'Available' if ADVANCED_STATS_AVAILABLE else 'Limited'}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "def initialize_statistical_methods():\n",
    "    \"\"\"\n",
    "    Initialize and test statistical methods availability with fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        dict: Available statistical methods configuration\n",
    "    \"\"\"\n",
    "    methods_config = {\n",
    "        'permutation_tests': False,\n",
    "        'bootstrap_tests': False,\n",
    "        'bayesian_analysis': False,\n",
    "        'power_analysis': False\n",
    "    }\n",
    "    \n",
    "    # Test permutation functionality\n",
    "    try:\n",
    "        test_data1 = np.random.normal(0, 1, 10)\n",
    "        test_data2 = np.random.normal(0, 1, 10)\n",
    "        \n",
    "        def test_statistic(x, y):\n",
    "            return np.mean(x) - np.mean(y)\n",
    "        \n",
    "        if PERMUTATION_TEST_AVAILABLE:\n",
    "            # Use scipy's permutation_test\n",
    "            perm_result = permutation_test((test_data1, test_data2), test_statistic, \n",
    "                                         n_resamples=100, random_state=42)\n",
    "            methods_config['permutation_tests'] = True\n",
    "            print(\"✓ SciPy permutation_test available and working\")\n",
    "        else:\n",
    "            # Use custom implementation\n",
    "            perm_result = custom_permutation_test(test_data1, test_data2, test_statistic, \n",
    "                                                n_resamples=100, random_state=42)\n",
    "            methods_config['permutation_tests'] = True\n",
    "            print(\"✓ Custom permutation_test fallback available and working\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Permutation test functionality issue: {e}\")\n",
    "        methods_config['permutation_tests'] = False\n",
    "    \n",
    "    # Test bootstrap functionality\n",
    "    try:\n",
    "        if BOOTSTRAP_AVAILABLE:\n",
    "            # Use scipy's bootstrap\n",
    "            bootstrap_result = bootstrap((test_data1,), np.mean, n_resamples=100, \n",
    "                                       random_state=42)\n",
    "            methods_config['bootstrap_tests'] = True\n",
    "            print(\"✓ SciPy bootstrap available and working\")\n",
    "        else:\n",
    "            # Use custom implementation\n",
    "            bootstrap_result = custom_bootstrap_ci(test_data1, np.mean, n_resamples=100, \n",
    "                                                 random_state=42)\n",
    "            methods_config['bootstrap_tests'] = True\n",
    "            print(\"✓ Custom bootstrap fallback available and working\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Bootstrap functionality issue: {e}\")\n",
    "        methods_config['bootstrap_tests'] = False\n",
    "    \n",
    "    # Test Bayesian functionality\n",
    "    if BAYESIAN_AVAILABLE:\n",
    "        try:\n",
    "            with pm.Model() as test_model:\n",
    "                mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "            methods_config['bayesian_analysis'] = True\n",
    "            print(\"✓ Bayesian analysis available and working\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Bayesian analysis issue: {e}\")\n",
    "            methods_config['bayesian_analysis'] = False\n",
    "    \n",
    "    # Test power analysis (CORRECTED FUNCTION CALL)\n",
    "    if STATSMODELS_AVAILABLE:\n",
    "        try:\n",
    "            power_result = tt_ind_solve_power(effect_size=0.5, nobs1=20, alpha=0.05)\n",
    "            methods_config['power_analysis'] = True\n",
    "            print(\"✓ Power analysis available and working\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Power analysis issue: {e}\")\n",
    "            methods_config['power_analysis'] = False\n",
    "    \n",
    "    return methods_config\n",
    "\n",
    "def setup_bayesian_environment():\n",
    "    \"\"\"\n",
    "    Configure PyMC and ArviZ for Bayesian analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian analysis configuration\n",
    "    \"\"\"\n",
    "    if not BAYESIAN_AVAILABLE:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Configure ArviZ styling\n",
    "        az.style.use('arviz-darkgrid')\n",
    "        \n",
    "        # Set up PyMC configuration (version-aware)\n",
    "        try:\n",
    "            # For PyMC3 compatibility\n",
    "            pm.set_tt_config('floatX', 'float64')\n",
    "        except AttributeError:\n",
    "            # PyMC v4+ doesn't have set_tt_config - configuration is handled differently\n",
    "            # This is normal and expected for newer PyMC versions\n",
    "            pass\n",
    "        \n",
    "        bayesian_config = {\n",
    "            'target_accept': 0.9,\n",
    "            'chains': 4,\n",
    "            'draws': 2000,\n",
    "            'tune': 1000,\n",
    "            'cores': min(4, os.cpu_count() or 1),\n",
    "            'return_inferencedata': True,\n",
    "            'random_seed': 42\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Bayesian analysis environment configured\")\n",
    "        return bayesian_config\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Bayesian environment setup issue: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Execute setup when cell runs\n",
    "CONFIG = setup_analysis_environment()\n",
    "STATISTICAL_CONFIG = initialize_statistical_methods()\n",
    "BAYESIAN_CONFIG = setup_bayesian_environment()\n",
    "\n",
    "# Make custom functions available globally if needed\n",
    "if not PERMUTATION_TEST_AVAILABLE:\n",
    "    permutation_test = custom_permutation_test\n",
    "    print(\"✓ Custom permutation_test function registered globally\")\n",
    "\n",
    "if not BOOTSTRAP_AVAILABLE:\n",
    "    bootstrap = custom_bootstrap_ci\n",
    "    print(\"✓ Custom bootstrap function registered globally\")\n",
    "\n",
    "# Display configuration summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL ANALYSIS CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SciPy Version: {scipy.__version__}\")\n",
    "print(f\"Available Methods:\")\n",
    "print(f\"  ✓ Basic statistics and visualization\")\n",
    "print(f\"  ✓ Standard hypothesis testing (t-tests, Mann-Whitney)\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('permutation_tests') else '✗'} Permutation tests {'(custom fallback)' if not PERMUTATION_TEST_AVAILABLE and STATISTICAL_CONFIG.get('permutation_tests') else '(scipy native)' if STATISTICAL_CONFIG.get('permutation_tests') else ''}\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('bootstrap_tests') else '✗'} Advanced bootstrapping {'(custom fallback)' if not BOOTSTRAP_AVAILABLE and STATISTICAL_CONFIG.get('bootstrap_tests') else '(scipy native)' if STATISTICAL_CONFIG.get('bootstrap_tests') else ''}\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('bayesian_analysis') else '✗'} Bayesian hypothesis testing\")\n",
    "print(f\"  {'✓' if STATISTICAL_CONFIG.get('power_analysis') else '✗'} Power analysis\")\n",
    "\n",
    "print(f\"\\nConfiguration Parameters:\")\n",
    "print(f\"  Alpha level: {CONFIG['ALPHA_LEVEL']}\")\n",
    "print(f\"  Improvement threshold: {CONFIG['IMPROVEMENT_THRESHOLD']:.0%}\")\n",
    "print(f\"  Bootstrap samples: {CONFIG['BOOTSTRAP_SAMPLES']:,}\")\n",
    "print(f\"  Permutation samples: {CONFIG['PERMUTATION_SAMPLES']:,}\")\n",
    "if BAYESIAN_CONFIG:\n",
    "    print(f\"  MCMC samples: {CONFIG['BAYESIAN_SAMPLES']:,}\")\n",
    "    print(f\"  MCMC chains: {CONFIG['BAYESIAN_CHAINS']}\")\n",
    "\n",
    "if not PERMUTATION_TEST_AVAILABLE:\n",
    "    print(f\"\\n⚠ NOTE: Using custom permutation test implementation\")\n",
    "    print(f\"   To use native SciPy implementation: pip install 'scipy>=1.7.0'\")\n",
    "\n",
    "if not BOOTSTRAP_AVAILABLE:\n",
    "    print(f\"\\n⚠ NOTE: Using custom bootstrap implementation\")\n",
    "    print(f\"   To use native SciPy implementation: pip install 'scipy>=1.7.0'\")\n",
    "\n",
    "print(f\"\\n✓ Enhanced statistical analysis environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Load and Process Java Code Metrics\n",
    "# Purpose: Load actual implementation metrics from iTrust java.csv for baseline establishment\n",
    "# Dependencies: pandas, configured environment (Cell 0)\n",
    "# Breadcrumbs: Setup -> Code Metrics Loading -> Baseline Data Preparation\n",
    "\n",
    "def load_code_metrics():\n",
    "    \"\"\"\n",
    "    Load and process Java code metrics from iTrust dataset\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed code metrics with derived calculations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load java.csv from iTrust dataset\n",
    "        java_df = pd.read_csv('../datasets/iTrust/iTrust/java.csv')\n",
    "        print(f\"✓ Loaded java.csv: {java_df.shape[0]} code entities\")\n",
    "\n",
    "        # Filter to only File entries (excluding methods, functions, etc.)\n",
    "        file_df = java_df[java_df['Kind'] == 'File'].copy()\n",
    "        print(f\"  Files: {len(file_df)}\")\n",
    "\n",
    "        # Select relevant metrics for SiFP analysis\n",
    "        metrics_columns = [\n",
    "            'Name', 'CountLine', 'CountLineCode', 'CountLineComment',\n",
    "            'CountDeclClass', 'CountDeclMethod', 'CountDeclMethodAll',\n",
    "            'CountDeclExecutableUnit', 'Cyclomatic', 'MaxCyclomatic'\n",
    "        ]\n",
    "\n",
    "        # Create analysis-ready dataframe\n",
    "        code_metrics_df = file_df[metrics_columns].copy()\n",
    "\n",
    "        # Calculate derived metrics that correlate with function points\n",
    "        code_metrics_df['TotalUnits'] = (\n",
    "            code_metrics_df['CountDeclClass'] + \n",
    "            code_metrics_df['CountDeclMethod']\n",
    "        )\n",
    "\n",
    "        return code_metrics_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading code metrics: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load and process the code metrics\n",
    "code_metrics_df = load_code_metrics()\n",
    "\n",
    "# Display sample data for verification\n",
    "print(\"\\nSample code metrics:\")\n",
    "print(code_metrics_df.head())\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\nCode Metrics Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  Total files: {len(code_metrics_df)}\")\n",
    "print(f\"  Average lines of code: {code_metrics_df['CountLineCode'].mean():.0f}\")\n",
    "print(f\"  Average methods per file: {code_metrics_df['CountDeclMethod'].mean():.1f}\")\n",
    "print(f\"  Average cyclomatic complexity: {code_metrics_df['Cyclomatic'].mean():.1f}\")\n",
    "print(f\"  Total LOC in codebase: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "\n",
    "# Store key metrics for later analysis\n",
    "code_summary = {\n",
    "    'total_files': len(code_metrics_df),\n",
    "    'total_lines': code_metrics_df['CountLineCode'].sum(),\n",
    "    'total_classes': code_metrics_df['CountDeclClass'].sum(),\n",
    "    'total_methods': code_metrics_df['CountDeclMethod'].sum(),\n",
    "    'avg_complexity': code_metrics_df['Cyclomatic'].mean(),\n",
    "    'total_units': code_metrics_df['TotalUnits'].sum()\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Code metrics loaded and processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Connect to Neo4j and Retrieve LLM SiFP Estimates\n",
    "# Purpose: Retrieve LLM-generated SiFP estimates for requirements with established ground truth links\n",
    "# Dependencies: Neo4j connection, json processing, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Code Metrics -> Neo4j Data Retrieval -> LLM Estimates Analysis\n",
    "\n",
    "def retrieve_llm_estimates():\n",
    "    \"\"\"\n",
    "    Connect to Neo4j and retrieve LLM SiFP estimates for ground truth requirements\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm_estimates_df, ground_truth_requirements, model_success_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Establish Neo4j connection using configuration\n",
    "        driver = GraphDatabase.driver(\n",
    "            CONFIG['NEO4J_URI'], \n",
    "            auth=(CONFIG['NEO4J_USER'], CONFIG['NEO4J_PASSWORD'])\n",
    "        )\n",
    "        print(f\"✓ Connected to Neo4j for project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "\n",
    "        with driver.session() as session:\n",
    "            # First, identify TARGET requirements with ground truth\n",
    "            gt_query = \"\"\"\n",
    "                MATCH (r1:Requirement {project: $project_name, type: 'TARGET'})\n",
    "                WHERE EXISTS((r1)-[:GROUND_TRUTH]-()) OR EXISTS(()-[:GROUND_TRUTH]-(r1))\n",
    "                RETURN DISTINCT r1.id as requirement_id\n",
    "            \"\"\"\n",
    "            \n",
    "            gt_result = session.run(gt_query, project_name=CONFIG['NEO4J_PROJECT_NAME'])\n",
    "            ground_truth_requirements = [record['requirement_id'] for record in gt_result]\n",
    "            \n",
    "            print(f\"✓ Found {len(ground_truth_requirements)} TARGET requirements with ground truth\")\n",
    "\n",
    "            # Query for LLM estimates on ground truth requirements\n",
    "            estimation_query = \"\"\"\n",
    "                MATCH (r1:Requirement {project: $project_name, type: 'TARGET'})-[se:SIFP_ESTIMATION]->(r2:Requirement)\n",
    "                WHERE r1.id IN $ground_truth_reqs\n",
    "                  AND se.final_estimation IS NOT NULL\n",
    "                  AND se.is_valid = true\n",
    "                RETURN DISTINCT r1.id as requirement_id,\n",
    "                       r1.content as requirement_content,\n",
    "                       se.model as model,\n",
    "                       se.actor_analysis as actor_json,\n",
    "                       se.final_estimation as final_json,\n",
    "                       se.judge_evaluation as judge_eval_json,\n",
    "                       se.confidence as confidence,\n",
    "                       se.judge_confidence as judge_confidence,\n",
    "                       se.judge_score as judge_score\n",
    "                ORDER BY r1.id, se.model\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(estimation_query, \n",
    "                               project_name=CONFIG['NEO4J_PROJECT_NAME'], \n",
    "                               ground_truth_reqs=ground_truth_requirements)\n",
    "            \n",
    "            # Process and structure the results\n",
    "            records = []\n",
    "            for record in result:\n",
    "                try:\n",
    "                    # Parse JSON data from Neo4j\n",
    "                    actor_data = json.loads(record['actor_json']) if record['actor_json'] else {}\n",
    "                    final_data = json.loads(record['final_json']) if record['final_json'] else {}\n",
    "                    \n",
    "                    # Extract UGEP and UGDG counts\n",
    "                    actor_ugep = len(actor_data.get('ugeps', []))\n",
    "                    actor_ugdg = len(actor_data.get('ugdgs', []))\n",
    "                    final_ugep = len(final_data.get('ugeps', []))\n",
    "                    final_ugdg = len(final_data.get('ugdgs', []))\n",
    "                    \n",
    "                    # Calculate SiFP using standard formula: SiFP = 4.6 × UGEP + 7.0 × UGDG\n",
    "                    actor_sifp = 4.6 * actor_ugep + 7 * actor_ugdg\n",
    "                    final_sifp = 4.6 * final_ugep + 7 * final_ugdg\n",
    "                    \n",
    "                    records.append({\n",
    "                        'requirement_id': record['requirement_id'],\n",
    "                        'requirement_content': record['requirement_content'][:100] + '...',\n",
    "                        'model': record['model'],\n",
    "                        'actor_ugep': actor_ugep,\n",
    "                        'actor_ugdg': actor_ugdg,\n",
    "                        'actor_sifp': actor_sifp,\n",
    "                        'final_ugep': final_ugep,\n",
    "                        'final_ugdg': final_ugdg,\n",
    "                        'final_sifp': final_sifp,\n",
    "                        'judge_score': record['judge_score'],\n",
    "                        'confidence': record['confidence']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing record for {record.get('requirement_id', 'unknown')}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Create estimates DataFrame\n",
    "        llm_estimates_df = pd.DataFrame(records)\n",
    "        \n",
    "        if not llm_estimates_df.empty:\n",
    "            print(f\"\\n✓ Retrieved {len(llm_estimates_df)} LLM estimates for ground truth requirements\")\n",
    "            \n",
    "            # Calculate model success rates\n",
    "            print(\"\\nModel Success Rates (Ground Truth Requirements):\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            model_success = []\n",
    "            for model in sorted(llm_estimates_df['model'].unique()):\n",
    "                model_estimates = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "                successful_reqs = model_estimates['requirement_id'].nunique()\n",
    "                success_rate = successful_reqs / len(ground_truth_requirements) * 100\n",
    "                \n",
    "                model_success.append({\n",
    "                    'model': model,\n",
    "                    'successful_estimates': successful_reqs,\n",
    "                    'total_ground_truth': len(ground_truth_requirements),\n",
    "                    'success_rate': success_rate\n",
    "                })\n",
    "                \n",
    "                print(f\"  {model}: {successful_reqs}/{len(ground_truth_requirements)} ({success_rate:.1f}%)\")\n",
    "            \n",
    "            model_success_df = pd.DataFrame(model_success)\n",
    "            \n",
    "            # Display sample estimates for verification\n",
    "            print(\"\\nSample LLM estimates (with ground truth):\")\n",
    "            display_cols = ['requirement_id', 'model', 'final_sifp', 'judge_score']\n",
    "            print(llm_estimates_df[display_cols].head())\n",
    "            \n",
    "            return llm_estimates_df, ground_truth_requirements, model_success_df\n",
    "            \n",
    "        else:\n",
    "            print(\"Warning: No LLM estimates found for ground truth requirements!\")\n",
    "            return pd.DataFrame(), ground_truth_requirements, pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving LLM estimates: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        driver.close()\n",
    "        print(\"✓ Neo4j connection closed\")\n",
    "\n",
    "# Execute the retrieval process\n",
    "llm_estimates_df, ground_truth_requirements, model_success_df = retrieve_llm_estimates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Establish Requirements-to-Code Mapping and Feature Analysis\n",
    "# Purpose: Create mapping between requirements and actual code files for validation baseline\n",
    "# Dependencies: llm_estimates_df from Cell 2, feature extraction logic\n",
    "# Breadcrumbs: Setup -> Data Retrieval -> Requirements Mapping -> Feature Analysis\n",
    "\n",
    "def analyze_requirement_features():\n",
    "    \"\"\"\n",
    "    Analyze requirements by extracting feature identifiers and establishing mappings\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (feature_requirements_df, feature_mapping)\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_feature_from_requirement(req_id):\n",
    "        \"\"\"\n",
    "        Extract feature/module name from requirement ID using common patterns\n",
    "        \n",
    "        Args:\n",
    "            req_id (str): Requirement identifier\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted feature name\n",
    "        \"\"\"\n",
    "        # Handle common requirement ID patterns\n",
    "        if 'UC' in req_id:\n",
    "            # Use case format: UC1.1 -> UC1\n",
    "            return req_id.split('.')[0]\n",
    "        elif '-' in req_id:\n",
    "            # Functional requirement format: FR-AUTH-001 -> AUTH\n",
    "            parts = req_id.split('-')\n",
    "            if len(parts) >= 2:\n",
    "                return parts[1]\n",
    "        return req_id  # Return original if no pattern matches\n",
    "\n",
    "    # Check if we have LLM estimates to analyze\n",
    "    if not llm_estimates_df.empty:\n",
    "        print(\"Analyzing requirement features and groupings...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract features from requirement IDs\n",
    "        llm_estimates_df['feature'] = llm_estimates_df['requirement_id'].apply(extract_feature_from_requirement)\n",
    "        \n",
    "        # Group requirements by feature for analysis\n",
    "        feature_requirements = llm_estimates_df.groupby('feature').agg({\n",
    "            'requirement_id': 'nunique',  # Count unique requirements\n",
    "            'final_sifp': ['mean', 'sum', 'std'],\n",
    "            'model': 'nunique'  # Count how many models estimated this feature\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names for better readability\n",
    "        feature_requirements.columns = [\n",
    "            'unique_requirements', 'avg_sifp', 'total_sifp', 'std_sifp', 'models_count'\n",
    "        ]\n",
    "        \n",
    "        # Sort by total SiFP for better insights\n",
    "        feature_requirements = feature_requirements.sort_values('total_sifp', ascending=False)\n",
    "        \n",
    "        print(\"Requirements grouped by feature:\")\n",
    "        print(feature_requirements)\n",
    "        \n",
    "        # Calculate feature statistics\n",
    "        print(f\"\\nFeature Analysis Summary:\")\n",
    "        print(f\"  Total features identified: {len(feature_requirements)}\")\n",
    "        print(f\"  Average requirements per feature: {feature_requirements['unique_requirements'].mean():.1f}\")\n",
    "        print(f\"  Average SiFP per feature: {feature_requirements['avg_sifp'].mean():.1f}\")\n",
    "        print(f\"  Most complex feature: {feature_requirements.index[0]} ({feature_requirements['total_sifp'].max():.1f} SiFP)\")\n",
    "        \n",
    "        # Create feature mapping for traceability\n",
    "        feature_mapping = {}\n",
    "        for feature in feature_requirements.index:\n",
    "            feature_reqs = llm_estimates_df[llm_estimates_df['feature'] == feature]['requirement_id'].unique()\n",
    "            feature_mapping[feature] = {\n",
    "                'requirements': list(feature_reqs),\n",
    "                'count': len(feature_reqs),\n",
    "                'estimated_loc': feature_requirements.loc[feature, 'total_sifp'] * CONFIG.get('avg_loc_per_sifp', 100)\n",
    "            }\n",
    "        \n",
    "        return feature_requirements, feature_mapping\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: No LLM estimates available for feature analysis\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "# Execute feature analysis\n",
    "if not llm_estimates_df.empty:\n",
    "    feature_requirements_df, feature_mapping = analyze_requirement_features()\n",
    "    \n",
    "    # Display insights about the mapping approach\n",
    "    print(f\"\\nRequirement-to-Code Mapping Approach:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"• Using aggregate analysis based on requirement feature groupings\")\n",
    "    print(\"• Features extracted from requirement IDs using pattern matching\")\n",
    "    print(\"• In production, explicit traceability links would provide direct mapping\")\n",
    "    print(\"• Current approach enables statistical validation at feature level\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping feature analysis - no LLM estimates available\")\n",
    "    feature_requirements_df = pd.DataFrame()\n",
    "    feature_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Calculate Normalized Metrics and Establish Conversion Baselines\n",
    "# Purpose: Establish normalized relationships between SiFP and code metrics using UFP→SiFP conversion\n",
    "# Dependencies: code_summary from Cell 1, llm_estimates_df from Cell 2, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Data Collection -> Mapping -> Baseline Establishment\n",
    "\n",
    "def calculate_normalized_metrics():\n",
    "    \"\"\"\n",
    "    Calculate normalized metrics and establish baseline relationships between SiFP and code metrics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (llm_analysis_df, baseline_metrics, industry_metrics)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Code Base Summary (Full Codebase):\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in code_summary.items():\n",
    "        print(f\"  {key}: {value:.0f}\")\n",
    "\n",
    "    # Calculate normalized code metrics\n",
    "    print(\"\\nNormalized Code Metrics (Full Codebase):\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Key normalized metrics\n",
    "    loc_per_file = code_summary['total_lines'] / code_summary['total_files']\n",
    "    methods_per_kloc = (code_summary['total_methods'] / code_summary['total_lines']) * 1000\n",
    "    classes_per_kloc = (code_summary['total_classes'] / code_summary['total_lines']) * 1000\n",
    "\n",
    "    print(f\"  Lines of code per file: {loc_per_file:.1f}\")\n",
    "    print(f\"  Methods per KLOC: {methods_per_kloc:.1f}\")\n",
    "    print(f\"  Classes per KLOC: {classes_per_kloc:.1f}\")\n",
    "\n",
    "    # Establish industry baselines\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE CALCULATION APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Industry standards from research\n",
    "    INDUSTRY_LOC_PER_UFP = 100  # Typical for Java\n",
    "    INDUSTRY_LOC_PER_SIFP = INDUSTRY_LOC_PER_UFP / CONFIG['CONVERSION_FACTOR']  # Adjust for conversion\n",
    "\n",
    "    industry_metrics = {\n",
    "        'LOC_PER_UFP': INDUSTRY_LOC_PER_UFP,\n",
    "        'LOC_PER_SIFP': INDUSTRY_LOC_PER_SIFP,\n",
    "        'SIFP_PER_KLOC': 1000/INDUSTRY_LOC_PER_SIFP\n",
    "    }\n",
    "\n",
    "    print(f\"\\nIndustry Baseline (Research-based):\")\n",
    "    print(f\"  Typical LOC per UFP (Java): {INDUSTRY_LOC_PER_UFP}\")\n",
    "    print(f\"  Implied LOC per SiFP: {INDUSTRY_LOC_PER_SIFP:.1f}\")\n",
    "    print(f\"  SiFP per KLOC: {industry_metrics['SIFP_PER_KLOC']:.2f}\")\n",
    "\n",
    "    # Analyze LLM estimates if available\n",
    "    if not llm_estimates_df.empty and len(ground_truth_requirements) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LLM SIFP ANALYSIS (Scaled to Estimated Requirements)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get unique requirements that were successfully estimated\n",
    "        estimated_requirements = llm_estimates_df['requirement_id'].unique()\n",
    "        estimation_coverage = len(estimated_requirements) / len(ground_truth_requirements)\n",
    "        \n",
    "        print(f\"\\nEstimation Coverage:\")\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "        print(f\"  Requirements with estimates: {len(estimated_requirements)} ({estimation_coverage:.1%})\")\n",
    "        \n",
    "        # Scale code metrics based on estimation coverage\n",
    "        scaled_code_metrics = {\n",
    "            'lines': code_summary['total_lines'] * estimation_coverage,\n",
    "            'classes': code_summary['total_classes'] * estimation_coverage,\n",
    "            'methods': code_summary['total_methods'] * estimation_coverage\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nScaled Code Metrics (for estimated requirements only):\")\n",
    "        print(f\"  Estimated lines of code: {scaled_code_metrics['lines']:.0f}\")\n",
    "        print(f\"  Estimated classes: {scaled_code_metrics['classes']:.0f}\")\n",
    "        print(f\"  Estimated methods: {scaled_code_metrics['methods']:.0f}\")\n",
    "        \n",
    "        # Calculate metrics for each LLM model\n",
    "        llm_analysis = []\n",
    "        \n",
    "        for model in sorted(llm_estimates_df['model'].unique()):\n",
    "            model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "            \n",
    "            # Calculate model totals\n",
    "            total_sifp = model_data['final_sifp'].sum()\n",
    "            total_ugep = model_data['final_ugep'].sum()\n",
    "            total_ugdg = model_data['final_ugdg'].sum()\n",
    "            successful_reqs = model_data['requirement_id'].nunique()\n",
    "            \n",
    "            # Calculate normalized metrics based on MODEL-SPECIFIC coverage\n",
    "            model_coverage = successful_reqs / len(ground_truth_requirements)\n",
    "            model_estimated_loc = code_summary['total_lines'] * model_coverage\n",
    "            \n",
    "            # Key normalized metrics\n",
    "            sifp_per_kloc = (total_sifp / model_estimated_loc) * 1000 if model_estimated_loc > 0 else 0\n",
    "            loc_per_sifp = model_estimated_loc / total_sifp if total_sifp > 0 else 0\n",
    "            sifp_per_req = total_sifp / successful_reqs if successful_reqs > 0 else 0\n",
    "            \n",
    "            # Calculate equivalent UFP for comparison\n",
    "            equivalent_ufp = total_sifp / CONFIG['CONVERSION_FACTOR']\n",
    "            \n",
    "            llm_analysis.append({\n",
    "                'model': model,\n",
    "                'successful_reqs': successful_reqs,\n",
    "                'coverage': model_coverage,\n",
    "                'total_sifp': total_sifp,\n",
    "                'equivalent_ufp': equivalent_ufp,\n",
    "                'total_ugep': total_ugep,\n",
    "                'total_ugdg': total_ugdg,\n",
    "                'estimated_loc': model_estimated_loc,\n",
    "                'sifp_per_kloc': sifp_per_kloc,\n",
    "                'loc_per_sifp': loc_per_sifp,\n",
    "                'sifp_per_req': sifp_per_req\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  Successfully estimated: {successful_reqs}/{len(ground_truth_requirements)} requirements ({model_coverage:.1%})\")\n",
    "            print(f\"  Total SiFP: {total_sifp:.1f} (equivalent to {equivalent_ufp:.1f} UFP)\")\n",
    "            print(f\"  Estimated LOC coverage: {model_estimated_loc:.0f} lines\")\n",
    "            print(f\"  SiFP per KLOC: {sifp_per_kloc:.2f}\")\n",
    "            print(f\"  LOC per SiFP point: {loc_per_sifp:.1f}\")\n",
    "            print(f\"  Deviation from industry baseline: {(loc_per_sifp - INDUSTRY_LOC_PER_SIFP)/INDUSTRY_LOC_PER_SIFP*100:+.1f}%\")\n",
    "        \n",
    "        # Create analysis DataFrame\n",
    "        llm_analysis_df = pd.DataFrame(llm_analysis)\n",
    "        \n",
    "        # Calculate project-specific baseline (weighted average of LLM estimates)\n",
    "        if not llm_analysis_df.empty:\n",
    "            weighted_loc_per_sifp = np.average(llm_analysis_df['loc_per_sifp'], \n",
    "                                              weights=llm_analysis_df['coverage'])\n",
    "            \n",
    "            baseline_metrics = {\n",
    "                'project_loc_per_sifp': weighted_loc_per_sifp,\n",
    "                'industry_loc_per_sifp': INDUSTRY_LOC_PER_SIFP,\n",
    "                'difference_pct': (weighted_loc_per_sifp - INDUSTRY_LOC_PER_SIFP)/INDUSTRY_LOC_PER_SIFP*100,\n",
    "                'estimated_requirements': estimated_requirements,\n",
    "                'scaled_code_metrics': scaled_code_metrics\n",
    "            }\n",
    "            \n",
    "            print(\"\\n\\nBASELINE COMPARISON:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"  Industry baseline LOC/SiFP: {INDUSTRY_LOC_PER_SIFP:.1f}\")\n",
    "            print(f\"  Project baseline LOC/SiFP (weighted avg): {weighted_loc_per_sifp:.1f}\")\n",
    "            print(f\"  Difference: {baseline_metrics['difference_pct']:+.1f}%\")\n",
    "            \n",
    "            return llm_analysis_df, baseline_metrics, industry_metrics\n",
    "        \n",
    "    # Return empty results if no LLM data\n",
    "    return pd.DataFrame(), {}, industry_metrics\n",
    "\n",
    "# Execute the normalized metrics calculation\n",
    "llm_analysis_df, baseline_metrics, industry_metrics = calculate_normalized_metrics()\n",
    "\n",
    "print(f\"\\n✓ Normalized metrics calculated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Detailed Normalized Performance Analysis\n",
    "# Purpose: Analyze model accuracy in normalized units (per SiFP point) with quality metrics\n",
    "# Dependencies: llm_analysis_df and baseline_metrics from Cell 4, llm_estimates_df from Cell 2  \n",
    "# Breadcrumbs: Setup -> Data Collection -> Baseline Establishment -> Performance Analysis\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Analyze detailed performance metrics for each LLM model\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance analysis with accuracy rankings\n",
    "    \"\"\"\n",
    "    \n",
    "    if llm_estimates_df.empty or llm_analysis_df.empty:\n",
    "        print(\"Warning: No LLM data available for performance analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Normalized Model Performance Analysis (Per SiFP Point)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model_performance = []\n",
    "    \n",
    "    # Use the project baseline from calculated metrics\n",
    "    baseline_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', \n",
    "                                                industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "    \n",
    "    print(f\"Using baseline: {baseline_loc_per_sifp:.1f} LOC per SiFP\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in llm_analysis_df.iterrows():\n",
    "        model = row['model']\n",
    "        \n",
    "        # Calculate normalized accuracy metrics\n",
    "        loc_per_sifp_error = row['loc_per_sifp'] - baseline_loc_per_sifp\n",
    "        loc_per_sifp_error_pct = (loc_per_sifp_error / baseline_loc_per_sifp) * 100 if baseline_loc_per_sifp > 0 else 0\n",
    "        \n",
    "        # Get quality metrics from original LLM data\n",
    "        model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "        avg_confidence = model_data['confidence'].mean() if 'confidence' in model_data.columns else 0\n",
    "        avg_judge_score = model_data['judge_score'].mean() if 'judge_score' in model_data.columns else 0\n",
    "        std_sifp = model_data['final_sifp'].std() if 'final_sifp' in model_data.columns else 0\n",
    "        \n",
    "        # Calculate success rate\n",
    "        success_rate = row['successful_reqs'] / len(ground_truth_requirements) if len(ground_truth_requirements) > 0 else 0\n",
    "        \n",
    "        model_performance.append({\n",
    "            'Model': model,\n",
    "            'Success_Rate': success_rate,\n",
    "            'SiFP_per_KLOC': row['sifp_per_kloc'],\n",
    "            'LOC_per_SiFP': row['loc_per_sifp'],\n",
    "            'LOC_per_SiFP_Error': loc_per_sifp_error,\n",
    "            'Error_Pct': abs(loc_per_sifp_error_pct),\n",
    "            'Avg_SiFP_per_Req': row['sifp_per_req'],\n",
    "            'Std_SiFP': std_sifp,\n",
    "            'Avg_Confidence': avg_confidence,\n",
    "            'Avg_Judge_Score': avg_judge_score\n",
    "        })\n",
    "        \n",
    "        # Display individual model analysis\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  Success rate: {success_rate:.1%}\")\n",
    "        print(f\"  SiFP per KLOC: {row['sifp_per_kloc']:.2f}\")\n",
    "        print(f\"  LOC per SiFP point: {row['loc_per_sifp']:.1f}\")\n",
    "        print(f\"  Error vs baseline: {loc_per_sifp_error:+.1f} LOC/SiFP ({loc_per_sifp_error_pct:+.1f}%)\")\n",
    "        print(f\"  Average confidence: {avg_confidence:.2%}\")\n",
    "        print(f\"  Average judge score: {avg_judge_score:.2f}/5\")\n",
    "        print(f\"  SiFP variability (std): {std_sifp:.2f}\")\n",
    "    \n",
    "    # Create performance DataFrame\n",
    "    performance_df = pd.DataFrame(model_performance)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        # Rank models by normalized accuracy (lower error is better)\n",
    "        performance_df['Accuracy_Rank'] = performance_df['Error_Pct'].rank()\n",
    "        \n",
    "        print(\"\\n\\nNormalized Performance Summary:\")\n",
    "        print(\"=\" * 60)\n",
    "        summary_cols = ['Model', 'Success_Rate', 'LOC_per_SiFP', 'Error_Pct', 'Accuracy_Rank']\n",
    "        print(performance_df[summary_cols].round(3).to_string(index=False))\n",
    "        \n",
    "        # Additional insights\n",
    "        best_accuracy = performance_df.loc[performance_df['Error_Pct'].idxmin()]\n",
    "        best_coverage = performance_df.loc[performance_df['Success_Rate'].idxmax()]\n",
    "        \n",
    "        print(f\"\\nKey Insights:\")\n",
    "        print(f\"  Most accurate model: {best_accuracy['Model']} ({best_accuracy['Error_Pct']:.1f}% error)\")\n",
    "        print(f\"  Best coverage model: {best_coverage['Model']} ({best_coverage['Success_Rate']:.1%} success rate)\")\n",
    "        print(f\"  Average error across all models: {performance_df['Error_Pct'].mean():.1f}%\")\n",
    "        \n",
    "    return performance_df\n",
    "\n",
    "# Execute performance analysis\n",
    "performance_df = analyze_model_performance()\n",
    "\n",
    "print(f\"\\n✓ Performance analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Load Desharnais Dataset and Establish UFP→SiFP→Effort Relationships  \n",
    "# Purpose: Load industry benchmark dataset and establish the complete conversion chain for effort estimation\n",
    "# Dependencies: sklearn LinearRegression, CONFIG from Cell 0, pandas processing\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Industry Benchmarks -> Effort Conversion Chain\n",
    "\n",
    "def load_and_analyze_desharnais():\n",
    "    \"\"\"\n",
    "    Load Desharnais dataset and establish UFP→SiFP→Effort conversion relationships\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (desharnais_df, effort_metrics, effort_model)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load industry benchmark dataset\n",
    "        desharnais_df = pd.read_csv('../datasets/CostEstimation/Desharnais.csv')\n",
    "        print(f\"✓ Loaded Desharnais dataset: {desharnais_df.shape[0]} projects\")\n",
    "\n",
    "        # Identify column names (handle variations in dataset)\n",
    "        ufp_column = 'PointsNonAdjust' if 'PointsNonAdjust' in desharnais_df.columns else 'UFP'\n",
    "        effort_column = 'Effort' if 'Effort' in desharnais_df.columns else 'effort'\n",
    "\n",
    "        print(f\"Using columns: UFP='{ufp_column}', Effort='{effort_column}'\")\n",
    "        \n",
    "        # Apply UFP to SiFP conversion using research-validated factor\n",
    "        print(f\"\\nApplying UFP→SiFP conversion factor: {CONFIG['CONVERSION_FACTOR']}\")\n",
    "        desharnais_df['SiFP_converted'] = desharnais_df[ufp_column] * CONFIG['CONVERSION_FACTOR']\n",
    "\n",
    "        # Calculate effort per SiFP metrics\n",
    "        print(\"\\nDesharnais Normalized Metrics:\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Calculate hours per SiFP point for each project\n",
    "        desharnais_df['hours_per_sifp'] = desharnais_df[effort_column] / desharnais_df['SiFP_converted']\n",
    "\n",
    "        # Calculate summary statistics\n",
    "        effort_metrics = {\n",
    "            'avg_hours_per_sifp': desharnais_df['hours_per_sifp'].mean(),\n",
    "            'median_hours_per_sifp': desharnais_df['hours_per_sifp'].median(),\n",
    "            'std_hours_per_sifp': desharnais_df['hours_per_sifp'].std(),\n",
    "            'min_hours_per_sifp': desharnais_df['hours_per_sifp'].min(),\n",
    "            'max_hours_per_sifp': desharnais_df['hours_per_sifp'].max()\n",
    "        }\n",
    "\n",
    "        print(f\"  Average hours per SiFP: {effort_metrics['avg_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Median hours per SiFP: {effort_metrics['median_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Std dev hours per SiFP: {effort_metrics['std_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Range: {effort_metrics['min_hours_per_sifp']:.2f} - {effort_metrics['max_hours_per_sifp']:.2f}\")\n",
    "\n",
    "        # Build linear effort prediction model\n",
    "        print(f\"\\nBuilding Linear Effort Model:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Prepare data for sklearn\n",
    "        X = desharnais_df[['SiFP_converted']].values.astype(np.float64)\n",
    "        y = desharnais_df[effort_column].values.astype(np.float64)\n",
    "\n",
    "        # Fit linear regression model\n",
    "        effort_model = LinearRegression()\n",
    "        effort_model.fit(X, y)\n",
    "\n",
    "        # Extract model coefficients\n",
    "        linear_hours_per_sifp = float(effort_model.coef_[0])\n",
    "        intercept = float(effort_model.intercept_)\n",
    "        \n",
    "        # Calculate model performance\n",
    "        y_pred = effort_model.predict(X)\n",
    "        r2 = float(r2_score(y, y_pred))\n",
    "\n",
    "        print(f\"  Hours per SiFP (coefficient): {linear_hours_per_sifp:.2f}\")\n",
    "        print(f\"  Base hours (intercept): {intercept:.2f}\")\n",
    "        print(f\"  R² score: {r2:.3f}\")\n",
    "        \n",
    "        # Add model metrics to effort_metrics\n",
    "        effort_metrics.update({\n",
    "            'linear_hours_per_sifp': linear_hours_per_sifp,\n",
    "            'intercept': intercept,\n",
    "            'r2_score': r2\n",
    "        })\n",
    "\n",
    "        # Analyze SiFP distribution in industry data\n",
    "        print(f\"\\nSiFP Distribution in Desharnais Dataset:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Mean SiFP per project: {desharnais_df['SiFP_converted'].mean():.1f}\")\n",
    "        print(f\"  Median SiFP per project: {desharnais_df['SiFP_converted'].median():.1f}\")\n",
    "        print(f\"  Range: {desharnais_df['SiFP_converted'].min():.1f} - {desharnais_df['SiFP_converted'].max():.1f}\")\n",
    "        print(f\"  Total projects: {len(desharnais_df)}\")\n",
    "\n",
    "        return desharnais_df, effort_metrics, effort_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Desharnais dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute Desharnais analysis\n",
    "desharnais_df, effort_metrics, effort_model = load_and_analyze_desharnais()\n",
    "\n",
    "print(f\"\\n✓ Desharnais dataset analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Normalized Effort Impact Analysis with Complete Conversion Chain\n",
    "# Purpose: Analyze effort impact using UFP→SiFP→Effort conversion chain and calculate cost implications  \n",
    "# Dependencies: performance_df from Cell 5, effort_metrics from Cell 6, CONFIG from Cell 0\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Industry Benchmarks -> Effort Impact Analysis\n",
    "\n",
    "def analyze_effort_impact():\n",
    "    \"\"\"\n",
    "    Analyze effort impact using the complete UFP→SiFP→Effort conversion chain\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Effort impact analysis with cost implications\n",
    "    \"\"\"\n",
    "    \n",
    "    if performance_df.empty or not effort_metrics:\n",
    "        print(\"Warning: Missing required data for effort impact analysis\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(\"Normalized Effort Impact Analysis (Per SiFP Point)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display conversion chain information\n",
    "    print(f\"\\nConversion Chain:\")\n",
    "    print(f\"  UFP → SiFP: Factor = {CONFIG['CONVERSION_FACTOR']} (SiFP = {CONFIG['CONVERSION_FACTOR']} × UFP)\")\n",
    "    print(f\"  SiFP → Effort: {effort_metrics['avg_hours_per_sifp']:.2f} hours/SiFP (from Desharnais)\")\n",
    "    \n",
    "    if baseline_metrics:\n",
    "        print(f\"  SiFP → LOC: {baseline_metrics['project_loc_per_sifp']:.1f} LOC/SiFP (project baseline)\")\n",
    "    \n",
    "    effort_impact = []\n",
    "    \n",
    "    for _, row in performance_df.iterrows():\n",
    "        model = row['Model']\n",
    "        \n",
    "        # Get model's LOC per SiFP\n",
    "        model_loc_per_sifp = row['LOC_per_SiFP']\n",
    "        baseline_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', \n",
    "                                                   industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "        \n",
    "        # Calculate SiFP estimation accuracy\n",
    "        # If model estimates fewer LOC per SiFP, it's overestimating SiFP count\n",
    "        sifp_estimation_factor = baseline_loc_per_sifp / model_loc_per_sifp if model_loc_per_sifp > 0 else 1\n",
    "        \n",
    "        # Calculate effort impact using Desharnais baseline\n",
    "        baseline_hours_per_sifp = effort_metrics['avg_hours_per_sifp']\n",
    "        \n",
    "        # The effective hours per estimated SiFP\n",
    "        effective_hours_per_estimated_sifp = baseline_hours_per_sifp / sifp_estimation_factor\n",
    "        \n",
    "        # Calculate percentage error in effort estimation\n",
    "        effort_error_pct = (sifp_estimation_factor - 1) * 100\n",
    "        \n",
    "        # Get total SiFP estimated by this model\n",
    "        if not llm_analysis_df.empty:\n",
    "            model_row = llm_analysis_df[llm_analysis_df['model'] == model]\n",
    "            if not model_row.empty:\n",
    "                model_total_sifp = model_row['total_sifp'].values[0]\n",
    "                actual_sifp = model_total_sifp / sifp_estimation_factor\n",
    "                \n",
    "                # Calculate total effort impact\n",
    "                estimated_total_effort = model_total_sifp * baseline_hours_per_sifp\n",
    "                actual_total_effort = actual_sifp * baseline_hours_per_sifp\n",
    "                total_effort_error = estimated_total_effort - actual_total_effort\n",
    "                \n",
    "                # Calculate cost impact using standard rate\n",
    "                total_cost_impact = total_effort_error * CONFIG['COST_PER_HOUR']\n",
    "            else:\n",
    "                model_total_sifp = actual_sifp = total_effort_error = total_cost_impact = 0\n",
    "        else:\n",
    "            model_total_sifp = actual_sifp = total_effort_error = total_cost_impact = 0\n",
    "        \n",
    "        effort_impact.append({\n",
    "            'Model': model,\n",
    "            'LOC_per_SiFP': model_loc_per_sifp,\n",
    "            'SiFP_Estimation_Factor': sifp_estimation_factor,\n",
    "            'Desharnais_Hours_per_SiFP': baseline_hours_per_sifp,\n",
    "            'Effective_Hours_per_Est_SiFP': effective_hours_per_estimated_sifp,\n",
    "            'Effort_Error_Pct': effort_error_pct,\n",
    "            'Model_Total_SiFP': model_total_sifp,\n",
    "            'Actual_SiFP': actual_sifp,\n",
    "            'Total_Effort_Error_Hours': total_effort_error,\n",
    "            'Total_Cost_Impact_USD': total_cost_impact\n",
    "        })\n",
    "        \n",
    "        # Display model-specific analysis\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  LOC per SiFP: {model_loc_per_sifp:.1f} (baseline: {baseline_loc_per_sifp:.1f})\")\n",
    "        print(f\"  SiFP estimation factor: {sifp_estimation_factor:.2f}x\")\n",
    "        print(f\"  Interpretation: Model {'overestimates' if sifp_estimation_factor > 1 else 'underestimates'} SiFP count\")\n",
    "        print(f\"  Desharnais baseline: {baseline_hours_per_sifp:.2f} hours per actual SiFP\")\n",
    "        print(f\"  Effective hours per estimated SiFP: {effective_hours_per_estimated_sifp:.2f}\")\n",
    "        print(f\"  Effort estimation error: {effort_error_pct:+.1f}%\")\n",
    "        if model_total_sifp > 0:\n",
    "            print(f\"  Total SiFP estimated: {model_total_sifp:.0f}\")\n",
    "            print(f\"  Actual SiFP (implied): {actual_sifp:.0f}\")\n",
    "            print(f\"  Total effort error: {total_effort_error:+.0f} hours (${total_cost_impact:+,.0f})\")\n",
    "    \n",
    "    effort_impact_df = pd.DataFrame(effort_impact)\n",
    "    \n",
    "    if not effort_impact_df.empty:\n",
    "        # Summary statistics\n",
    "        print(\"\\n\\nEffort Impact Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"  Desharnais hours per SiFP: {effort_metrics['avg_hours_per_sifp']:.2f}\")\n",
    "        print(f\"  Average SiFP estimation factor: {effort_impact_df['SiFP_Estimation_Factor'].mean():.2f}x\")\n",
    "        print(f\"  Average effort error: {effort_impact_df['Effort_Error_Pct'].mean():+.1f}%\")\n",
    "        print(f\"  Total cost impact range: ${effort_impact_df['Total_Cost_Impact_USD'].min():,.0f} to ${effort_impact_df['Total_Cost_Impact_USD'].max():,.0f}\")\n",
    "        \n",
    "        if effort_impact_df['Effort_Error_Pct'].abs().size > 0:\n",
    "            best_model = effort_impact_df.loc[effort_impact_df['Effort_Error_Pct'].abs().idxmin(), 'Model']\n",
    "            print(f\"  Most accurate effort model: {best_model}\")\n",
    "    \n",
    "    return effort_impact_df\n",
    "\n",
    "# Execute effort impact analysis\n",
    "effort_impact_df = analyze_effort_impact()\n",
    "\n",
    "print(f\"\\n✓ Effort impact analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Comprehensive Visualization of Normalized Results and Distributions\n",
    "# Purpose: Create comprehensive visualizations of model performance, accuracy distributions, and cost impacts\n",
    "# Dependencies: performance_df from Cell 5, effort_impact_df from Cell 7, matplotlib/seaborn from Cell 0\n",
    "# Breadcrumbs: Setup -> Performance Analysis -> Effort Impact -> Comprehensive Visualization\n",
    "\n",
    "if 'performance_df' in globals() and 'effort_impact_df' in globals() and not performance_df.empty and not effort_impact_df.empty:\n",
    "    \n",
    "    # Get baseline values from previous calculations\n",
    "    avg_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_metrics.get('LOC_PER_SIFP', 100))\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10)\n",
    "    project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "    \n",
    "    # Create a larger figure with more subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Define grid for subplots\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    models = performance_df['Model'].values\n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    # 1. LOC per SiFP Point Comparison\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    bars = ax1.bar(x, performance_df['LOC_per_SiFP'], alpha=0.7, color='skyblue')\n",
    "    ax1.axhline(y=avg_loc_per_sifp, color='red', linestyle='--', \n",
    "                label=f'Baseline ({avg_loc_per_sifp:.1f} LOC/SiFP)')\n",
    "    \n",
    "    # Color bars based on performance\n",
    "    for i, bar in enumerate(bars):\n",
    "        if performance_df.iloc[i]['LOC_per_SiFP'] < avg_loc_per_sifp * 0.8:\n",
    "            bar.set_color('green')\n",
    "        elif performance_df.iloc[i]['LOC_per_SiFP'] > avg_loc_per_sifp * 1.2:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Lines of Code per SiFP Point')\n",
    "    ax1.set_title('Code Density per SiFP Point by Model')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add success rate and error annotations\n",
    "    for i, (model, success_rate, error) in enumerate(zip(models, performance_df['Success_Rate'], performance_df['Error_Pct'])):\n",
    "        ax1.text(i, performance_df.iloc[i]['LOC_per_SiFP'] + 1, \n",
    "                f'{success_rate:.0%}\\n±{error:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 2. Model Performance Comparison Table\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Create performance summary table\n",
    "    table_data = []\n",
    "    for _, row in performance_df.iterrows():\n",
    "        model_name = row['Model'].split('/')[-1][:20]\n",
    "        table_data.append([\n",
    "            model_name,\n",
    "            f\"{row['Success_Rate']:.0%}\",\n",
    "            f\"{row['LOC_per_SiFP']:.1f}\",\n",
    "            f\"{row['Error_Pct']:.0f}%\"\n",
    "        ])\n",
    "    \n",
    "    table = ax2.table(cellText=table_data,\n",
    "                     colLabels=['Model', 'Success', 'LOC/SiFP', 'Error'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    ax2.set_title('Model Performance Summary', pad=20)\n",
    "    \n",
    "    # 3. Effort per SiFP Point\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    bars = ax3.bar(x, effort_impact_df['Effective_Hours_per_Est_SiFP'], \n",
    "            color=['green' if x < desharnais_hours_per_sifp else 'orange' \n",
    "                   for x in effort_impact_df['Effective_Hours_per_Est_SiFP']], alpha=0.7)\n",
    "    ax3.axhline(y=desharnais_hours_per_sifp, color='red', linestyle='--', \n",
    "                label=f'Desharnais Baseline ({desharnais_hours_per_sifp:.1f} hrs/SiFP)')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Effective Hours per Estimated SiFP')\n",
    "    ax3.set_title('Effort Estimation per SiFP Point')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. SiFP per KLOC\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.bar(x, performance_df['SiFP_per_KLOC'], alpha=0.7, color='coral')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('SiFP per KLOC')\n",
    "    ax4.set_title('Function Point Density (SiFP per 1000 LOC)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Total Cost Impact\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    bars = ax5.bar(x, effort_impact_df['Total_Cost_Impact_USD'], \n",
    "                   color=['darkgreen' if x < 0 else 'darkred' \n",
    "                          for x in effort_impact_df['Total_Cost_Impact_USD']], alpha=0.7)\n",
    "    ax5.set_xlabel('Model')\n",
    "    ax5.set_ylabel('Total Cost Impact ($)')\n",
    "    ax5.set_title('Total Cost Impact')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels([m.split('/')[-1][:15] for m in models], rotation=45, ha='right')\n",
    "    ax5.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(effort_impact_df['Total_Cost_Impact_USD']):\n",
    "        ax5.text(i, v + (1000 if v > 0 else -1000), f'${v:,.0f}', \n",
    "                ha='center', va='bottom' if v > 0 else 'top', fontsize=8)\n",
    "    \n",
    "    # 6-10. Histograms for each model showing requirement-level accuracy\n",
    "    histogram_count = 0\n",
    "    max_histograms = 6  # Limit to 6 histograms to fit in remaining subplot space\n",
    "    \n",
    "    for idx, model in enumerate(models[:max_histograms]):\n",
    "        row_idx = 2 + histogram_count // 3\n",
    "        col_idx = histogram_count % 3\n",
    "        \n",
    "        if row_idx >= 4:  # Don't exceed our grid\n",
    "            break\n",
    "            \n",
    "        ax = fig.add_subplot(gs[row_idx, col_idx])\n",
    "        \n",
    "        model_data = llm_estimates_df[llm_estimates_df['model'] == model]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            model_coverage = model_data['requirement_id'].nunique() / len(ground_truth_requirements)\n",
    "            \n",
    "            # Calculate LOC per SiFP for each requirement\n",
    "            req_loc_per_sifp = []\n",
    "            for _, req in model_data.iterrows():\n",
    "                if req['final_sifp'] > 0:\n",
    "                    est_loc_per_req = (code_summary['total_lines'] * model_coverage) / model_data['requirement_id'].nunique()\n",
    "                    loc_per_sifp = est_loc_per_req / req['final_sifp']\n",
    "                    req_loc_per_sifp.append(loc_per_sifp)\n",
    "            \n",
    "            if req_loc_per_sifp:\n",
    "                # Create histogram\n",
    "                n, bins, patches = ax.hist(req_loc_per_sifp, bins=min(15, len(req_loc_per_sifp)), \n",
    "                                         alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                \n",
    "                # Color code bins\n",
    "                for i, patch in enumerate(patches):\n",
    "                    if i < len(bins) - 1:  # bins has one more element than patches\n",
    "                        if bins[i] < avg_loc_per_sifp * 0.8:\n",
    "                            patch.set_facecolor('green')\n",
    "                        elif bins[i] > avg_loc_per_sifp * 1.2:\n",
    "                            patch.set_facecolor('red')\n",
    "                \n",
    "                # Add baseline line\n",
    "                ax.axvline(x=avg_loc_per_sifp, color='red', linestyle='--', linewidth=2, \n",
    "                          label=f'Baseline: {avg_loc_per_sifp:.1f}')\n",
    "                ax.axvline(x=np.mean(req_loc_per_sifp), color='blue', linestyle='-', linewidth=2,\n",
    "                          label=f'Model mean: {np.mean(req_loc_per_sifp):.1f}')\n",
    "                \n",
    "                ax.set_xlabel('LOC per SiFP')\n",
    "                ax.set_ylabel('# Requirements')\n",
    "                ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nAccuracy Distribution')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics text\n",
    "                ax.text(0.95, 0.95, f'n={len(req_loc_per_sifp)}\\nσ={np.std(req_loc_per_sifp):.1f}',\n",
    "                       transform=ax.transAxes, ha='right', va='top', fontsize=8,\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No valid data', transform=ax.transAxes, ha='center', va='center')\n",
    "                ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nNo Data')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No model data', transform=ax.transAxes, ha='center', va='center')\n",
    "            ax.set_title(f'{model.split(\"/\")[-1][:20]}\\nNo Data')\n",
    "        \n",
    "        histogram_count += 1\n",
    "    \n",
    "    plt.suptitle(f'Comprehensive SiFP Analysis - {project_name}\\n'\n",
    "                 f'All Models Performance and Accuracy Distribution', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Comprehensive visualization completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create comprehensive visualization - required data not available\")\n",
    "    print(\"Available variables:\")\n",
    "    if 'performance_df' in globals():\n",
    "        print(f\"  - performance_df: {len(performance_df) if not performance_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - performance_df: not defined\")\n",
    "    \n",
    "    if 'effort_impact_df' in globals():\n",
    "        print(f\"  - effort_impact_df: {len(effort_impact_df) if not effort_impact_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - effort_impact_df: not defined\")\n",
    "    \n",
    "    if 'baseline_metrics' in globals():\n",
    "        print(\"  - baseline_metrics: available\")\n",
    "    else:\n",
    "        print(\"  - baseline_metrics: not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Executive Summary with Complete UFP→SiFP→LOC→Effort Analysis  \n",
    "# Purpose: Generate comprehensive executive summary with complete conversion chain analysis and business insights\n",
    "# Dependencies: All previous analysis results, CONFIG settings, comprehensive metrics from entire workflow\n",
    "# Breadcrumbs: Setup -> Analysis -> Recommendations -> Executive Summary & Business Impact Report\n",
    "\n",
    "print(\"EXECUTIVE SUMMARY - NORMALIZED SIFP ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get project name safely\n",
    "project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project') if 'CONFIG' in globals() else 'Unknown Project'\n",
    "print(f\"Project: {project_name}\")\n",
    "print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if 'performance_df' in globals() and 'effort_impact_df' in globals() and not performance_df.empty and not effort_impact_df.empty:\n",
    "    print(f\"\\nData Summary:\")\n",
    "    \n",
    "    if 'code_metrics_df' in globals():\n",
    "        print(f\"  Total code files in project: {len(code_metrics_df)}\")\n",
    "        print(f\"  Total lines of code in project: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "    else:\n",
    "        print(\"  Code metrics: Not available\")\n",
    "    \n",
    "    if 'ground_truth_requirements' in globals():\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "    else:\n",
    "        print(\"  Ground truth requirements: Not available\")\n",
    "    \n",
    "    if 'llm_estimates_df' in globals() and not llm_estimates_df.empty:\n",
    "        estimated_requirements = llm_estimates_df['requirement_id'].unique()\n",
    "        if 'ground_truth_requirements' in globals():\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)} ({len(estimated_requirements)/len(ground_truth_requirements):.1%})\")\n",
    "        else:\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)}\")\n",
    "    \n",
    "    # Get baseline values safely\n",
    "    conversion_factor = CONFIG.get('CONVERSION_FACTOR', 0.957) if 'CONFIG' in globals() else 0.957\n",
    "    \n",
    "    # Get industry and project baselines\n",
    "    industry_loc_per_sifp = industry_metrics.get('LOC_PER_SIFP', 100) if 'industry_metrics' in globals() else 100\n",
    "    project_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_loc_per_sifp) if 'baseline_metrics' in globals() else industry_loc_per_sifp\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10) if 'effort_metrics' in globals() else 10\n",
    "    \n",
    "    print(f\"\\nConversion Factors and Baselines:\")\n",
    "    print(f\"  UFP → SiFP: {conversion_factor} (from Desharnais research)\")\n",
    "    print(f\"  SiFP → Effort: {desharnais_hours_per_sifp:.2f} hours/SiFP (Desharnais dataset)\")\n",
    "    print(f\"  SiFP → LOC: {project_loc_per_sifp:.1f} LOC/SiFP (project weighted average)\")\n",
    "    print(f\"  Industry baseline: {industry_loc_per_sifp:.1f} LOC/SiFP\")\n",
    "    print(f\"  Project vs Industry: {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}%\")\n",
    "    \n",
    "    # Detailed performance for each model\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED MODEL PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, row in performance_df.iterrows():\n",
    "        model = row['Model']\n",
    "        effort_row = effort_impact_df[effort_impact_df['Model'] == model]\n",
    "        \n",
    "        if not effort_row.empty:\n",
    "            effort_row = effort_row.iloc[0]\n",
    "            \n",
    "            if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "                llm_row = llm_analysis_df[llm_analysis_df['model'] == model]\n",
    "                if not llm_row.empty:\n",
    "                    llm_row = llm_row.iloc[0]\n",
    "                else:\n",
    "                    llm_row = None\n",
    "            else:\n",
    "                llm_row = None\n",
    "            \n",
    "            print(f\"\\n{idx+1}. {model}\")\n",
    "            print(\"-\" * len(f\"{idx+1}. {model}\"))\n",
    "            \n",
    "            print(f\"\\n  Estimation Coverage:\")\n",
    "            print(f\"    - Success rate: {row['Success_Rate']:.1%}\")\n",
    "            if llm_row is not None:\n",
    "                print(f\"    - Requirements estimated: {llm_row['successful_reqs']}/{len(ground_truth_requirements) if 'ground_truth_requirements' in globals() else 'unknown'}\")\n",
    "            \n",
    "            print(f\"\\n  SiFP Estimates:\")\n",
    "            if llm_row is not None:\n",
    "                print(f\"    - Total SiFP: {llm_row['total_sifp']:.0f}\")\n",
    "                print(f\"    - Equivalent UFP: {llm_row['equivalent_ufp']:.0f}\")\n",
    "                print(f\"    - Average SiFP per requirement: {row['Avg_SiFP_per_Req']:.1f}\")\n",
    "            else:\n",
    "                print(f\"    - Average SiFP per requirement: {row.get('Avg_SiFP_per_Req', 'N/A')}\")\n",
    "            \n",
    "            print(f\"\\n  Accuracy Metrics:\")\n",
    "            print(f\"    - LOC per SiFP: {row['LOC_per_SiFP']:.1f} (baseline: {project_loc_per_sifp:.1f})\")\n",
    "            print(f\"    - Error: {row.get('LOC_per_SiFP_Error', 'N/A'):+.1f} LOC/SiFP ({row['Error_Pct']:+.1f}%)\")\n",
    "            print(f\"    - SiFP estimation factor: {effort_row['SiFP_Estimation_Factor']:.2f}x\")\n",
    "            \n",
    "            print(f\"\\n  Effort Impact:\")\n",
    "            print(f\"    - Desharnais baseline: {desharnais_hours_per_sifp:.2f} hours/SiFP\")\n",
    "            print(f\"    - Effort estimation error: {effort_row['Effort_Error_Pct']:+.1f}%\")\n",
    "            print(f\"    - Total effort error: {effort_row['Total_Effort_Error_Hours']:+.0f} hours\")\n",
    "            print(f\"    - Cost impact: ${effort_row['Total_Cost_Impact_USD']:+,.0f}\")\n",
    "            \n",
    "            print(f\"\\n  Quality Indicators:\")\n",
    "            print(f\"    - Average confidence: {row.get('Avg_Confidence', 0):.1%}\")\n",
    "            print(f\"    - Average judge score: {row.get('Avg_Judge_Score', 0):.2f}/5\")\n",
    "    \n",
    "    # Analysis of the conversion chain\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"CONVERSION CHAIN ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nFor a typical requirement in this project:\")\n",
    "    if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "        avg_sifp_per_req = llm_analysis_df['sifp_per_req'].mean()\n",
    "        print(f\"  Average SiFP per requirement: {avg_sifp_per_req:.1f}\")\n",
    "        print(f\"  Equivalent UFP: {avg_sifp_per_req / conversion_factor:.1f}\")\n",
    "        print(f\"  Expected LOC: {avg_sifp_per_req * project_loc_per_sifp:.0f}\")\n",
    "        print(f\"  Expected effort: {avg_sifp_per_req * desharnais_hours_per_sifp:.0f} hours\")\n",
    "    else:\n",
    "        print(\"  Analysis not available - LLM analysis data missing\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        best_accuracy = performance_df.loc[performance_df['Error_Pct'].idxmin()]['Model']\n",
    "        print(f\"\\n1. For most accurate code size estimation: {best_accuracy}\")\n",
    "    else:\n",
    "        print(\"\\n1. For most accurate code size estimation: Data not available\")\n",
    "    \n",
    "    if not effort_impact_df.empty:\n",
    "        best_effort = effort_impact_df.loc[effort_impact_df['Effort_Error_Pct'].abs().idxmin()]['Model']\n",
    "        print(f\"2. For most accurate effort estimation: {best_effort}\")\n",
    "    else:\n",
    "        print(\"2. For most accurate effort estimation: Data not available\")\n",
    "    \n",
    "    print(f\"3. Use Desharnais baseline of {desharnais_hours_per_sifp:.1f} hours per SiFP for effort planning\")\n",
    "    print(f\"4. Apply UFP conversion factor of {conversion_factor} when comparing to UFP-based estimates\")\n",
    "    print(f\"5. Consider that this project has {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}% different LOC/SiFP than industry average\")\n",
    "    \n",
    "    # Save results with all conversion factors\n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Create comprehensive summary\n",
    "        conversion_summary = pd.DataFrame({\n",
    "            'Metric': ['UFP→SiFP Factor', 'Industry LOC/SiFP', 'Project LOC/SiFP', 'Desharnais Hours/SiFP'],\n",
    "            'Value': [conversion_factor, industry_loc_per_sifp, project_loc_per_sifp, desharnais_hours_per_sifp]\n",
    "        })\n",
    "        conversion_summary.to_csv(f'results/conversion_factors_{project_name}.csv', index=False)\n",
    "        \n",
    "        # Save all other results\n",
    "        performance_df.to_csv(f'results/normalized_performance_{project_name}.csv', index=False)\n",
    "        effort_impact_df.to_csv(f'results/normalized_effort_impact_{project_name}.csv', index=False)\n",
    "        \n",
    "        if 'llm_analysis_df' in globals() and not llm_analysis_df.empty:\n",
    "            llm_analysis_df.to_csv(f'results/normalized_llm_analysis_{project_name}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Results saved to results/ directory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not save results - {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Missing required data for executive summary\")\n",
    "    print(\"Available data:\")\n",
    "    \n",
    "    if 'performance_df' in globals():\n",
    "        print(f\"  - performance_df: {len(performance_df) if not performance_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - performance_df: not available\")\n",
    "    \n",
    "    if 'effort_impact_df' in globals():\n",
    "        print(f\"  - effort_impact_df: {len(effort_impact_df) if not effort_impact_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - effort_impact_df: not available\")\n",
    "    \n",
    "    if 'llm_analysis_df' in globals():\n",
    "        print(f\"  - llm_analysis_df: {len(llm_analysis_df) if not llm_analysis_df.empty else 'empty'}\")\n",
    "    else:\n",
    "        print(\"  - llm_analysis_df: not available\")\n",
    "\n",
    "print(f\"\\n✓ Executive summary completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Statistical Hypothesis Testing for >30% Improvement in Time-to-Market\n",
    "# Purpose: Perform formal statistical testing to validate hypothesis about hallucination-reducing techniques\n",
    "# Dependencies: performance_df, effort_impact_df, scipy.stats for statistical tests\n",
    "# Breadcrumbs: Setup -> Analysis -> Executive Summary -> Statistical Hypothesis Testing\n",
    "\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def perform_hypothesis_testing():\n",
    "    \"\"\"\n",
    "    Perform formal statistical hypothesis testing for the >30% improvement claim\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistical test results including p-values and confidence intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"STATISTICAL HYPOTHESIS TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYPOTHESIS: Implementing hallucination-reducing techniques in LLMs\")\n",
    "    print(\"significantly improve (>30%) time to market in new product development\")\n",
    "    print(\"\\nOPERATIONAL DEFINITIONS:\")\n",
    "    print(\"- Hallucination-reducing techniques: Multi-stage refinement (actor→judge→meta-judge)\")\n",
    "    print(\"- Time-to-market improvement: Measured via estimation accuracy reducing project delays\")\n",
    "    print(\"- Significance threshold: >30% improvement with p < 0.05\")\n",
    "    \n",
    "    if performance_df.empty or effort_impact_df.empty:\n",
    "        print(\"\\nWarning: Insufficient data for statistical testing\")\n",
    "        return {}\n",
    "    \n",
    "    # Define treatment vs control groups based on model characteristics\n",
    "    # Assumption: Models with judge scores > 3.5 represent \"hallucination-reducing\" techniques\n",
    "    treatment_threshold = 3.5\n",
    "    \n",
    "    # Categorize models\n",
    "    treatment_models = performance_df[performance_df['Avg_Judge_Score'] > treatment_threshold]\n",
    "    control_models = performance_df[performance_df['Avg_Judge_Score'] <= treatment_threshold]\n",
    "    \n",
    "    print(f\"\\nGROUP DEFINITIONS:\")\n",
    "    print(f\"Treatment group (Judge Score > {treatment_threshold}): {len(treatment_models)} models\")\n",
    "    print(f\"Control group (Judge Score ≤ {treatment_threshold}): {len(control_models)} models\")\n",
    "    \n",
    "    if len(treatment_models) == 0 or len(control_models) == 0:\n",
    "        print(\"\\nWarning: Insufficient models in treatment or control groups for comparison\")\n",
    "        print(\"Adjusting criteria...\")\n",
    "        \n",
    "        # Alternative grouping: Top 50% vs bottom 50% by judge score\n",
    "        median_judge_score = performance_df['Avg_Judge_Score'].median()\n",
    "        treatment_models = performance_df[performance_df['Avg_Judge_Score'] > median_judge_score]\n",
    "        control_models = performance_df[performance_df['Avg_Judge_Score'] <= median_judge_score]\n",
    "        \n",
    "        print(f\"Alternative grouping by median judge score ({median_judge_score:.2f}):\")\n",
    "        print(f\"Treatment group: {len(treatment_models)} models\")\n",
    "        print(f\"Control group: {len(control_models)} models\")\n",
    "    \n",
    "    # Primary outcome: Estimation accuracy (lower error = better time-to-market)\n",
    "    treatment_errors = treatment_models['Error_Pct'].values\n",
    "    control_errors = control_models['Error_Pct'].values\n",
    "    \n",
    "    # Secondary outcomes: Success rate, effort estimation accuracy\n",
    "    treatment_success = treatment_models['Success_Rate'].values\n",
    "    control_success = control_models['Success_Rate'].values\n",
    "    \n",
    "    print(f\"\\nDESCRIPTIVE STATISTICS:\")\n",
    "    print(f\"Treatment Group (n={len(treatment_errors)}):\")\n",
    "    print(f\"  Mean error: {np.mean(treatment_errors):.2f}% (±{np.std(treatment_errors):.2f})\")\n",
    "    print(f\"  Mean success rate: {np.mean(treatment_success):.2%} (±{np.std(treatment_success):.2%})\")\n",
    "    \n",
    "    print(f\"\\nControl Group (n={len(control_errors)}):\")\n",
    "    print(f\"  Mean error: {np.mean(control_errors):.2f}% (±{np.std(control_errors):.2f})\")\n",
    "    print(f\"  Mean success rate: {np.mean(control_success):.2%} (±{np.std(control_success):.2%})\")\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    error_improvement = (np.mean(control_errors) - np.mean(treatment_errors)) / np.mean(control_errors) * 100\n",
    "    success_improvement = (np.mean(treatment_success) - np.mean(control_success)) / np.mean(control_success) * 100\n",
    "    \n",
    "    print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "    print(f\"  Error reduction: {error_improvement:+.1f}%\")\n",
    "    print(f\"  Success rate improvement: {success_improvement:+.1f}%\")\n",
    "    print(f\"  Meets >30% threshold: {'YES' if abs(error_improvement) > 30 or success_improvement > 30 else 'NO'}\")\n",
    "    \n",
    "    # Statistical tests\n",
    "    test_results = {}\n",
    "    \n",
    "    # 1. T-test for estimation errors (assuming normal distribution)\n",
    "    if len(treatment_errors) > 1 and len(control_errors) > 1:\n",
    "        t_stat, t_pvalue = ttest_ind(treatment_errors, control_errors)\n",
    "        \n",
    "        # Calculate confidence interval for difference\n",
    "        pooled_se = np.sqrt(np.var(treatment_errors)/len(treatment_errors) + \n",
    "                           np.var(control_errors)/len(control_errors))\n",
    "        mean_diff = np.mean(treatment_errors) - np.mean(control_errors)\n",
    "        margin_error = 1.96 * pooled_se  # 95% CI\n",
    "        ci_lower = mean_diff - margin_error\n",
    "        ci_upper = mean_diff + margin_error\n",
    "        \n",
    "        test_results['t_test'] = {\n",
    "            'statistic': t_stat,\n",
    "            'p_value': t_pvalue,\n",
    "            'mean_difference': mean_diff,\n",
    "            'ci_95': (ci_lower, ci_upper),\n",
    "            'significant': t_pvalue < 0.05\n",
    "        }\n",
    "    \n",
    "    # 2. Mann-Whitney U test (non-parametric alternative)\n",
    "    if len(treatment_errors) > 1 and len(control_errors) > 1:\n",
    "        u_stat, u_pvalue = mannwhitneyu(treatment_errors, control_errors, alternative='two-sided')\n",
    "        \n",
    "        test_results['mann_whitney'] = {\n",
    "            'statistic': u_stat,\n",
    "            'p_value': u_pvalue,\n",
    "            'significant': u_pvalue < 0.05\n",
    "        }\n",
    "    \n",
    "    # 3. Effect size (Cohen's d)\n",
    "    if len(treatment_errors) > 1 and len(control_errors) > 1:\n",
    "        pooled_std = np.sqrt(((len(treatment_errors)-1)*np.var(treatment_errors) + \n",
    "                             (len(control_errors)-1)*np.var(control_errors)) / \n",
    "                            (len(treatment_errors) + len(control_errors) - 2))\n",
    "        cohens_d = (np.mean(treatment_errors) - np.mean(control_errors)) / pooled_std\n",
    "        \n",
    "        test_results['effect_size'] = {\n",
    "            'cohens_d': cohens_d,\n",
    "            'interpretation': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'\n",
    "        }\n",
    "    \n",
    "    # Display statistical test results\n",
    "    print(f\"\\nSTATISTICAL TEST RESULTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 't_test' in test_results:\n",
    "        t_result = test_results['t_test']\n",
    "        print(f\"\\n1. Independent Samples T-Test:\")\n",
    "        print(f\"   H₀: No difference in estimation errors between groups\")\n",
    "        print(f\"   H₁: Significant difference exists\")\n",
    "        print(f\"   t-statistic: {t_result['statistic']:.3f}\")\n",
    "        print(f\"   p-value: {t_result['p_value']:.4f}\")\n",
    "        print(f\"   Mean difference: {t_result['mean_difference']:.2f}% error\")\n",
    "        print(f\"   95% CI: ({t_result['ci_95'][0]:.2f}, {t_result['ci_95'][1]:.2f})\")\n",
    "        print(f\"   Significant: {'YES' if t_result['significant'] else 'NO'} (α = 0.05)\")\n",
    "    \n",
    "    if 'mann_whitney' in test_results:\n",
    "        u_result = test_results['mann_whitney']\n",
    "        print(f\"\\n2. Mann-Whitney U Test (Non-parametric):\")\n",
    "        print(f\"   U-statistic: {u_result['statistic']:.3f}\")\n",
    "        print(f\"   p-value: {u_result['p_value']:.4f}\")\n",
    "        print(f\"   Significant: {'YES' if u_result['significant'] else 'NO'} (α = 0.05)\")\n",
    "    \n",
    "    if 'effect_size' in test_results:\n",
    "        effect = test_results['effect_size']\n",
    "        print(f\"\\n3. Effect Size Analysis:\")\n",
    "        print(f\"   Cohen's d: {effect['cohens_d']:.3f}\")\n",
    "        print(f\"   Interpretation: {effect['interpretation']} effect\")\n",
    "    \n",
    "    # Power analysis (post-hoc)\n",
    "    if 'effect_size' in test_results and len(treatment_errors) > 1:\n",
    "        from scipy.stats import norm\n",
    "        alpha = 0.05\n",
    "        n1, n2 = len(treatment_errors), len(control_errors)\n",
    "        effect_size = abs(test_results['effect_size']['cohens_d'])\n",
    "        \n",
    "        # Simplified power calculation\n",
    "        se = np.sqrt(1/n1 + 1/n2)\n",
    "        critical_t = norm.ppf(1 - alpha/2)\n",
    "        power = 1 - norm.cdf(critical_t - effect_size/se) + norm.cdf(-critical_t - effect_size/se)\n",
    "        \n",
    "        print(f\"\\n4. Statistical Power Analysis:\")\n",
    "        print(f\"   Observed power: {power:.3f}\")\n",
    "        print(f\"   Sample size (treatment): {n1}\")\n",
    "        print(f\"   Sample size (control): {n2}\")\n",
    "        print(f\"   Power interpretation: {'Adequate' if power > 0.8 else 'Inadequate'} (target: 0.8)\")\n",
    "    \n",
    "    # Conclusion for hypothesis\n",
    "    print(f\"\\nHYPOTHESIS TESTING CONCLUSION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    significant_improvement = (abs(error_improvement) > 30 or success_improvement > 30)\n",
    "    statistically_significant = (test_results.get('t_test', {}).get('significant', False) or \n",
    "                               test_results.get('mann_whitney', {}).get('significant', False))\n",
    "    \n",
    "    print(f\"\\n✓ Magnitude Test: {'PASS' if significant_improvement else 'FAIL'}\")\n",
    "    print(f\"  - Required: >30% improvement\")\n",
    "    print(f\"  - Observed: {max(abs(error_improvement), success_improvement):.1f}% improvement\")\n",
    "    \n",
    "    print(f\"\\n✓ Statistical Significance: {'PASS' if statistically_significant else 'FAIL'}\")\n",
    "    print(f\"  - Required: p < 0.05\")\n",
    "    print(f\"  - Observed: p = {test_results.get('t_test', {}).get('p_value', 'N/A')}\")\n",
    "    \n",
    "    hypothesis_supported = significant_improvement and statistically_significant\n",
    "    \n",
    "    print(f\"\\n🎯 FINAL VERDICT: {'HYPOTHESIS SUPPORTED' if hypothesis_supported else 'HYPOTHESIS NOT SUPPORTED'}\")\n",
    "    \n",
    "    if not hypothesis_supported:\n",
    "        print(f\"\\nRECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
    "        print(f\"  1. Increase sample size (current: {len(performance_df)} models)\")\n",
    "        print(f\"  2. Define clearer treatment/control groups\")\n",
    "        print(f\"  3. Collect direct time-to-market measurements\")\n",
    "        print(f\"  4. Implement randomized controlled trial design\")\n",
    "        print(f\"  5. Establish baseline measurements before intervention\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Execute statistical hypothesis testing\n",
    "if 'performance_df' in globals() and not performance_df.empty:\n",
    "    statistical_results = perform_hypothesis_testing()\n",
    "else:\n",
    "    print(\"Cannot perform hypothesis testing - performance data not available\")\n",
    "    statistical_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11A] - Advanced Permutation Testing for Hallucination-Reducing Techniques\n",
    "# Purpose: Implement robust permutation tests that don't rely on distributional assumptions\n",
    "# Dependencies: performance_df from Cell 5, STATISTICAL_CONFIG from Cell 0, scipy.stats permutation_test\n",
    "# Breadcrumbs: Setup -> Analysis -> Statistical Testing -> Advanced Permutation Testing\n",
    "\n",
    "def perform_advanced_permutation_testing():\n",
    "    \"\"\"\n",
    "    Perform comprehensive permutation testing for the >30% improvement hypothesis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Permutation test results with exact p-values and effect sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ADVANCED PERMUTATION TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve time-to-market by >30%\")\n",
    "    print(\"METHOD: Distribution-free permutation tests with exact p-values\")\n",
    "    \n",
    "    if performance_df.empty:\n",
    "        print(\"\\nWarning: Insufficient data for permutation testing\")\n",
    "        return {}\n",
    "    \n",
    "    # Define treatment vs control groups (same logic as Cell 11 but enhanced)\n",
    "    median_judge_score = performance_df['Avg_Judge_Score'].median()\n",
    "    treatment_models = performance_df[performance_df['Avg_Judge_Score'] > median_judge_score]\n",
    "    control_models = performance_df[performance_df['Avg_Judge_Score'] <= median_judge_score]\n",
    "    \n",
    "    print(f\"\\nGROUP DEFINITIONS:\")\n",
    "    print(f\"Treatment group (Judge Score > {median_judge_score:.2f}): {len(treatment_models)} models\")\n",
    "    print(f\"Control group (Judge Score ≤ {median_judge_score:.2f}): {len(control_models)} models\")\n",
    "    \n",
    "    if len(treatment_models) == 0 or len(control_models) == 0:\n",
    "        print(\"Warning: Cannot perform permutation tests - insufficient group sizes\")\n",
    "        return {}\n",
    "    \n",
    "    # Extract data arrays for testing\n",
    "    treatment_errors = treatment_models['Error_Pct'].values\n",
    "    control_errors = control_models['Error_Pct'].values\n",
    "    treatment_success = treatment_models['Success_Rate'].values\n",
    "    control_success = control_models['Success_Rate'].values\n",
    "    \n",
    "    permutation_results = {}\n",
    "    n_resamples = CONFIG.get('PERMUTATION_SAMPLES', 10000)\n",
    "    \n",
    "    print(f\"\\nPERMUTATION TEST CONFIGURATION:\")\n",
    "    print(f\"  Number of resamples: {n_resamples:,}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    print(f\"  Test type: Two-sided\")\n",
    "    \n",
    "    # 1. Permutation test for estimation error reduction\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"1. PERMUTATION TEST: ESTIMATION ERROR REDUCTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"H₀: No difference in estimation errors between groups\")\n",
    "    print(f\"H₁: Treatment group has different (lower) estimation errors\")\n",
    "    \n",
    "    def error_difference_statistic(treatment, control):\n",
    "        \"\"\"Test statistic: difference in means (control - treatment)\"\"\"\n",
    "        return np.mean(control) - np.mean(treatment)\n",
    "    \n",
    "    try:\n",
    "        # Perform permutation test for error reduction\n",
    "        error_perm_result = permutation_test(\n",
    "            (treatment_errors, control_errors),\n",
    "            error_difference_statistic,\n",
    "            n_resamples=n_resamples,\n",
    "            alternative='greater',  # We expect treatment to have lower errors\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        observed_error_reduction = error_difference_statistic(treatment_errors, control_errors)\n",
    "        error_improvement_pct = (observed_error_reduction / np.mean(control_errors)) * 100\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed error reduction: {observed_error_reduction:.2f} percentage points\")\n",
    "        print(f\"  Improvement percentage: {error_improvement_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {error_perm_result.pvalue:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if error_perm_result.pvalue < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if abs(error_improvement_pct) > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['error_reduction'] = {\n",
    "            'observed_statistic': observed_error_reduction,\n",
    "            'improvement_pct': error_improvement_pct,\n",
    "            'p_value': error_perm_result.pvalue,\n",
    "            'significant': error_perm_result.pvalue < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': abs(error_improvement_pct) > 30,\n",
    "            'null_distribution': error_perm_result.null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in error reduction permutation test: {e}\")\n",
    "        permutation_results['error_reduction'] = {}\n",
    "    \n",
    "    # 2. Permutation test for success rate improvement\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"2. PERMUTATION TEST: SUCCESS RATE IMPROVEMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"H₀: No difference in success rates between groups\")\n",
    "    print(f\"H₁: Treatment group has higher success rates\")\n",
    "    \n",
    "    def success_difference_statistic(treatment, control):\n",
    "        \"\"\"Test statistic: difference in means (treatment - control)\"\"\"\n",
    "        return np.mean(treatment) - np.mean(control)\n",
    "    \n",
    "    try:\n",
    "        # Perform permutation test for success rate improvement\n",
    "        success_perm_result = permutation_test(\n",
    "            (treatment_success, control_success),\n",
    "            success_difference_statistic,\n",
    "            n_resamples=n_resamples,\n",
    "            alternative='greater',  # We expect treatment to have higher success rates\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        observed_success_improvement = success_difference_statistic(treatment_success, control_success)\n",
    "        success_improvement_pct = (observed_success_improvement / np.mean(control_success)) * 100\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed success improvement: {observed_success_improvement:.4f}\")\n",
    "        print(f\"  Improvement percentage: {success_improvement_pct:.1f}%\")\n",
    "        print(f\"  Permutation p-value: {success_perm_result.pvalue:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if success_perm_result.pvalue < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if success_improvement_pct > 30 else 'NO'}\")\n",
    "        \n",
    "        permutation_results['success_improvement'] = {\n",
    "            'observed_statistic': observed_success_improvement,\n",
    "            'improvement_pct': success_improvement_pct,\n",
    "            'p_value': success_perm_result.pvalue,\n",
    "            'significant': success_perm_result.pvalue < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': success_improvement_pct > 30,\n",
    "            'null_distribution': success_perm_result.null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in success rate permutation test: {e}\")\n",
    "        permutation_results['success_improvement'] = {}\n",
    "    \n",
    "    # 3. Combined permutation test for overall improvement\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"3. COMBINED PERMUTATION TEST: OVERALL IMPROVEMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"H₀: No overall improvement in treatment group\")\n",
    "    print(f\"H₁: Treatment group shows overall improvement (error reduction OR success improvement)\")\n",
    "    \n",
    "    def combined_improvement_statistic(treatment_err, control_err, treatment_succ, control_succ):\n",
    "        \"\"\"Combined test statistic: max of standardized improvements\"\"\"\n",
    "        error_improvement = (np.mean(control_err) - np.mean(treatment_err)) / np.std(control_err)\n",
    "        success_improvement = (np.mean(treatment_succ) - np.mean(control_succ)) / np.std(control_succ)\n",
    "        return max(error_improvement, success_improvement)\n",
    "    \n",
    "    try:\n",
    "        # Custom permutation test for combined statistic\n",
    "        def combined_statistic_wrapper(combined_data):\n",
    "            \"\"\"Wrapper for combined data permutation\"\"\"\n",
    "            n_treatment = len(treatment_errors)\n",
    "            treatment_err_perm = combined_data[0][:n_treatment]\n",
    "            control_err_perm = combined_data[0][n_treatment:]\n",
    "            treatment_succ_perm = combined_data[1][:n_treatment]\n",
    "            control_succ_perm = combined_data[1][n_treatment:]\n",
    "            return combined_improvement_statistic(treatment_err_perm, control_err_perm, \n",
    "                                                treatment_succ_perm, control_succ_perm)\n",
    "        \n",
    "        # Combine data for permutation\n",
    "        combined_errors = np.concatenate([treatment_errors, control_errors])\n",
    "        combined_success = np.concatenate([treatment_success, control_success])\n",
    "        \n",
    "        combined_perm_result = permutation_test(\n",
    "            (combined_errors, combined_success),\n",
    "            combined_statistic_wrapper,\n",
    "            n_resamples=n_resamples,\n",
    "            alternative='greater',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        observed_combined = combined_improvement_statistic(treatment_errors, control_errors, \n",
    "                                                         treatment_success, control_success)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Combined improvement statistic: {observed_combined:.3f}\")\n",
    "        print(f\"  Permutation p-value: {combined_perm_result.pvalue:.6f}\")\n",
    "        print(f\"  Significant: {'YES' if combined_perm_result.pvalue < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        \n",
    "        permutation_results['combined_improvement'] = {\n",
    "            'observed_statistic': observed_combined,\n",
    "            'p_value': combined_perm_result.pvalue,\n",
    "            'significant': combined_perm_result.pvalue < CONFIG['ALPHA_LEVEL'],\n",
    "            'null_distribution': combined_perm_result.null_distribution\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in combined permutation test: {e}\")\n",
    "        permutation_results['combined_improvement'] = {}\n",
    "    \n",
    "    # 4. Permutation-based confidence intervals\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"4. PERMUTATION-BASED CONFIDENCE INTERVALS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Bootstrap-style permutation for confidence intervals\n",
    "        def permutation_ci(data1, data2, statistic_func, confidence_level=0.95, n_resamples=n_resamples):\n",
    "            \"\"\"Calculate permutation-based confidence interval\"\"\"\n",
    "            combined_data = np.concatenate([data1, data2])\n",
    "            n1 = len(data1)\n",
    "            \n",
    "            # Generate permutation statistics\n",
    "            perm_statistics = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_resamples):\n",
    "                perm_data = np.random.permutation(combined_data)\n",
    "                perm1 = perm_data[:n1]\n",
    "                perm2 = perm_data[n1:]\n",
    "                perm_stat = statistic_func(perm1, perm2)\n",
    "                perm_statistics.append(perm_stat)\n",
    "            \n",
    "            # Calculate confidence interval\n",
    "            alpha = 1 - confidence_level\n",
    "            lower_percentile = (alpha / 2) * 100\n",
    "            upper_percentile = (1 - alpha / 2) * 100\n",
    "            \n",
    "            ci_lower = np.percentile(perm_statistics, lower_percentile)\n",
    "            ci_upper = np.percentile(perm_statistics, upper_percentile)\n",
    "            \n",
    "            return ci_lower, ci_upper, np.array(perm_statistics)\n",
    "        \n",
    "        # CI for error reduction\n",
    "        error_ci_lower, error_ci_upper, error_perm_dist = permutation_ci(\n",
    "            treatment_errors, control_errors, error_difference_statistic\n",
    "        )\n",
    "        \n",
    "        # CI for success improvement\n",
    "        success_ci_lower, success_ci_upper, success_perm_dist = permutation_ci(\n",
    "            treatment_success, control_success, success_difference_statistic\n",
    "        )\n",
    "        \n",
    "        print(f\"95% Confidence Intervals (Permutation-based):\")\n",
    "        print(f\"  Error reduction: [{error_ci_lower:.2f}, {error_ci_upper:.2f}] percentage points\")\n",
    "        print(f\"  Success improvement: [{success_ci_lower:.4f}, {success_ci_upper:.4f}]\")\n",
    "        \n",
    "        # Check if CI excludes zero (indicates significance)\n",
    "        error_ci_significant = error_ci_lower > 0 or error_ci_upper < 0\n",
    "        success_ci_significant = success_ci_lower > 0 or success_ci_upper < 0\n",
    "        \n",
    "        print(f\"\\nConfidence Interval Significance:\")\n",
    "        print(f\"  Error reduction CI excludes 0: {'YES' if error_ci_significant else 'NO'}\")\n",
    "        print(f\"  Success improvement CI excludes 0: {'YES' if success_ci_significant else 'NO'}\")\n",
    "        \n",
    "        permutation_results['confidence_intervals'] = {\n",
    "            'error_reduction_ci': (error_ci_lower, error_ci_upper),\n",
    "            'success_improvement_ci': (success_ci_lower, success_ci_upper),\n",
    "            'error_ci_significant': error_ci_significant,\n",
    "            'success_ci_significant': success_ci_significant\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating permutation confidence intervals: {e}\")\n",
    "        permutation_results['confidence_intervals'] = {}\n",
    "    \n",
    "    # 5. Multiple comparisons adjustment\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"5. MULTIPLE COMPARISONS ADJUSTMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        \n",
    "        # Collect all p-values for adjustment\n",
    "        p_values = []\n",
    "        test_names = []\n",
    "        \n",
    "        if 'error_reduction' in permutation_results and permutation_results['error_reduction']:\n",
    "            p_values.append(permutation_results['error_reduction']['p_value'])\n",
    "            test_names.append('Error Reduction')\n",
    "        \n",
    "        if 'success_improvement' in permutation_results and permutation_results['success_improvement']:\n",
    "            p_values.append(permutation_results['success_improvement']['p_value'])\n",
    "            test_names.append('Success Improvement')\n",
    "        \n",
    "        if 'combined_improvement' in permutation_results and permutation_results['combined_improvement']:\n",
    "            p_values.append(permutation_results['combined_improvement']['p_value'])\n",
    "            test_names.append('Combined Improvement')\n",
    "        \n",
    "        if p_values:\n",
    "            # Apply Bonferroni correction\n",
    "            bonferroni_rejected, bonferroni_pvals, _, _ = multipletests(p_values, method='bonferroni')\n",
    "            \n",
    "            # Apply Benjamini-Hochberg (FDR) correction\n",
    "            fdr_rejected, fdr_pvals, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "            \n",
    "            print(f\"Multiple Comparisons Results:\")\n",
    "            print(f\"{'Test':<20} {'Raw p-value':<12} {'Bonferroni':<12} {'FDR (B-H)':<12}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, (test_name, raw_p) in enumerate(zip(test_names, p_values)):\n",
    "                print(f\"{test_name:<20} {raw_p:<12.6f} {bonferroni_pvals[i]:<12.6f} {fdr_pvals[i]:<12.6f}\")\n",
    "            \n",
    "            permutation_results['multiple_comparisons'] = {\n",
    "                'raw_p_values': p_values,\n",
    "                'test_names': test_names,\n",
    "                'bonferroni_p_values': bonferroni_pvals.tolist(),\n",
    "                'bonferroni_rejected': bonferroni_rejected.tolist(),\n",
    "                'fdr_p_values': fdr_pvals.tolist(),\n",
    "                'fdr_rejected': fdr_rejected.tolist()\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiple comparisons adjustment: {e}\")\n",
    "        permutation_results['multiple_comparisons'] = {}\n",
    "    \n",
    "    # Summary of permutation testing results\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"PERMUTATION TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    significant_tests = 0\n",
    "    total_tests = 0\n",
    "    meets_threshold_tests = 0\n",
    "    \n",
    "    for test_name, results in permutation_results.items():\n",
    "        if test_name == 'multiple_comparisons' or test_name == 'confidence_intervals':\n",
    "            continue\n",
    "        if results and 'significant' in results:\n",
    "            total_tests += 1\n",
    "            if results['significant']:\n",
    "                significant_tests += 1\n",
    "            if results.get('meets_threshold', False):\n",
    "                meets_threshold_tests += 1\n",
    "    \n",
    "    print(f\"\\nOverall Results:\")\n",
    "    print(f\"  Total permutation tests: {total_tests}\")\n",
    "    print(f\"  Statistically significant: {significant_tests}/{total_tests}\")\n",
    "    print(f\"  Meet >30% threshold: {meets_threshold_tests}/{total_tests}\")\n",
    "    \n",
    "    # Final verdict for permutation testing\n",
    "    any_significant = significant_tests > 0\n",
    "    any_threshold = meets_threshold_tests > 0\n",
    "    \n",
    "    print(f\"\\n🎯 PERMUTATION TESTING VERDICT:\")\n",
    "    print(f\"  Statistical Evidence: {'Found' if any_significant else 'Not Found'}\")\n",
    "    print(f\"  Practical Significance: {'Found' if any_threshold else 'Not Found'}\")\n",
    "    print(f\"  Overall Support: {'STRONG' if any_significant and any_threshold else 'MODERATE' if any_significant or any_threshold else 'WEAK'}\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF PERMUTATION TESTING:\")\n",
    "    print(f\"  ✓ No distributional assumptions required\")\n",
    "    print(f\"  ✓ Exact p-values (not asymptotic)\")\n",
    "    print(f\"  ✓ Robust to outliers\")\n",
    "    print(f\"  ✓ Suitable for small sample sizes\")\n",
    "    print(f\"  ✓ Multiple comparison corrections applied\")\n",
    "    \n",
    "    return permutation_results\n",
    "\n",
    "# Execute advanced permutation testing\n",
    "if 'performance_df' in globals() and not performance_df.empty and STATISTICAL_CONFIG.get('permutation_tests', False):\n",
    "    permutation_results = perform_advanced_permutation_testing()\n",
    "    print(f\"\\n✓ Advanced permutation testing completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform advanced permutation testing:\")\n",
    "    if 'performance_df' not in globals() or performance_df.empty:\n",
    "        print(\"  - Performance data not available\")\n",
    "    if not STATISTICAL_CONFIG.get('permutation_tests', False):\n",
    "        print(\"  - Permutation testing functionality not available\")\n",
    "        print(\"  - Install with: pip install 'praxis-requirements-analyzer[dev]'\")\n",
    "    \n",
    "    permutation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11B] - Expanded Bootstrap Hypothesis Testing\n",
    "# Purpose: Bootstrap-based hypothesis testing for >30% improvement with confidence intervals\n",
    "# Dependencies: performance_df, effort_impact_df, bootstrap functionality from Cell 0\n",
    "# Breadcrumbs: Setup -> Analysis -> Permutation Testing -> Bootstrap Hypothesis Testing\n",
    "\n",
    "def perform_bootstrap_hypothesis_testing():\n",
    "    \"\"\"\n",
    "    Perform comprehensive bootstrap hypothesis testing for the >30% improvement claim\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap test results with confidence intervals and bias corrections\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"EXPANDED BOOTSTRAP HYPOTHESIS TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve time-to-market by >30%\")\n",
    "    print(\"METHOD: Bootstrap resampling with bias-corrected confidence intervals\")\n",
    "    \n",
    "    if performance_df.empty:\n",
    "        print(\"\\nWarning: Insufficient data for bootstrap testing\")\n",
    "        return {}\n",
    "    \n",
    "    # Define treatment vs control groups (consistent with Cell 11A)\n",
    "    median_judge_score = performance_df['Avg_Judge_Score'].median()\n",
    "    treatment_models = performance_df[performance_df['Avg_Judge_Score'] > median_judge_score]\n",
    "    control_models = performance_df[performance_df['Avg_Judge_Score'] <= median_judge_score]\n",
    "    \n",
    "    print(f\"\\nGROUP DEFINITIONS:\")\n",
    "    print(f\"Treatment group (Judge Score > {median_judge_score:.2f}): {len(treatment_models)} models\")\n",
    "    print(f\"Control group (Judge Score ≤ {median_judge_score:.2f}): {len(control_models)} models\")\n",
    "    \n",
    "    if len(treatment_models) == 0 or len(control_models) == 0:\n",
    "        print(\"Warning: Cannot perform bootstrap tests - insufficient group sizes\")\n",
    "        return {}\n",
    "    \n",
    "    # Extract data arrays for testing\n",
    "    treatment_errors = treatment_models['Error_Pct'].values\n",
    "    control_errors = control_models['Error_Pct'].values\n",
    "    treatment_success = treatment_models['Success_Rate'].values\n",
    "    control_success = control_models['Success_Rate'].values\n",
    "    \n",
    "    bootstrap_results = {}\n",
    "    n_bootstrap = CONFIG.get('BOOTSTRAP_SAMPLES', 10000)\n",
    "    \n",
    "    print(f\"\\nBOOTSTRAP TEST CONFIGURATION:\")\n",
    "    print(f\"  Number of bootstrap samples: {n_bootstrap:,}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    print(f\"  Confidence level: 95%\")\n",
    "    print(f\"  Bias correction: BCa (Bias-Corrected and accelerated)\")\n",
    "    \n",
    "    # 1. Bootstrap test for error reduction\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"1. BOOTSTRAP TEST: ESTIMATION ERROR REDUCTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"H₀: No difference in estimation errors between groups\")\n",
    "    print(f\"H₁: Treatment group has lower estimation errors (>30% improvement)\")\n",
    "    \n",
    "    try:\n",
    "        def error_improvement_statistic(control_sample, treatment_sample):\n",
    "            \"\"\"Calculate improvement percentage: (control - treatment) / control * 100\"\"\"\n",
    "            control_mean = np.mean(control_sample)\n",
    "            treatment_mean = np.mean(treatment_sample)\n",
    "            if control_mean == 0:\n",
    "                return 0\n",
    "            return ((control_mean - treatment_mean) / control_mean) * 100\n",
    "        \n",
    "        # Bootstrap for error improvement\n",
    "        def bootstrap_error_improvement(n_samples=n_bootstrap):\n",
    "            improvements = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                # Resample with replacement from each group\n",
    "                control_sample = np.random.choice(control_errors, len(control_errors), replace=True)\n",
    "                treatment_sample = np.random.choice(treatment_errors, len(treatment_errors), replace=True)\n",
    "                \n",
    "                improvement = error_improvement_statistic(control_sample, treatment_sample)\n",
    "                improvements.append(improvement)\n",
    "            \n",
    "            return np.array(improvements)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        error_bootstrap_dist = bootstrap_error_improvement()\n",
    "        observed_error_improvement = error_improvement_statistic(control_errors, treatment_errors)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        error_ci_lower = np.percentile(error_bootstrap_dist, 2.5)\n",
    "        error_ci_upper = np.percentile(error_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis test: H₀: improvement ≤ 0\n",
    "        # Count how many bootstrap samples show improvement ≤ 0\n",
    "        null_violations = np.sum(error_bootstrap_dist <= 0)\n",
    "        error_bootstrap_p_value = null_violations / len(error_bootstrap_dist)\n",
    "        \n",
    "        # Test for >30% improvement specifically\n",
    "        improvement_30_violations = np.sum(error_bootstrap_dist < 30)\n",
    "        improvement_30_p_value = improvement_30_violations / len(error_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed error improvement: {observed_error_improvement:.1f}%\")\n",
    "        print(f\"  Bootstrap mean improvement: {np.mean(error_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std improvement: {np.std(error_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{error_ci_lower:.1f}%, {error_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(improvement ≤ 0): {error_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(improvement < 30%): {improvement_30_p_value:.4f}\")\n",
    "        print(f\"  Significant improvement: {'YES' if error_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_error_improvement > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - improvement_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['error_reduction'] = {\n",
    "            'observed_improvement': observed_error_improvement,\n",
    "            'bootstrap_mean': np.mean(error_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(error_bootstrap_dist),\n",
    "            'confidence_interval': (error_ci_lower, error_ci_upper),\n",
    "            'p_value_improvement': error_bootstrap_p_value,\n",
    "            'p_value_30_percent': improvement_30_p_value,\n",
    "            'significant': error_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_error_improvement > 30,\n",
    "            'threshold_confidence': 1 - improvement_30_p_value,\n",
    "            'bootstrap_distribution': error_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap error reduction test: {e}\")\n",
    "        bootstrap_results['error_reduction'] = {}\n",
    "    \n",
    "    # 2. Bootstrap test for success rate improvement\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"2. BOOTSTRAP TEST: SUCCESS RATE IMPROVEMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"H₀: No difference in success rates between groups\")\n",
    "    print(f\"H₁: Treatment group has higher success rates (>30% improvement)\")\n",
    "    \n",
    "    try:\n",
    "        def success_improvement_statistic(control_sample, treatment_sample):\n",
    "            \"\"\"Calculate improvement percentage: (treatment - control) / control * 100\"\"\"\n",
    "            control_mean = np.mean(control_sample)\n",
    "            treatment_mean = np.mean(treatment_sample)\n",
    "            if control_mean == 0:\n",
    "                return 0\n",
    "            return ((treatment_mean - control_mean) / control_mean) * 100\n",
    "        \n",
    "        # Bootstrap for success rate improvement\n",
    "        def bootstrap_success_improvement(n_samples=n_bootstrap):\n",
    "            improvements = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                # Resample with replacement from each group\n",
    "                control_sample = np.random.choice(control_success, len(control_success), replace=True)\n",
    "                treatment_sample = np.random.choice(treatment_success, len(treatment_success), replace=True)\n",
    "                \n",
    "                improvement = success_improvement_statistic(control_sample, treatment_sample)\n",
    "                improvements.append(improvement)\n",
    "            \n",
    "            return np.array(improvements)\n",
    "        \n",
    "        # Generate bootstrap distribution\n",
    "        success_bootstrap_dist = bootstrap_success_improvement()\n",
    "        observed_success_improvement = success_improvement_statistic(control_success, treatment_success)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        success_ci_lower = np.percentile(success_bootstrap_dist, 2.5)\n",
    "        success_ci_upper = np.percentile(success_bootstrap_dist, 97.5)\n",
    "        \n",
    "        # Bootstrap hypothesis test: H₀: improvement ≤ 0\n",
    "        null_violations = np.sum(success_bootstrap_dist <= 0)\n",
    "        success_bootstrap_p_value = null_violations / len(success_bootstrap_dist)\n",
    "        \n",
    "        # Test for >30% improvement specifically\n",
    "        improvement_30_violations = np.sum(success_bootstrap_dist < 30)\n",
    "        success_30_p_value = improvement_30_violations / len(success_bootstrap_dist)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Observed success improvement: {observed_success_improvement:.1f}%\")\n",
    "        print(f\"  Bootstrap mean improvement: {np.mean(success_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  Bootstrap std improvement: {np.std(success_bootstrap_dist):.1f}%\")\n",
    "        print(f\"  95% Bootstrap CI: [{success_ci_lower:.1f}%, {success_ci_upper:.1f}%]\")\n",
    "        print(f\"  P(improvement ≤ 0): {success_bootstrap_p_value:.4f}\")\n",
    "        print(f\"  P(improvement < 30%): {success_30_p_value:.4f}\")\n",
    "        print(f\"  Significant improvement: {'YES' if success_bootstrap_p_value < CONFIG['ALPHA_LEVEL'] else 'NO'}\")\n",
    "        print(f\"  Meets >30% threshold: {'YES' if observed_success_improvement > 30 else 'NO'}\")\n",
    "        print(f\"  30% threshold confidence: {1 - success_30_p_value:.3f}\")\n",
    "        \n",
    "        bootstrap_results['success_improvement'] = {\n",
    "            'observed_improvement': observed_success_improvement,\n",
    "            'bootstrap_mean': np.mean(success_bootstrap_dist),\n",
    "            'bootstrap_std': np.std(success_bootstrap_dist),\n",
    "            'confidence_interval': (success_ci_lower, success_ci_upper),\n",
    "            'p_value_improvement': success_bootstrap_p_value,\n",
    "            'p_value_30_percent': success_30_p_value,\n",
    "            'significant': success_bootstrap_p_value < CONFIG['ALPHA_LEVEL'],\n",
    "            'meets_threshold': observed_success_improvement > 30,\n",
    "            'threshold_confidence': 1 - success_30_p_value,\n",
    "            'bootstrap_distribution': success_bootstrap_dist\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap success rate test: {e}\")\n",
    "        bootstrap_results['success_improvement'] = {}\n",
    "    \n",
    "    # 3. Bias-Corrected and Accelerated (BCa) Bootstrap Intervals\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"3. BIAS-CORRECTED AND ACCELERATED (BCa) INTERVALS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        def calculate_bca_interval(data1, data2, statistic_func, alpha=0.05, n_bootstrap=n_bootstrap):\n",
    "            \"\"\"\n",
    "            Calculate BCa (Bias-Corrected and accelerated) bootstrap confidence interval\n",
    "            \"\"\"\n",
    "            # Original statistic\n",
    "            original_stat = statistic_func(data1, data2)\n",
    "            \n",
    "            # Bootstrap statistics\n",
    "            bootstrap_stats = []\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            for _ in range(n_bootstrap):\n",
    "                sample1 = np.random.choice(data1, len(data1), replace=True)\n",
    "                sample2 = np.random.choice(data2, len(data2), replace=True)\n",
    "                bootstrap_stats.append(statistic_func(sample1, sample2))\n",
    "            \n",
    "            bootstrap_stats = np.array(bootstrap_stats)\n",
    "            \n",
    "            # Bias correction\n",
    "            num_less = np.sum(bootstrap_stats < original_stat)\n",
    "            bias_correction = stats.norm.ppf(num_less / n_bootstrap)\n",
    "            \n",
    "            # Acceleration (jackknife)\n",
    "            n1, n2 = len(data1), len(data2)\n",
    "            jackknife_stats = []\n",
    "            \n",
    "            # Jackknife for data1\n",
    "            for i in range(n1):\n",
    "                jack_data1 = np.delete(data1, i)\n",
    "                jackknife_stats.append(statistic_func(jack_data1, data2))\n",
    "            \n",
    "            # Jackknife for data2\n",
    "            for i in range(n2):\n",
    "                jack_data2 = np.delete(data2, i)\n",
    "                jackknife_stats.append(statistic_func(data1, jack_data2))\n",
    "            \n",
    "            jackknife_stats = np.array(jackknife_stats)\n",
    "            jackknife_mean = np.mean(jackknife_stats)\n",
    "            \n",
    "            # Acceleration parameter\n",
    "            numerator = np.sum((jackknife_mean - jackknife_stats)**3)\n",
    "            denominator = 6 * (np.sum((jackknife_mean - jackknife_stats)**2))**1.5\n",
    "            acceleration = numerator / denominator if denominator != 0 else 0\n",
    "            \n",
    "            # BCa percentiles\n",
    "            z_alpha_2 = stats.norm.ppf(alpha/2)\n",
    "            z_1_alpha_2 = stats.norm.ppf(1 - alpha/2)\n",
    "            \n",
    "            alpha_1 = stats.norm.cdf(bias_correction + (bias_correction + z_alpha_2)/(1 - acceleration*(bias_correction + z_alpha_2)))\n",
    "            alpha_2 = stats.norm.cdf(bias_correction + (bias_correction + z_1_alpha_2)/(1 - acceleration*(bias_correction + z_1_alpha_2)))\n",
    "            \n",
    "            # Ensure percentiles are within valid range\n",
    "            alpha_1 = max(0, min(1, alpha_1))\n",
    "            alpha_2 = max(0, min(1, alpha_2))\n",
    "            \n",
    "            lower_percentile = alpha_1 * 100\n",
    "            upper_percentile = alpha_2 * 100\n",
    "            \n",
    "            bca_lower = np.percentile(bootstrap_stats, lower_percentile)\n",
    "            bca_upper = np.percentile(bootstrap_stats, upper_percentile)\n",
    "            \n",
    "            return bca_lower, bca_upper, bias_correction, acceleration\n",
    "        \n",
    "        # BCa intervals for error improvement\n",
    "        def error_improvement_for_bca(control_sample, treatment_sample):\n",
    "            return error_improvement_statistic(control_sample, treatment_sample)\n",
    "        \n",
    "        error_bca_lower, error_bca_upper, error_bias, error_accel = calculate_bca_interval(\n",
    "            control_errors, treatment_errors, error_improvement_for_bca\n",
    "        )\n",
    "        \n",
    "        # BCa intervals for success improvement\n",
    "        def success_improvement_for_bca(control_sample, treatment_sample):\n",
    "            return success_improvement_statistic(control_sample, treatment_sample)\n",
    "        \n",
    "        success_bca_lower, success_bca_upper, success_bias, success_accel = calculate_bca_interval(\n",
    "            control_success, treatment_success, success_improvement_for_bca\n",
    "        )\n",
    "        \n",
    "        print(f\"BCa Confidence Intervals (95%):\")\n",
    "        print(f\"  Error reduction:\")\n",
    "        print(f\"    Standard CI: [{bootstrap_results['error_reduction']['confidence_interval'][0]:.1f}%, {bootstrap_results['error_reduction']['confidence_interval'][1]:.1f}%]\")\n",
    "        print(f\"    BCa CI: [{error_bca_lower:.1f}%, {error_bca_upper:.1f}%]\")\n",
    "        print(f\"    Bias correction: {error_bias:.3f}\")\n",
    "        print(f\"    Acceleration: {error_accel:.3f}\")\n",
    "        \n",
    "        print(f\"  Success rate improvement:\")\n",
    "        print(f\"    Standard CI: [{bootstrap_results['success_improvement']['confidence_interval'][0]:.1f}%, {bootstrap_results['success_improvement']['confidence_interval'][1]:.1f}%]\")\n",
    "        print(f\"    BCa CI: [{success_bca_lower:.1f}%, {success_bca_upper:.1f}%]\")\n",
    "        print(f\"    Bias correction: {success_bias:.3f}\")\n",
    "        print(f\"    Acceleration: {success_accel:.3f}\")\n",
    "        \n",
    "        bootstrap_results['bca_intervals'] = {\n",
    "            'error_reduction_bca': (error_bca_lower, error_bca_upper),\n",
    "            'success_improvement_bca': (success_bca_lower, success_bca_upper),\n",
    "            'error_bias_correction': error_bias,\n",
    "            'error_acceleration': error_accel,\n",
    "            'success_bias_correction': success_bias,\n",
    "            'success_acceleration': success_accel\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BCa intervals: {e}\")\n",
    "        bootstrap_results['bca_intervals'] = {}\n",
    "    \n",
    "    # 4. Bootstrap Power Analysis\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"4. BOOTSTRAP POWER ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        def bootstrap_power_analysis(effect_sizes, sample_sizes, alpha=0.05):\n",
    "            \"\"\"\n",
    "            Estimate statistical power using bootstrap simulation\n",
    "            \"\"\"\n",
    "            power_results = {}\n",
    "            \n",
    "            for n in sample_sizes:\n",
    "                power_results[n] = {}\n",
    "                for effect in effect_sizes:\n",
    "                    # Simulate data with known effect size\n",
    "                    sim_control = np.random.normal(50, 15, n)  # Mean error ~50%\n",
    "                    sim_treatment = np.random.normal(50 * (1 - effect), 15, n)  # Reduced by effect\n",
    "                    \n",
    "                    # Bootstrap test\n",
    "                    bootstrap_improvements = []\n",
    "                    for _ in range(1000):  # Reduced for speed\n",
    "                        boot_control = np.random.choice(sim_control, n, replace=True)\n",
    "                        boot_treatment = np.random.choice(sim_treatment, n, replace=True)\n",
    "                        improvement = error_improvement_statistic(boot_control, boot_treatment)\n",
    "                        bootstrap_improvements.append(improvement)\n",
    "                    \n",
    "                    # Calculate power (proportion of significant results)\n",
    "                    null_violations = np.sum(np.array(bootstrap_improvements) <= 0)\n",
    "                    p_value = null_violations / len(bootstrap_improvements)\n",
    "                    power = 1 - (p_value >= alpha)\n",
    "                    \n",
    "                    power_results[n][effect] = power\n",
    "            \n",
    "            return power_results\n",
    "        \n",
    "        effect_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]  # 10%, 20%, 30%, 40%, 50% improvements\n",
    "        sample_sizes = [5, 10, 15, 20] if len(performance_df) <= 20 else [10, 20, 30, 50]\n",
    "        \n",
    "        power_analysis = bootstrap_power_analysis(effect_sizes, sample_sizes)\n",
    "        \n",
    "        print(f\"Bootstrap Power Analysis Results:\")\n",
    "        print(f\"{'Sample Size':<12} {'10%':<8} {'20%':<8} {'30%':<8} {'40%':<8} {'50%':<8}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for n in sample_sizes:\n",
    "            power_row = f\"{n:<12}\"\n",
    "            for effect in effect_sizes:\n",
    "                power_row += f\"{power_analysis[n][effect]:.2f}    \"\n",
    "            print(power_row)\n",
    "        \n",
    "        bootstrap_results['power_analysis'] = power_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap power analysis: {e}\")\n",
    "        bootstrap_results['power_analysis'] = {}\n",
    "    \n",
    "    # 5. Bootstrap Hypothesis Testing Summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"BOOTSTRAP HYPOTHESIS TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    significant_tests = 0\n",
    "    total_tests = 0\n",
    "    meets_threshold_tests = 0\n",
    "    high_confidence_tests = 0\n",
    "    \n",
    "    for test_name, results in bootstrap_results.items():\n",
    "        if test_name in ['bca_intervals', 'power_analysis']:\n",
    "            continue\n",
    "        if results and 'significant' in results:\n",
    "            total_tests += 1\n",
    "            if results['significant']:\n",
    "                significant_tests += 1\n",
    "            if results.get('meets_threshold', False):\n",
    "                meets_threshold_tests += 1\n",
    "            if results.get('threshold_confidence', 0) > 0.8:\n",
    "                high_confidence_tests += 1\n",
    "    \n",
    "    print(f\"\\nOverall Results:\")\n",
    "    print(f\"  Total bootstrap tests: {total_tests}\")\n",
    "    print(f\"  Statistically significant: {significant_tests}/{total_tests}\")\n",
    "    print(f\"  Meet >30% threshold: {meets_threshold_tests}/{total_tests}\")\n",
    "    print(f\"  High confidence (>80%) for 30% threshold: {high_confidence_tests}/{total_tests}\")\n",
    "    \n",
    "    # Final verdict for bootstrap testing\n",
    "    any_significant = significant_tests > 0\n",
    "    any_threshold = meets_threshold_tests > 0\n",
    "    high_confidence = high_confidence_tests > 0\n",
    "    \n",
    "    print(f\"\\n🎯 BOOTSTRAP TESTING VERDICT:\")\n",
    "    print(f\"  Statistical Evidence: {'Found' if any_significant else 'Not Found'}\")\n",
    "    print(f\"  Practical Significance: {'Found' if any_threshold else 'Not Found'}\")\n",
    "    print(f\"  High Confidence in 30% Threshold: {'YES' if high_confidence else 'NO'}\")\n",
    "    print(f\"  Overall Support: {'STRONG' if any_significant and any_threshold and high_confidence else 'MODERATE' if any_significant or any_threshold else 'WEAK'}\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF BOOTSTRAP TESTING:\")\n",
    "    print(f\"  ✓ Provides confidence intervals for complex statistics\")\n",
    "    print(f\"  ✓ Accounts for bias in small samples (BCa intervals)\")\n",
    "    print(f\"  ✓ Direct probability statements about thresholds\")\n",
    "    print(f\"  ✓ Robust to non-normal distributions\")\n",
    "    print(f\"  ✓ Intuitive interpretation of results\")\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "# Execute bootstrap hypothesis testing\n",
    "if 'performance_df' in globals() and not performance_df.empty and STATISTICAL_CONFIG.get('bootstrap_tests', False):\n",
    "    bootstrap_results = perform_bootstrap_hypothesis_testing()\n",
    "    print(f\"\\n✓ Bootstrap hypothesis testing completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform bootstrap hypothesis testing:\")\n",
    "    if 'performance_df' not in globals() or performance_df.empty:\n",
    "        print(\"  - Performance data not available\")\n",
    "    if not STATISTICAL_CONFIG.get('bootstrap_tests', False):\n",
    "        print(\"  - Bootstrap testing functionality not available\")\n",
    "        print(\"  - Install with: pip install 'praxis-requirements-analyzer[dev]'\")\n",
    "    \n",
    "    bootstrap_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11C] - Bayesian Analysis for >30% Improvement Hypothesis\n",
    "# Purpose: Bayesian hypothesis testing with posterior probability statements\n",
    "# Dependencies: performance_df, pymc, arviz from Cell 0, BAYESIAN_CONFIG\n",
    "# Breadcrumbs: Setup -> Analysis -> Bootstrap Testing -> Bayesian Analysis\n",
    "\n",
    "def perform_bayesian_analysis():\n",
    "    \"\"\"\n",
    "    Perform comprehensive Bayesian analysis for the >30% improvement hypothesis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian analysis results with posterior probabilities and credible intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"BAYESIAN ANALYSIS FOR >30% IMPROVEMENT HYPOTHESIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYPOTHESIS: Hallucination-reducing techniques improve time-to-market by >30%\")\n",
    "    print(\"METHOD: Bayesian inference with posterior probability statements\")\n",
    "    \n",
    "    if performance_df.empty or not STATISTICAL_CONFIG.get('bayesian_analysis', False):\n",
    "        print(\"\\nWarning: Insufficient data or Bayesian functionality not available\")\n",
    "        return {}\n",
    "    \n",
    "    # Define treatment vs control groups (consistent with Cells 11A and 11B)\n",
    "    median_judge_score = performance_df['Avg_Judge_Score'].median()\n",
    "    treatment_models = performance_df[performance_df['Avg_Judge_Score'] > median_judge_score]\n",
    "    control_models = performance_df[performance_df['Avg_Judge_Score'] <= median_judge_score]\n",
    "    \n",
    "    print(f\"\\nGROUP DEFINITIONS:\")\n",
    "    print(f\"Treatment group (Judge Score > {median_judge_score:.2f}): {len(treatment_models)} models\")\n",
    "    print(f\"Control group (Judge Score ≤ {median_judge_score:.2f}): {len(control_models)} models\")\n",
    "    \n",
    "    if len(treatment_models) == 0 or len(control_models) == 0:\n",
    "        print(\"Warning: Cannot perform Bayesian analysis - insufficient group sizes\")\n",
    "        return {}\n",
    "    \n",
    "    # Extract data arrays for analysis\n",
    "    treatment_errors = treatment_models['Error_Pct'].values\n",
    "    control_errors = control_models['Error_Pct'].values\n",
    "    treatment_success = treatment_models['Success_Rate'].values\n",
    "    control_success = control_models['Success_Rate'].values\n",
    "    \n",
    "    bayesian_results = {}\n",
    "    \n",
    "    print(f\"\\nBAYESIAN ANALYSIS CONFIGURATION:\")\n",
    "    print(f\"  MCMC samples: {CONFIG.get('BAYESIAN_SAMPLES', 2000):,}\")\n",
    "    print(f\"  MCMC chains: {CONFIG.get('BAYESIAN_CHAINS', 4)}\")\n",
    "    print(f\"  Improvement threshold: {CONFIG.get('IMPROVEMENT_THRESHOLD', 0.30):.0%}\")\n",
    "    print(f\"  Random seed: 42 (for reproducibility)\")\n",
    "    \n",
    "    # 1. Bayesian analysis for error reduction\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"1. BAYESIAN ANALYSIS: ESTIMATION ERROR REDUCTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Bayesian model for error reduction...\")\n",
    "        \n",
    "        with pm.Model() as error_model:\n",
    "            # Priors for group means (weakly informative)\n",
    "            mu_control = pm.Normal('mu_control', mu=50, sigma=20)  # Expected ~50% error\n",
    "            mu_treatment = pm.Normal('mu_treatment', mu=50, sigma=20)\n",
    "            \n",
    "            # Priors for group standard deviations\n",
    "            sigma_control = pm.HalfNormal('sigma_control', sigma=15)\n",
    "            sigma_treatment = pm.HalfNormal('sigma_treatment', sigma=15)\n",
    "            \n",
    "            # Likelihood for observed data\n",
    "            control_obs = pm.Normal('control_obs', mu=mu_control, sigma=sigma_control, \n",
    "                                  observed=control_errors)\n",
    "            treatment_obs = pm.Normal('treatment_obs', mu=mu_treatment, sigma=sigma_treatment, \n",
    "                                    observed=treatment_errors)\n",
    "            \n",
    "            # Derived quantities of interest\n",
    "            error_difference = pm.Deterministic('error_difference', mu_control - mu_treatment)\n",
    "            error_improvement_pct = pm.Deterministic('error_improvement_pct', \n",
    "                                                   100 * error_difference / mu_control)\n",
    "            \n",
    "            # Probability of >30% improvement\n",
    "            improvement_30_indicator = pm.Deterministic('improvement_30_plus', \n",
    "                                                      pm.math.switch(error_improvement_pct > 30, 1, 0))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace_error = pm.sample(\n",
    "                draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=42,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        error_improvement_samples = trace_error.posterior['error_improvement_pct'].values.flatten()\n",
    "        error_difference_samples = trace_error.posterior['error_difference'].values.flatten()\n",
    "        \n",
    "        # Calculate posterior statistics\n",
    "        error_posterior_mean = np.mean(error_improvement_samples)\n",
    "        error_posterior_std = np.std(error_improvement_samples)\n",
    "        error_credible_interval = np.percentile(error_improvement_samples, [2.5, 97.5])\n",
    "        \n",
    "        # Probability of >30% improvement\n",
    "        prob_30_plus = np.mean(error_improvement_samples > 30)\n",
    "        prob_any_improvement = np.mean(error_improvement_samples > 0)\n",
    "        \n",
    "        # ROPE analysis (Region of Practical Equivalence: -5% to +5%)\n",
    "        rope_lower, rope_upper = -5, 5\n",
    "        prob_in_rope = np.mean((error_improvement_samples >= rope_lower) & \n",
    "                              (error_improvement_samples <= rope_upper))\n",
    "        prob_above_rope = np.mean(error_improvement_samples > rope_upper)\n",
    "        prob_below_rope = np.mean(error_improvement_samples < rope_lower)\n",
    "        \n",
    "        print(f\"\\nBayesian Results for Error Reduction:\")\n",
    "        print(f\"  Posterior mean improvement: {error_posterior_mean:.1f}%\")\n",
    "        print(f\"  Posterior std improvement: {error_posterior_std:.1f}%\")\n",
    "        print(f\"  95% Credible interval: [{error_credible_interval[0]:.1f}%, {error_credible_interval[1]:.1f}%]\")\n",
    "        print(f\"  P(improvement > 0%): {prob_any_improvement:.3f}\")\n",
    "        print(f\"  P(improvement > 30%): {prob_30_plus:.3f}\")\n",
    "        print(f\"  P(improvement in ROPE [-5%, +5%]): {prob_in_rope:.3f}\")\n",
    "        print(f\"  P(improvement > ROPE): {prob_above_rope:.3f}\")\n",
    "        \n",
    "        bayesian_results['error_reduction'] = {\n",
    "            'posterior_mean': error_posterior_mean,\n",
    "            'posterior_std': error_posterior_std,\n",
    "            'credible_interval': error_credible_interval,\n",
    "            'prob_any_improvement': prob_any_improvement,\n",
    "            'prob_30_plus': prob_30_plus,\n",
    "            'prob_in_rope': prob_in_rope,\n",
    "            'prob_above_rope': prob_above_rope,\n",
    "            'trace': trace_error,\n",
    "            'posterior_samples': error_improvement_samples\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian error reduction analysis: {e}\")\n",
    "        bayesian_results['error_reduction'] = {}\n",
    "    \n",
    "    # 2. Bayesian analysis for success rate improvement\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"2. BAYESIAN ANALYSIS: SUCCESS RATE IMPROVEMENT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        print(\"Building Bayesian model for success rate improvement...\")\n",
    "        \n",
    "        with pm.Model() as success_model:\n",
    "            # Priors for group success rates (Beta distribution appropriate for rates)\n",
    "            # Using Beta(2, 2) as weakly informative prior centered at 0.5\n",
    "            p_control = pm.Beta('p_control', alpha=2, beta=2)\n",
    "            p_treatment = pm.Beta('p_treatment', alpha=2, beta=2)\n",
    "            \n",
    "            # Transform to account for observed success rates (assume bounded [0, 1])\n",
    "            success_control_scaled = control_success  # Already in [0, 1] range\n",
    "            success_treatment_scaled = treatment_success\n",
    "            \n",
    "            # Likelihood (using Normal approximation for success rates)\n",
    "            control_success_obs = pm.Normal('control_success_obs', \n",
    "                                          mu=p_control, \n",
    "                                          sigma=0.1,  # Assumed measurement error\n",
    "                                          observed=success_control_scaled)\n",
    "            treatment_success_obs = pm.Normal('treatment_success_obs', \n",
    "                                            mu=p_treatment, \n",
    "                                            sigma=0.1,\n",
    "                                            observed=success_treatment_scaled)\n",
    "            \n",
    "            # Derived quantities\n",
    "            success_difference = pm.Deterministic('success_difference', p_treatment - p_control)\n",
    "            success_improvement_pct = pm.Deterministic('success_improvement_pct', \n",
    "                                                     100 * success_difference / p_control)\n",
    "            \n",
    "            # Probability of >30% improvement\n",
    "            success_30_indicator = pm.Deterministic('success_30_plus', \n",
    "                                                  pm.math.switch(success_improvement_pct > 30, 1, 0))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace_success = pm.sample(\n",
    "                draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=42,\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        success_improvement_samples = trace_success.posterior['success_improvement_pct'].values.flatten()\n",
    "        success_difference_samples = trace_success.posterior['success_difference'].values.flatten()\n",
    "        \n",
    "        # Calculate posterior statistics\n",
    "        success_posterior_mean = np.mean(success_improvement_samples)\n",
    "        success_posterior_std = np.std(success_improvement_samples)\n",
    "        success_credible_interval = np.percentile(success_improvement_samples, [2.5, 97.5])\n",
    "        \n",
    "        # Probability calculations\n",
    "        prob_success_30_plus = np.mean(success_improvement_samples > 30)\n",
    "        prob_success_any_improvement = np.mean(success_improvement_samples > 0)\n",
    "        \n",
    "        # ROPE analysis for success rates\n",
    "        success_rope_lower, success_rope_upper = -5, 5\n",
    "        prob_success_in_rope = np.mean((success_improvement_samples >= success_rope_lower) & \n",
    "                                     (success_improvement_samples <= success_rope_upper))\n",
    "        prob_success_above_rope = np.mean(success_improvement_samples > success_rope_upper)\n",
    "        \n",
    "        print(f\"\\nBayesian Results for Success Rate Improvement:\")\n",
    "        print(f\"  Posterior mean improvement: {success_posterior_mean:.1f}%\")\n",
    "        print(f\"  Posterior std improvement: {success_posterior_std:.1f}%\")\n",
    "        print(f\"  95% Credible interval: [{success_credible_interval[0]:.1f}%, {success_credible_interval[1]:.1f}%]\")\n",
    "        print(f\"  P(improvement > 0%): {prob_success_any_improvement:.3f}\")\n",
    "        print(f\"  P(improvement > 30%): {prob_success_30_plus:.3f}\")\n",
    "        print(f\"  P(improvement in ROPE [-5%, +5%]): {prob_success_in_rope:.3f}\")\n",
    "        print(f\"  P(improvement > ROPE): {prob_success_above_rope:.3f}\")\n",
    "        \n",
    "        bayesian_results['success_improvement'] = {\n",
    "            'posterior_mean': success_posterior_mean,\n",
    "            'posterior_std': success_posterior_std,\n",
    "            'credible_interval': success_credible_interval,\n",
    "            'prob_any_improvement': prob_success_any_improvement,\n",
    "            'prob_30_plus': prob_success_30_plus,\n",
    "            'prob_in_rope': prob_success_in_rope,\n",
    "            'prob_above_rope': prob_success_above_rope,\n",
    "            'trace': trace_success,\n",
    "            'posterior_samples': success_improvement_samples\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian success rate analysis: {e}\")\n",
    "        bayesian_results['success_improvement'] = {}\n",
    "    \n",
    "    # 3. Bayesian Model Comparison using WAIC\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"3. BAYESIAN MODEL COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        print(\"Comparing models with and without treatment effect...\")\n",
    "        \n",
    "        # Model with treatment effect (already computed above)\n",
    "        if 'error_reduction' in bayesian_results and bayesian_results['error_reduction']:\n",
    "            trace_with_effect = bayesian_results['error_reduction']['trace']\n",
    "            \n",
    "            # Null model (no treatment effect)\n",
    "            with pm.Model() as null_model:\n",
    "                # Single population model\n",
    "                mu_pooled = pm.Normal('mu_pooled', mu=50, sigma=20)\n",
    "                sigma_pooled = pm.HalfNormal('sigma_pooled', sigma=15)\n",
    "                \n",
    "                # All observations from same distribution\n",
    "                all_errors = np.concatenate([control_errors, treatment_errors])\n",
    "                all_obs = pm.Normal('all_obs', mu=mu_pooled, sigma=sigma_pooled, \n",
    "                                  observed=all_errors)\n",
    "                \n",
    "                trace_null = pm.sample(\n",
    "                    draws=CONFIG.get('BAYESIAN_SAMPLES', 2000),\n",
    "                    chains=CONFIG.get('BAYESIAN_CHAINS', 4),\n",
    "                    tune=1000,\n",
    "                    random_seed=42,\n",
    "                    return_inferencedata=True\n",
    "                )\n",
    "            \n",
    "            # Calculate WAIC for model comparison\n",
    "            waic_with_effect = az.waic(trace_with_effect)\n",
    "            waic_null = az.waic(trace_null)\n",
    "            \n",
    "            # Bayes Factor approximation using WAIC\n",
    "            delta_waic = waic_with_effect.waic - waic_null.waic\n",
    "            \n",
    "            print(f\"Model Comparison Results:\")\n",
    "            print(f\"  Treatment effect model WAIC: {waic_with_effect.waic:.2f} ± {waic_with_effect.waic_se:.2f}\")\n",
    "            print(f\"  Null model WAIC: {waic_null.waic:.2f} ± {waic_null.waic_se:.2f}\")\n",
    "            print(f\"  ΔWAIC (effect - null): {delta_waic:.2f}\")\n",
    "            \n",
    "            if delta_waic < -2:\n",
    "                model_preference = \"Strong evidence for treatment effect\"\n",
    "            elif delta_waic < 0:\n",
    "                model_preference = \"Moderate evidence for treatment effect\"\n",
    "            elif delta_waic < 2:\n",
    "                model_preference = \"Weak evidence either way\"\n",
    "            else:\n",
    "                model_preference = \"Evidence against treatment effect\"\n",
    "            \n",
    "            print(f\"  Model preference: {model_preference}\")\n",
    "            \n",
    "            bayesian_results['model_comparison'] = {\n",
    "                'waic_with_effect': waic_with_effect.waic,\n",
    "                'waic_null': waic_null.waic,\n",
    "                'delta_waic': delta_waic,\n",
    "                'preference': model_preference\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian model comparison: {e}\")\n",
    "        bayesian_results['model_comparison'] = {}\n",
    "    \n",
    "    # 4. Posterior Predictive Checks\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"4. POSTERIOR PREDICTIVE CHECKS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        if 'error_reduction' in bayesian_results and bayesian_results['error_reduction']:\n",
    "            trace_error = bayesian_results['error_reduction']['trace']\n",
    "            \n",
    "            # Generate posterior predictive samples\n",
    "            with error_model:\n",
    "                ppc = pm.sample_posterior_predictive(trace_error, random_seed=42)\n",
    "            \n",
    "            # Compare observed vs predicted statistics\n",
    "            obs_control_mean = np.mean(control_errors)\n",
    "            obs_treatment_mean = np.mean(treatment_errors)\n",
    "            obs_difference = obs_control_mean - obs_treatment_mean\n",
    "            \n",
    "            pred_control_means = np.mean(ppc.posterior_predictive['control_obs'], axis=(0, 1, 2))\n",
    "            pred_treatment_means = np.mean(ppc.posterior_predictive['treatment_obs'], axis=(0, 1, 2))\n",
    "            pred_differences = pred_control_means - pred_treatment_means\n",
    "            \n",
    "            # Calculate Bayesian p-values\n",
    "            p_value_control = np.mean(np.abs(pred_control_means - obs_control_mean) >= \n",
    "                                    np.abs(pred_control_means - obs_control_mean))\n",
    "            p_value_treatment = np.mean(np.abs(pred_treatment_means - obs_treatment_mean) >= \n",
    "                                      np.abs(pred_treatment_means - obs_treatment_mean))\n",
    "            p_value_difference = np.mean(pred_differences >= obs_difference)\n",
    "            \n",
    "            print(f\"Posterior Predictive Check Results:\")\n",
    "            print(f\"  Observed control mean: {obs_control_mean:.2f}%\")\n",
    "            print(f\"  Predicted control mean: {np.mean(pred_control_means):.2f}% ± {np.std(pred_control_means):.2f}\")\n",
    "            print(f\"  Observed treatment mean: {obs_treatment_mean:.2f}%\")\n",
    "            print(f\"  Predicted treatment mean: {np.mean(pred_treatment_means):.2f}% ± {np.std(pred_treatment_means):.2f}\")\n",
    "            print(f\"  Bayesian p-value (difference): {p_value_difference:.3f}\")\n",
    "            \n",
    "            bayesian_results['posterior_predictive'] = {\n",
    "                'obs_control_mean': obs_control_mean,\n",
    "                'obs_treatment_mean': obs_treatment_mean,\n",
    "                'pred_control_means': pred_control_means,\n",
    "                'pred_treatment_means': pred_treatment_means,\n",
    "                'p_value_difference': p_value_difference\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in posterior predictive checks: {e}\")\n",
    "        bayesian_results['posterior_predictive'] = {}\n",
    "    \n",
    "    # 5. Bayesian Decision Analysis\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"5. BAYESIAN DECISION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        if ('error_reduction' in bayesian_results and bayesian_results['error_reduction'] and\n",
    "            'success_improvement' in bayesian_results and bayesian_results['success_improvement']):\n",
    "            \n",
    "            error_samples = bayesian_results['error_reduction']['posterior_samples']\n",
    "            success_samples = bayesian_results['success_improvement']['posterior_samples']\n",
    "            \n",
    "            # Combined probability of achieving >30% in either metric\n",
    "            prob_either_30 = np.mean((error_samples > 30) | (success_samples > 30))\n",
    "            \n",
    "            # Combined probability of achieving >30% in both metrics\n",
    "            prob_both_30 = np.mean((error_samples > 30) & (success_samples > 30))\n",
    "            \n",
    "            # Expected value of improvement\n",
    "            expected_error_improvement = np.mean(error_samples)\n",
    "            expected_success_improvement = np.mean(success_samples)\n",
    "            \n",
    "            # Risk assessment (probability of negative outcomes)\n",
    "            prob_error_worse = np.mean(error_samples < -10)  # >10% worse performance\n",
    "            prob_success_worse = np.mean(success_samples < -10)\n",
    "            \n",
    "            print(f\"Bayesian Decision Analysis:\")\n",
    "            print(f\"  P(>30% improvement in either metric): {prob_either_30:.3f}\")\n",
    "            print(f\"  P(>30% improvement in both metrics): {prob_both_30:.3f}\")\n",
    "            print(f\"  Expected error improvement: {expected_error_improvement:.1f}%\")\n",
    "            print(f\"  Expected success improvement: {expected_success_improvement:.1f}%\")\n",
    "            print(f\"  P(>10% worse error performance): {prob_error_worse:.3f}\")\n",
    "            print(f\"  P(>10% worse success performance): {prob_success_worse:.3f}\")\n",
    "            \n",
    "            # Decision recommendation\n",
    "            if prob_either_30 > 0.8:\n",
    "                decision = \"STRONG RECOMMENDATION: Adopt hallucination-reducing techniques\"\n",
    "            elif prob_either_30 > 0.6:\n",
    "                decision = \"MODERATE RECOMMENDATION: Consider adoption with monitoring\"\n",
    "            elif prob_either_30 > 0.4:\n",
    "                decision = \"WEAK RECOMMENDATION: Proceed with caution\"\n",
    "            else:\n",
    "                decision = \"NOT RECOMMENDED: Insufficient evidence of benefit\"\n",
    "            \n",
    "            print(f\"\\nDecision Recommendation: {decision}\")\n",
    "            \n",
    "            bayesian_results['decision_analysis'] = {\n",
    "                'prob_either_30': prob_either_30,\n",
    "                'prob_both_30': prob_both_30,\n",
    "                'expected_error_improvement': expected_error_improvement,\n",
    "                'expected_success_improvement': expected_success_improvement,\n",
    "                'prob_error_worse': prob_error_worse,\n",
    "                'prob_success_worse': prob_success_worse,\n",
    "                'recommendation': decision\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Bayesian decision analysis: {e}\")\n",
    "        bayesian_results['decision_analysis'] = {}\n",
    "    \n",
    "    # 6. Bayesian Analysis Summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"BAYESIAN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nKey Posterior Probabilities:\")\n",
    "    if 'error_reduction' in bayesian_results and bayesian_results['error_reduction']:\n",
    "        print(f\"  P(Error reduction > 30%): {bayesian_results['error_reduction']['prob_30_plus']:.3f}\")\n",
    "    if 'success_improvement' in bayesian_results and bayesian_results['success_improvement']:\n",
    "        print(f\"  P(Success improvement > 30%): {bayesian_results['success_improvement']['prob_30_plus']:.3f}\")\n",
    "    if 'decision_analysis' in bayesian_results and bayesian_results['decision_analysis']:\n",
    "        print(f\"  P(Either metric > 30%): {bayesian_results['decision_analysis']['prob_either_30']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nModel Evidence:\")\n",
    "    if 'model_comparison' in bayesian_results and bayesian_results['model_comparison']:\n",
    "        print(f\"  {bayesian_results['model_comparison']['preference']}\")\n",
    "        print(f\"  ΔWAIC: {bayesian_results['model_comparison']['delta_waic']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF BAYESIAN ANALYSIS:\")\n",
    "    print(f\"  ✓ Direct probability statements about hypotheses\")\n",
    "    print(f\"  ✓ Credible intervals with intuitive interpretation\")\n",
    "    print(f\"  ✓ Incorporates prior knowledge and uncertainty\")\n",
    "    print(f\"  ✓ Model comparison and selection capabilities\")\n",
    "    print(f\"  ✓ Decision-theoretic framework for recommendations\")\n",
    "    \n",
    "    return bayesian_results\n",
    "\n",
    "# Execute Bayesian analysis\n",
    "if ('performance_df' in globals() and not performance_df.empty and \n",
    "    STATISTICAL_CONFIG.get('bayesian_analysis', False)):\n",
    "    bayesian_results = perform_bayesian_analysis()\n",
    "    print(f\"\\n✓ Bayesian analysis completed successfully\")\n",
    "else:\n",
    "    print(\"Cannot perform Bayesian analysis:\")\n",
    "    if 'performance_df' not in globals() or performance_df.empty:\n",
    "        print(\"  - Performance data not available\")\n",
    "    if not STATISTICAL_CONFIG.get('bayesian_analysis', False):\n",
    "        print(\"  - Bayesian analysis functionality not available\")\n",
    "        print(\"  - Install with: pip install 'praxis-requirements-analyzer[dev]'\")\n",
    "    \n",
    "    bayesian_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Enhanced Statistical Validation Visualizations with Advanced Methods\n",
    "# Purpose: Create comprehensive visualizations for all statistical tests including permutation, bootstrap, and Bayesian results\n",
    "# Dependencies: statistical_results, permutation_results, bootstrap_results, bayesian_results from previous cells\n",
    "# Breadcrumbs: Setup -> Analysis -> Advanced Statistical Testing -> Enhanced Validation Visualizations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "import numpy as np\n",
    "\n",
    "def create_enhanced_statistical_visualizations():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations to support all statistical hypothesis testing methods\n",
    "    \"\"\"\n",
    "    \n",
    "    if performance_df.empty:\n",
    "        print(\"No data available for statistical visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Set up the enhanced figure with more subplots\n",
    "    fig = plt.figure(figsize=(24, 20), constrained_layout=True)  # Use constrained_layout instead\n",
    "    fig.suptitle('Enhanced Statistical Validation of Hallucination-Reducing Techniques\\n' +\n",
    "                 'Hypothesis: >30% Improvement in Time-to-Market\\n' +\n",
    "                 'Methods: Classical, Permutation, Bootstrap, and Bayesian Analysis', fontsize=18)\n",
    "    \n",
    "    # Create a more complex grid layout\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Define groups based on judge scores (using median split)\n",
    "    median_judge_score = performance_df['Avg_Judge_Score'].median()\n",
    "    treatment_group = performance_df[performance_df['Avg_Judge_Score'] > median_judge_score]\n",
    "    control_group = performance_df[performance_df['Avg_Judge_Score'] <= median_judge_score]\n",
    "    \n",
    "    # 1. Enhanced Box plot comparison with all test results\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    data_for_box = [control_group['Error_Pct'].values, treatment_group['Error_Pct'].values]\n",
    "    box_plot = ax1.boxplot(data_for_box, labels=['Control\\n(Low Judge Score)', 'Treatment\\n(High Judge Score)'], \n",
    "                          patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightcoral')\n",
    "    box_plot['boxes'][1].set_facecolor('lightgreen')\n",
    "    ax1.set_ylabel('Estimation Error (%)')\n",
    "    ax1.set_title('Error Rate Distribution by Group\\nwith Statistical Test Results')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add comprehensive significance indicators\n",
    "    test_results_text = \"\"\n",
    "    if 'statistical_results' in globals() and 't_test' in statistical_results:\n",
    "        p_val = statistical_results['t_test']['p_value']\n",
    "        test_results_text += f\"t-test: p={p_val:.4f}\\n\"\n",
    "    if 'permutation_results' in globals() and 'error_reduction' in permutation_results:\n",
    "        perm_p = permutation_results['error_reduction'].get('p_value', 'N/A')\n",
    "        test_results_text += f\"Permutation: p={perm_p:.4f}\\n\" if perm_p != 'N/A' else \"\"\n",
    "    if 'bootstrap_results' in globals() and 'error_reduction' in bootstrap_results:\n",
    "        boot_p = bootstrap_results['error_reduction'].get('p_value_improvement', 'N/A')\n",
    "        test_results_text += f\"Bootstrap: p={boot_p:.4f}\\n\" if boot_p != 'N/A' else \"\"\n",
    "    if 'bayesian_results' in globals() and 'error_reduction' in bayesian_results:\n",
    "        bayes_prob = bayesian_results['error_reduction'].get('prob_30_plus', 'N/A')\n",
    "        test_results_text += f\"Bayesian P(>30%): {bayes_prob:.3f}\" if bayes_prob != 'N/A' else \"\"\n",
    "    \n",
    "    ax1.text(0.02, 0.98, test_results_text, transform=ax1.transAxes, \n",
    "             verticalalignment='top', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Permutation Test Results\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    if 'permutation_results' in globals() and 'error_reduction' in permutation_results:\n",
    "        perm_data = permutation_results['error_reduction']\n",
    "        if 'null_distribution' in perm_data:\n",
    "            null_dist = perm_data['null_distribution']\n",
    "            observed_stat = perm_data['observed_statistic']\n",
    "            \n",
    "            ax2.hist(null_dist, bins=50, alpha=0.7, color='lightblue', density=True, \n",
    "                    label='Null Distribution')\n",
    "            ax2.axvline(observed_stat, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Observed: {observed_stat:.2f}')\n",
    "            ax2.set_xlabel('Error Difference (Control - Treatment)')\n",
    "            ax2.set_ylabel('Density')\n",
    "            ax2.set_title('Permutation Test: Error Reduction\\nNull Distribution vs Observed')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add p-value annotation\n",
    "            p_val = perm_data.get('p_value', 'N/A')\n",
    "            ax2.text(0.95, 0.95, f'p = {p_val:.4f}', transform=ax2.transAxes, \n",
    "                    ha='right', va='top', fontsize=12,\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow' if p_val < 0.05 else 'white'))\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Permutation Test\\nNot Available', ha='center', va='center', \n",
    "                transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('Permutation Test Results')\n",
    "    \n",
    "    # 3. Bootstrap Confidence Intervals\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    if 'bootstrap_results' in globals():\n",
    "        models = []\n",
    "        improvements = []\n",
    "        ci_lowers = []\n",
    "        ci_uppers = []\n",
    "        \n",
    "        if 'error_reduction' in bootstrap_results:\n",
    "            models.append('Error\\nReduction')\n",
    "            improvements.append(bootstrap_results['error_reduction'].get('observed_improvement', 0))\n",
    "            ci = bootstrap_results['error_reduction'].get('confidence_interval', (0, 0))\n",
    "            ci_lowers.append(ci[0])\n",
    "            ci_uppers.append(ci[1])\n",
    "        \n",
    "        if 'success_improvement' in bootstrap_results:\n",
    "            models.append('Success\\nImprovement')\n",
    "            improvements.append(bootstrap_results['success_improvement'].get('observed_improvement', 0))\n",
    "            ci = bootstrap_results['success_improvement'].get('confidence_interval', (0, 0))\n",
    "            ci_lowers.append(ci[0])\n",
    "            ci_uppers.append(ci[1])\n",
    "        \n",
    "        if models:\n",
    "            x_pos = np.arange(len(models))\n",
    "            bars = ax3.bar(x_pos, improvements, alpha=0.7, \n",
    "                          color=['green' if imp > 30 else 'orange' if imp > 0 else 'red' \n",
    "                                for imp in improvements])\n",
    "            \n",
    "            # Add error bars for confidence intervals\n",
    "            errors = [[imp - ci_low for imp, ci_low in zip(improvements, ci_lowers)],\n",
    "                     [ci_up - imp for imp, ci_up in zip(improvements, ci_uppers)]]\n",
    "            ax3.errorbar(x_pos, improvements, yerr=errors, fmt='none', \n",
    "                        color='black', capsize=5, capthick=2)\n",
    "            \n",
    "            ax3.axhline(y=30, color='red', linestyle='--', linewidth=2, label='30% Threshold')\n",
    "            ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "            ax3.set_xticks(x_pos)\n",
    "            ax3.set_xticklabels(models)\n",
    "            ax3.set_ylabel('Improvement (%)')\n",
    "            ax3.set_title('Bootstrap Confidence Intervals\\nfor Improvement Metrics')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Bootstrap Results\\nNot Available', ha='center', va='center',\n",
    "                    transform=ax3.transAxes, fontsize=14)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Bootstrap Results\\nNot Available', ha='center', va='center',\n",
    "                transform=ax3.transAxes, fontsize=14)\n",
    "        ax3.set_title('Bootstrap Confidence Intervals')\n",
    "    \n",
    "    # 4. Bayesian Posterior Distributions\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    if 'bayesian_results' in globals():\n",
    "        if 'error_reduction' in bayesian_results and 'posterior_samples' in bayesian_results['error_reduction']:\n",
    "            error_samples = bayesian_results['error_reduction']['posterior_samples']\n",
    "            ax4.hist(error_samples, bins=50, alpha=0.7, color='skyblue', density=True,\n",
    "                    label='Error Reduction')\n",
    "            \n",
    "            # Add credible interval\n",
    "            ci = bayesian_results['error_reduction'].get('credible_interval', [0, 0])\n",
    "            ax4.axvline(ci[0], color='blue', linestyle=':', alpha=0.7, label=f'95% CI')\n",
    "            ax4.axvline(ci[1], color='blue', linestyle=':', alpha=0.7)\n",
    "            \n",
    "        if 'success_improvement' in bayesian_results and 'posterior_samples' in bayesian_results['success_improvement']:\n",
    "            success_samples = bayesian_results['success_improvement']['posterior_samples']\n",
    "            ax4.hist(success_samples, bins=50, alpha=0.7, color='lightgreen', density=True,\n",
    "                    label='Success Improvement')\n",
    "        \n",
    "        ax4.axvline(30, color='red', linestyle='--', linewidth=2, label='30% Threshold')\n",
    "        ax4.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "        ax4.set_xlabel('Improvement (%)')\n",
    "        ax4.set_ylabel('Posterior Density')\n",
    "        ax4.set_title('Bayesian Posterior Distributions')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add probability annotations\n",
    "        if 'error_reduction' in bayesian_results:\n",
    "            prob_30 = bayesian_results['error_reduction'].get('prob_30_plus', 0)\n",
    "            ax4.text(0.02, 0.95, f'P(Error >30%): {prob_30:.3f}', \n",
    "                    transform=ax4.transAxes, va='top', fontsize=10,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Bayesian Results\\nNot Available', ha='center', va='center',\n",
    "                transform=ax4.transAxes, fontsize=14)\n",
    "        ax4.set_title('Bayesian Posterior Distributions')\n",
    "    \n",
    "    # 5. Model Performance Scatter with Advanced Statistics\n",
    "    ax5 = fig.add_subplot(gs[1, :2])\n",
    "    scatter = ax5.scatter(performance_df['Avg_Judge_Score'], performance_df['Error_Pct'], \n",
    "                         c=performance_df['Success_Rate'], cmap='RdYlGn', s=150, alpha=0.8)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(performance_df['Avg_Judge_Score'], performance_df['Error_Pct'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax5.plot(performance_df['Avg_Judge_Score'], p(performance_df['Avg_Judge_Score']), \n",
    "             \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Add median split line\n",
    "    ax5.axvline(median_judge_score, color='purple', linestyle=':', linewidth=2, \n",
    "               label=f'Median Split: {median_judge_score:.2f}')\n",
    "    \n",
    "    # Enhanced correlation and statistical info\n",
    "    corr_coef = performance_df['Avg_Judge_Score'].corr(performance_df['Error_Pct'])\n",
    "    stats_text = f'Correlation: r = {corr_coef:.3f}\\n'\n",
    "    \n",
    "    if 'statistical_results' in globals() and 'effect_size' in statistical_results:\n",
    "        cohens_d = statistical_results['effect_size'].get('cohens_d', 'N/A')\n",
    "        stats_text += f\"Cohen's d: {cohens_d:.3f}\\n\" if cohens_d != 'N/A' else \"\"\n",
    "    \n",
    "    if 'bayesian_results' in globals() and 'model_comparison' in bayesian_results:\n",
    "        delta_waic = bayesian_results['model_comparison'].get('delta_waic', 'N/A')\n",
    "        stats_text += f\"ΔWAIC: {delta_waic:.2f}\" if delta_waic != 'N/A' else \"\"\n",
    "    \n",
    "    ax5.text(0.02, 0.98, stats_text, transform=ax5.transAxes, \n",
    "             verticalalignment='top', fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax5.set_xlabel('Average Judge Score')\n",
    "    ax5.set_ylabel('Estimation Error (%)')\n",
    "    ax5.set_title('Model Performance: Judge Score vs Accuracy\\nwith Statistical Relationship Analysis')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # Create colorbar separately to avoid layout issues\n",
    "    cbar = fig.colorbar(scatter, ax=ax5, label='Success Rate', shrink=0.8)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Statistical Methods Comparison Table\n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create comprehensive comparison table\n",
    "    methods = ['Classical t-test', 'Permutation Test', 'Bootstrap Test', 'Bayesian Analysis']\n",
    "    metrics = ['p-value/Probability', 'Effect Size', 'Confidence/Credible Interval', 'Interpretation']\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    # Classical t-test\n",
    "    if 'statistical_results' in globals() and 't_test' in statistical_results:\n",
    "        t_result = statistical_results['t_test']\n",
    "        classical_row = [\n",
    "            f\"{t_result.get('p_value', 'N/A'):.4f}\" if t_result.get('p_value') != 'N/A' else 'N/A',\n",
    "            f\"{statistical_results.get('effect_size', {}).get('cohens_d', 'N/A'):.3f}\" if 'effect_size' in statistical_results else 'N/A',\n",
    "            f\"[{t_result.get('ci_95', [0,0])[0]:.2f}, {t_result.get('ci_95', [0,0])[1]:.2f}]\" if 'ci_95' in t_result else 'N/A',\n",
    "            'Significant' if t_result.get('significant', False) else 'Not Significant'\n",
    "        ]\n",
    "    else:\n",
    "        classical_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(classical_row)\n",
    "    \n",
    "    # Permutation test\n",
    "    if 'permutation_results' in globals() and 'error_reduction' in permutation_results:\n",
    "        perm_result = permutation_results['error_reduction']\n",
    "        perm_row = [\n",
    "            f\"{perm_result.get('p_value', 'N/A'):.4f}\" if perm_result.get('p_value') != 'N/A' else 'N/A',\n",
    "            f\"{perm_result.get('improvement_pct', 'N/A'):.1f}%\" if perm_result.get('improvement_pct') != 'N/A' else 'N/A',\n",
    "            'Exact CI Available' if 'confidence_intervals' in permutation_results else 'N/A',\n",
    "            'Significant' if perm_result.get('significant', False) else 'Not Significant'\n",
    "        ]\n",
    "    else:\n",
    "        perm_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(perm_row)\n",
    "    \n",
    "    # Bootstrap test\n",
    "    if 'bootstrap_results' in globals() and 'error_reduction' in bootstrap_results:\n",
    "        boot_result = bootstrap_results['error_reduction']\n",
    "        boot_row = [\n",
    "            f\"{boot_result.get('p_value_30_percent', 'N/A'):.4f}\" if boot_result.get('p_value_30_percent') != 'N/A' else 'N/A',\n",
    "            f\"{boot_result.get('observed_improvement', 'N/A'):.1f}%\" if boot_result.get('observed_improvement') != 'N/A' else 'N/A',\n",
    "            f\"[{boot_result.get('confidence_interval', [0,0])[0]:.1f}, {boot_result.get('confidence_interval', [0,0])[1]:.1f}]\" if 'confidence_interval' in boot_result else 'N/A',\n",
    "            'Meets Threshold' if boot_result.get('meets_threshold', False) else 'Below Threshold'\n",
    "        ]\n",
    "    else:\n",
    "        boot_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(boot_row)\n",
    "    \n",
    "    # Bayesian analysis\n",
    "    if 'bayesian_results' in globals() and 'error_reduction' in bayesian_results:\n",
    "        bayes_result = bayesian_results['error_reduction']\n",
    "        bayes_row = [\n",
    "            f\"{bayes_result.get('prob_30_plus', 'N/A'):.3f}\" if bayes_result.get('prob_30_plus') != 'N/A' else 'N/A',\n",
    "            f\"{bayes_result.get('posterior_mean', 'N/A'):.1f}%\" if bayes_result.get('posterior_mean') != 'N/A' else 'N/A',\n",
    "            f\"[{bayes_result.get('credible_interval', [0,0])[0]:.1f}, {bayes_result.get('credible_interval', [0,0])[1]:.1f}]\" if 'credible_interval' in bayes_result else 'N/A',\n",
    "            'Strong Evidence' if bayes_result.get('prob_30_plus', 0) > 0.8 else 'Moderate Evidence' if bayes_result.get('prob_30_plus', 0) > 0.6 else 'Weak Evidence'\n",
    "        ]\n",
    "    else:\n",
    "        bayes_row = ['N/A', 'N/A', 'N/A', 'N/A']\n",
    "    table_data.append(bayes_row)\n",
    "    \n",
    "    table = ax6.table(cellText=table_data,\n",
    "                     rowLabels=methods,\n",
    "                     colLabels=metrics,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Color code the table based on results\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(metrics)):\n",
    "            cell = table[(i+1, j)]\n",
    "            if 'Significant' in table_data[i][j] or 'Strong Evidence' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightgreen')\n",
    "            elif 'Not Significant' in table_data[i][j] or 'Weak Evidence' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightcoral')\n",
    "            elif 'Moderate' in table_data[i][j]:\n",
    "                cell.set_facecolor('lightyellow')\n",
    "    \n",
    "    ax6.set_title('Statistical Methods Comparison Summary', pad=20, fontsize=14)\n",
    "    \n",
    "    # 7-10. Bootstrap and Bayesian Distribution Details (if available)\n",
    "    methods_available = 0\n",
    "    \n",
    "    # Bootstrap distributions\n",
    "    if 'bootstrap_results' in globals():\n",
    "        for idx, (test_name, label) in enumerate([('error_reduction', 'Error Reduction'), \n",
    "                                                 ('success_improvement', 'Success Improvement')]):\n",
    "            if test_name in bootstrap_results and 'bootstrap_distribution' in bootstrap_results[test_name]:\n",
    "                ax = fig.add_subplot(gs[2, methods_available])\n",
    "                \n",
    "                boot_dist = bootstrap_results[test_name]['bootstrap_distribution']\n",
    "                observed = bootstrap_results[test_name]['observed_improvement']\n",
    "                ci = bootstrap_results[test_name]['confidence_interval']\n",
    "                \n",
    "                ax.hist(boot_dist, bins=40, alpha=0.7, color='steelblue', density=True)\n",
    "                ax.axvline(observed, color='red', linestyle='-', linewidth=2, \n",
    "                          label=f'Observed: {observed:.1f}%')\n",
    "                ax.axvline(ci[0], color='orange', linestyle='--', alpha=0.7)\n",
    "                ax.axvline(ci[1], color='orange', linestyle='--', alpha=0.7, \n",
    "                          label=f'95% CI: [{ci[0]:.1f}, {ci[1]:.1f}]')\n",
    "                ax.axvline(30, color='green', linestyle=':', linewidth=2, label='30% Threshold')\n",
    "                \n",
    "                ax.set_xlabel('Improvement (%)')\n",
    "                ax.set_ylabel('Bootstrap Density')\n",
    "                ax.set_title(f'Bootstrap Distribution\\n{label}')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                methods_available += 1\n",
    "                if methods_available >= 2:\n",
    "                    break\n",
    "    \n",
    "    # Bayesian posterior distributions (detailed)\n",
    "    if 'bayesian_results' in globals() and methods_available < 4:\n",
    "        for idx, (test_name, label) in enumerate([('error_reduction', 'Error Reduction'), \n",
    "                                                 ('success_improvement', 'Success Improvement')]):\n",
    "            if (test_name in bayesian_results and 'posterior_samples' in bayesian_results[test_name] \n",
    "                and methods_available < 4):\n",
    "                ax = fig.add_subplot(gs[2, methods_available])\n",
    "                \n",
    "                samples = bayesian_results[test_name]['posterior_samples']\n",
    "                ci = bayesian_results[test_name]['credible_interval']\n",
    "                prob_30 = bayesian_results[test_name]['prob_30_plus']\n",
    "                \n",
    "                ax.hist(samples, bins=40, alpha=0.7, color='lightcoral', density=True)\n",
    "                ax.axvline(ci[0], color='blue', linestyle='--', alpha=0.7)\n",
    "                ax.axvline(ci[1], color='blue', linestyle='--', alpha=0.7, \n",
    "                          label=f'95% Credible: [{ci[0]:.1f}, {ci[1]:.1f}]')\n",
    "                ax.axvline(30, color='green', linestyle=':', linewidth=2, label='30% Threshold')\n",
    "                \n",
    "                # Shade area above 30%\n",
    "                x_range = np.linspace(samples.min(), samples.max(), 1000)\n",
    "                density = stats.gaussian_kde(samples)(x_range)\n",
    "                ax.fill_between(x_range, 0, density, where=(x_range >= 30), \n",
    "                               alpha=0.3, color='green', label=f'P(>30%) = {prob_30:.3f}')\n",
    "                \n",
    "                ax.set_xlabel('Improvement (%)')\n",
    "                ax.set_ylabel('Posterior Density')\n",
    "                ax.set_title(f'Bayesian Posterior\\n{label}')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                methods_available += 1\n",
    "                if methods_available >= 4:\n",
    "                    break\n",
    "    \n",
    "    # 11. Overall Statistical Evidence Summary\n",
    "    ax11 = fig.add_subplot(gs[3, :])\n",
    "    ax11.axis('off')\n",
    "    \n",
    "    # Create evidence summary\n",
    "    evidence_summary = \"COMPREHENSIVE STATISTICAL EVIDENCE SUMMARY\\n\" + \"=\"*80 + \"\\n\\n\"\n",
    "    \n",
    "    # Classical statistics\n",
    "    if 'statistical_results' in globals() and 't_test' in statistical_results:\n",
    "        t_sig = statistical_results['t_test'].get('significant', False)\n",
    "        evidence_summary += f\"📊 CLASSICAL STATISTICS: {'SIGNIFICANT' if t_sig else 'NOT SIGNIFICANT'}\\n\"\n",
    "        evidence_summary += f\"   t-test p-value: {statistical_results['t_test'].get('p_value', 'N/A'):.4f}\\n\"\n",
    "        if 'effect_size' in statistical_results:\n",
    "            effect_size = statistical_results['effect_size'].get('cohens_d', 'N/A')\n",
    "            evidence_summary += f\"   Effect size (Cohen's d): {effect_size:.3f} ({statistical_results['effect_size'].get('interpretation', 'N/A')} effect)\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Permutation tests\n",
    "    if 'permutation_results' in globals():\n",
    "        perm_sig_count = sum(1 for test in ['error_reduction', 'success_improvement', 'combined_improvement'] \n",
    "                           if test in permutation_results and permutation_results[test].get('significant', False))\n",
    "        perm_total = len([test for test in ['error_reduction', 'success_improvement', 'combined_improvement'] \n",
    "                         if test in permutation_results])\n",
    "        evidence_summary += f\"🔄 PERMUTATION TESTS: {perm_sig_count}/{perm_total} SIGNIFICANT\\n\"\n",
    "        if 'error_reduction' in permutation_results:\n",
    "            evidence_summary += f\"   Error reduction p-value: {permutation_results['error_reduction'].get('p_value', 'N/A'):.4f}\\n\"\n",
    "        if 'success_improvement' in permutation_results:\n",
    "            evidence_summary += f\"   Success improvement p-value: {permutation_results['success_improvement'].get('p_value', 'N/A'):.4f}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Bootstrap results\n",
    "    if 'bootstrap_results' in globals():\n",
    "        boot_threshold_count = sum(1 for test in ['error_reduction', 'success_improvement'] \n",
    "                                 if test in bootstrap_results and bootstrap_results[test].get('meets_threshold', False))\n",
    "        boot_total = len([test for test in ['error_reduction', 'success_improvement'] \n",
    "                         if test in bootstrap_results])\n",
    "        evidence_summary += f\"🔀 BOOTSTRAP ANALYSIS: {boot_threshold_count}/{boot_total} MEET >30% THRESHOLD\\n\"\n",
    "        if 'error_reduction' in bootstrap_results:\n",
    "            conf = bootstrap_results['error_reduction'].get('threshold_confidence', 0)\n",
    "            evidence_summary += f\"   Error reduction confidence for 30%: {conf:.3f}\\n\"\n",
    "        if 'success_improvement' in bootstrap_results:\n",
    "            conf = bootstrap_results['success_improvement'].get('threshold_confidence', 0)\n",
    "            evidence_summary += f\"   Success improvement confidence for 30%: {conf:.3f}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\"\n",
    "    \n",
    "    # Bayesian results\n",
    "    if 'bayesian_results' in globals():\n",
    "        evidence_summary += f\"🎯 BAYESIAN ANALYSIS: POSTERIOR PROBABILITIES\\n\"\n",
    "        if 'error_reduction' in bayesian_results:\n",
    "            prob = bayesian_results['error_reduction'].get('prob_30_plus', 0)\n",
    "            evidence_summary += f\"   P(Error reduction > 30%): {prob:.3f}\\n\"\n",
    "        if 'success_improvement' in bayesian_results:\n",
    "            prob = bayesian_results['success_improvement'].get('prob_30_plus', 0)\n",
    "            evidence_summary += f\"   P(Success improvement > 30%): {prob:.3f}\\n\"\n",
    "        if 'decision_analysis' in bayesian_results:\n",
    "            prob_either = bayesian_results['decision_analysis'].get('prob_either_30', 0)\n",
    "            evidence_summary += f\"   P(Either metric > 30%): {prob_either:.3f}\\n\"\n",
    "            recommendation = bayesian_results['decision_analysis'].get('recommendation', 'N/A')\n",
    "            evidence_summary += f\"   Decision: {recommendation}\\n\"\n",
    "    \n",
    "    evidence_summary += \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "    evidence_summary += \"🎯 OVERALL CONCLUSION: \"\n",
    "    \n",
    "    # Determine overall conclusion\n",
    "    significant_methods = 0\n",
    "    total_methods = 0\n",
    "    \n",
    "    if 'statistical_results' in globals() and 't_test' in statistical_results:\n",
    "        total_methods += 1\n",
    "        if statistical_results['t_test'].get('significant', False):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'permutation_results' in globals() and 'error_reduction' in permutation_results:\n",
    "        total_methods += 1\n",
    "        if permutation_results['error_reduction'].get('significant', False):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'bootstrap_results' in globals() and 'error_reduction' in bootstrap_results:\n",
    "        total_methods += 1\n",
    "        if bootstrap_results['error_reduction'].get('meets_threshold', False):\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if 'bayesian_results' in globals() and 'error_reduction' in bayesian_results:\n",
    "        total_methods += 1\n",
    "        if bayesian_results['error_reduction'].get('prob_30_plus', 0) > 0.8:\n",
    "            significant_methods += 1\n",
    "    \n",
    "    if significant_methods >= total_methods * 0.75:\n",
    "        conclusion = \"STRONG EVIDENCE for >30% improvement hypothesis\"\n",
    "    elif significant_methods >= total_methods * 0.5:\n",
    "        conclusion = \"MODERATE EVIDENCE for >30% improvement hypothesis\"\n",
    "    else:\n",
    "        conclusion = \"WEAK EVIDENCE for >30% improvement hypothesis\"\n",
    "    \n",
    "    evidence_summary += conclusion\n",
    "    \n",
    "    ax11.text(0.05, 0.95, evidence_summary, transform=ax11.transAxes, \n",
    "              fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Use try/except for layout to handle potential colorbar conflicts\n",
    "    try:\n",
    "        # Don't use tight_layout, rely on constrained_layout set at figure creation\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Layout adjustment warning: {e}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Enhanced statistical visualizations completed successfully\")\n",
    "    print(f\"✓ Integrated results from {total_methods} statistical methods\")\n",
    "    print(f\"✓ {significant_methods}/{total_methods} methods show positive evidence\")\n",
    "\n",
    "# Execute enhanced statistical visualizations\n",
    "if 'performance_df' in globals() and not performance_df.empty:\n",
    "    create_enhanced_statistical_visualizations()\n",
    "else:\n",
    "    print(\"Cannot create enhanced statistical visualizations - performance data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - Enhanced Final Executive Summary with Advanced Statistical Analysis\n",
    "# Purpose: Generate comprehensive executive summary integrating all statistical methods and business insights\n",
    "# Dependencies: All previous analysis results, statistical_results, permutation_results, bootstrap_results, bayesian_results\n",
    "# Breadcrumbs: Setup -> Analysis -> Advanced Statistical Testing -> Enhanced Final Executive Summary & Business Impact Report\n",
    "\n",
    "print(\"ENHANCED EXECUTIVE SUMMARY - COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get project name safely\n",
    "project_name = CONFIG.get('NEO4J_PROJECT_NAME', 'Unknown Project') if 'CONFIG' in globals() else 'Unknown Project'\n",
    "print(f\"Project: {project_name}\")\n",
    "print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Analysis Framework: Classical, Permutation, Bootstrap, and Bayesian Methods\")\n",
    "\n",
    "if 'performance_df' in globals() and not performance_df.empty:\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'code_metrics_df' in globals():\n",
    "        print(f\"  Total code files in project: {len(code_metrics_df)}\")\n",
    "        print(f\"  Total lines of code in project: {code_metrics_df['CountLineCode'].sum():,}\")\n",
    "    else:\n",
    "        print(\"  Code metrics: Not available\")\n",
    "    \n",
    "    if 'ground_truth_requirements' in globals():\n",
    "        print(f\"  Ground truth requirements: {len(ground_truth_requirements)}\")\n",
    "    else:\n",
    "        print(\"  Ground truth requirements: Not available\")\n",
    "    \n",
    "    if 'llm_estimates_df' in globals() and not llm_estimates_df.empty:\n",
    "        estimated_requirements = llm_estimates_df['requirement_id'].unique()\n",
    "        if 'ground_truth_requirements' in globals():\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)} ({len(estimated_requirements)/len(ground_truth_requirements):.1%})\")\n",
    "        else:\n",
    "            print(f\"  Requirements with estimates: {len(estimated_requirements)}\")\n",
    "    \n",
    "    # Get baseline values safely\n",
    "    conversion_factor = CONFIG.get('CONVERSION_FACTOR', 0.957) if 'CONFIG' in globals() else 0.957\n",
    "    industry_loc_per_sifp = industry_metrics.get('LOC_PER_SIFP', 100) if 'industry_metrics' in globals() else 100\n",
    "    project_loc_per_sifp = baseline_metrics.get('project_loc_per_sifp', industry_loc_per_sifp) if 'baseline_metrics' in globals() else industry_loc_per_sifp\n",
    "    desharnais_hours_per_sifp = effort_metrics.get('avg_hours_per_sifp', 10) if 'effort_metrics' in globals() else 10\n",
    "    \n",
    "    print(f\"\\nConversion Factors and Baselines:\")\n",
    "    print(f\"  UFP → SiFP: {conversion_factor} (from Desharnais research)\")\n",
    "    print(f\"  SiFP → Effort: {desharnais_hours_per_sifp:.2f} hours/SiFP (Desharnais dataset)\")\n",
    "    print(f\"  SiFP → LOC: {project_loc_per_sifp:.1f} LOC/SiFP (project weighted average)\")\n",
    "    print(f\"  Industry baseline: {industry_loc_per_sifp:.1f} LOC/SiFP\")\n",
    "    print(f\"  Project vs Industry: {(project_loc_per_sifp - industry_loc_per_sifp)/industry_loc_per_sifp*100:+.1f}%\")\n",
    "    \n",
    "    # COMPREHENSIVE STATISTICAL HYPOTHESIS TESTING SUMMARY\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE STATISTICAL HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nHypothesis: Hallucination-reducing techniques improve time-to-market by >30%\")\n",
    "    print(f\"Operational Definition: Multi-stage refinement (actor→judge→meta-judge)\")\n",
    "    print(f\"Statistical Significance Threshold: α = 0.05\")\n",
    "    print(f\"Practical Significance Threshold: >30% improvement\")\n",
    "    \n",
    "    # Collect results from all statistical methods\n",
    "    statistical_evidence = {\n",
    "        'classical': {},\n",
    "        'permutation': {},\n",
    "        'bootstrap': {},\n",
    "        'bayesian': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Classical Statistics\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"1. CLASSICAL STATISTICAL ANALYSIS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if 'statistical_results' in globals() and statistical_results:\n",
    "        if 't_test' in statistical_results:\n",
    "            t_result = statistical_results['t_test']\n",
    "            print(f\"  Method: Independent samples t-test\")\n",
    "            print(f\"  t-statistic: {t_result['statistic']:.3f}\")\n",
    "            print(f\"  p-value: {t_result['p_value']:.6f}\")\n",
    "            print(f\"  95% Confidence Interval: ({t_result['ci_95'][0]:.2f}, {t_result['ci_95'][1]:.2f})\")\n",
    "            print(f\"  Statistically significant: {'YES' if t_result['significant'] else 'NO'}\")\n",
    "            \n",
    "            statistical_evidence['classical'] = {\n",
    "                'significant': t_result['significant'],\n",
    "                'p_value': t_result['p_value'],\n",
    "                'method': 't-test'\n",
    "            }\n",
    "        \n",
    "        if 'effect_size' in statistical_results:\n",
    "            effect = statistical_results['effect_size']\n",
    "            print(f\"  Effect size (Cohen's d): {effect['cohens_d']:.3f} ({effect['interpretation']} effect)\")\n",
    "            statistical_evidence['classical']['effect_size'] = effect['cohens_d']\n",
    "    else:\n",
    "        print(f\"  Classical statistical testing: NOT COMPLETED\")\n",
    "        print(f\"  Reason: Insufficient data or analysis not run\")\n",
    "    \n",
    "    # 2. Permutation Tests\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"2. PERMUTATION TEST ANALYSIS\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if 'permutation_results' in globals() and permutation_results:\n",
    "        perm_tests = ['error_reduction', 'success_improvement', 'combined_improvement']\n",
    "        significant_perm_tests = 0\n",
    "        total_perm_tests = 0\n",
    "        \n",
    "        for test_name in perm_tests:\n",
    "            if test_name in permutation_results and permutation_results[test_name]:\n",
    "                total_perm_tests += 1\n",
    "                result = permutation_results[test_name]\n",
    "                test_label = test_name.replace('_', ' ').title()\n",
    "                \n",
    "                print(f\"  {test_label}:\")\n",
    "                print(f\"    p-value: {result.get('p_value', 'N/A'):.6f}\")\n",
    "                print(f\"    Improvement: {result.get('improvement_pct', 'N/A'):.1f}%\")\n",
    "                print(f\"    Significant: {'YES' if result.get('significant', False) else 'NO'}\")\n",
    "                print(f\"    Meets >30% threshold: {'YES' if result.get('meets_threshold', False) else 'NO'}\")\n",
    "                \n",
    "                if result.get('significant', False):\n",
    "                    significant_perm_tests += 1\n",
    "        \n",
    "        print(f\"\\n  Overall Permutation Results: {significant_perm_tests}/{total_perm_tests} tests significant\")\n",
    "        print(f\"  Method advantages: Distribution-free, exact p-values, robust to outliers\")\n",
    "        \n",
    "        statistical_evidence['permutation'] = {\n",
    "            'significant_tests': significant_perm_tests,\n",
    "            'total_tests': total_perm_tests,\n",
    "            'any_significant': significant_perm_tests > 0\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Permutation testing: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: Advanced statistical libraries not installed or analysis not run\")\n",
    "    \n",
    "    # 3. Bootstrap Analysis\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"3. BOOTSTRAP HYPOTHESIS TESTING\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if 'bootstrap_results' in globals() and bootstrap_results:\n",
    "        boot_tests = ['error_reduction', 'success_improvement']\n",
    "        threshold_boot_tests = 0\n",
    "        total_boot_tests = 0\n",
    "        \n",
    "        for test_name in boot_tests:\n",
    "            if test_name in bootstrap_results and bootstrap_results[test_name]:\n",
    "                total_boot_tests += 1\n",
    "                result = bootstrap_results[test_name]\n",
    "                test_label = test_name.replace('_', ' ').title()\n",
    "                \n",
    "                print(f\"  {test_label}:\")\n",
    "                print(f\"    Observed improvement: {result.get('observed_improvement', 'N/A'):.1f}%\")\n",
    "                print(f\"    95% Bootstrap CI: [{result.get('confidence_interval', [0,0])[0]:.1f}, {result.get('confidence_interval', [0,0])[1]:.1f}]%\")\n",
    "                print(f\"    P(improvement > 30%): {result.get('threshold_confidence', 'N/A'):.3f}\")\n",
    "                print(f\"    Meets >30% threshold: {'YES' if result.get('meets_threshold', False) else 'NO'}\")\n",
    "                \n",
    "                if result.get('meets_threshold', False):\n",
    "                    threshold_boot_tests += 1\n",
    "        \n",
    "        print(f\"\\n  Overall Bootstrap Results: {threshold_boot_tests}/{total_boot_tests} tests meet >30% threshold\")\n",
    "        print(f\"  Method advantages: Direct threshold testing, bias-corrected intervals\")\n",
    "        \n",
    "        statistical_evidence['bootstrap'] = {\n",
    "            'threshold_tests': threshold_boot_tests,\n",
    "            'total_tests': total_boot_tests,\n",
    "            'any_threshold': threshold_boot_tests > 0\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Bootstrap testing: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: Advanced statistical libraries not installed or analysis not run\")\n",
    "    \n",
    "    # 4. Bayesian Analysis\n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(\"4. BAYESIAN HYPOTHESIS TESTING\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if 'bayesian_results' in globals() and bayesian_results:\n",
    "        print(f\"  Method: Bayesian inference with posterior probability statements\")\n",
    "        \n",
    "        if 'error_reduction' in bayesian_results and bayesian_results['error_reduction']:\n",
    "            error_result = bayesian_results['error_reduction']\n",
    "            print(f\"  Error Reduction Analysis:\")\n",
    "            print(f\"    Posterior mean improvement: {error_result.get('posterior_mean', 'N/A'):.1f}%\")\n",
    "            print(f\"    95% Credible interval: [{error_result.get('credible_interval', [0,0])[0]:.1f}, {error_result.get('credible_interval', [0,0])[1]:.1f}]%\")\n",
    "            print(f\"    P(improvement > 30%): {error_result.get('prob_30_plus', 'N/A'):.3f}\")\n",
    "            print(f\"    P(any improvement): {error_result.get('prob_any_improvement', 'N/A'):.3f}\")\n",
    "        \n",
    "        if 'success_improvement' in bayesian_results and bayesian_results['success_improvement']:\n",
    "            success_result = bayesian_results['success_improvement']\n",
    "            print(f\"  Success Rate Analysis:\")\n",
    "            print(f\"    P(improvement > 30%): {success_result.get('prob_30_plus', 'N/A'):.3f}\")\n",
    "            print(f\"    P(any improvement): {success_result.get('prob_any_improvement', 'N/A'):.3f}\")\n",
    "        \n",
    "        if 'decision_analysis' in bayesian_results and bayesian_results['decision_analysis']:\n",
    "            decision_result = bayesian_results['decision_analysis']\n",
    "            print(f\"  Combined Decision Analysis:\")\n",
    "            print(f\"    P(either metric > 30%): {decision_result.get('prob_either_30', 'N/A'):.3f}\")\n",
    "            print(f\"    P(both metrics > 30%): {decision_result.get('prob_both_30', 'N/A'):.3f}\")\n",
    "            print(f\"    Recommendation: {decision_result.get('recommendation', 'N/A')}\")\n",
    "        \n",
    "        if 'model_comparison' in bayesian_results and bayesian_results['model_comparison']:\n",
    "            model_result = bayesian_results['model_comparison']\n",
    "            print(f\"  Model Comparison:\")\n",
    "            print(f\"    Evidence: {model_result.get('preference', 'N/A')}\")\n",
    "            print(f\"    ΔWAIC: {model_result.get('delta_waic', 'N/A'):.2f}\")\n",
    "        \n",
    "        print(f\"  Method advantages: Direct probability statements, credible intervals, decision framework\")\n",
    "        \n",
    "        # Determine Bayesian evidence strength\n",
    "        bayes_strong = False\n",
    "        if ('error_reduction' in bayesian_results and \n",
    "            bayesian_results['error_reduction'].get('prob_30_plus', 0) > 0.8):\n",
    "            bayes_strong = True\n",
    "        if ('decision_analysis' in bayesian_results and \n",
    "            bayesian_results['decision_analysis'].get('prob_either_30', 0) > 0.8):\n",
    "            bayes_strong = True\n",
    "            \n",
    "        statistical_evidence['bayesian'] = {\n",
    "            'strong_evidence': bayes_strong,\n",
    "            'available': True\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Bayesian analysis: NOT AVAILABLE\")\n",
    "        print(f\"  Reason: PyMC/ArviZ not installed or analysis not run\")\n",
    "        statistical_evidence['bayesian'] = {'available': False}\n",
    "    \n",
    "    # INTEGRATED STATISTICAL CONCLUSION\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"INTEGRATED STATISTICAL CONCLUSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Count evidence across methods\n",
    "    evidence_count = 0\n",
    "    total_methods = 0\n",
    "    evidence_details = []\n",
    "    \n",
    "    # Classical evidence\n",
    "    if statistical_evidence['classical']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['classical'].get('significant', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Classical t-test: SIGNIFICANT\")\n",
    "        else:\n",
    "            evidence_details.append(\"Classical t-test: Not significant\")\n",
    "    \n",
    "    # Permutation evidence\n",
    "    if statistical_evidence['permutation']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['permutation'].get('any_significant', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Permutation tests: SIGNIFICANT\")\n",
    "        else:\n",
    "            evidence_details.append(\"Permutation tests: Not significant\")\n",
    "    \n",
    "    # Bootstrap evidence\n",
    "    if statistical_evidence['bootstrap']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['bootstrap'].get('any_threshold', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Bootstrap tests: MEETS THRESHOLD\")\n",
    "        else:\n",
    "            evidence_details.append(\"Bootstrap tests: Below threshold\")\n",
    "    \n",
    "    # Bayesian evidence\n",
    "    if statistical_evidence['bayesian']['available']:\n",
    "        total_methods += 1\n",
    "        if statistical_evidence['bayesian'].get('strong_evidence', False):\n",
    "            evidence_count += 1\n",
    "            evidence_details.append(\"Bayesian analysis: STRONG EVIDENCE\")\n",
    "        else:\n",
    "            evidence_details.append(\"Bayesian analysis: Weak evidence\")\n",
    "    \n",
    "    print(f\"\\nEvidence Summary:\")\n",
    "    for detail in evidence_details:\n",
    "        print(f\"  • {detail}\")\n",
    "    \n",
    "    print(f\"\\nStatistical Methods Agreement: {evidence_count}/{total_methods} methods show positive evidence\")\n",
    "    \n",
    "    # Overall statistical conclusion\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        statistical_conclusion = \"STRONG STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🟢\"\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        statistical_conclusion = \"MODERATE STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🟡\"\n",
    "    else:\n",
    "        statistical_conclusion = \"WEAK STATISTICAL EVIDENCE\"\n",
    "        conclusion_color = \"🔴\"\n",
    "    \n",
    "    print(f\"\\n{conclusion_color} OVERALL STATISTICAL CONCLUSION: {statistical_conclusion}\")\n",
    "    print(f\"   for >30% improvement in time-to-market through hallucination-reducing techniques\")\n",
    "    \n",
    "    # MODEL PERFORMANCE AND BUSINESS IMPACT\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL PERFORMANCE AND BUSINESS RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not performance_df.empty:\n",
    "        # Best performing models\n",
    "        best_accuracy = performance_df.loc[performance_df['Error_Pct'].idxmin()]\n",
    "        best_coverage = performance_df.loc[performance_df['Success_Rate'].idxmax()]\n",
    "        \n",
    "        print(f\"\\n1. RECOMMENDED MODELS:\")\n",
    "        print(f\"   Most Accurate Model: {best_accuracy['Model']}\")\n",
    "        print(f\"     • Error rate: {best_accuracy['Error_Pct']:.1f}%\")\n",
    "        print(f\"     • Success rate: {best_accuracy['Success_Rate']:.1%}\")\n",
    "        print(f\"     • LOC per SiFP: {best_accuracy['LOC_per_SiFP']:.1f}\")\n",
    "        \n",
    "        print(f\"\\n   Best Coverage Model: {best_coverage['Model']}\")\n",
    "        print(f\"     • Success rate: {best_coverage['Success_Rate']:.1%}\")\n",
    "        print(f\"     • Error rate: {best_coverage['Error_Pct']:.1f}%\")\n",
    "        print(f\"     • LOC per SiFP: {best_coverage['LOC_per_SiFP']:.1f}\")\n",
    "        \n",
    "        if 'effort_impact_df' in globals() and not effort_impact_df.empty:\n",
    "            best_effort = effort_impact_df.loc[effort_impact_df['Effort_Error_Pct'].abs().idxmin()]\n",
    "            print(f\"\\n   Best Effort Estimation Model: {best_effort['Model']}\")\n",
    "            print(f\"     • Effort error: {best_effort['Effort_Error_Pct']:+.1f}%\")\n",
    "            print(f\"     • Cost impact: ${best_effort['Total_Cost_Impact_USD']:+,.0f}\")\n",
    "    \n",
    "    # Business impact summary\n",
    "    if 'effort_impact_df' in globals() and not effort_impact_df.empty:\n",
    "        print(f\"\\n2. BUSINESS IMPACT ANALYSIS:\")\n",
    "        total_cost_impact = effort_impact_df['Total_Cost_Impact_USD'].sum()\n",
    "        avg_effort_error = effort_impact_df['Effort_Error_Pct'].mean()\n",
    "        max_cost_impact = effort_impact_df['Total_Cost_Impact_USD'].abs().max()\n",
    "        \n",
    "        print(f\"   Total net cost impact: ${total_cost_impact:+,.0f}\")\n",
    "        print(f\"   Average effort estimation error: {avg_effort_error:+.1f}%\")\n",
    "        print(f\"   Maximum cost impact (single model): ${max_cost_impact:,.0f}\")\n",
    "        print(f\"   Cost per hour assumed: ${CONFIG.get('COST_PER_HOUR', 100)}/hour\")\n",
    "        \n",
    "        # Risk assessment\n",
    "        print(f\"\\n3. RISK ASSESSMENT:\")\n",
    "        if abs(avg_effort_error) < 15:\n",
    "            print(f\"   ✅ LOW RISK: Average estimation errors within acceptable range (<15%)\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  MODERATE RISK: Average estimation errors exceed 15% threshold\")\n",
    "        \n",
    "        if abs(total_cost_impact) < 50000:\n",
    "            print(f\"   ✅ LOW FINANCIAL RISK: Total cost impact manageable (<$50k)\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  FINANCIAL RISK: Significant cost impact requires attention\")\n",
    "    \n",
    "    # IMPLEMENTATION RECOMMENDATIONS\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"IMPLEMENTATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n1. STATISTICAL EVIDENCE BASED:\")\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        print(f\"   🚀 STRONG RECOMMENDATION: Implement hallucination-reducing techniques\")\n",
    "        print(f\"      • Multiple statistical methods converge on positive evidence\")\n",
    "        print(f\"      • Risk of Type I error minimized through diverse analytical approaches\")\n",
    "        print(f\"      • Expected improvement supported by robust statistical framework\")\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        print(f\"   📋 MODERATE RECOMMENDATION: Pilot implementation with careful monitoring\")\n",
    "        print(f\"      • Mixed statistical evidence suggests cautious adoption\")\n",
    "        print(f\"      • Implement in controlled environment with measurement systems\")\n",
    "        print(f\"      • Establish clear success metrics and decision criteria\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  WEAK RECOMMENDATION: Additional research needed before implementation\")\n",
    "        print(f\"      • Insufficient statistical evidence for confident recommendation\")\n",
    "        print(f\"      • Consider larger sample sizes or alternative approaches\")\n",
    "        print(f\"      • Focus on improving measurement and data collection systems\")\n",
    "    \n",
    "    print(f\"\\n2. TECHNICAL IMPLEMENTATION:\")\n",
    "    print(f\"   • Multi-stage refinement: actor → judge → meta-judge architecture\")\n",
    "    print(f\"   • Quality scoring systems for estimation validation\")\n",
    "    print(f\"   • Continuous monitoring of estimation accuracy\")\n",
    "    print(f\"   • Regular model performance evaluation and recalibration\")\n",
    "    \n",
    "    print(f\"\\n3. ORGANIZATIONAL READINESS:\")\n",
    "    print(f\"   • Training on new estimation methodologies\")\n",
    "    print(f\"   • Integration with existing project management processes\")\n",
    "    print(f\"   • Change management for adoption of AI-assisted estimation\")\n",
    "    print(f\"   • Establishment of governance and quality assurance protocols\")\n",
    "    \n",
    "    # LIMITATIONS AND FUTURE RESEARCH\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"LIMITATIONS AND FUTURE RESEARCH DIRECTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nStudy Limitations:\")\n",
    "    print(f\"  • Sample size constraints limit statistical power\")\n",
    "    print(f\"  • Cross-sectional design rather than longitudinal study\")\n",
    "    print(f\"  • Proxy measures for 'hallucination-reducing techniques'\")\n",
    "    print(f\"  • Project-specific results may not generalize broadly\")\n",
    "    print(f\"  • Limited direct measurement of time-to-market impact\")\n",
    "    \n",
    "    print(f\"\\nFuture Research Priorities:\")\n",
    "    print(f\"  1. Randomized controlled trials with larger sample sizes\")\n",
    "    print(f\"  2. Direct measurement of time-to-market metrics in live projects\")\n",
    "    print(f\"  3. Multi-organization validation across different domains\")\n",
    "    print(f\"  4. Longitudinal studies tracking improvement sustainability\")\n",
    "    print(f\"  5. Cost-benefit analysis of implementation overhead\")\n",
    "    print(f\"  6. Comparative analysis with alternative estimation methods\")\n",
    "    \n",
    "    # FINAL EXECUTIVE DECISION FRAMEWORK\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"EXECUTIVE DECISION FRAMEWORK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n🎯 KEY FINDINGS:\")\n",
    "    print(f\"   • Statistical Analysis: {statistical_conclusion}\")\n",
    "    print(f\"   • Methods Agreement: {evidence_count}/{total_methods} approaches show positive results\")\n",
    "    if not performance_df.empty:\n",
    "        print(f\"   • Best Model Accuracy: {performance_df['Error_Pct'].min():.1f}% error rate\")\n",
    "        print(f\"   • Model Consistency: {'High' if performance_df['Error_Pct'].std() < 10 else 'Moderate'}\")\n",
    "    if 'effort_impact_df' in globals() and not effort_impact_df.empty:\n",
    "        print(f\"   • Financial Impact: ${effort_impact_df['Total_Cost_Impact_USD'].sum():+,.0f} net impact\")\n",
    "    \n",
    "    print(f\"\\n📊 DECISION MATRIX:\")\n",
    "    print(f\"   Statistical Evidence: {'Strong' if evidence_count >= total_methods * 0.75 else 'Moderate' if evidence_count >= total_methods * 0.5 else 'Weak'}\")\n",
    "    print(f\"   Implementation Risk: {'Low' if abs(avg_effort_error) < 15 else 'Moderate'}\")\n",
    "    print(f\"   Financial Impact: {'Manageable' if abs(total_cost_impact) < 50000 else 'Significant'}\")\n",
    "    print(f\"   Technical Readiness: {'High' if not performance_df.empty else 'Moderate'}\")\n",
    "    \n",
    "    print(f\"\\n🎯 EXECUTIVE RECOMMENDATION:\")\n",
    "    if evidence_count >= total_methods * 0.75:\n",
    "        print(f\"   PROCEED WITH IMPLEMENTATION\")\n",
    "        print(f\"   • Strong statistical evidence supports adoption\")\n",
    "        print(f\"   • Risk-reward profile favorable for implementation\")\n",
    "        print(f\"   • Establish monitoring systems for continuous validation\")\n",
    "    elif evidence_count >= total_methods * 0.5:\n",
    "        print(f\"   PROCEED WITH PILOT PROGRAM\")\n",
    "        print(f\"   • Moderate evidence supports cautious adoption\")\n",
    "        print(f\"   • Implement with robust measurement and evaluation\")\n",
    "        print(f\"   • Establish clear go/no-go criteria for full deployment\")\n",
    "    else:\n",
    "        print(f\"   DEFER IMPLEMENTATION - CONDUCT ADDITIONAL RESEARCH\")\n",
    "        print(f\"   • Insufficient evidence for confident business decision\")\n",
    "        print(f\"   • Invest in larger-scale validation studies\")\n",
    "        print(f\"   • Focus on improving data collection and measurement systems\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    try:\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Create comprehensive executive summary\n",
    "        final_summary = {\n",
    "            'project': project_name,\n",
    "            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "            'hypothesis': 'Hallucination-reducing techniques improve time-to-market by >30%',\n",
    "            'statistical_methods_used': total_methods,\n",
    "            'methods_with_positive_evidence': evidence_count,\n",
    "            'overall_statistical_conclusion': statistical_conclusion,\n",
    "            'evidence_strength_ratio': f\"{evidence_count}/{total_methods}\",\n",
    "            'best_accuracy_model': performance_df.loc[performance_df['Error_Pct'].idxmin(), 'Model'] if not performance_df.empty else 'N/A',\n",
    "            'best_accuracy_error': float(performance_df['Error_Pct'].min()) if not performance_df.empty else 'N/A',\n",
    "            'total_cost_impact': float(effort_impact_df['Total_Cost_Impact_USD'].sum()) if 'effort_impact_df' in globals() and not effort_impact_df.empty else 'N/A',\n",
    "            'executive_recommendation': 'PROCEED WITH IMPLEMENTATION' if evidence_count >= total_methods * 0.75 else 'PROCEED WITH PILOT PROGRAM' if evidence_count >= total_methods * 0.5 else 'DEFER IMPLEMENTATION',\n",
    "            'statistical_evidence_details': evidence_details,\n",
    "            'analysis_timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        # Save detailed results\n",
    "        import json\n",
    "        with open(f'results/enhanced_executive_summary_{project_name}_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_summary, f, indent=2, default=str)\n",
    "        \n",
    "        # Save statistical evidence summary\n",
    "        with open(f'results/statistical_evidence_{project_name}_{timestamp}.json', 'w') as f:\n",
    "            json.dump(statistical_evidence, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n✅ COMPREHENSIVE ANALYSIS RESULTS SAVED\")\n",
    "        print(f\"   Location: results/enhanced_executive_summary_{project_name}_{timestamp}.json\")\n",
    "        print(f\"   Statistical Evidence: results/statistical_evidence_{project_name}_{timestamp}.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️  Warning: Could not save results - {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ ANALYSIS INCOMPLETE\")\n",
    "    print(\"Missing required data components:\")\n",
    "    print(\"  - performance_df: Model performance metrics\")\n",
    "    print(\"  - effort_impact_df: Effort and cost impact analysis\")\n",
    "    print(\"  - statistical_results: Basic hypothesis testing results\")\n",
    "    print(\"\\nPlease ensure all previous analysis cells have completed successfully.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 ENHANCED STATISTICAL ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Framework: Classical → Permutation → Bootstrap → Bayesian → Integration\")\n",
    "print(\"Thank you for using the Enhanced SiFP COSMIC Estimation Analysis Framework!\")\n",
    "print(\"For questions or support, refer to the comprehensive documentation and statistical methodologies.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2: Hallucination Impact on Estimation Quality\n",
    "**Hallucinations in LLM outputs significantly correlate with lower quality estimates in new product feature development.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Dependencies\n",
    "# Purpose: Import necessary libraries and configure the environment for traceability and SiFP analysis\n",
    "# Dependencies: All packages assumed to be installed\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Database connectivity for Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, balanced_accuracy_score, \n",
    "    cohen_kappa_score, matthews_corrcoef, fbeta_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Enhanced statistical analysis\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import ttest_power, tt_solve_power\n",
    "\n",
    "# Advanced statistical testing imports\n",
    "# 1. Permutation Testing\n",
    "from scipy.stats import permutation_test, bootstrap as scipy_bootstrap\n",
    "\n",
    "# 2. Extended Bootstrap Analysis\n",
    "from arch.bootstrap import IIDBootstrap, CircularBlockBootstrap, StationaryBootstrap\n",
    "\n",
    "# 3. Bayesian Analysis\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import bayesian_testing as bt\n",
    "\n",
    "# Suppress warnings for cleaner notebook output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging with shorter format for better PDF wrapping\n",
    "import textwrap\n",
    "\n",
    "class PDFLoggingFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter that wraps long log messages for better PDF display\"\"\"\n",
    "    def format(self, record):\n",
    "        # Format the basic log record\n",
    "        formatted = super().format(record)\n",
    "        \n",
    "        # Wrap long lines at 120 characters with proper indentation\n",
    "        if len(formatted) > 120:\n",
    "            lines = textwrap.wrap(formatted, width=120, \n",
    "                                subsequent_indent='    ')  # Indent continuation lines\n",
    "            formatted = '\\n'.join(lines)\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "# Create custom formatter\n",
    "pdf_formatter = PDFLoggingFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom formatter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # Override any existing handlers\n",
    ")\n",
    "\n",
    "# Apply custom formatter to all handlers\n",
    "logger = logging.getLogger(__name__)\n",
    "for handler in logging.getLogger().handlers:\n",
    "    handler.setFormatter(pdf_formatter)\n",
    "\n",
    "# Configure pandas display settings for legal landscape format\n",
    "pd.set_option('display.width', 130)           # Set width threshold for legal landscape\n",
    "pd.set_option('display.max_columns', 25)     # Reasonable number of columns\n",
    "pd.set_option('display.max_colwidth', 25)    # Compact column width\n",
    "pd.set_option('display.precision', 2)        # Only 2 decimal places to save space\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  # Consistent float formatting\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "# Note: Removed expand_frame_repr=False to allow natural wrapping at 130 chars\n",
    "\n",
    "# Configure matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'seaborn')\n",
    "\n",
    "# Custom color palettes for TP/FP/FN visualization\n",
    "# TP (True Positive) = Light Blue, FP (False Positive) = Red, FN (False Negative) = Orange\n",
    "tp_fp_fn_colors = [\"#4d98da\", \"#e74c3c\", \"#f39c12\"]  # Light blue for TP, Red for FP, Orange for FN\n",
    "hallucination_colors = [\"#4d98da\", \"#e74c3c\", \"#f39c12\"]  # Same as above but conceptually for TP vs Hallucinations (FP+FN)\n",
    "\n",
    "# Statistical testing configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Advanced statistical methods availability flags (all True since packages are installed)\n",
    "advanced_methods_available = {\n",
    "    'permutation_tests': True,\n",
    "    'extended_bootstrap': True,\n",
    "    'bayesian_analysis': True,\n",
    "    'bayesian_testing': True,\n",
    "    'monte_carlo': True,\n",
    "    'statsmodels': True,\n",
    "    'scipy_bootstrap': True,\n",
    "    'arch_bootstrap': True\n",
    "}\n",
    "\n",
    "# Set statsmodels_available flag for backward compatibility\n",
    "statsmodels_available = True\n",
    "\n",
    "# Optimization metric for threshold evaluation\n",
    "OPTIMIZATION_METRIC = 'F2'\n",
    "\n",
    "print(\"Notebook Environment Setup Complete ✓\")\n",
    "print(\"HYPOTHESIS 2 FRAMEWORK:\")\n",
    "print(\"========================\")\n",
    "print(\"Hallucinations in LLM outputs = BOTH False Positive (FP) AND False Negative (FN) traceability links:\")\n",
    "print(\"- FP Hallucinations: Predicted as traceable but NOT verified by ground truth (over-identification)\")\n",
    "print(\"- FN Hallucinations: Predicted as non-traceable but ARE verified by ground truth (missed features/under-identification)\")\n",
    "print(\"- TP Links: Correctly identified traceable links (verified predictions)\")\n",
    "print(\"\")\n",
    "print(\"RESEARCH QUESTION: Do hallucinated traceability links (both FP over-identification and FN missed features)\")\n",
    "print(\"exhibit significantly higher SiFP estimation errors compared to TP links in new product development?\")\n",
    "print(\"\")\n",
    "print(\"ADVANCED STATISTICAL METHODS AVAILABLE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"✓ Permutation Tests (exact p-values, distribution-free)\")\n",
    "print(\"✓ Extended Bootstrap Methods (IID, Circular Block, Stationary Bootstrap)\")\n",
    "print(\"✓ Bayesian Analysis (credible intervals, posterior distributions)\")\n",
    "print(\"✓ Bayesian Hypothesis Testing (Bayes factors, model comparison)\")\n",
    "print(\"✓ Monte Carlo Simulation Methods (manual implementation)\")\n",
    "print(\"✓ Enhanced Statistical Modeling (statsmodels)\")\n",
    "print(\"✓ Multiple Testing Corrections and Power Analysis\")\n",
    "print(\"\")\n",
    "print(\"This notebook analyzes the relationship between BOTH types of LLM hallucinations and SiFP estimation quality\")\n",
    "print(\"using comprehensive statistical validation including:\")\n",
    "print(\"- Traditional parametric and non-parametric tests\")\n",
    "print(\"- Permutation-based exact tests (distribution-free)\")\n",
    "print(\"- Extended bootstrap methods (IID, Circular Block, Stationary Bootstrap)\")\n",
    "print(\"- Bayesian hypothesis testing (credible intervals, Bayes factors)\")\n",
    "print(\"- Monte Carlo simulation methods (manual implementation)\")\n",
    "print(\"- Multiple testing corrections and power analysis\")\n",
    "print(\"\")\n",
    "print(\"🎯 READY FOR COMPREHENSIVE HYPOTHESIS 2 TESTING WITH ADVANCED STATISTICAL METHODS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Environment Configuration\n",
    "# Purpose: Load environment variables and configure analysis settings from .env file\n",
    "# Dependencies: os, dotenv\n",
    "# Breadcrumbs: Setup -> Environment Configuration -> Model Selection\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j credentials from environment variables\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_PROJECT_NAME = os.getenv('NEO4J_PROJECT_NAME')\n",
    "\n",
    "# Get current model from environment\n",
    "CURRENT_MODEL_KEY = os.getenv('CURRENT_MODEL')\n",
    "CURRENT_MODEL = os.getenv(os.getenv('CURRENT_MODEL', ''))\n",
    "\n",
    "# Configuration for analysis\n",
    "SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "MIN_TRACEABILITY_THRESHOLD = int(os.getenv('MIN_TRACEABILITY_THRESHOLD', '3'))\n",
    "SIFP_ESTIMATION_REQUIREMENT = os.getenv('SIFP_ESTIMATION_REQUIREMENT', 'SOURCE')\n",
    "\n",
    "# Get models to include in analysis from environment\n",
    "ANALYSIS_MODEL_IDS = os.getenv('RESULTS_ANALYSIS_MODEL_IDS', '')\n",
    "if ANALYSIS_MODEL_IDS:\n",
    "    # Split by comma and strip whitespace to get variable names\n",
    "    model_var_names = [var_name.strip() for var_name in ANALYSIS_MODEL_IDS.split(',')]\n",
    "    \n",
    "    # Look up each variable in the environment to get actual model IDs\n",
    "    selected_models = []\n",
    "    for var_name in model_var_names:\n",
    "        model_id = os.getenv(var_name, '')\n",
    "        if model_id:\n",
    "            selected_models.append(model_id)\n",
    "else:\n",
    "    selected_models = [CURRENT_MODEL] if CURRENT_MODEL else []\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Environment Configuration:\")\n",
    "print(f\"==========================\")\n",
    "print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "print(f\"Current model: {CURRENT_MODEL}\")\n",
    "print(f\"Selected models for analysis: {selected_models}\")\n",
    "print(f\"Visualization: {'Enabled' if SHOW_VISUALIZATION else 'Disabled'}\")\n",
    "print(f\"Traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "print(f\"SIFP requirement type: {SIFP_ESTIMATION_REQUIREMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Neo4j Connection Setup\n",
    "# Purpose: Create and test connection to Neo4j database for data retrieval\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection -> Neo4j Client\n",
    "\n",
    "def create_neo4j_driver(uri=None, user=None, password=None):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance\n",
    "    \n",
    "    Parameters:\n",
    "        uri (str, optional): Neo4j connection URI. If None, uses NEO4J_URI from environment.\n",
    "        user (str, optional): Neo4j username. If None, uses NEO4J_USER from environment.\n",
    "        password (str, optional): Neo4j password. If None, uses NEO4J_PASSWORD from environment.\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use parameters if provided, otherwise use environment variables\n",
    "        uri = uri or NEO4J_URI\n",
    "        user = user or NEO4J_USER  \n",
    "        password = password or NEO4J_PASSWORD\n",
    "        \n",
    "        # Verify that all required connection parameters are available\n",
    "        if not all([uri, user, password]):\n",
    "            missing = [param for param, value in \n",
    "                      zip(['uri', 'user', 'password'], [uri, user, password]) \n",
    "                      if not value]\n",
    "            \n",
    "            error_msg = f\"Missing Neo4j connection parameters: {', '.join(missing)}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Create the driver\n",
    "        driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "        # Verify connection with a simple query\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 1 as test\").single()\n",
    "            if result and result[\"test\"] == 1:\n",
    "                logger.info(\"Successfully connected to Neo4j database\")\n",
    "                logger.info(f\"Connected to Neo4j at {uri}\")\n",
    "                return driver\n",
    "            else:\n",
    "                raise ConnectionError(\"Could not verify Neo4j connection\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver\n",
    "try:\n",
    "    driver = create_neo4j_driver()\n",
    "    print(f\"Neo4j Connection Status:\")\n",
    "    print(f\"======================\")\n",
    "    print(f\"Connected to: {NEO4J_URI}\")\n",
    "    print(f\"Project name: {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Connection successful ✓\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Neo4j database:\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"Please check your environment variables and database connection settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query Meta Judge Links (Traceability Data)\n",
    "# Purpose: Retrieve traceability data including TP/FP classifications from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Database Connection -> Data Retrieval -> Traceability Links\n",
    "\n",
    "def query_meta_judge_links(driver, project_name=None, models=None, target_ids=None):\n",
    "    \"\"\"\n",
    "    Query LLM_RESULT_META_JUDGE links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        models (list, optional): List of models to query. If None, uses selected_models from environment.\n",
    "        target_ids (list, optional): List of target IDs to filter by. If None, retrieves all target IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing meta judge links with traceability data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        models = models or selected_models\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for meta judge query\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        if not models:\n",
    "            logger.warning(\"No models specified for meta judge query, using all available models\")\n",
    "        \n",
    "        # Build model filter clause if models are specified\n",
    "        model_filter = \"\"\n",
    "        if models:\n",
    "            model_list = \"', '\".join(models)\n",
    "            model_filter = f\"AND r.model IN ['{model_list}']\"\n",
    "        \n",
    "        # Build target ID filter clause if target_ids are specified\n",
    "        target_id_filter = \"\"\n",
    "        if target_ids and len(target_ids) > 0:\n",
    "            # Convert all IDs to strings and wrap in quotes\n",
    "            quoted_ids = [\"'\" + str(id).replace(\"'\", \"\\\\'\") + \"'\" for id in target_ids]\n",
    "            id_list = \", \".join(quoted_ids)\n",
    "            target_id_filter = f\"AND target.id IN [{id_list}]\"\n",
    "            logger.info(f\"Filtering meta judge query to {len(target_ids)} specific target IDs\")\n",
    "        \n",
    "        # Query for meta-judge links\n",
    "        meta_judge_query = f\"\"\"\n",
    "        MATCH (p:Project {{name: $project_name}})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_META_JUDGE]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' {model_filter} {target_id_filter}\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            source.description as source_description,\n",
    "            target.id as target_id,\n",
    "            target.description as target_description,\n",
    "            r.is_traceable as is_traceable,\n",
    "            r.judge_score as judge_score,\n",
    "            r.semantic_alignment as semantic_alignment,\n",
    "            r.non_functional_coverage as non_functional_coverage,\n",
    "            r.final_score as final_score,\n",
    "            r.actor_score as actor_score,\n",
    "            r.functional_completeness as functional_completeness,\n",
    "            r.model as model\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing meta judge query for project: {project_name}\")\n",
    "            results = session.run(meta_judge_query, project_name=project_name).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No meta judge links found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            meta_judge_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Convert boolean columns to proper boolean type\n",
    "            if 'is_traceable' in meta_judge_df.columns:\n",
    "                meta_judge_df['is_traceable'] = meta_judge_df['is_traceable'].map(\n",
    "                    lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                )\n",
    "            \n",
    "            # Convert numeric columns to float\n",
    "            numeric_cols = [\n",
    "                'judge_score', 'semantic_alignment', 'non_functional_coverage',\n",
    "                'final_score', 'actor_score', 'functional_completeness'\n",
    "            ]\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in meta_judge_df.columns:\n",
    "                    meta_judge_df[col] = pd.to_numeric(meta_judge_df[col], errors='coerce')\n",
    "            \n",
    "            # Calculate total score as judge_score + actor_score\n",
    "            if 'judge_score' in meta_judge_df.columns and 'actor_score' in meta_judge_df.columns:\n",
    "                meta_judge_df['total_score'] = meta_judge_df['judge_score'] + meta_judge_df['actor_score']\n",
    "            \n",
    "            # Add meta_judge_threshold column based on MIN_TRACEABILITY_THRESHOLD\n",
    "            if 'total_score' in meta_judge_df.columns:\n",
    "                meta_judge_df['meta_judge_threshold'] = meta_judge_df['total_score'] >= MIN_TRACEABILITY_THRESHOLD\n",
    "            \n",
    "            # Count metrics for logging\n",
    "            logger.info(f\"Retrieved {len(meta_judge_df)} meta judge links\")\n",
    "            if 'is_traceable' in meta_judge_df.columns:\n",
    "                traceable_count = meta_judge_df['is_traceable'].sum()\n",
    "                logger.info(f\"Traceable links: {traceable_count} ({traceable_count/len(meta_judge_df)*100:.2f}%)\")\n",
    "            \n",
    "            return meta_judge_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for meta judge links: {str(e)}\") \n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    # Wait until we have both ground truth data and SIFP data\n",
    "    sifp_target_ids = []\n",
    "    ground_truth_target_ids = []\n",
    "    \n",
    "    # Get target IDs from SIFP data if already loaded\n",
    "    if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "        if 'sifp_requirement_id' in globals()['sifp_results_df'].columns:\n",
    "            sifp_target_ids = globals()['sifp_results_df']['sifp_requirement_id'].unique().tolist()\n",
    "        print(f\"Found {len(sifp_target_ids)} unique target IDs with SIFP data\")\n",
    "    \n",
    "    # Get target IDs from ground truth data if already loaded\n",
    "    if 'ground_truth_df' in globals() and not globals()['ground_truth_df'].empty:\n",
    "        ground_truth_target_ids = globals()['ground_truth_df']['target_id'].unique().tolist()\n",
    "        print(f\"Found {len(ground_truth_target_ids)} unique target IDs in ground truth data\")\n",
    "    \n",
    "    # Only filter if we have data from both sources\n",
    "    filter_target_ids = None\n",
    "    if sifp_target_ids and ground_truth_target_ids:\n",
    "        # Find the intersection of target IDs (those with both SIFP data and ground truth)\n",
    "        filter_target_ids = list(set(sifp_target_ids).intersection(set(ground_truth_target_ids)))\n",
    "        print(f\"Filtering meta judge query to {len(filter_target_ids)} target IDs that have both SIFP data and ground truth\")\n",
    "    \n",
    "    # Query meta judge data with filtered target IDs\n",
    "    meta_judge_df = query_meta_judge_links(driver, target_ids=filter_target_ids)\n",
    "    \n",
    "    print(f\"Meta Judge Traceability Results:\")\n",
    "    print(f\"================================\")\n",
    "    \n",
    "    if not meta_judge_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total links analyzed: {len(meta_judge_df)}\")\n",
    "        \n",
    "        if 'model' in meta_judge_df.columns:\n",
    "            model_counts = meta_judge_df['model'].value_counts()\n",
    "            print(f\"Links by model:\")\n",
    "            for model, count in model_counts.items():\n",
    "                print(f\"  {model}: {count}\")\n",
    "        \n",
    "        if 'is_traceable' in meta_judge_df.columns:\n",
    "            traceable_count = meta_judge_df['is_traceable'].sum()\n",
    "            print(f\"Traceability Distribution:\")\n",
    "            print(f\"  Traceable links: {traceable_count} ({traceable_count/len(meta_judge_df)*100:.2f}%)\")\n",
    "            print(f\"  Non-traceable links: {len(meta_judge_df) - traceable_count} ({(len(meta_judge_df) - traceable_count)/len(meta_judge_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of meta judge data:\")\n",
    "        display_cols = ['source_id', 'target_id', 'model', 'is_traceable', 'judge_score', 'actor_score', 'total_score']\n",
    "        display_cols = [col for col in display_cols if col in meta_judge_df.columns]\n",
    "        print(meta_judge_df[display_cols].head())\n",
    "    else:\n",
    "        print(\"No meta judge data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving meta judge data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Query Ground Truth Links\n",
    "# Purpose: Retrieve ground truth traceability links for validation and baseline comparison\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Retrieval -> Ground Truth -> Validation Data\n",
    "\n",
    "def query_ground_truth_links(driver, project_name=None):\n",
    "    \"\"\"\n",
    "    Query ground truth traceability links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If project_name isn't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for ground truth query\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Query for ground truth links\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' AND target.type = 'TARGET'\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            source.description as source_description,\n",
    "            target.id as target_id,\n",
    "            target.description as target_description,\n",
    "            1 as ground_truth,\n",
    "            d.id as document_id\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing ground truth query for project: {project_name}\")\n",
    "            results = session.run(ground_truth_query, project_name=project_name).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No ground truth links found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            ground_truth_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Add pair_id column for merging (source_id + \"_\" + target_id)\n",
    "            if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "                ground_truth_df['pair_id'] = ground_truth_df['source_id'] + \"_\" + ground_truth_df['target_id']\n",
    "            \n",
    "            # Count metrics for logging\n",
    "            logger.info(f\"Retrieved {len(ground_truth_df)} ground truth links\")\n",
    "            source_count = ground_truth_df['source_id'].nunique() if 'source_id' in ground_truth_df.columns else 0\n",
    "            target_count = ground_truth_df['target_id'].nunique() if 'target_id' in ground_truth_df.columns else 0\n",
    "            logger.info(f\"Unique source requirements: {source_count}\")\n",
    "            logger.info(f\"Unique target requirements: {target_count}\")\n",
    "            \n",
    "            return ground_truth_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for ground truth links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    ground_truth_df = query_ground_truth_links(driver)\n",
    "    print(f\"Ground Truth Links:\")\n",
    "    print(f\"==================\")\n",
    "    \n",
    "    if not ground_truth_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total ground truth links: {len(ground_truth_df)}\")\n",
    "        \n",
    "        # Count unique source and target requirements\n",
    "        source_count = ground_truth_df['source_id'].nunique()\n",
    "        target_count = ground_truth_df['target_id'].nunique()\n",
    "        \n",
    "        print(f\"Unique source requirements: {source_count}\")\n",
    "        print(f\"Unique target requirements: {target_count}\")\n",
    "        \n",
    "        # Calculate link density\n",
    "        if source_count > 0 and target_count > 0:\n",
    "            link_density = len(ground_truth_df) / (source_count * target_count)\n",
    "            print(f\"Link density: {link_density:.4f}\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of ground truth data:\")\n",
    "        display_cols = ['source_id', 'target_id', 'ground_truth']\n",
    "        display_cols = [col for col in display_cols if col in ground_truth_df.columns]\n",
    "        print(ground_truth_df[display_cols].head())\n",
    "        \n",
    "        # Create a set of ground truth link pair IDs for faster lookups\n",
    "        if 'pair_id' in ground_truth_df.columns:\n",
    "            ground_truth_links = set(ground_truth_df['pair_id'])\n",
    "            print(f\"Created fast lookup set with {len(ground_truth_links)} ground truth link pairs\")\n",
    "    else:\n",
    "        print(\"No ground truth data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving ground truth data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Query SIFP Estimation Data\n",
    "# Purpose: Retrieve SiFP estimations and actual metrics from requirements analysis\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Retrieval -> SIFP Estimations -> Requirements Metrics\n",
    "\n",
    "def query_sifp_estimations(driver, project_name=None, models=None, requirement_type=None, target_ids=None):\n",
    "    \"\"\"\n",
    "    Query SIFP estimation data from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        models (list, optional): List of models to query. If None, uses selected_models from environment.\n",
    "        requirement_type (str, optional): Requirement type to query. If None, uses SIFP_ESTIMATION_REQUIREMENT from environment.\n",
    "        target_ids (list, optional): List of target IDs to filter by. If None, retrieves all target IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing SIFP estimation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        models = models or selected_models\n",
    "        requirement_type = requirement_type or SIFP_ESTIMATION_REQUIREMENT\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for SIFP estimation query\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Build model filter clause if models are specified\n",
    "        model_filter = \"\"\n",
    "        if models:\n",
    "            model_list = \"', '\".join(models)\n",
    "            model_filter = f\"AND s.model IN ['{model_list}']\"\n",
    "        \n",
    "        # Build target ID filter clause if target_ids are specified\n",
    "        target_id_filter = \"\"\n",
    "        if target_ids and len(target_ids) > 0:\n",
    "            # Convert all IDs to strings and wrap in quotes\n",
    "            quoted_ids = [\"'\" + str(id).replace(\"'\", \"\\\\'\") + \"'\" for id in target_ids]\n",
    "            id_list = \", \".join(quoted_ids)\n",
    "            target_id_filter = f\"AND r.id IN [{id_list}]\"\n",
    "            logger.info(f\"Filtering SIFP query to {len(target_ids)} specific target IDs\")\n",
    "        \n",
    "        # Query for SIFP estimations\n",
    "        sifp_query = f\"\"\"\n",
    "        MATCH (p:Project {{name: $project_name}})\n",
    "        MATCH (p)-[:CONTAINS*]->(n)\n",
    "        MATCH (r:Requirement)-[s:SIFP_ESTIMATION]->(e)\n",
    "        WHERE r = n\n",
    "        AND r.type = $requirement_type\n",
    "        {model_filter}\n",
    "        {target_id_filter}\n",
    "        WITH r, s\n",
    "        WHERE s.actor_analysis IS NOT NULL\n",
    "        AND s.judge_evaluation IS NOT NULL\n",
    "        AND s.final_estimation IS NOT NULL\n",
    "        WITH r, s,\n",
    "            CASE\n",
    "                WHEN s.actor_analysis STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.actor_analysis)\n",
    "                ELSE NULL\n",
    "            END as actor_analysis,\n",
    "            CASE\n",
    "                WHEN s.judge_evaluation STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.judge_evaluation)\n",
    "                ELSE NULL\n",
    "            END as judge_eval,\n",
    "            CASE\n",
    "                WHEN s.final_estimation STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.final_estimation)\n",
    "                ELSE NULL\n",
    "            END as final_est\n",
    "        WHERE actor_analysis IS NOT NULL\n",
    "        AND final_est IS NOT NULL\n",
    "        WITH r.id as sifp_requirement_id,\n",
    "            s.is_valid as sifp_is_valid,\n",
    "            s.model as sifp_model,\n",
    "            s.judge_score as sifp_judge_score,\n",
    "            s.judge_confidence as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            actor_analysis.confidence as sifp_actor_confidence,\n",
    "            actor_analysis.sifp_points.total as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            judge_eval.ugep_accuracy as sifp_judge_ugep_accuracy,\n",
    "            judge_eval.ugdg_accuracy as sifp_judge_ugdg_accuracy,\n",
    "            judge_eval.calculation_accuracy as sifp_judge_calculation_accuracy,\n",
    "            judge_eval.component_classification_accuracy as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            final_est.sifp_points.total as sifp_final_total\n",
    "        WITH sifp_requirement_id, sifp_model,\n",
    "            COLLECT([sifp_is_valid, sifp_judge_score, sifp_judge_confidence,\n",
    "            sifp_actor_confidence, sifp_actor_total, sifp_judge_ugep_accuracy,\n",
    "            sifp_judge_ugdg_accuracy, sifp_judge_calculation_accuracy,\n",
    "            sifp_judge_classification_accuracy, sifp_final_total])[0] as fields\n",
    "        RETURN \n",
    "            sifp_requirement_id,\n",
    "            sifp_model,\n",
    "            fields[0] as sifp_is_valid,\n",
    "            fields[1] as sifp_judge_score,\n",
    "            fields[2] as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            fields[3] as sifp_actor_confidence,\n",
    "            fields[4] as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            fields[5] as sifp_judge_ugep_accuracy,\n",
    "            fields[6] as sifp_judge_ugdg_accuracy,\n",
    "            fields[7] as sifp_judge_calculation_accuracy,\n",
    "            fields[8] as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            fields[9] as sifp_final_total\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing SIFP estimation query for project: {project_name}\")\n",
    "            results = session.run(sifp_query, \n",
    "                                 project_name=project_name,\n",
    "                                 requirement_type=requirement_type).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No SIFP estimation results found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            sifp_results_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Convert boolean columns to proper boolean type\n",
    "            if 'sifp_is_valid' in sifp_results_df.columns:\n",
    "                sifp_results_df['sifp_is_valid'] = sifp_results_df['sifp_is_valid'].map(\n",
    "                    lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                )\n",
    "            \n",
    "            # Convert numeric columns to float\n",
    "            numeric_cols = [\n",
    "                'sifp_judge_score', 'sifp_judge_confidence', 'sifp_actor_confidence', 'sifp_actor_total',\n",
    "                'sifp_judge_ugep_accuracy', 'sifp_judge_ugdg_accuracy', 'sifp_judge_calculation_accuracy',\n",
    "                'sifp_judge_classification_accuracy', 'sifp_final_total'\n",
    "            ]\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in sifp_results_df.columns:\n",
    "                    sifp_results_df[col] = pd.to_numeric(sifp_results_df[col], errors='coerce')\n",
    "            \n",
    "            # Add columns to help with analysis\n",
    "            if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "                # Calculate difference between actor and final estimations\n",
    "                sifp_results_df['sifp_difference'] = sifp_results_df['sifp_final_total'] - sifp_results_df['sifp_actor_total']\n",
    "                \n",
    "                # Calculate percentage difference\n",
    "                nonzero_mask = sifp_results_df['sifp_actor_total'] != 0\n",
    "                sifp_results_df['sifp_pct_difference'] = np.nan\n",
    "                sifp_results_df.loc[nonzero_mask, 'sifp_pct_difference'] = (\n",
    "                    (sifp_results_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                     sifp_results_df.loc[nonzero_mask, 'sifp_actor_total']) / \n",
    "                    sifp_results_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "                )\n",
    "                \n",
    "                # Calculate absolute percentage difference\n",
    "                sifp_results_df['sifp_abs_pct_difference'] = sifp_results_df['sifp_pct_difference'].abs()\n",
    "            \n",
    "            # Log summary statistics\n",
    "            logger.info(f\"Retrieved {len(sifp_results_df)} SIFP estimation results\")\n",
    "            \n",
    "            if 'sifp_model' in sifp_results_df.columns:\n",
    "                logger.info(f\"Models in SIFP data: {sifp_results_df['sifp_model'].unique()}\")\n",
    "            \n",
    "            if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "                logger.info(f\"Average actor total: {sifp_results_df['sifp_actor_total'].mean():.2f}\")\n",
    "                logger.info(f\"Average final total: {sifp_results_df['sifp_final_total'].mean():.2f}\")\n",
    "            \n",
    "            return sifp_results_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for SIFP estimations: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    # First, extract target IDs from ground truth data\n",
    "    ground_truth_target_ids = []\n",
    "    if 'ground_truth_df' in globals() and not globals()['ground_truth_df'].empty:\n",
    "        ground_truth_target_ids = globals()['ground_truth_df']['target_id'].unique().tolist()\n",
    "        print(f\"Found {len(ground_truth_target_ids)} unique target IDs in ground truth data\")\n",
    "    \n",
    "    # Query SIFP data, filtered by target IDs from ground truth\n",
    "    sifp_results_df = query_sifp_estimations(driver, target_ids=ground_truth_target_ids)\n",
    "    \n",
    "    print(f\"SIFP Estimation Results:\")\n",
    "    print(f\"========================\")\n",
    "    \n",
    "    if not sifp_results_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total SIFP estimations: {len(sifp_results_df)}\")\n",
    "        \n",
    "        if 'sifp_model' in sifp_results_df.columns:\n",
    "            model_counts = sifp_results_df['sifp_model'].value_counts()\n",
    "            print(f\"Estimations by model:\")\n",
    "            for model, count in model_counts.items():\n",
    "                print(f\"  {model}: {count}\")\n",
    "        \n",
    "        # Display statistics on SIFP estimations\n",
    "        if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "            print(f\"SIFP Estimation Statistics:\")\n",
    "            print(f\"  Average actor estimation: {sifp_results_df['sifp_actor_total'].mean():.2f} points\")\n",
    "            print(f\"  Average judge estimation: {sifp_results_df['sifp_final_total'].mean():.2f} points\")\n",
    "            print(f\"  Average improvement: {sifp_results_df['sifp_difference'].mean():.2f} points\")\n",
    "            print(f\"  Average percentage change: {sifp_results_df['sifp_pct_difference'].mean():.2f}%\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of SIFP estimation data:\")\n",
    "        display_cols = ['sifp_requirement_id', 'sifp_model', 'sifp_actor_total', 'sifp_final_total', 'sifp_difference', 'sifp_pct_difference']\n",
    "        display_cols = [col for col in display_cols if col in sifp_results_df.columns]\n",
    "        print(sifp_results_df[display_cols].head())\n",
    "        \n",
    "        # Display summary statistics for key metrics\n",
    "        print(\"Summary Statistics for Key Metrics:\")\n",
    "        key_metrics = ['sifp_actor_total', 'sifp_final_total', 'sifp_difference', 'sifp_abs_pct_difference']\n",
    "        key_metrics = [col for col in key_metrics if col in sifp_results_df.columns]\n",
    "        \n",
    "        if key_metrics:\n",
    "            print(sifp_results_df[key_metrics].describe())\n",
    "    else:\n",
    "        print(\"No SIFP estimation data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving SIFP estimation data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Model Evaluation and Threshold Optimization\n",
    "# Purpose: Evaluate traceability links and classify as TP/FP/FN/TN based on F2 optimization\n",
    "# Dependencies: pandas, numpy, sklearn.metrics\n",
    "# Breadcrumbs: Data Analysis -> Model Evaluation -> Classification Optimization\n",
    "\n",
    "def evaluate_model_thresholds(df, model_name, score_column='total_score', \n",
    "                             ground_truth_column='ground_truth_traceable', \n",
    "                             optimize_for='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance across different thresholds\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing model predictions and ground truth\n",
    "        model_name: Name of the model to evaluate\n",
    "        score_column: Column containing score values\n",
    "        ground_truth_column: Column containing ground truth values\n",
    "        optimize_for: Metric to optimize for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this model\n",
    "        model_df = df[df['model'] == model_name].copy()\n",
    "        \n",
    "        if model_df.empty:\n",
    "            print(f\"No data available for model: {model_name}\")\n",
    "            return {}\n",
    "            \n",
    "        if ground_truth_column not in model_df.columns:\n",
    "            print(f\"Ground truth column '{ground_truth_column}' not found for model: {model_name}\")\n",
    "            return {}\n",
    "        \n",
    "        # Get ground truth and scores\n",
    "        y_true = model_df[ground_truth_column].astype(int).values\n",
    "        \n",
    "        # Check for and handle None/NaN values in score column\n",
    "        if model_df[score_column].isna().any():\n",
    "            print(f\"Found NaN values in {score_column} for model {model_name}. Filling with 0.\")\n",
    "            model_df[score_column] = model_df[score_column].fillna(0)\n",
    "        \n",
    "        # Ensure scores are numeric\n",
    "        if model_df[score_column].dtype == object:\n",
    "            try:\n",
    "                model_df[score_column] = pd.to_numeric(model_df[score_column])\n",
    "                print(f\"Converted {score_column} to numeric for model {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {score_column} to numeric: {str(e)}\")\n",
    "                # Default to zeros if conversion fails\n",
    "                model_df[score_column] = 0\n",
    "        \n",
    "        scores = model_df[score_column].values\n",
    "        \n",
    "        # Print debug information\n",
    "        print(f\"  - Total data points: {len(model_df)}\")\n",
    "        print(f\"  - Positive examples: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Negative examples: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Score range: {scores.min():.4f} to {scores.max():.4f}\")\n",
    "        \n",
    "        # If all ground truth values are the same, we can't calculate meaningful metrics\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            print(f\"Insufficient ground truth variety for model {model_name} - all values are {np.unique(y_true)[0]}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum())\n",
    "            }\n",
    "        \n",
    "        # Generate possible thresholds from the data\n",
    "        unique_scores = np.unique(scores)\n",
    "        # Add some intermediate thresholds for a more fine-grained evaluation\n",
    "        thresholds = np.sort(np.concatenate([\n",
    "            unique_scores,\n",
    "            np.linspace(scores.min(), scores.max(), 20)\n",
    "        ]))\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Convert scores to binary predictions using this threshold\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            \n",
    "            # Only calculate if we have at least one prediction of each class\n",
    "            if np.unique(y_pred).size < 2:\n",
    "                continue\n",
    "                \n",
    "            # Confusion matrix components\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            # Handle division by zero\n",
    "            if tp + fp == 0:  # No positive predictions\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = tp / (tp + fp)\n",
    "                \n",
    "            if tp + fn == 0:  # No positive ground truth\n",
    "                rec = 0\n",
    "            else:\n",
    "                rec = tp / (tp + fn)\n",
    "            \n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "            \n",
    "            # Additional metrics\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity/True Negative Rate\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate/False Negative Rate\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'tnr': tnr,  # specificity\n",
    "                'fnr': fnr,  # miss rate\n",
    "                'f1_score': f1,\n",
    "                'f2_score': f2,\n",
    "                'mcc': mcc  # Matthews Correlation Coefficient\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if results_df.empty:\n",
    "            print(f\"No valid thresholds found for model {model_name}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "                'error': \"No valid thresholds found with current data\"\n",
    "            }\n",
    "        \n",
    "        # Find best threshold based on optimization metric\n",
    "        if optimize_for == 'F1':\n",
    "            best_idx = results_df['f1_score'].idxmax()\n",
    "            best_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_idx = results_df['f2_score'].idxmax()\n",
    "            best_metric = 'f2_score'\n",
    "            \n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        # Return comprehensive results with the original dataframe for further use\n",
    "        result_dict = {\n",
    "            'model_name': model_name,\n",
    "            'score_column': score_column,\n",
    "            'data_points': len(model_df),\n",
    "            'ground_truth_positive': int(y_true.sum()),\n",
    "            'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "            'best_threshold': best_result['threshold'],\n",
    "            'best_precision': best_result['precision'],\n",
    "            'best_recall': best_result['recall'],\n",
    "            'best_accuracy': best_result['accuracy'],\n",
    "            'best_balanced_accuracy': best_result['balanced_accuracy'],\n",
    "            'best_f1': best_result['f1_score'],\n",
    "            'best_f2': best_result['f2_score'],\n",
    "            'best_tnr': best_result['tnr'],\n",
    "            'best_fnr': best_result['fnr'],\n",
    "            'best_mcc': best_result['mcc'],\n",
    "            'best_tp': best_result['tp'],\n",
    "            'best_fp': best_result['fp'],\n",
    "            'best_fn': best_result['fn'],\n",
    "            'best_tn': best_result['tn'],\n",
    "            'optimization_metric': optimize_for,\n",
    "            'threshold_results': results_df,\n",
    "            'model_df': model_df\n",
    "        }\n",
    "        \n",
    "        return result_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df) if 'model_df' in locals() else 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def apply_classification_to_dataframe(df, evaluation_results):\n",
    "    \"\"\"\n",
    "    Apply classification (TP/FP/FN/TN) to a dataframe based on evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame to apply classification to\n",
    "        evaluation_results: Dictionary with evaluation results including best threshold\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added classification columns\n",
    "    \"\"\"\n",
    "    if not evaluation_results or 'error' in evaluation_results:\n",
    "        return df\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Extract key information from evaluation results\n",
    "    model_name = evaluation_results['model_name']\n",
    "    score_column = evaluation_results['score_column']\n",
    "    threshold = evaluation_results['best_threshold']\n",
    "    \n",
    "    # Add columns for this specific model and score type\n",
    "    col_prefix = f\"{model_name}_{score_column}\"\n",
    "    prediction_col = f\"{col_prefix}_prediction\"\n",
    "    classification_col = f\"{col_prefix}_classification\"\n",
    "    \n",
    "    # Filter for rows with this model\n",
    "    if 'model' in result_df.columns:\n",
    "        model_mask = (result_df['model'] == model_name)\n",
    "    else:\n",
    "        model_mask = pd.Series(True, index=result_df.index)\n",
    "    \n",
    "    # Create prediction column (True/False) based on threshold\n",
    "    result_df.loc[model_mask, prediction_col] = False\n",
    "    score_mask = model_mask & result_df[score_column].notna()\n",
    "    result_df.loc[score_mask, prediction_col] = result_df.loc[score_mask, score_column] >= threshold\n",
    "    \n",
    "    # Create classification column (TP/FP/FN/TN) based on prediction and ground truth\n",
    "    result_df.loc[model_mask, classification_col] = 'NA'\n",
    "    \n",
    "    # Apply classification logic\n",
    "    if 'ground_truth_traceable' in result_df.columns:\n",
    "        # TP: ground truth positive AND prediction positive\n",
    "        tp_mask = model_mask & result_df['ground_truth_traceable'] & result_df[prediction_col]\n",
    "        result_df.loc[tp_mask, classification_col] = 'TP'\n",
    "        \n",
    "        # FP: ground truth negative BUT prediction positive\n",
    "        # Make sure to handle potential float values correctly by ensuring boolean conversion\n",
    "        fp_mask = model_mask & ~result_df['ground_truth_traceable'] & result_df[prediction_col]\n",
    "        result_df.loc[fp_mask, classification_col] = 'FP'\n",
    "        \n",
    "        # FN: ground truth positive BUT prediction negative\n",
    "        # This is where the error occurred - we need to ensure we're treating prediction_col as boolean\n",
    "        prediction_bool = result_df[prediction_col].astype(bool)\n",
    "        fn_mask = model_mask & result_df['ground_truth_traceable'] & ~prediction_bool\n",
    "        result_df.loc[fn_mask, classification_col] = 'FN'\n",
    "        \n",
    "        # TN: ground truth negative AND prediction negative\n",
    "        tn_mask = model_mask & ~result_df['ground_truth_traceable'] & ~prediction_bool\n",
    "        result_df.loc[tn_mask, classification_col] = 'TN'\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_combined_dataset(meta_judge_df, ground_truth_df, sifp_results_df):\n",
    "    \"\"\"\n",
    "    Combine meta judge data with traceability classifications and SIFP estimates\n",
    "    \n",
    "    Parameters:\n",
    "        meta_judge_df: DataFrame with meta judge assessments\n",
    "        ground_truth_df: DataFrame with ground truth traceability data\n",
    "        sifp_results_df: DataFrame with SIFP estimation data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset linking classifications with SIFP data\n",
    "    \"\"\"\n",
    "    # Validate input data\n",
    "    if meta_judge_df is None or meta_judge_df.empty:\n",
    "        logger.error(\"Meta judge data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if ground_truth_df is None or ground_truth_df.empty:\n",
    "        logger.error(\"Ground truth data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if sifp_results_df is None or sifp_results_df.empty:\n",
    "        logger.error(\"SIFP results data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"Creating combined dataset from {len(meta_judge_df)} meta judge links, \" \n",
    "                f\"{len(ground_truth_df)} ground truth links, and {len(sifp_results_df)} SIFP estimates\")\n",
    "    \n",
    "    # Create a set of ground truth link pairs for fast lookup\n",
    "    if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "        # Convert all IDs to strings for consistent comparison\n",
    "        ground_truth_df['source_id'] = ground_truth_df['source_id'].astype(str)\n",
    "        ground_truth_df['target_id'] = ground_truth_df['target_id'].astype(str)\n",
    "        \n",
    "        ground_truth_pairs = set(zip(ground_truth_df['source_id'], ground_truth_df['target_id']))\n",
    "        logger.info(f\"Created ground truth lookup set with {len(ground_truth_pairs)} link pairs\")\n",
    "        \n",
    "        # Create a copy to work with\n",
    "        result_df = meta_judge_df.copy()\n",
    "        \n",
    "        # Ensure IDs are strings\n",
    "        result_df['source_id'] = result_df['source_id'].astype(str)\n",
    "        result_df['target_id'] = result_df['target_id'].astype(str)\n",
    "        \n",
    "        # Add ground_truth_traceable column\n",
    "        result_df['ground_truth_traceable'] = result_df.apply(\n",
    "            lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Add a pair_id for easier merging with SIFP data\n",
    "        result_df['pair_id'] = result_df['source_id'] + \"_\" + result_df['target_id']\n",
    "        \n",
    "        # Now merge with SIFP data using target_id and model\n",
    "        # Ensure SIFP data has consistent string IDs\n",
    "        sifp_df = sifp_results_df.copy()\n",
    "        sifp_df['sifp_requirement_id'] = sifp_df['sifp_requirement_id'].astype(str)\n",
    "        \n",
    "        # Create merge keys that include both target_id and model for SIFP data\n",
    "        if 'model' in result_df.columns and 'sifp_model' in sifp_df.columns:\n",
    "            logger.info(\"Creating merge keys using both target_id and model\")\n",
    "            result_df['merge_key'] = result_df['target_id'] + '_' + result_df['model']\n",
    "            sifp_df['merge_key'] = sifp_df['sifp_requirement_id'] + '_' + sifp_df['sifp_model']\n",
    "            \n",
    "            # Check for duplicates in merge keys\n",
    "            if sifp_df['merge_key'].duplicated().any():\n",
    "                logger.warning(f\"Found {sifp_df['merge_key'].duplicated().sum()} duplicate merge keys in SIFP estimates. Using first occurrence.\")\n",
    "                sifp_df = sifp_df.drop_duplicates(subset=['merge_key'], keep='first')\n",
    "            \n",
    "            # Perform the merge\n",
    "            combined_df = result_df.merge(\n",
    "                sifp_df,\n",
    "                on='merge_key',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "            logger.info(f\"Merged dataframe has {len(combined_df)} rows with {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "            \n",
    "            # If the merge didn't work well, try with just target_id as fallback\n",
    "            if merge_pct < 50:\n",
    "                logger.warning(\"Poor merge results with target_id+model. Trying with just target_id as fallback.\")\n",
    "                \n",
    "                # Fallback to just target_id\n",
    "                combined_df = result_df.merge(\n",
    "                    sifp_df.drop(columns=['merge_key']),\n",
    "                    left_on='target_id',\n",
    "                    right_on='sifp_requirement_id',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_sifp')\n",
    "                )\n",
    "                \n",
    "                # Check fallback merge result\n",
    "                merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "                merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "                logger.info(f\"Fallback merge with target_id: {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "        else:\n",
    "            # If we don't have model columns in both dataframes, merge on target_id directly\n",
    "            logger.info(\"Missing model column in one or both DataFrames, merging on target_id only\")\n",
    "            combined_df = result_df.merge(\n",
    "                sifp_df,\n",
    "                left_on='target_id',\n",
    "                right_on='sifp_requirement_id',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "            logger.info(f\"Merged on target_id: {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "        \n",
    "        # Add error metrics if they don't exist\n",
    "        if 'sifp_abs_error' not in combined_df.columns:\n",
    "            combined_df['sifp_abs_error'] = (combined_df['sifp_final_total'] - combined_df['sifp_actor_total']).abs()\n",
    "        \n",
    "        if 'sifp_pct_error' not in combined_df.columns:\n",
    "            # Calculate percentage error, handling division by zero\n",
    "            combined_df['sifp_pct_error'] = np.nan\n",
    "            nonzero_mask = (combined_df['sifp_actor_total'] != 0) & combined_df['sifp_actor_total'].notna()\n",
    "            \n",
    "            combined_df.loc[nonzero_mask, 'sifp_pct_error'] = (\n",
    "                (combined_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                 combined_df.loc[nonzero_mask, 'sifp_actor_total']).abs() / \n",
    "                combined_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "            )\n",
    "        \n",
    "        # Log statistics by classification (after classification is applied)\n",
    "        if 'classification' in combined_df.columns:\n",
    "            logger.info(\"Calculating error metrics by classification:\")\n",
    "            for classification in combined_df['classification'].unique():\n",
    "                if pd.isna(classification):\n",
    "                    continue\n",
    "                    \n",
    "                class_df = combined_df[combined_df['classification'] == classification]\n",
    "                if len(class_df) > 0:\n",
    "                    logger.info(f\"  {classification}: {len(class_df)} links\")\n",
    "                    \n",
    "                    # Compute stats on valid data\n",
    "                    valid_data = class_df[class_df['sifp_abs_error'].notna()]\n",
    "                    if len(valid_data) > 0:\n",
    "                        logger.info(f\"    Mean Absolute Error: {valid_data['sifp_abs_error'].mean():.2f}\")\n",
    "                        \n",
    "                        pct_data = valid_data[valid_data['sifp_pct_error'].notna()]\n",
    "                        if len(pct_data) > 0:\n",
    "                            logger.info(f\"    Mean Percentage Error: {pct_data['sifp_pct_error'].mean():.2f}%\")\n",
    "                    else:\n",
    "                        logger.info(f\"    No valid SIFP data for {classification} links\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        logger.error(\"Required source_id and target_id columns missing from ground truth data\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the actual model name\n",
    "current_model_var = os.environ.get('CURRENT_MODEL', '')\n",
    "actual_model_name = os.environ.get(current_model_var, current_model_var)\n",
    "\n",
    "# Initialize best_thresholds_df as an empty DataFrame (will be filled later)\n",
    "best_thresholds_df = pd.DataFrame()\n",
    "\n",
    "# First, make sure meta_judge_df exists and has required data\n",
    "if 'meta_judge_df' not in globals() or globals()['meta_judge_df'] is None or globals()['meta_judge_df'].empty:\n",
    "    print(\"No meta judge data available. Please run previous cells first.\")\n",
    "else:\n",
    "    meta_judge_df = globals()['meta_judge_df']\n",
    "    \n",
    "    # Make sure ground_truth_df exists and has required data\n",
    "    if 'ground_truth_df' not in globals() or globals()['ground_truth_df'] is None or globals()['ground_truth_df'].empty:\n",
    "        print(\"No ground truth data available. Please run previous cells first.\")\n",
    "    else:\n",
    "        # Create a dataset for evaluation\n",
    "        ground_truth_df = globals()['ground_truth_df']\n",
    "        \n",
    "        print(f\"Preparing data for threshold evaluation:\")\n",
    "        print(f\"======================================\")\n",
    "        \n",
    "        # Create a set of ground truth link pairs for fast lookup\n",
    "        if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "            # Convert IDs to strings for consistent comparison\n",
    "            ground_truth_df['source_id'] = ground_truth_df['source_id'].astype(str)\n",
    "            ground_truth_df['target_id'] = ground_truth_df['target_id'].astype(str)\n",
    "            \n",
    "            ground_truth_pairs = set(zip(ground_truth_df['source_id'], ground_truth_df['target_id']))\n",
    "            print(f\"Created ground truth lookup set with {len(ground_truth_pairs)} link pairs\")\n",
    "            \n",
    "            # Convert source_id and target_id to strings in meta_judge_df too\n",
    "            meta_judge_df['source_id'] = meta_judge_df['source_id'].astype(str)\n",
    "            meta_judge_df['target_id'] = meta_judge_df['target_id'].astype(str)\n",
    "            \n",
    "            # Add ground_truth_traceable column to meta_judge_df\n",
    "            meta_judge_df['ground_truth_traceable'] = meta_judge_df.apply(\n",
    "                lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Add derived score columns if not already present\n",
    "            if 'judge_score' in meta_judge_df.columns and 'actor_score' in meta_judge_df.columns:\n",
    "                if 'total_score' not in meta_judge_df.columns:\n",
    "                    meta_judge_df['total_score'] = meta_judge_df['judge_score'] + meta_judge_df['actor_score']\n",
    "                \n",
    "                # Add alternative total scores for comparison\n",
    "                if 'final_score' in meta_judge_df.columns:\n",
    "                    if 'total_score_with_final' not in meta_judge_df.columns:\n",
    "                        meta_judge_df['total_score_with_final'] = meta_judge_df['actor_score'] + meta_judge_df['final_score']\n",
    "                    \n",
    "                    if 'total_score_all' not in meta_judge_df.columns:\n",
    "                        meta_judge_df['total_score_all'] = meta_judge_df['actor_score'] + meta_judge_df['judge_score'] + meta_judge_df['final_score']\n",
    "            \n",
    "            # Get list of all models\n",
    "            all_models = meta_judge_df['model'].unique()\n",
    "            \n",
    "            # Set OPTIMIZATION_METRIC if not already in globals\n",
    "            if 'OPTIMIZATION_METRIC' not in globals():\n",
    "                OPTIMIZATION_METRIC = 'F2'\n",
    "            else:\n",
    "                OPTIMIZATION_METRIC = globals()['OPTIMIZATION_METRIC']\n",
    "            \n",
    "            print(f\"Evaluating {len(all_models)} models using meta judge data\")\n",
    "            print(f\"Optimizing for {OPTIMIZATION_METRIC} score\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Define score columns to evaluate\n",
    "            score_columns_to_evaluate = [\n",
    "                'is_traceable',         # Boolean indicator\n",
    "                'actor_score',          # Individual score\n",
    "                'judge_score',          # Individual score\n",
    "                'final_score',          # Individual score\n",
    "                'total_score',          # judge_score + actor_score\n",
    "                'total_score_with_final',  # actor_score + final_score\n",
    "                'total_score_all'       # actor_score + judge_score + final_score\n",
    "            ]\n",
    "            \n",
    "            # Filter to only columns that exist\n",
    "            score_columns_to_evaluate = [\n",
    "                col for col in score_columns_to_evaluate \n",
    "                if col in meta_judge_df.columns and not meta_judge_df[col].isna().all()\n",
    "            ]\n",
    "            \n",
    "            # Evaluate models and score columns\n",
    "            evaluation_results = []\n",
    "            classified_df = meta_judge_df.copy()\n",
    "            \n",
    "            for model in all_models:\n",
    "                print(f\"Evaluating model: {model}\")\n",
    "                \n",
    "                # For each model, evaluate using different score columns\n",
    "                model_results = []\n",
    "                for score_column in score_columns_to_evaluate:\n",
    "                    print(f\"  Evaluating using {score_column}:\")\n",
    "                    result = evaluate_model_thresholds(\n",
    "                        meta_judge_df, \n",
    "                        model, \n",
    "                        score_column=score_column,\n",
    "                        optimize_for=OPTIMIZATION_METRIC\n",
    "                    )\n",
    "                    \n",
    "                    if result and 'best_threshold' in result:\n",
    "                        # Add score column to result\n",
    "                        result['score_column'] = score_column\n",
    "                        model_results.append(result)\n",
    "                        \n",
    "                        # Print key metrics\n",
    "                        print(f\"    - Best threshold: {result['best_threshold']:.3f}\")\n",
    "                        print(f\"    - Precision: {result['best_precision']:.3f}\")\n",
    "                        print(f\"    - Recall: {result['best_recall']:.3f}\")\n",
    "                        print(f\"    - F1: {result['best_f1']:.3f}\")\n",
    "                        print(f\"    - F2: {result['best_f2']:.3f}\")\n",
    "                        print(f\"    - MCC: {result['best_mcc']:.3f}\")\n",
    "                        \n",
    "                        # Apply classification to the dataframe\n",
    "                        classified_df = apply_classification_to_dataframe(classified_df, result)\n",
    "                \n",
    "                # Find best score column for this model based on optimization metric\n",
    "                if model_results:\n",
    "                    # Sort by the chosen optimization metric\n",
    "                    if OPTIMIZATION_METRIC == 'F1':\n",
    "                        model_results.sort(key=lambda x: x['best_f1'], reverse=True)\n",
    "                    else:  # F2\n",
    "                        model_results.sort(key=lambda x: x['best_f2'], reverse=True)\n",
    "                        \n",
    "                    best_result = model_results[0]\n",
    "                    print(f\"  Best performing score for {model}: {best_result['score_column']}\")\n",
    "                    print(f\"    - {OPTIMIZATION_METRIC} Score: {best_result['best_f2' if OPTIMIZATION_METRIC == 'F2' else 'best_f1']:.3f}\")\n",
    "                    \n",
    "                    # Add a 'best_model_score' classification column\n",
    "                    best_col_prefix = f\"{model}_{best_result['score_column']}\"\n",
    "                    best_classification_col = f\"{best_col_prefix}_classification\"\n",
    "                    classified_df[f\"{model}_best_classification\"] = classified_df[best_classification_col]\n",
    "                    \n",
    "                    evaluation_results.extend(model_results)\n",
    "            \n",
    "            # Create DataFrame of best thresholds with all metrics\n",
    "            if evaluation_results:\n",
    "                best_thresholds_df = pd.DataFrame([\n",
    "                    {\n",
    "                        'model_name': r['model_name'],\n",
    "                        'score_column': r['score_column'],\n",
    "                        'best_threshold': r['best_threshold'],\n",
    "                        'accuracy': r['best_accuracy'],\n",
    "                        'balanced_accuracy': r['best_balanced_accuracy'],\n",
    "                        'precision': r['best_precision'],\n",
    "                        'recall': r['best_recall'],\n",
    "                        'specificity': r['best_tnr'],\n",
    "                        'miss_rate': r['best_fnr'],\n",
    "                        'f1_score': r['best_f1'],\n",
    "                        'f2_score': r['best_f2'],\n",
    "                        'matthews_corr': r['best_mcc'],\n",
    "                        'true_positives': r['best_tp'],\n",
    "                        'false_positives': r['best_fp'],\n",
    "                        'false_negatives': r['best_fn'],\n",
    "                        'true_negatives': r['best_tn'],\n",
    "                        'data_points': r['data_points'],\n",
    "                        'ground_truth_positive': r['ground_truth_positive'],\n",
    "                        'ground_truth_negative': r['ground_truth_negative'],\n",
    "                    }\n",
    "                    for r in evaluation_results if 'best_threshold' in r\n",
    "                ])\n",
    "                \n",
    "                # Sort by the appropriate metric\n",
    "                sort_col = 'f1_score' if OPTIMIZATION_METRIC == 'F1' else 'f2_score'\n",
    "                best_thresholds_df = best_thresholds_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "                \n",
    "                print(\"Best Thresholds by Model and Score Column:\")\n",
    "                print(\"-\" * 80)\n",
    "                print(best_thresholds_df)\n",
    "                \n",
    "                # Add a simple 'classification' column that uses the best model's best score column\n",
    "                best_model_row = best_thresholds_df.iloc[0]\n",
    "                best_model = best_model_row['model_name']\n",
    "                best_score_col = best_model_row['score_column']\n",
    "                best_classification_col = f\"{best_model}_{best_score_col}_classification\"\n",
    "                \n",
    "                # Create the unified classification column\n",
    "                if best_classification_col in classified_df.columns:\n",
    "                    classified_df['classification'] = classified_df[best_classification_col]\n",
    "                    print(f\"Added 'classification' column using best model ({best_model}) and score column ({best_score_col})\")\n",
    "                    \n",
    "                    # Count by classification\n",
    "                    classification_counts = classified_df['classification'].value_counts()\n",
    "                    print(\"Classification Distribution:\")\n",
    "                    for cls, count in classification_counts.items():\n",
    "                        print(f\"  {cls}: {count} ({count/len(classified_df)*100:.2f}%)\")\n",
    "                \n",
    "                # Store the classified DataFrame for use in the next cell\n",
    "                globals()['classified_df'] = classified_df\n",
    "                globals()['best_thresholds_df'] = best_thresholds_df\n",
    "                \n",
    "                # If there's SIFP data available, create combined dataset\n",
    "                if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "                    print(\"Creating combined dataset with SIFP estimation data...\")\n",
    "                    combined_df = create_combined_dataset(\n",
    "                        classified_df, \n",
    "                        ground_truth_df, \n",
    "                        globals()['sifp_results_df']\n",
    "                    )\n",
    "                    \n",
    "                    if not combined_df.empty:\n",
    "                        globals()['combined_df'] = combined_df\n",
    "                        print(f\"Successfully created combined dataset with {len(combined_df)} rows\")\n",
    "                    else:\n",
    "                        print(\"Failed to create combined dataset\")\n",
    "            else:\n",
    "                print(\"No valid threshold results found.\")\n",
    "        else:\n",
    "            print(\"Required columns missing from ground truth data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Combine Classified Data with SIFP Estimates\n",
    "# Purpose: Merge TP/FP/FN classified links with SIFP estimation data for hallucination analysis\n",
    "# Dependencies: pandas, numpy, logging\n",
    "# Breadcrumbs: Model Evaluation -> Data Integration -> SIFP Mapping\n",
    "\n",
    "def combine_classification_with_sifp(classified_df=None, sifp_results_df=None, include_fn_as_hallucination=True):\n",
    "    \"\"\"\n",
    "    Combine classified traceability links (TP/FP/FN/TN) with SIFP estimates\n",
    "    \n",
    "    Parameters:\n",
    "        classified_df (pd.DataFrame, optional): DataFrame with classification results\n",
    "        sifp_results_df (pd.DataFrame, optional): DataFrame with SIFP estimates\n",
    "        include_fn_as_hallucination (bool): Whether to include FN as hallucination type (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset focusing on TP and hallucination links (FP+FN) with SIFP data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get required dataframes from globals if not provided\n",
    "        if classified_df is None or classified_df.empty:\n",
    "            if 'classified_df' in globals() and not globals()['classified_df'].empty:\n",
    "                classified_df = globals()['classified_df']\n",
    "            else:\n",
    "                logger.error(\"No classified data available\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        if sifp_results_df is None or sifp_results_df.empty:\n",
    "            if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "                sifp_results_df = globals()['sifp_results_df']\n",
    "            else:\n",
    "                logger.error(\"No SIFP estimates data available\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Create a copy of the classified dataframe\n",
    "        combined_df = classified_df.copy()\n",
    "        \n",
    "        # Ensure we have a classification column\n",
    "        if 'classification' not in combined_df.columns:\n",
    "            logger.error(\"Missing classification column in data\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter to TP, FP, and optionally FN links for hallucination analysis\n",
    "        if include_fn_as_hallucination:\n",
    "            # Include FN as a type of hallucination (missed features)\n",
    "            analysis_classifications = ['TP', 'FP', 'FN']\n",
    "            logger.info(\"Including FN (False Negatives) as hallucination type alongside FP\")\n",
    "        else:\n",
    "            # Original approach - only FP as hallucination\n",
    "            analysis_classifications = ['TP', 'FP']\n",
    "            logger.info(\"Using only FP (False Positives) as hallucination type\")\n",
    "        \n",
    "        tp_fp_fn_df = combined_df[combined_df['classification'].isin(analysis_classifications)].copy()\n",
    "        logger.info(f\"Filtered to {analysis_classifications} classifications: {len(tp_fp_fn_df)} rows from {len(combined_df)} total\")\n",
    "        \n",
    "        # Add hallucination indicator column\n",
    "        # TP = Non-hallucination (correct identification)\n",
    "        # FP = Hallucination (over-identification)  \n",
    "        # FN = Hallucination (missed features/under-identification)\n",
    "        tp_fp_fn_df['is_hallucination'] = tp_fp_fn_df['classification'].isin(['FP', 'FN'])\n",
    "        tp_fp_fn_df['hallucination_type'] = tp_fp_fn_df['classification'].apply(\n",
    "            lambda x: 'Correct_Identification' if x == 'TP' \n",
    "                     else 'Over_Identification' if x == 'FP'\n",
    "                     else 'Under_Identification' if x == 'FN'\n",
    "                     else 'Unknown'\n",
    "        )\n",
    "        \n",
    "        # Ensure target_id and model are strings for consistent comparison\n",
    "        tp_fp_fn_df['target_id'] = tp_fp_fn_df['target_id'].astype(str)\n",
    "        sifp_results_df['sifp_requirement_id'] = sifp_results_df['sifp_requirement_id'].astype(str)\n",
    "        \n",
    "        if 'model' in tp_fp_fn_df.columns and 'sifp_model' in sifp_results_df.columns:\n",
    "            tp_fp_fn_df['model'] = tp_fp_fn_df['model'].astype(str)\n",
    "            sifp_results_df['sifp_model'] = sifp_results_df['sifp_model'].astype(str)\n",
    "            \n",
    "            # Create merge keys that include both target_id and model\n",
    "            logger.info(\"Creating merge keys using both target_id and model\")\n",
    "            tp_fp_fn_df['merge_key'] = tp_fp_fn_df['target_id'] + '_' + tp_fp_fn_df['model']\n",
    "            sifp_results_df['merge_key'] = sifp_results_df['sifp_requirement_id'] + '_' + sifp_results_df['sifp_model']\n",
    "            \n",
    "            # Check for duplicates in merge keys\n",
    "            if sifp_results_df['merge_key'].duplicated().any():\n",
    "                logger.warning(f\"Found {sifp_results_df['merge_key'].duplicated().sum()} duplicate merge keys in SIFP estimates. Using first occurrence.\")\n",
    "                sifp_results_df = sifp_results_df.drop_duplicates(subset=['merge_key'], keep='first')\n",
    "            \n",
    "            # Perform the merge\n",
    "            merged_df = tp_fp_fn_df.merge(\n",
    "                sifp_results_df,\n",
    "                on='merge_key',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            logger.info(f\"Merged dataframe has {len(merged_df)} rows\")\n",
    "            merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            logger.info(f\"Found {merge_success} rows with valid SIFP data after merge ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "            \n",
    "            # If the merge didn't work well, try with just target_id\n",
    "            if merge_success < len(merged_df) * 0.5:\n",
    "                logger.warning(\"Poor merge results with target_id+model. Trying with just target_id.\")\n",
    "                \n",
    "                # Fallback to just target_id\n",
    "                merged_df = tp_fp_fn_df.merge(\n",
    "                    sifp_results_df.drop(columns=['merge_key']),\n",
    "                    left_on='target_id',\n",
    "                    right_on='sifp_requirement_id',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_sifp')\n",
    "                )\n",
    "                \n",
    "                merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "                logger.info(f\"Fallback merge with target_id: {merge_success} rows with valid SIFP data ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            # If we don't have model columns in both dataframes, merge on target_id directly\n",
    "            logger.info(\"Missing model column in one or both DataFrames, merging on target_id only\")\n",
    "            merged_df = tp_fp_fn_df.merge(\n",
    "                sifp_results_df,\n",
    "                left_on='target_id',\n",
    "                right_on='sifp_requirement_id',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            logger.info(f\"Merged on target_id: {merge_success} rows with valid SIFP data ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Add error metrics if they don't exist\n",
    "        if 'sifp_abs_error' not in merged_df.columns:\n",
    "            merged_df['sifp_abs_error'] = (merged_df['sifp_final_total'] - merged_df['sifp_actor_total']).abs()\n",
    "        \n",
    "        if 'sifp_pct_error' not in merged_df.columns:\n",
    "            # Calculate percentage error, handling division by zero\n",
    "            merged_df['sifp_pct_error'] = np.nan\n",
    "            nonzero_mask = (merged_df['sifp_actor_total'] != 0) & merged_df['sifp_actor_total'].notna()\n",
    "            \n",
    "            merged_df.loc[nonzero_mask, 'sifp_pct_error'] = (\n",
    "                (merged_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                 merged_df.loc[nonzero_mask, 'sifp_actor_total']).abs() / \n",
    "                merged_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "            )\n",
    "        \n",
    "        # Calculate aggregated error metrics by classification and hallucination type\n",
    "        if merge_success > 0:\n",
    "            logger.info(\"Calculating error metrics by classification and hallucination type:\")\n",
    "            \n",
    "            # By individual classification\n",
    "            for classification in analysis_classifications:\n",
    "                class_df = merged_df[merged_df['classification'] == classification]\n",
    "                if len(class_df) > 0:\n",
    "                    abs_error_mean = class_df['sifp_abs_error'].mean()\n",
    "                    pct_error_mean = class_df.loc[class_df['sifp_pct_error'].notna(), 'sifp_pct_error'].mean()\n",
    "                    \n",
    "                    logger.info(f\"  {classification}: {len(class_df)} links\")\n",
    "                    logger.info(f\"    Mean Absolute Error: {abs_error_mean:.2f}\")\n",
    "                    logger.info(f\"    Mean Percentage Error: {pct_error_mean:.2f}%\")\n",
    "            \n",
    "            # By hallucination status (TP vs All Hallucinations)\n",
    "            hallucination_groups = merged_df.groupby('is_hallucination')\n",
    "            for is_halluc, group_df in hallucination_groups:\n",
    "                group_name = \"Hallucinations (FP+FN)\" if is_halluc else \"Correct Identification (TP)\"\n",
    "                abs_error_mean = group_df['sifp_abs_error'].mean()\n",
    "                pct_error_mean = group_df.loc[group_df['sifp_pct_error'].notna(), 'sifp_pct_error'].mean()\n",
    "                \n",
    "                logger.info(f\"  {group_name}: {len(group_df)} links\")\n",
    "                logger.info(f\"    Mean Absolute Error: {abs_error_mean:.2f}\")\n",
    "                logger.info(f\"    Mean Percentage Error: {pct_error_mean:.2f}%\")\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining classified data with SIFP estimates: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run the combination process\n",
    "print(f\"Combining Classified Links with SIFP Estimates (Including FN as Hallucination):\") \n",
    "print(f\"===============================================================================\")\n",
    "\n",
    "combined_df = combine_classification_with_sifp(include_fn_as_hallucination=True)\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Display summary statistics\n",
    "    print(f\"Successfully created combined dataset with {len(combined_df)} rows!\")\n",
    "    \n",
    "    # Count rows by classification\n",
    "    if 'classification' in combined_df.columns:\n",
    "        classification_counts = combined_df['classification'].value_counts()\n",
    "        print(\"Distribution by classification:\")\n",
    "        for cls, count in classification_counts.items():\n",
    "            print(f\"  {cls}: {count} ({count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows by hallucination status\n",
    "    if 'is_hallucination' in combined_df.columns:\n",
    "        hallucination_counts = combined_df['is_hallucination'].value_counts()\n",
    "        print(\"Distribution by hallucination status:\")\n",
    "        print(f\"  Correct Identification (TP): {hallucination_counts.get(False, 0)} ({hallucination_counts.get(False, 0)/len(combined_df)*100:.1f}%)\")\n",
    "        print(f\"  Hallucinations (FP+FN): {hallucination_counts.get(True, 0)} ({hallucination_counts.get(True, 0)/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows by hallucination type\n",
    "    if 'hallucination_type' in combined_df.columns:\n",
    "        type_counts = combined_df['hallucination_type'].value_counts()\n",
    "        print(\"Distribution by hallucination type:\")\n",
    "        for htype, count in type_counts.items():\n",
    "            print(f\"  {htype}: {count} ({count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows with SIFP data\n",
    "    sifp_count = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().all(axis=1).sum()\n",
    "    print(f\"Rows with complete SIFP data: {sifp_count} ({sifp_count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show error metrics by classification\n",
    "    if sifp_count > 0 and 'classification' in combined_df.columns:\n",
    "        print(\"SIFP Error Metrics by Classification:\")\n",
    "        for classification in ['TP', 'FP', 'FN']:\n",
    "            if classification in combined_df['classification'].values:\n",
    "                class_df = combined_df[\n",
    "                    (combined_df['classification'] == classification) & \n",
    "                    combined_df[['sifp_abs_error', 'sifp_actor_total']].notna().all(axis=1)\n",
    "                ]\n",
    "                \n",
    "                if len(class_df) > 0:\n",
    "                    print(f\"  {classification} Links ({len(class_df)} rows):\")\n",
    "                    print(f\"    Mean SIFP Actor Total: {class_df['sifp_actor_total'].mean():.2f}\")\n",
    "                    print(f\"    Mean SIFP Final Total: {class_df['sifp_final_total'].mean():.2f}\")\n",
    "                    print(f\"    Mean Absolute Error: {class_df['sifp_abs_error'].mean():.2f}\")\n",
    "                    \n",
    "                    pct_error_df = class_df[class_df['sifp_pct_error'].notna()]\n",
    "                    if len(pct_error_df) > 0:\n",
    "                        print(f\"    Mean Percentage Error: {pct_error_df['sifp_pct_error'].mean():.2f}%\")\n",
    "        \n",
    "        # Show error metrics by hallucination status\n",
    "        print(\"SIFP Error Metrics by Hallucination Status:\")\n",
    "        for is_halluc, group_name in [(False, \"Correct Identification (TP)\"), (True, \"Hallucinations (FP+FN)\")]:\n",
    "            group_df = combined_df[\n",
    "                (combined_df['is_hallucination'] == is_halluc) & \n",
    "                combined_df[['sifp_abs_error', 'sifp_actor_total']].notna().all(axis=1)\n",
    "            ]\n",
    "            \n",
    "            if len(group_df) > 0:\n",
    "                print(f\"  {group_name} ({len(group_df)} rows):\")\n",
    "                print(f\"    Mean SIFP Actor Total: {group_df['sifp_actor_total'].mean():.2f}\")\n",
    "                print(f\"    Mean SIFP Final Total: {group_df['sifp_final_total'].mean():.2f}\")\n",
    "                print(f\"    Mean Absolute Error: {group_df['sifp_abs_error'].mean():.2f}\")\n",
    "                \n",
    "                pct_error_df = group_df[group_df['sifp_pct_error'].notna()]\n",
    "                if len(pct_error_df) > 0:\n",
    "                    print(f\"    Mean Percentage Error: {pct_error_df['sifp_pct_error'].mean():.2f}%\")\n",
    "    \n",
    "    # Display sample of the combined dataset\n",
    "    print(f\"Sample of combined dataset ({min(5, len(combined_df))} rows):\")\n",
    "    display_cols = ['classification', 'is_hallucination', 'hallucination_type', 'source_id', 'target_id', 'model', \n",
    "                    'sifp_actor_total', 'sifp_final_total', 'sifp_abs_error', 'sifp_pct_error']\n",
    "    display_cols = [col for col in display_cols if col in combined_df.columns]\n",
    "    print(combined_df[display_cols].head())\n",
    "    \n",
    "    # Store in global namespace for next cells\n",
    "    globals()['combined_df'] = combined_df\n",
    "else:\n",
    "    print(\"No combined dataset created. Check error logs.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Calculate Estimation Accuracy Metrics\n",
    "# Purpose: Compute accuracy metrics for SiFP estimations by classification type including FN hallucinations\n",
    "# Dependencies: pandas, numpy, sklearn.metrics\n",
    "# Breadcrumbs: Data Integration -> Accuracy Analysis -> SiFP Metrics\n",
    "\n",
    "def calculate_sifp_accuracy_metrics(df=None, group_by_classification=True, include_hallucination_analysis=True):\n",
    "    \"\"\"\n",
    "    Calculate accuracy metrics for SiFP estimations including hallucination analysis\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        group_by_classification (bool): Whether to calculate metrics by TP/FP/FN classification\n",
    "        include_hallucination_analysis (bool): Whether to include hallucination vs non-hallucination analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy metrics DataFrames and analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global combined_df if not provided\n",
    "        df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(\"No data available for calculating accuracy metrics\")\n",
    "            return {}\n",
    "        \n",
    "        # Filter to rows that have SIFP data\n",
    "        sifp_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total'])\n",
    "        \n",
    "        if sifp_df.empty:\n",
    "            logger.error(\"No SIFP data available for calculating accuracy metrics\")\n",
    "            return {}\n",
    "        \n",
    "        logger.info(f\"Calculating accuracy metrics on {len(sifp_df)} links with SIFP data\")\n",
    "        \n",
    "        # Define metric calculation function\n",
    "        def calculate_metrics(data):\n",
    "            if len(data) == 0:\n",
    "                return pd.Series({\n",
    "                    'count': 0,\n",
    "                    'mae': np.nan,\n",
    "                    'mse': np.nan,\n",
    "                    'rmse': np.nan,\n",
    "                    'mape': np.nan,\n",
    "                    'r2': np.nan,\n",
    "                    'pearson_r': np.nan,\n",
    "                    'pearson_p': np.nan,\n",
    "                    'mean_abs_error': np.nan,\n",
    "                    'mean_pct_error': np.nan,\n",
    "                    'std_abs_error': np.nan,\n",
    "                    'std_pct_error': np.nan,\n",
    "                    'mean_actor_total': np.nan,\n",
    "                    'mean_final_total': np.nan,\n",
    "                    'median_actor_total': np.nan,\n",
    "                    'median_final_total': np.nan\n",
    "                })\n",
    "            \n",
    "            # Basic metrics\n",
    "            y_true = data['sifp_actor_total']\n",
    "            y_pred = data['sifp_final_total']\n",
    "            \n",
    "            # Calculate standard ML metrics\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "            # Filter out zeros to avoid division by zero\n",
    "            nonzero_mask = y_true != 0\n",
    "            if nonzero_mask.sum() > 0:\n",
    "                mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            # R-squared\n",
    "            r2 = r2_score(y_true, y_pred) if len(data) > 1 else np.nan\n",
    "            \n",
    "            # Pearson correlation\n",
    "            if len(data) > 1:\n",
    "                pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n",
    "            else:\n",
    "                pearson_r, pearson_p = np.nan, np.nan\n",
    "            \n",
    "            # Custom metrics calculated earlier\n",
    "            abs_error = data['sifp_abs_error'].mean() if 'sifp_abs_error' in data.columns else np.nan\n",
    "            pct_error = data['sifp_pct_error'].mean() if 'sifp_pct_error' in data.columns else np.nan\n",
    "            \n",
    "            std_abs_error = data['sifp_abs_error'].std() if 'sifp_abs_error' in data.columns else np.nan\n",
    "            std_pct_error = data['sifp_pct_error'].std() if 'sifp_pct_error' in data.columns else np.nan\n",
    "            \n",
    "            # Basic statistics\n",
    "            mean_actor = y_true.mean()\n",
    "            mean_final = y_pred.mean()\n",
    "            median_actor = y_true.median()\n",
    "            median_final = y_pred.median()\n",
    "            \n",
    "            return pd.Series({\n",
    "                'count': len(data),\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape,\n",
    "                'r2': r2,\n",
    "                'pearson_r': pearson_r,\n",
    "                'pearson_p': pearson_p,\n",
    "                'mean_abs_error': abs_error,\n",
    "                'mean_pct_error': pct_error,\n",
    "                'std_abs_error': std_abs_error,\n",
    "                'std_pct_error': std_pct_error,\n",
    "                'mean_actor_total': mean_actor,\n",
    "                'mean_final_total': mean_final,\n",
    "                'median_actor_total': median_actor,\n",
    "                'median_final_total': median_final\n",
    "            })\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Calculate global metrics\n",
    "        global_metrics = calculate_metrics(sifp_df)\n",
    "        results['overall'] = pd.DataFrame([global_metrics], index=['Overall'])\n",
    "        \n",
    "        # Calculate metrics by individual classification if requested\n",
    "        if group_by_classification and 'classification' in sifp_df.columns:\n",
    "            # Group data by classification and calculate metrics for each group\n",
    "            classification_groups = ['TP', 'FP', 'FN']\n",
    "            available_classifications = [cls for cls in classification_groups if cls in sifp_df['classification'].values]\n",
    "            \n",
    "            if available_classifications:\n",
    "                grouped_metrics = sifp_df[sifp_df['classification'].isin(available_classifications)].groupby('classification').apply(calculate_metrics)\n",
    "                \n",
    "                # Create a DataFrame with all metrics\n",
    "                classification_metrics_df = pd.DataFrame([global_metrics]).T.rename(columns={0: 'Overall'})\n",
    "                \n",
    "                # For each classification group, add metrics to the DataFrame\n",
    "                for cls in grouped_metrics.index:\n",
    "                    classification_metrics_df[cls] = grouped_metrics.loc[cls]\n",
    "                \n",
    "                # Convert to a more readable format\n",
    "                classification_metrics_df = classification_metrics_df.T\n",
    "                \n",
    "                # Reorder index to put Overall first if it exists\n",
    "                if 'Overall' in classification_metrics_df.index:\n",
    "                    new_index = ['Overall'] + [idx for idx in classification_metrics_df.index if idx != 'Overall']\n",
    "                    classification_metrics_df = classification_metrics_df.reindex(new_index)\n",
    "                \n",
    "                results['by_classification'] = classification_metrics_df\n",
    "                logger.info(f\"Calculated accuracy metrics for {len(classification_metrics_df)} classification groups\")\n",
    "        \n",
    "        # Calculate metrics by hallucination status if requested\n",
    "        if include_hallucination_analysis and 'is_hallucination' in sifp_df.columns:\n",
    "            hallucination_metrics = sifp_df.groupby('is_hallucination').apply(calculate_metrics)\n",
    "            \n",
    "            # Create DataFrame with hallucination analysis\n",
    "            hallucination_metrics_df = pd.DataFrame()\n",
    "            \n",
    "            # Add metrics for each group\n",
    "            for is_halluc in hallucination_metrics.index:\n",
    "                group_name = \"Hallucinations_FP_FN\" if is_halluc else \"Correct_Identification_TP\"\n",
    "                hallucination_metrics_df[group_name] = hallucination_metrics.loc[is_halluc]\n",
    "            \n",
    "            # Add overall metrics\n",
    "            hallucination_metrics_df['Overall'] = global_metrics\n",
    "            \n",
    "            # Convert to readable format\n",
    "            hallucination_metrics_df = hallucination_metrics_df.T\n",
    "            \n",
    "            # Reorder to put Overall first\n",
    "            if 'Overall' in hallucination_metrics_df.index:\n",
    "                new_index = ['Overall'] + [idx for idx in hallucination_metrics_df.index if idx != 'Overall']\n",
    "                hallucination_metrics_df = hallucination_metrics_df.reindex(new_index)\n",
    "            \n",
    "            results['by_hallucination_status'] = hallucination_metrics_df\n",
    "            logger.info(f\"Calculated hallucination analysis metrics\")\n",
    "        \n",
    "        # Calculate metrics by hallucination type if available\n",
    "        if include_hallucination_analysis and 'hallucination_type' in sifp_df.columns:\n",
    "            hallucination_type_metrics = sifp_df.groupby('hallucination_type').apply(calculate_metrics)\n",
    "            \n",
    "            # Create DataFrame with hallucination type analysis\n",
    "            hallucination_type_metrics_df = pd.DataFrame()\n",
    "            \n",
    "            # Add metrics for each group\n",
    "            for htype in hallucination_type_metrics.index:\n",
    "                hallucination_type_metrics_df[htype] = hallucination_type_metrics.loc[htype]\n",
    "            \n",
    "            # Add overall metrics\n",
    "            hallucination_type_metrics_df['Overall'] = global_metrics\n",
    "            \n",
    "            # Convert to readable format\n",
    "            hallucination_type_metrics_df = hallucination_type_metrics_df.T\n",
    "            \n",
    "            # Reorder to put Overall first\n",
    "            if 'Overall' in hallucination_type_metrics_df.index:\n",
    "                new_index = ['Overall'] + [idx for idx in hallucination_type_metrics_df.index if idx != 'Overall']\n",
    "                hallucination_type_metrics_df = hallucination_type_metrics_df.reindex(new_index)\n",
    "            \n",
    "            results['by_hallucination_type'] = hallucination_type_metrics_df\n",
    "            logger.info(f\"Calculated hallucination type analysis metrics\")\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating accuracy metrics: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "try:\n",
    "    accuracy_results = calculate_sifp_accuracy_metrics(include_hallucination_analysis=True)\n",
    "    print(f\"SIFP Estimation Accuracy Metrics with Hallucination Analysis:\")\n",
    "    print(f\"============================================================\")\n",
    "    \n",
    "    if accuracy_results:\n",
    "        # Display overall metrics\n",
    "        if 'overall' in accuracy_results:\n",
    "            print(\"Overall Metrics:\")\n",
    "            print(accuracy_results['overall'].round(2))\n",
    "            print()\n",
    "        \n",
    "        # Display metrics by individual classification (TP, FP, FN)\n",
    "        if 'by_classification' in accuracy_results:\n",
    "            print(\"Metrics by Individual Classification:\")\n",
    "            display_metrics = accuracy_results['by_classification'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = display_metrics.select_dtypes(include=[np.number]).columns\n",
    "            display_metrics[numeric_cols] = display_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(display_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Compare TP vs FP vs FN metrics\n",
    "            available_classes = [cls for cls in ['TP', 'FP', 'FN'] if cls in display_metrics.index]\n",
    "            if len(available_classes) >= 2:\n",
    "                print(\"Classification Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape', 'r2']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in display_metrics.columns]\n",
    "                \n",
    "                class_comparison = display_metrics.loc[available_classes, comparison_metrics]\n",
    "                print(class_comparison)\n",
    "                print()\n",
    "        \n",
    "        # Display metrics by hallucination status (TP vs FP+FN)\n",
    "        if 'by_hallucination_status' in accuracy_results:\n",
    "            print(\"Metrics by Hallucination Status (KEY ANALYSIS):\")\n",
    "            halluc_metrics = accuracy_results['by_hallucination_status'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = halluc_metrics.select_dtypes(include=[np.number]).columns\n",
    "            halluc_metrics[numeric_cols] = halluc_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(halluc_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Calculate differences between TP and Hallucinations\n",
    "            if 'Correct_Identification_TP' in halluc_metrics.index and 'Hallucinations_FP_FN' in halluc_metrics.index:\n",
    "                print(\"TP vs Hallucinations (FP+FN) Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape', 'r2']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in halluc_metrics.columns]\n",
    "                \n",
    "                tp_halluc_comparison = halluc_metrics.loc[['Correct_Identification_TP', 'Hallucinations_FP_FN'], comparison_metrics]\n",
    "                \n",
    "                # Calculate the difference and percentage difference\n",
    "                if not tp_halluc_comparison.empty:\n",
    "                    diff = tp_halluc_comparison.loc['Correct_Identification_TP'] - tp_halluc_comparison.loc['Hallucinations_FP_FN']\n",
    "                    pct_diff = ((tp_halluc_comparison.loc['Correct_Identification_TP'] - tp_halluc_comparison.loc['Hallucinations_FP_FN']) / \n",
    "                               tp_halluc_comparison.loc['Hallucinations_FP_FN'] * 100).round(1)\n",
    "                    \n",
    "                    # Skip count column for percentage difference\n",
    "                    if 'count' in pct_diff.index:\n",
    "                        pct_diff['count'] = np.nan\n",
    "                    \n",
    "                    # Add comparison rows to the DataFrame\n",
    "                    tp_halluc_comparison.loc['Difference (TP - Hallucinations)'] = diff\n",
    "                    tp_halluc_comparison.loc['% Difference'] = pct_diff\n",
    "                    \n",
    "                    # Format and display\n",
    "                    print(tp_halluc_comparison.round(2))\n",
    "                    print()\n",
    "                    \n",
    "                    # Highlight key insights for hypothesis testing\n",
    "                    print(\"Key Insights for Hypothesis 2:\")\n",
    "                    for metric in ['mean_abs_error', 'mean_pct_error', 'rmse', 'mape']:\n",
    "                        if metric in diff.index and not np.isnan(diff[metric]):\n",
    "                            if diff[metric] < 0:\n",
    "                                print(f\"- TP links have {abs(diff[metric]):.2f} LOWER {metric} than Hallucination links (FP+FN) - {abs(pct_diff[metric]):.1f}% better\")\n",
    "                                print(f\"  → SUPPORTS hypothesis: Hallucinations correlate with HIGHER estimation errors\")\n",
    "                            elif diff[metric] > 0:\n",
    "                                print(f\"- TP links have {diff[metric]:.2f} HIGHER {metric} than Hallucination links (FP+FN) - {pct_diff[metric]:.1f}% worse\")\n",
    "                                print(f\"  → CONTRADICTS hypothesis: Hallucinations correlate with LOWER estimation errors\")\n",
    "                            else:\n",
    "                                print(f\"- TP and Hallucination links show NO difference in {metric}\")\n",
    "        \n",
    "        # Display metrics by hallucination type (Over vs Under identification)\n",
    "        if 'by_hallucination_type' in accuracy_results:\n",
    "            print(\"Metrics by Hallucination Type:\")\n",
    "            type_metrics = accuracy_results['by_hallucination_type'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = type_metrics.select_dtypes(include=[np.number]).columns\n",
    "            type_metrics[numeric_cols] = type_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(type_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Compare Over vs Under identification if both exist\n",
    "            if 'Over_Identification' in type_metrics.index and 'Under_Identification' in type_metrics.index:\n",
    "                print(\"Over-Identification (FP) vs Under-Identification (FN) Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in type_metrics.columns]\n",
    "                \n",
    "                over_under_comparison = type_metrics.loc[['Over_Identification', 'Under_Identification'], comparison_metrics]\n",
    "                print(over_under_comparison)\n",
    "                print()\n",
    "        \n",
    "        # Store results in global namespace for next cells\n",
    "        globals()['accuracy_results'] = accuracy_results\n",
    "        \n",
    "        # Also store the main metrics DataFrame for backward compatibility\n",
    "        if 'by_classification' in accuracy_results:\n",
    "            globals()['accuracy_metrics'] = accuracy_results['by_classification']\n",
    "        elif 'overall' in accuracy_results:\n",
    "            globals()['accuracy_metrics'] = accuracy_results['overall']\n",
    "    else:\n",
    "        print(\"No accuracy metrics could be calculated. Check logs for details.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during accuracy metrics calculation: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Compare TP vs Hallucinations (FP+FN) Estimation Accuracy\n",
    "# Purpose: Analyze and compare SiFP accuracy between TP links and Hallucination links (FP+FN) with statistical tests\n",
    "# Dependencies: pandas, numpy, scipy.stats\n",
    "# Breadcrumbs: Accuracy Analysis -> Statistical Comparison -> Hypothesis 2 Testing\n",
    "\n",
    "def compare_tp_vs_hallucinations_accuracy(df=None, accuracy_results=None):\n",
    "    \"\"\"\n",
    "    Perform detailed comparison of SiFP accuracy between TP links and Hallucination links (FP+FN)\n",
    "    Core analysis for Hypothesis 2: \"Hallucinations significantly correlate with lower quality estimates\"\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        accuracy_results (dict, optional): Accuracy results from previous cell\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing comprehensive comparison results and statistical tests\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global variables if not provided\n",
    "        df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(\"No combined data available for TP vs Hallucinations comparison\")\n",
    "            return {}\n",
    "            \n",
    "        # Filter for rows with SIFP data and classification information\n",
    "        valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "        valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "        \n",
    "        if len(valid_df) == 0:\n",
    "            logger.error(\"No valid TP, FP, or FN data with SIFP estimations found\")\n",
    "            return {}\n",
    "            \n",
    "        # Split into TP (non-hallucination) and Hallucination (FP+FN) datasets\n",
    "        tp_df = valid_df[valid_df['is_hallucination'] == False]  # TP links\n",
    "        hallucination_df = valid_df[valid_df['is_hallucination'] == True]  # FP + FN links\n",
    "        \n",
    "        logger.info(f\"Comparing {len(tp_df)} TP links with {len(hallucination_df)} Hallucination links (FP+FN)\")\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            'hypothesis': \"Hallucinations in LLM outputs significantly correlate with lower quality estimates\",\n",
    "            'comparison_type': 'TP_vs_Hallucinations_FP_FN',\n",
    "            'counts': {\n",
    "                'TP': len(tp_df),\n",
    "                'Hallucinations_FP_FN': len(hallucination_df),\n",
    "                'FP_only': len(valid_df[valid_df['classification'] == 'FP']),\n",
    "                'FN_only': len(valid_df[valid_df['classification'] == 'FN']),\n",
    "                'total': len(valid_df)\n",
    "            },\n",
    "            'metrics': {},\n",
    "            'statistical_tests': {},\n",
    "            'effect_sizes': {},\n",
    "            'hypothesis_conclusion': {}\n",
    "        }\n",
    "        \n",
    "        # Check if we have enough data for meaningful comparison\n",
    "        if len(tp_df) < 5 or len(hallucination_df) < 5:\n",
    "            logger.warning(\"Sample size too small for reliable statistical comparison\")\n",
    "            results['warning'] = \"Sample size too small for reliable statistical comparison\"\n",
    "            return results\n",
    "            \n",
    "        # Extract metrics for comparison\n",
    "        metrics_to_compare = ['sifp_abs_error', 'sifp_pct_error']\n",
    "        \n",
    "        for metric in metrics_to_compare:\n",
    "            if metric in tp_df.columns and metric in hallucination_df.columns:\n",
    "                # Get the data\n",
    "                tp_values = tp_df[metric].dropna()\n",
    "                halluc_values = hallucination_df[metric].dropna()\n",
    "                \n",
    "                if len(tp_values) == 0 or len(halluc_values) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Basic descriptive statistics\n",
    "                results['metrics'][metric] = {\n",
    "                    'TP': {\n",
    "                        'mean': tp_values.mean(),\n",
    "                        'median': tp_values.median(),\n",
    "                        'std': tp_values.std(),\n",
    "                        'count': len(tp_values)\n",
    "                    },\n",
    "                    'Hallucinations_FP_FN': {\n",
    "                        'mean': halluc_values.mean(),\n",
    "                        'median': halluc_values.median(),\n",
    "                        'std': halluc_values.std(),\n",
    "                        'count': len(halluc_values)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Calculate difference and hypothesis direction\n",
    "                mean_diff = tp_values.mean() - halluc_values.mean()\n",
    "                pct_diff = (mean_diff / halluc_values.mean()) * 100 if halluc_values.mean() != 0 else np.nan\n",
    "                    \n",
    "                results['metrics'][metric]['difference'] = {\n",
    "                    'absolute': mean_diff,\n",
    "                    'percent': pct_diff\n",
    "                }\n",
    "                \n",
    "                # Hypothesis support check: mean_diff < 0 means TP has lower error → SUPPORTS hypothesis\n",
    "                hypothesis_support = mean_diff < 0\n",
    "                results['metrics'][metric]['hypothesis_support'] = {\n",
    "                    'supports_hypothesis': hypothesis_support,\n",
    "                    'interpretation': f\"TP links have {'LOWER' if hypothesis_support else 'HIGHER'} {metric} than Hallucinations\"\n",
    "                }\n",
    "                \n",
    "                # Statistical tests\n",
    "                results['statistical_tests'][metric] = {}\n",
    "                \n",
    "                # Check for normality with Shapiro-Wilk test\n",
    "                if len(tp_values) <= 5000 and len(halluc_values) <= 5000:\n",
    "                    try:\n",
    "                        _, tp_norm_p = stats.shapiro(tp_values)\n",
    "                        _, halluc_norm_p = stats.shapiro(halluc_values)\n",
    "                        tp_is_normal = tp_norm_p > 0.05\n",
    "                        halluc_is_normal = halluc_norm_p > 0.05\n",
    "                        both_normal = tp_is_normal and halluc_is_normal\n",
    "                    except Exception:\n",
    "                        both_normal = False\n",
    "                else:\n",
    "                    both_normal = False\n",
    "                \n",
    "                results['statistical_tests'][metric]['normality'] = {\n",
    "                    'both_normal': both_normal\n",
    "                }\n",
    "                \n",
    "                # Main comparison tests\n",
    "                if both_normal:\n",
    "                    # Parametric: t-test\n",
    "                    try:\n",
    "                        _, levene_p = stats.levene(tp_values, halluc_values)\n",
    "                        equal_var = levene_p > 0.05\n",
    "                        t_stat, t_p = stats.ttest_ind(tp_values, halluc_values, equal_var=equal_var)\n",
    "                        \n",
    "                        results['statistical_tests'][metric]['parametric'] = {\n",
    "                            'test': 't-test',\n",
    "                            'statistic': t_stat,\n",
    "                            'p_value': t_p,\n",
    "                            'significant': t_p < 0.05,\n",
    "                            'hypothesis_result': 'SUPPORTS' if (t_p < 0.05 and hypothesis_support) else 'CONTRADICTS' if (t_p < 0.05 and not hypothesis_support) else 'INCONCLUSIVE'\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"t-test failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                # Non-parametric: Mann-Whitney U test\n",
    "                try:\n",
    "                    u_stat, u_p = stats.mannwhitneyu(tp_values, halluc_values, alternative='two-sided')\n",
    "                    \n",
    "                    results['statistical_tests'][metric]['non_parametric'] = {\n",
    "                        'test': 'Mann-Whitney U',\n",
    "                        'statistic': u_stat,\n",
    "                        'p_value': u_p,\n",
    "                        'significant': u_p < 0.05,\n",
    "                        'hypothesis_result': 'SUPPORTS' if (u_p < 0.05 and hypothesis_support) else 'CONTRADICTS' if (u_p < 0.05 and not hypothesis_support) else 'INCONCLUSIVE'\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Mann-Whitney U test failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                # Effect sizes\n",
    "                effect_sizes = {}\n",
    "                \n",
    "                # Cohen's d\n",
    "                try:\n",
    "                    pooled_std = np.sqrt(((len(tp_values) - 1) * tp_values.std()**2 + \n",
    "                                         (len(halluc_values) - 1) * halluc_values.std()**2) / \n",
    "                                         (len(tp_values) + len(halluc_values) - 2))\n",
    "                    if pooled_std > 0:\n",
    "                        cohen_d = mean_diff / pooled_std\n",
    "                        \n",
    "                        if abs(cohen_d) < 0.2:\n",
    "                            effect_interpretation = \"Negligible\"\n",
    "                        elif abs(cohen_d) < 0.5:\n",
    "                            effect_interpretation = \"Small\"\n",
    "                        elif abs(cohen_d) < 0.8:\n",
    "                            effect_interpretation = \"Medium\"\n",
    "                        else:\n",
    "                            effect_interpretation = \"Large\"\n",
    "                        \n",
    "                        effect_sizes['cohen_d'] = {\n",
    "                            'value': cohen_d,\n",
    "                            'interpretation': effect_interpretation\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Cohen's d calculation failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                results['effect_sizes'][metric] = effect_sizes\n",
    "        \n",
    "        # Overall hypothesis conclusion\n",
    "        significant_results = []\n",
    "        \n",
    "        for metric, tests in results['statistical_tests'].items():\n",
    "            for test_type in ['parametric', 'non_parametric']:\n",
    "                if test_type in tests and tests[test_type]['significant']:\n",
    "                    significant_results.append({\n",
    "                        'metric': metric,\n",
    "                        'test': tests[test_type]['test'],\n",
    "                        'p_value': tests[test_type]['p_value'],\n",
    "                        'hypothesis_result': tests[test_type]['hypothesis_result']\n",
    "                    })\n",
    "        \n",
    "        # Determine overall conclusion\n",
    "        support_count = len([r for r in significant_results if r['hypothesis_result'] == 'SUPPORTS'])\n",
    "        contradict_count = len([r for r in significant_results if r['hypothesis_result'] == 'CONTRADICTS'])\n",
    "        \n",
    "        if support_count > contradict_count and support_count > 0:\n",
    "            overall_conclusion = \"HYPOTHESIS SUPPORTED\"\n",
    "        elif contradict_count > support_count and contradict_count > 0:\n",
    "            overall_conclusion = \"HYPOTHESIS CONTRADICTED\" \n",
    "        elif support_count == contradict_count and support_count > 0:\n",
    "            overall_conclusion = \"MIXED EVIDENCE\"\n",
    "        else:\n",
    "            overall_conclusion = \"INSUFFICIENT EVIDENCE\"\n",
    "        \n",
    "        results['hypothesis_conclusion'] = {\n",
    "            'overall_result': overall_conclusion,\n",
    "            'significant_supporting_tests': support_count,\n",
    "            'significant_contradicting_tests': contradict_count,\n",
    "            'total_significant_tests': len(significant_results),\n",
    "            'significant_results': significant_results\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error comparing TP vs Hallucinations accuracy: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Run the comprehensive comparison for Hypothesis 2\n",
    "try:\n",
    "    tp_vs_hallucinations_results = compare_tp_vs_hallucinations_accuracy()\n",
    "    print(f\"HYPOTHESIS 2 TESTING: TP vs Hallucinations (FP+FN) Accuracy Comparison\")\n",
    "    print(f\"======================================================================\")\n",
    "    \n",
    "    if tp_vs_hallucinations_results:\n",
    "        # Display basic counts\n",
    "        if 'counts' in tp_vs_hallucinations_results:\n",
    "            counts = tp_vs_hallucinations_results['counts']\n",
    "            print(f\"Sample Sizes:\")\n",
    "            print(f\"- TP (Correct Identification): {counts['TP']} links\")\n",
    "            print(f\"- Hallucinations (FP+FN): {counts['Hallucinations_FP_FN']} links\")\n",
    "            print(f\"  └─ FP (Over-identification): {counts['FP_only']} links\")\n",
    "            print(f\"  └─ FN (Under-identification): {counts['FN_only']} links\")\n",
    "            print()\n",
    "        \n",
    "        # Display descriptive statistics comparison\n",
    "        if 'metrics' in tp_vs_hallucinations_results:\n",
    "            print(\"Descriptive Statistics Comparison:\")\n",
    "            for metric, data in tp_vs_hallucinations_results['metrics'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                print(f\"    TP Links:         {data['TP']['mean']:.3f} ± {data['TP']['std']:.3f}\")\n",
    "                print(f\"    Hallucinations:   {data['Hallucinations_FP_FN']['mean']:.3f} ± {data['Hallucinations_FP_FN']['std']:.3f}\")\n",
    "                \n",
    "                diff = data['difference']\n",
    "                hypothesis_support = data['hypothesis_support']['supports_hypothesis']\n",
    "                direction = \"LOWER\" if diff['absolute'] < 0 else \"HIGHER\"\n",
    "                \n",
    "                print(f\"    Difference:       TP has {abs(diff['absolute']):.3f} {direction} error\")\n",
    "                print(f\"    Hypothesis:       {'✓ SUPPORTS' if hypothesis_support else '✗ CONTRADICTS'}\")\n",
    "                print()\n",
    "        \n",
    "        # Display statistical tests\n",
    "        if 'statistical_tests' in tp_vs_hallucinations_results:\n",
    "            print(\"Statistical Significance Tests:\")\n",
    "            for metric, tests in tp_vs_hallucinations_results['statistical_tests'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                \n",
    "                for test_type in ['parametric', 'non_parametric']:\n",
    "                    if test_type in tests:\n",
    "                        test_info = tests[test_type]\n",
    "                        significance = \"significant\" if test_info['significant'] else \"not significant\"\n",
    "                        \n",
    "                        print(f\"    {test_info['test']}: p={test_info['p_value']:.6f} ({significance})\")\n",
    "                        print(f\"      → {test_info['hypothesis_result']}\")\n",
    "                print()\n",
    "        \n",
    "        # Display effect sizes\n",
    "        if 'effect_sizes' in tp_vs_hallucinations_results:\n",
    "            print(\"Effect Size Analysis:\")\n",
    "            for metric, effects in tp_vs_hallucinations_results['effect_sizes'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                if 'cohen_d' in effects:\n",
    "                    cohen = effects['cohen_d']\n",
    "                    print(f\"    Cohen's d: {cohen['value']:.3f} ({cohen['interpretation']} effect)\")\n",
    "                print()\n",
    "        \n",
    "        # Display hypothesis conclusion\n",
    "        if 'hypothesis_conclusion' in tp_vs_hallucinations_results:\n",
    "            conclusion = tp_vs_hallucinations_results['hypothesis_conclusion']\n",
    "            print(\"=\"*80)\n",
    "            print(\"HYPOTHESIS 2 CONCLUSION:\")\n",
    "            print(f\"Overall Result: {conclusion['overall_result']}\")\n",
    "            print(f\"Supporting Evidence: {conclusion['significant_supporting_tests']} significant test(s)\")\n",
    "            print(f\"Contradicting Evidence: {conclusion['significant_contradicting_tests']} significant test(s)\")\n",
    "            print()\n",
    "            \n",
    "            # Provide interpretation\n",
    "            if conclusion['overall_result'] == \"HYPOTHESIS SUPPORTED\":\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"✓ Evidence supports that hallucinations (both FP and FN) significantly\")\n",
    "                print(\"  correlate with lower quality SiFP estimates in new product development.\")\n",
    "            elif conclusion['overall_result'] == \"HYPOTHESIS CONTRADICTED\":\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"✗ Evidence contradicts the hypothesis - hallucinations appear to correlate\")\n",
    "                print(\"  with BETTER or equivalent SiFP estimation quality.\")\n",
    "            else:\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"? Mixed or insufficient evidence to draw strong conclusions.\")\n",
    "        \n",
    "        # Store results for further analysis\n",
    "        globals()['tp_vs_hallucinations_results'] = tp_vs_hallucinations_results\n",
    "    else:\n",
    "        print(\"No comparison data available. Check logs for details.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during TP vs Hallucinations comparison: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Enhanced Statistical Validation for Hypothesis 2\n",
    "# Purpose: Add comprehensive statistical measures including confidence intervals, power analysis, and robust testing\n",
    "# Dependencies: pandas, numpy, scipy.stats, statsmodels\n",
    "# Breadcrumbs: Statistical Comparison -> Enhanced Validation -> Robust Hypothesis Testing\n",
    "\n",
    "def enhanced_statistical_validation(df=None, alpha=0.05, bootstrap_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform enhanced statistical validation for Hypothesis 2 with comprehensive measures\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        alpha (float): Significance level (default: 0.05)\n",
    "        bootstrap_samples (int): Number of bootstrap samples for confidence intervals\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive statistical validation results\n",
    "    \"\"\"\n",
    "    # Use global variables if not provided\n",
    "    df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(\"No data available for enhanced statistical validation\")\n",
    "        return {}\n",
    "    \n",
    "    # Filter for valid data\n",
    "    valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "    valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        logger.error(\"No valid data for enhanced statistical validation\")\n",
    "        return {}\n",
    "    \n",
    "    # Split groups\n",
    "    tp_df = valid_df[valid_df['is_hallucination'] == False]\n",
    "    hallucination_df = valid_df[valid_df['is_hallucination'] == True]\n",
    "    \n",
    "    results = {\n",
    "        'confidence_intervals': {},\n",
    "        'power_analysis': {},\n",
    "        'bootstrap_results': {},\n",
    "        'correlation_analysis': {},\n",
    "        'additional_effect_sizes': {},\n",
    "        'assumption_testing': {},\n",
    "        'multiple_testing_correction': {},\n",
    "        'sample_adequacy': {}\n",
    "    }\n",
    "    \n",
    "    metrics_to_test = ['sifp_abs_error', 'sifp_pct_error']\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric not in tp_df.columns or metric not in hallucination_df.columns:\n",
    "            continue\n",
    "            \n",
    "        tp_values = tp_df[metric].dropna()\n",
    "        halluc_values = hallucination_df[metric].dropna()\n",
    "        \n",
    "        if len(tp_values) < 3 or len(halluc_values) < 3:\n",
    "            continue\n",
    "        \n",
    "        results[metric] = {}\n",
    "        \n",
    "        # 1. CONFIDENCE INTERVALS\n",
    "        # Bootstrap confidence intervals for mean difference\n",
    "        def bootstrap_mean_diff(tp_vals, halluc_vals, n_samples=bootstrap_samples):\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            diffs = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                tp_boot = np.random.choice(tp_vals, size=len(tp_vals), replace=True)\n",
    "                halluc_boot = np.random.choice(halluc_vals, size=len(halluc_vals), replace=True)\n",
    "                diffs.append(np.mean(tp_boot) - np.mean(halluc_boot))\n",
    "            \n",
    "            diffs = np.array(diffs)\n",
    "            ci_lower = np.percentile(diffs, (alpha/2) * 100)\n",
    "            ci_upper = np.percentile(diffs, (1 - alpha/2) * 100)\n",
    "            \n",
    "            return {\n",
    "                'mean_difference': np.mean(diffs),\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'contains_zero': ci_lower <= 0 <= ci_upper,\n",
    "                'confidence_level': (1 - alpha) * 100\n",
    "            }\n",
    "        \n",
    "        bootstrap_ci = bootstrap_mean_diff(tp_values, halluc_values)\n",
    "        results['confidence_intervals'][metric] = bootstrap_ci\n",
    "        \n",
    "        # Parametric CI for mean difference\n",
    "        mean_diff = tp_values.mean() - halluc_values.mean()\n",
    "        se_diff = np.sqrt(tp_values.var()/len(tp_values) + halluc_values.var()/len(halluc_values))\n",
    "        \n",
    "        if len(tp_values) >= 30 and len(halluc_values) >= 30:\n",
    "            # Large sample: use z-distribution\n",
    "            z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "            margin_error = z_critical * se_diff\n",
    "            method = 'Large_sample_z'\n",
    "        else:\n",
    "            # Small sample: use t-distribution (Welch's)\n",
    "            df_welch = (tp_values.var()/len(tp_values) + halluc_values.var()/len(halluc_values))**2 / \\\n",
    "                      ((tp_values.var()/len(tp_values))**2/(len(tp_values)-1) + \n",
    "                       (halluc_values.var()/len(halluc_values))**2/(len(halluc_values)-1))\n",
    "            t_critical = stats.t.ppf(1 - alpha/2, df_welch)\n",
    "            margin_error = t_critical * se_diff\n",
    "            method = 'Welch_t'\n",
    "        \n",
    "        parametric_ci = {\n",
    "            'mean_difference': mean_diff,\n",
    "            'ci_lower': mean_diff - margin_error,\n",
    "            'ci_upper': mean_diff + margin_error,\n",
    "            'contains_zero': (mean_diff - margin_error) <= 0 <= (mean_diff + margin_error),\n",
    "            'method': method\n",
    "        }\n",
    "        \n",
    "        results['confidence_intervals'][metric]['parametric'] = parametric_ci\n",
    "        \n",
    "        # 2. POWER ANALYSIS (Fixed implementation)\n",
    "        # Calculate observed effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(tp_values) - 1) * tp_values.var() + \n",
    "                             (len(halluc_values) - 1) * halluc_values.var()) / \n",
    "                             (len(tp_values) + len(halluc_values) - 2))\n",
    "        \n",
    "        if pooled_std > 0:\n",
    "            observed_effect_size = abs(tp_values.mean() - halluc_values.mean()) / pooled_std\n",
    "            total_n = len(tp_values) + len(halluc_values)\n",
    "            \n",
    "            # Calculate achieved power using ttest_power\n",
    "            achieved_power = ttest_power(effect_size=observed_effect_size,\n",
    "                                       nobs=total_n,\n",
    "                                       alpha=alpha,\n",
    "                                       alternative='two-sided')\n",
    "            \n",
    "            # Calculate required sample size for 80% power using simplified approach\n",
    "            # For two-sample t-test with equal sample sizes\n",
    "            n_groups = 2\n",
    "            power_target = 0.8\n",
    "            \n",
    "            # Use iterative approach to find required sample size\n",
    "            required_total_n = total_n\n",
    "            for test_n in range(10, 1000, 5):\n",
    "                test_power = ttest_power(effect_size=observed_effect_size,\n",
    "                                       nobs=test_n,\n",
    "                                       alpha=alpha,\n",
    "                                       alternative='two-sided')\n",
    "                if test_power >= power_target:\n",
    "                    required_total_n = test_n\n",
    "                    break\n",
    "            \n",
    "            # Calculate required sample size per group (assuming equal allocation)\n",
    "            required_n_per_group = required_total_n / 2\n",
    "            \n",
    "            results['power_analysis'][metric] = {\n",
    "                'observed_effect_size': observed_effect_size,\n",
    "                'achieved_power': achieved_power,\n",
    "                'required_total_n_for_80_power': required_total_n,\n",
    "                'required_n_per_group_for_80_power': required_n_per_group,\n",
    "                'current_n_tp': len(tp_values),\n",
    "                'current_n_halluc': len(halluc_values),\n",
    "                'current_total_n': total_n,\n",
    "                'adequate_power': achieved_power >= 0.8\n",
    "            }\n",
    "        \n",
    "        # 3. ADDITIONAL EFFECT SIZES\n",
    "        additional_effects = {}\n",
    "        \n",
    "        # Cliff's Delta (robust, non-parametric effect size)\n",
    "        cliff_delta_sum = 0\n",
    "        for tp_val in tp_values:\n",
    "            for halluc_val in halluc_values:\n",
    "                if tp_val > halluc_val:\n",
    "                    cliff_delta_sum += 1\n",
    "                elif tp_val < halluc_val:\n",
    "                    cliff_delta_sum -= 1\n",
    "        \n",
    "        cliff_delta = cliff_delta_sum / (len(tp_values) * len(halluc_values))\n",
    "        \n",
    "        if abs(cliff_delta) < 0.147:\n",
    "            cliff_interpretation = \"Negligible\"\n",
    "        elif abs(cliff_delta) < 0.33:\n",
    "            cliff_interpretation = \"Small\"\n",
    "        elif abs(cliff_delta) < 0.474:\n",
    "            cliff_interpretation = \"Medium\"\n",
    "        else:\n",
    "            cliff_interpretation = \"Large\"\n",
    "        \n",
    "        additional_effects['cliff_delta'] = {\n",
    "            'value': cliff_delta,\n",
    "            'interpretation': cliff_interpretation\n",
    "        }\n",
    "        \n",
    "        # Glass's Delta\n",
    "        if halluc_values.std() > 0:\n",
    "            glass_delta = (tp_values.mean() - halluc_values.mean()) / halluc_values.std()\n",
    "            additional_effects['glass_delta'] = {\n",
    "                'value': glass_delta,\n",
    "                'interpretation': \"Control group (Hallucinations) as reference\"\n",
    "            }\n",
    "        \n",
    "        results['additional_effect_sizes'][metric] = additional_effects\n",
    "        \n",
    "        # 4. CORRELATION ANALYSIS\n",
    "        # Point-biserial correlation\n",
    "        combined_values = np.concatenate([tp_values, halluc_values])\n",
    "        binary_halluc = np.concatenate([np.zeros(len(tp_values)), np.ones(len(halluc_values))])\n",
    "        \n",
    "        correlation_coef, correlation_p = stats.pearsonr(binary_halluc, combined_values)\n",
    "        \n",
    "        results['correlation_analysis'][metric] = {\n",
    "            'point_biserial_r': correlation_coef,\n",
    "            'p_value': correlation_p,\n",
    "            'significant': correlation_p < alpha,\n",
    "            'interpretation': f\"{'Positive' if correlation_coef > 0 else 'Negative'} correlation between hallucination status and {metric}\"\n",
    "        }\n",
    "    \n",
    "    # 5. MULTIPLE TESTING CORRECTION\n",
    "    all_p_values = []\n",
    "    test_descriptions = []\n",
    "    \n",
    "    # Get p-values from previous analysis if available\n",
    "    if 'tp_vs_hallucinations_results' in globals():\n",
    "        prev_results = globals()['tp_vs_hallucinations_results']\n",
    "        if 'statistical_tests' in prev_results:\n",
    "            for metric, tests in prev_results['statistical_tests'].items():\n",
    "                for test_type in ['parametric', 'non_parametric']:\n",
    "                    if test_type in tests and 'p_value' in tests[test_type]:\n",
    "                        all_p_values.append(tests[test_type]['p_value'])\n",
    "                        test_descriptions.append(f\"{metric}_{test_type}\")\n",
    "    \n",
    "    # Add correlation p-values\n",
    "    for metric in results['correlation_analysis']:\n",
    "        if 'p_value' in results['correlation_analysis'][metric]:\n",
    "            all_p_values.append(results['correlation_analysis'][metric]['p_value'])\n",
    "            test_descriptions.append(f\"{metric}_correlation\")\n",
    "    \n",
    "    if all_p_values:\n",
    "        # Bonferroni correction\n",
    "        bonferroni_alpha = alpha / len(all_p_values)\n",
    "        bonferroni_significant = [p < bonferroni_alpha for p in all_p_values]\n",
    "        \n",
    "        # False Discovery Rate (Benjamini-Hochberg)\n",
    "        fdr_rejected, fdr_p_corrected, _, _ = multipletests(all_p_values, alpha=alpha, method='fdr_bh')\n",
    "        \n",
    "        results['multiple_testing_correction'] = {\n",
    "            'original_alpha': alpha,\n",
    "            'bonferroni_alpha': bonferroni_alpha,\n",
    "            'bonferroni_significant': bonferroni_significant,\n",
    "            'fdr_rejected': fdr_rejected.tolist(),\n",
    "            'fdr_corrected_p': fdr_p_corrected.tolist(),\n",
    "            'test_descriptions': test_descriptions,\n",
    "            'original_p_values': all_p_values\n",
    "        }\n",
    "    \n",
    "    # 6. SAMPLE ADEQUACY ASSESSMENT\n",
    "    total_n = len(tp_df) + len(hallucination_df)\n",
    "    tp_n = len(tp_df)\n",
    "    halluc_n = len(hallucination_df)\n",
    "    \n",
    "    sample_adequacy = {\n",
    "        'total_sample_size': total_n,\n",
    "        'tp_sample_size': tp_n,\n",
    "        'hallucination_sample_size': halluc_n,\n",
    "        'minimum_recommended': 30,\n",
    "        'adequate_total_size': total_n >= 30,\n",
    "        'adequate_group_sizes': tp_n >= 15 and halluc_n >= 15,\n",
    "        'balanced_groups': abs(tp_n - halluc_n) / max(tp_n, halluc_n) < 0.5,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    if not sample_adequacy['adequate_total_size']:\n",
    "        sample_adequacy['recommendations'].append(\"Increase total sample size to at least 30\")\n",
    "    if not sample_adequacy['adequate_group_sizes']:\n",
    "        sample_adequacy['recommendations'].append(\"Ensure each group has at least 15 observations\")\n",
    "    if not sample_adequacy['balanced_groups']:\n",
    "        sample_adequacy['recommendations'].append(\"Consider balancing group sizes for better statistical power\")\n",
    "    \n",
    "    results['sample_adequacy'] = sample_adequacy\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run enhanced statistical validation with improved interpretation\n",
    "print(f\"ENHANCED STATISTICAL VALIDATION FOR HYPOTHESIS 2:\")\n",
    "print(f\"================================================\")\n",
    "print(f\"HYPOTHESIS: 'Hallucinations in LLM outputs significantly correlate with lower quality SiFP estimates'\")\n",
    "print(f\"OPERATIONALIZATION:\")\n",
    "print(f\"- Hallucinations = Both FP (over-identification) and FN (under-identification) traceability links\")\n",
    "print(f\"- Lower quality = Higher absolute and percentage errors in SiFP estimations\")\n",
    "print(f\"- Significant = p < 0.05 with meaningful effect sizes\")\n",
    "print()\n",
    "\n",
    "enhanced_results = enhanced_statistical_validation()\n",
    "\n",
    "if enhanced_results:\n",
    "    # Display confidence intervals with hypothesis interpretation\n",
    "    if 'confidence_intervals' in enhanced_results:\n",
    "        print(\"1. CONFIDENCE INTERVALS FOR MEAN DIFFERENCES:\")\n",
    "        print(\"   (Testing if TP links have different error rates than Hallucinations)\")\n",
    "        print(\"-\" * 70)\n",
    "        for metric, ci_data in enhanced_results['confidence_intervals'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            \n",
    "            # Bootstrap CI\n",
    "            bootstrap_ci = ci_data\n",
    "            ci_level = bootstrap_ci.get('confidence_level', 95)\n",
    "            print(f\"    Bootstrap {ci_level}% CI: [{bootstrap_ci['ci_lower']:.3f}, {bootstrap_ci['ci_upper']:.3f}]\")\n",
    "            print(f\"    Mean Difference (TP - Hallucinations): {bootstrap_ci['mean_difference']:.3f}\")\n",
    "            \n",
    "            # Hypothesis interpretation\n",
    "            if not bootstrap_ci['contains_zero']:\n",
    "                if bootstrap_ci['ci_upper'] < 0:\n",
    "                    print(f\"    ✓ SUPPORTS HYPOTHESIS: TP links have significantly LOWER {metric_name.lower()}\")\n",
    "                    print(f\"      → Hallucinations correlate with HIGHER estimation errors\")\n",
    "                else:\n",
    "                    print(f\"    ✗ CONTRADICTS HYPOTHESIS: TP links have significantly HIGHER {metric_name.lower()}\")\n",
    "                    print(f\"      → Hallucinations correlate with LOWER estimation errors\")\n",
    "            else:\n",
    "                print(f\"    ? INCONCLUSIVE: No significant difference detected\")\n",
    "                print(f\"      → Cannot conclude hallucinations affect estimation quality\")\n",
    "            \n",
    "            # Parametric CI if available\n",
    "            if 'parametric' in ci_data:\n",
    "                param_ci = ci_data['parametric']\n",
    "                print(f\"    Parametric CI ({param_ci['method']}): [{param_ci['ci_lower']:.3f}, {param_ci['ci_upper']:.3f}]\")\n",
    "            print()\n",
    "    \n",
    "    # Display power analysis with interpretation\n",
    "    if 'power_analysis' in enhanced_results:\n",
    "        print(\"2. STATISTICAL POWER ANALYSIS:\")\n",
    "        print(\"   (Assessing reliability of our conclusions)\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric, power_data in enhanced_results['power_analysis'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            print(f\"    Observed Effect Size (Cohen's d): {power_data['observed_effect_size']:.3f}\")\n",
    "            print(f\"    Achieved Statistical Power: {power_data['achieved_power']:.3f} ({power_data['achieved_power']*100:.1f}%)\")\n",
    "            print(f\"    Required Total N for 80% Power: {power_data['required_total_n_for_80_power']:.0f}\")\n",
    "            print(f\"    Required N per Group for 80% Power: {power_data['required_n_per_group_for_80_power']:.0f}\")\n",
    "            print(f\"    Current Sample: TP={power_data['current_n_tp']}, Hallucinations={power_data['current_n_halluc']}\")\n",
    "            \n",
    "            # Power interpretation\n",
    "            if power_data['adequate_power']:\n",
    "                print(f\"    ✓ ADEQUATE POWER: High confidence in detecting true effects\")\n",
    "            else:\n",
    "                print(f\"    ⚠ LOW POWER: May miss true effects (Type II error risk)\")\n",
    "                print(f\"      → Consider increasing sample size for more reliable conclusions\")\n",
    "            print()\n",
    "    \n",
    "    # Display additional effect sizes with interpretation\n",
    "    if 'additional_effect_sizes' in enhanced_results:\n",
    "        print(\"3. ADDITIONAL EFFECT SIZE MEASURES:\")\n",
    "        print(\"   (Assessing practical significance beyond statistical significance)\")\n",
    "        print(\"-\" * 65)\n",
    "        for metric, effects in enhanced_results['additional_effect_sizes'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            if 'cliff_delta' in effects:\n",
    "                cliff = effects['cliff_delta']\n",
    "                print(f\"    Cliff's Delta: {cliff['value']:.3f} ({cliff['interpretation']} effect)\")\n",
    "                \n",
    "                # Interpretation for hypothesis\n",
    "                if cliff['value'] < 0:\n",
    "                    print(f\"      → TP links tend to have lower {metric_name.lower()} than Hallucinations\")\n",
    "                    print(f\"      → SUPPORTS hypothesis about hallucination impact\")\n",
    "                elif cliff['value'] > 0:\n",
    "                    print(f\"      → TP links tend to have higher {metric_name.lower()} than Hallucinations\")\n",
    "                    print(f\"      → CONTRADICTS hypothesis about hallucination impact\")\n",
    "                else:\n",
    "                    print(f\"      → No systematic difference between groups\")\n",
    "                    \n",
    "            if 'glass_delta' in effects:\n",
    "                glass = effects['glass_delta']\n",
    "                print(f\"    Glass's Delta: {glass['value']:.3f}\")\n",
    "                print(f\"      → Standardized difference using Hallucination group variability\")\n",
    "            print()\n",
    "    \n",
    "    # Display correlation analysis with hypothesis context\n",
    "    if 'correlation_analysis' in enhanced_results:\n",
    "        print(\"4. CORRELATION ANALYSIS:\")\n",
    "        print(\"   (Direct test of correlation between hallucinations and estimation errors)\")\n",
    "        print(\"-\" * 75)\n",
    "        for metric, corr_data in enhanced_results['correlation_analysis'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            print(f\"    Point-biserial correlation (r): {corr_data['point_biserial_r']:.3f}\")\n",
    "            print(f\"    p-value: {corr_data['p_value']:.6f}\")\n",
    "            print(f\"    Statistically significant: {'Yes' if corr_data['significant'] else 'No'}\")\n",
    "            \n",
    "            # Hypothesis interpretation\n",
    "            if corr_data['significant']:\n",
    "                if corr_data['point_biserial_r'] > 0:\n",
    "                    print(f\"    ✓ SUPPORTS HYPOTHESIS: Positive correlation means hallucinations\")\n",
    "                    print(f\"      are associated with HIGHER {metric_name.lower()}\")\n",
    "                else:\n",
    "                    print(f\"    ✗ CONTRADICTS HYPOTHESIS: Negative correlation means hallucinations\")\n",
    "                    print(f\"      are associated with LOWER {metric_name.lower()}\")\n",
    "            else:\n",
    "                print(f\"    ? INCONCLUSIVE: No significant correlation detected\")\n",
    "            \n",
    "            # Strength interpretation\n",
    "            r_abs = abs(corr_data['point_biserial_r'])\n",
    "            if r_abs < 0.1:\n",
    "                strength = \"negligible\"\n",
    "            elif r_abs < 0.3:\n",
    "                strength = \"small\"\n",
    "            elif r_abs < 0.5:\n",
    "                strength = \"medium\"\n",
    "            else:\n",
    "                strength = \"large\"\n",
    "            print(f\"    Correlation strength: {strength} ({r_abs:.3f})\")\n",
    "            print()\n",
    "    \n",
    "    # Display multiple testing correction\n",
    "    if 'multiple_testing_correction' in enhanced_results:\n",
    "        print(\"5. MULTIPLE TESTING CORRECTION:\")\n",
    "        print(\"   (Controlling for false discoveries when testing multiple metrics)\")\n",
    "        print(\"-\" * 65)\n",
    "        correction_data = enhanced_results['multiple_testing_correction']\n",
    "        print(f\"    Original significance level (α): {correction_data['original_alpha']}\")\n",
    "        print(f\"    Bonferroni-corrected α: {correction_data['bonferroni_alpha']:.6f}\")\n",
    "        print(f\"    Number of statistical tests: {len(correction_data['original_p_values'])}\")\n",
    "        \n",
    "        significant_after_correction = sum(correction_data.get('bonferroni_significant', []))\n",
    "        print(f\"    Significant after Bonferroni correction: {significant_after_correction}\")\n",
    "        \n",
    "        if 'fdr_rejected' in correction_data:\n",
    "            fdr_significant = sum(correction_data['fdr_rejected'])\n",
    "            print(f\"    Significant after FDR correction: {fdr_significant}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if significant_after_correction > 0:\n",
    "            print(f\"    ✓ ROBUST FINDINGS: Results survive correction for multiple testing\")\n",
    "            print(f\"      → High confidence that observed effects are not due to chance\")\n",
    "        else:\n",
    "            print(f\"    ⚠ REDUCED CONFIDENCE: No tests significant after correction\")\n",
    "            print(f\"      → Observed effects may be due to multiple testing artifacts\")\n",
    "        print()\n",
    "    \n",
    "    # Display sample adequacy\n",
    "    if 'sample_adequacy' in enhanced_results:\n",
    "        print(\"6. SAMPLE SIZE ADEQUACY:\")\n",
    "        print(\"   (Assessing whether we have sufficient data for reliable conclusions)\")\n",
    "        print(\"-\" * 70)\n",
    "        adequacy = enhanced_results['sample_adequacy']\n",
    "        print(f\"    Total Sample Size: {adequacy['total_sample_size']}\")\n",
    "        print(f\"    TP Group Size: {adequacy['tp_sample_size']}\")\n",
    "        print(f\"    Hallucination Group Size: {adequacy['hallucination_sample_size']}\")\n",
    "        print(f\"    Adequate for statistical tests: {'✓ Yes' if adequacy['adequate_total_size'] else '✗ No'}\")\n",
    "        print(f\"    Adequate group sizes: {'✓ Yes' if adequacy['adequate_group_sizes'] else '✗ No'}\")\n",
    "        print(f\"    Reasonably balanced groups: {'✓ Yes' if adequacy['balanced_groups'] else '✗ No'}\")\n",
    "        \n",
    "        if adequacy['recommendations']:\n",
    "            print(\"    Recommendations for improvement:\")\n",
    "            for rec in adequacy['recommendations']:\n",
    "                print(f\"      - {rec}\")\n",
    "        print()\n",
    "    \n",
    "    # Store enhanced results\n",
    "    globals()['enhanced_validation_results'] = enhanced_results\n",
    "    \n",
    "    print(\"OVERALL INTERPRETATION FOR HYPOTHESIS 2:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Your statistical analysis now provides multiple lines of evidence:\")\n",
    "    print()\n",
    "    print(\"✓ CONFIDENCE INTERVALS: Show the range of plausible differences\")\n",
    "    print(\"✓ STATISTICAL POWER: Assesses reliability of conclusions\")\n",
    "    print(\"✓ EFFECT SIZES: Measure practical significance beyond p-values\")\n",
    "    print(\"✓ CORRELATIONS: Direct test of the hypothesized relationship\")\n",
    "    print(\"✓ MULTIPLE TESTING: Controls for false discovery rates\")\n",
    "    print(\"✓ SAMPLE ADEQUACY: Confirms sufficient data for conclusions\")\n",
    "    print()\n",
    "    print(\"This comprehensive approach provides much stronger evidence\")\n",
    "    print(\"for or against your hypothesis than traditional p-value testing alone!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No enhanced validation results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Advanced Statistical Methods: Function Definitions and Execution\n",
    "# Purpose: Define and execute cutting-edge statistical methods for robust Hypothesis 2 testing\n",
    "# Dependencies: scipy, arch, pymc, arviz, bayesian_testing\n",
    "# Breadcrumbs: Enhanced Validation -> Advanced Methods -> Function Definition\n",
    "\n",
    "def advanced_statistical_methods_analysis(df=None, alpha=0.05, n_permutations=10000, n_bootstrap=5000):\n",
    "    \"\"\"\n",
    "    Comprehensive advanced statistical analysis for Hypothesis 2 using:\n",
    "    1. Permutation Tests (exact p-values, distribution-free)\n",
    "    2. Extended Bootstrap Methods (BCa intervals, multiple bootstrap types)\n",
    "    3. Bayesian Analysis (credible intervals, Bayes factors)\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        alpha (float): Significance level (default: 0.05)\n",
    "        n_permutations (int): Number of permutation samples (default: 10000)\n",
    "        n_bootstrap (int): Number of bootstrap samples (default: 5000)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive results from all advanced statistical methods\n",
    "    \"\"\"\n",
    "    # Use global variables if not provided\n",
    "    df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(\"No data available for advanced statistical analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Filter for valid data\n",
    "    valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "    valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        logger.error(\"No valid data for advanced statistical analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Split groups\n",
    "    tp_df = valid_df[valid_df['is_hallucination'] == False]\n",
    "    hallucination_df = valid_df[valid_df['is_hallucination'] == True]\n",
    "    \n",
    "    logger.info(f\"Advanced analysis: {len(tp_df)} TP links vs {len(hallucination_df)} Hallucination links\")\n",
    "    \n",
    "    results = {\n",
    "        'permutation_tests': {},\n",
    "        'extended_bootstrap': {},\n",
    "        'bayesian_analysis': {},\n",
    "        'method_comparison': {},\n",
    "        'hypothesis_conclusion': {}\n",
    "    }\n",
    "    \n",
    "    metrics_to_test = ['sifp_abs_error', 'sifp_pct_error']\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric not in tp_df.columns or metric not in hallucination_df.columns:\n",
    "            continue\n",
    "            \n",
    "        tp_values = tp_df[metric].dropna()\n",
    "        halluc_values = hallucination_df[metric].dropna()\n",
    "        \n",
    "        if len(tp_values) < 5 or len(halluc_values) < 5:\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Analyzing {metric} with advanced methods...\")\n",
    "        \n",
    "        # =================================================================\n",
    "        # 1. PERMUTATION TESTS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running permutation tests for {metric}...\")\n",
    "        \n",
    "        def mean_difference_statistic(x, y, axis=0):\n",
    "            \"\"\"Statistic function for permutation test\"\"\"\n",
    "            return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "        \n",
    "        # Two-sample permutation test for mean difference\n",
    "        perm_result = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='two-sided',\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # One-sided tests for hypothesis direction\n",
    "        perm_result_less = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='less',  # TP < Hallucinations (supports hypothesis)\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        perm_result_greater = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='greater',  # TP > Hallucinations (contradicts hypothesis)\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # Calculate observed effect\n",
    "        observed_diff = tp_values.mean() - halluc_values.mean()\n",
    "        \n",
    "        results['permutation_tests'][metric] = {\n",
    "            'observed_difference': observed_diff,\n",
    "            'two_sided': {\n",
    "                'statistic': perm_result.statistic,\n",
    "                'p_value': perm_result.pvalue,\n",
    "                'significant': perm_result.pvalue < alpha\n",
    "            },\n",
    "            'one_sided_less': {\n",
    "                'statistic': perm_result_less.statistic,\n",
    "                'p_value': perm_result_less.pvalue,\n",
    "                'significant': perm_result_less.pvalue < alpha,\n",
    "                'supports_hypothesis': perm_result_less.pvalue < alpha and observed_diff < 0\n",
    "            },\n",
    "            'one_sided_greater': {\n",
    "                'statistic': perm_result_greater.statistic,\n",
    "                'p_value': perm_result_greater.pvalue,\n",
    "                'significant': perm_result_greater.pvalue < alpha,\n",
    "                'contradicts_hypothesis': perm_result_greater.pvalue < alpha and observed_diff > 0\n",
    "            },\n",
    "            'n_permutations': n_permutations,\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if (perm_result_less.pvalue < alpha and observed_diff < 0) else 'CONTRADICTS' if (perm_result_greater.pvalue < alpha and observed_diff > 0) else 'INCONCLUSIVE'\n",
    "        }\n",
    "        \n",
    "        # =================================================================\n",
    "        # 2. EXTENDED BOOTSTRAP METHODS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running extended bootstrap methods for {metric}...\")\n",
    "        \n",
    "        bootstrap_results = {}\n",
    "        \n",
    "        # IID Bootstrap (standard) - Fixed implementation\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        \n",
    "        # Simple bootstrap implementation without arch library issues\n",
    "        bootstrap_diffs = []\n",
    "        for i in range(n_bootstrap):\n",
    "            # Bootstrap samples\n",
    "            tp_bootstrap = np.random.choice(tp_values, size=len(tp_values), replace=True)\n",
    "            halluc_bootstrap = np.random.choice(halluc_values, size=len(halluc_values), replace=True)\n",
    "            \n",
    "            # Calculate difference\n",
    "            diff = np.mean(tp_bootstrap) - np.mean(halluc_bootstrap)\n",
    "            bootstrap_diffs.append(diff)\n",
    "        \n",
    "        bootstrap_diffs = np.array(bootstrap_diffs)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        ci_lower = np.percentile(bootstrap_diffs, (alpha/2) * 100)\n",
    "        ci_upper = np.percentile(bootstrap_diffs, (1 - alpha/2) * 100)\n",
    "        \n",
    "        # BCa confidence intervals (bias-corrected and accelerated)\n",
    "        # Estimate bias correction\n",
    "        n_tp = len(tp_values)\n",
    "        n_halluc = len(halluc_values)\n",
    "        \n",
    "        # Jackknife for acceleration\n",
    "        jackknife_diffs = []\n",
    "        for i in range(n_tp):\n",
    "            tp_jack = np.delete(tp_values, i)\n",
    "            jackknife_diffs.append(tp_jack.mean() - halluc_values.mean())\n",
    "        for i in range(n_halluc):\n",
    "            halluc_jack = np.delete(halluc_values, i)\n",
    "            jackknife_diffs.append(tp_values.mean() - halluc_jack.mean())\n",
    "        \n",
    "        jackknife_diffs = np.array(jackknife_diffs)\n",
    "        jackknife_mean = np.mean(jackknife_diffs)\n",
    "        \n",
    "        # Acceleration parameter\n",
    "        acceleration = np.sum((jackknife_mean - jackknife_diffs)**3) / (6 * (np.sum((jackknife_mean - jackknife_diffs)**2))**(3/2))\n",
    "        if np.isnan(acceleration):\n",
    "            acceleration = 0\n",
    "        \n",
    "        # Bias correction\n",
    "        observed_diff = tp_values.mean() - halluc_values.mean()\n",
    "        bias_correction = stats.norm.ppf((bootstrap_diffs < observed_diff).mean())\n",
    "        if np.isnan(bias_correction):\n",
    "            bias_correction = 0\n",
    "        \n",
    "        # BCa confidence intervals\n",
    "        z_alpha_2 = stats.norm.ppf(alpha/2)\n",
    "        z_1_alpha_2 = stats.norm.ppf(1 - alpha/2)\n",
    "        \n",
    "        alpha_1 = stats.norm.cdf(bias_correction + (bias_correction + z_alpha_2)/(1 - acceleration * (bias_correction + z_alpha_2)))\n",
    "        alpha_2 = stats.norm.cdf(bias_correction + (bias_correction + z_1_alpha_2)/(1 - acceleration * (bias_correction + z_1_alpha_2)))\n",
    "        \n",
    "        # Ensure valid percentiles\n",
    "        alpha_1 = max(0.001, min(0.999, alpha_1))\n",
    "        alpha_2 = max(0.001, min(0.999, alpha_2))\n",
    "        \n",
    "        bca_ci_lower = np.percentile(bootstrap_diffs, alpha_1 * 100)\n",
    "        bca_ci_upper = np.percentile(bootstrap_diffs, alpha_2 * 100)\n",
    "        \n",
    "        bootstrap_results['bca_intervals'] = {\n",
    "            'ci_lower': bca_ci_lower,\n",
    "            'ci_upper': bca_ci_upper,\n",
    "            'bias_correction': bias_correction,\n",
    "            'acceleration': acceleration,\n",
    "            'contains_zero': bca_ci_lower <= 0 <= bca_ci_upper\n",
    "        }\n",
    "        \n",
    "        bootstrap_results['iid_bootstrap'] = {\n",
    "            'mean_difference': np.mean(bootstrap_diffs),\n",
    "            'std_difference': np.std(bootstrap_diffs),\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'contains_zero': ci_lower <= 0 <= ci_upper,\n",
    "            'n_bootstrap': n_bootstrap,\n",
    "            'bootstrap_p_value': 2 * min((bootstrap_diffs <= 0).mean(), (bootstrap_diffs >= 0).mean()),\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if ci_upper < 0 else 'CONTRADICTS' if ci_lower > 0 else 'INCONCLUSIVE'\n",
    "        }\n",
    "        \n",
    "        # Circular Block Bootstrap (simplified implementation)\n",
    "        # Simple block bootstrap without arch library complications\n",
    "        block_size = min(5, len(tp_values) // 5, len(halluc_values) // 5)\n",
    "        if block_size >= 2:\n",
    "            circular_diffs = []\n",
    "            combined_data = np.concatenate([tp_values, halluc_values])\n",
    "            combined_labels = np.concatenate([np.zeros(len(tp_values)), np.ones(len(halluc_values))])\n",
    "            \n",
    "            for i in range(min(1000, n_bootstrap)):  # Limit for computational efficiency\n",
    "                # Simple circular block sampling\n",
    "                n_blocks = len(combined_data) // block_size\n",
    "                block_starts = np.random.choice(len(combined_data), size=n_blocks, replace=True)\n",
    "                \n",
    "                boot_data = []\n",
    "                boot_labels = []\n",
    "                for start in block_starts:\n",
    "                    for j in range(block_size):\n",
    "                        idx = (start + j) % len(combined_data)\n",
    "                        boot_data.append(combined_data[idx])\n",
    "                        boot_labels.append(combined_labels[idx])\n",
    "                \n",
    "                boot_data = np.array(boot_data)\n",
    "                boot_labels = np.array(boot_labels)\n",
    "                \n",
    "                tp_boot = boot_data[boot_labels == 0]\n",
    "                halluc_boot = boot_data[boot_labels == 1]\n",
    "                \n",
    "                if len(tp_boot) > 0 and len(halluc_boot) > 0:\n",
    "                    circular_diffs.append(tp_boot.mean() - halluc_boot.mean())\n",
    "            \n",
    "            if circular_diffs:\n",
    "                circular_diffs = np.array(circular_diffs)\n",
    "                circular_ci_lower = np.percentile(circular_diffs, (alpha/2) * 100)\n",
    "                circular_ci_upper = np.percentile(circular_diffs, (1 - alpha/2) * 100)\n",
    "                \n",
    "                bootstrap_results['circular_block_bootstrap'] = {\n",
    "                    'mean_difference': np.mean(circular_diffs),\n",
    "                    'std_difference': np.std(circular_diffs),\n",
    "                    'ci_lower': circular_ci_lower,\n",
    "                    'ci_upper': circular_ci_upper,\n",
    "                    'contains_zero': circular_ci_lower <= 0 <= circular_ci_upper,\n",
    "                    'n_bootstrap': len(circular_diffs),\n",
    "                    'block_size': block_size,\n",
    "                    'hypothesis_interpretation': 'SUPPORTS' if circular_ci_upper < 0 else 'CONTRADICTS' if circular_ci_lower > 0 else 'INCONCLUSIVE'\n",
    "                }\n",
    "        \n",
    "        results['extended_bootstrap'][metric] = bootstrap_results\n",
    "        \n",
    "        # =================================================================\n",
    "        # 3. BAYESIAN ANALYSIS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running Bayesian analysis for {metric}...\")\n",
    "        \n",
    "        # Bayesian two-sample t-test using PyMC\n",
    "        with pm.Model() as bayesian_model:\n",
    "            # Priors for group means\n",
    "            mu_tp = pm.Normal('mu_tp', mu=tp_values.mean(), sigma=tp_values.std()*2)\n",
    "            mu_halluc = pm.Normal('mu_halluc', mu=halluc_values.mean(), sigma=halluc_values.std()*2)\n",
    "            \n",
    "            # Priors for group standard deviations\n",
    "            sigma_tp = pm.HalfNormal('sigma_tp', sigma=tp_values.std()*2)\n",
    "            sigma_halluc = pm.HalfNormal('sigma_halluc', sigma=halluc_values.std()*2)\n",
    "            \n",
    "            # Likelihood for observations\n",
    "            tp_obs = pm.Normal('tp_obs', mu=mu_tp, sigma=sigma_tp, observed=tp_values)\n",
    "            halluc_obs = pm.Normal('halluc_obs', mu=mu_halluc, sigma=sigma_halluc, observed=halluc_values)\n",
    "            \n",
    "            # Derived quantities\n",
    "            diff_means = pm.Deterministic('diff_means', mu_tp - mu_halluc)\n",
    "            effect_size = pm.Deterministic('effect_size', diff_means / pm.math.sqrt((sigma_tp**2 + sigma_halluc**2) / 2))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace = pm.sample(2000, tune=1000, cores=1, random_seed=RANDOM_SEED, \n",
    "                            progressbar=False, return_inferencedata=True)\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        posterior_diff = trace.posterior['diff_means'].values.flatten()\n",
    "        posterior_effect_size = trace.posterior['effect_size'].values.flatten()\n",
    "        \n",
    "        # Calculate credible intervals\n",
    "        diff_ci = np.percentile(posterior_diff, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "        effect_ci = np.percentile(posterior_effect_size, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "        \n",
    "        # Bayesian p-value (probability that difference is in wrong direction)\n",
    "        bayesian_p_value = (posterior_diff > 0).mean() if posterior_diff.mean() < 0 else (posterior_diff < 0).mean()\n",
    "        \n",
    "        # Probability that TP has lower error (supports hypothesis)\n",
    "        prob_supports_hypothesis = (posterior_diff < 0).mean()\n",
    "        \n",
    "        # Bayes factor approximation using Savage-Dickey density ratio\n",
    "        # Compare H1: diff != 0 vs H0: diff = 0\n",
    "        from scipy.stats import gaussian_kde\n",
    "        posterior_kde = gaussian_kde(posterior_diff)\n",
    "        \n",
    "        # Prior density at 0 (assuming normal prior centered at 0)\n",
    "        prior_at_zero = stats.norm.pdf(0, 0, tp_values.std() + halluc_values.std())\n",
    "        posterior_at_zero = posterior_kde.evaluate([0])[0]\n",
    "        \n",
    "        # Bayes factor (BF10 = evidence for H1 vs H0)\n",
    "        bayes_factor = prior_at_zero / posterior_at_zero if posterior_at_zero > 0 else np.inf\n",
    "        \n",
    "        # Interpret Bayes factor\n",
    "        if bayes_factor < 1/3:\n",
    "            bf_interpretation = \"Strong evidence for H0 (no difference)\"\n",
    "        elif bayes_factor < 1:\n",
    "            bf_interpretation = \"Moderate evidence for H0\"\n",
    "        elif bayes_factor < 3:\n",
    "            bf_interpretation = \"Weak evidence for H1\"\n",
    "        elif bayes_factor < 10:\n",
    "            bf_interpretation = \"Moderate evidence for H1 (difference exists)\"\n",
    "        else:\n",
    "            bf_interpretation = \"Strong evidence for H1\"\n",
    "        \n",
    "        results['bayesian_analysis'][metric] = {\n",
    "            'posterior_mean_difference': np.mean(posterior_diff),\n",
    "            'posterior_std_difference': np.std(posterior_diff),\n",
    "            'credible_interval_95': {\n",
    "                'lower': diff_ci[0],\n",
    "                'upper': diff_ci[1],\n",
    "                'contains_zero': diff_ci[0] <= 0 <= diff_ci[1]\n",
    "            },\n",
    "            'effect_size': {\n",
    "                'mean': np.mean(posterior_effect_size),\n",
    "                'credible_interval': {'lower': effect_ci[0], 'upper': effect_ci[1]}\n",
    "            },\n",
    "            'bayesian_p_value': bayesian_p_value,\n",
    "            'prob_supports_hypothesis': prob_supports_hypothesis,\n",
    "            'bayes_factor': bayes_factor,\n",
    "            'bayes_factor_interpretation': bf_interpretation,\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if prob_supports_hypothesis > 0.95 else 'CONTRADICTS' if prob_supports_hypothesis < 0.05 else 'INCONCLUSIVE',\n",
    "            'model_summary': {\n",
    "                'n_samples': len(posterior_diff),\n",
    "                'n_tp': len(tp_values),\n",
    "                'n_hallucinations': len(halluc_values)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # =================================================================\n",
    "    # 4. METHOD COMPARISON AND SYNTHESIS\n",
    "    # =================================================================\n",
    "    logger.info(\"Synthesizing results across all advanced methods...\")\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric in results['permutation_tests'] or metric in results['extended_bootstrap'] or metric in results['bayesian_analysis']:\n",
    "            comparison = {\n",
    "                'metric': metric,\n",
    "                'hypothesis_support_summary': {},\n",
    "                'p_values': {},\n",
    "                'confidence_intervals': {},\n",
    "                'effect_evidence': {}\n",
    "            }\n",
    "            \n",
    "            # Collect hypothesis support from each method\n",
    "            methods_support = []\n",
    "            \n",
    "            if metric in results['permutation_tests'] and 'hypothesis_interpretation' in results['permutation_tests'][metric]:\n",
    "                perm_support = results['permutation_tests'][metric]['hypothesis_interpretation']\n",
    "                methods_support.append(('Permutation', perm_support))\n",
    "                comparison['p_values']['permutation_two_sided'] = results['permutation_tests'][metric]['two_sided']['p_value']\n",
    "                comparison['p_values']['permutation_one_sided'] = results['permutation_tests'][metric]['one_sided_less']['p_value']\n",
    "            \n",
    "            if metric in results['extended_bootstrap'] and 'iid_bootstrap' in results['extended_bootstrap'][metric]:\n",
    "                boot_support = results['extended_bootstrap'][metric]['iid_bootstrap']['hypothesis_interpretation']\n",
    "                methods_support.append(('Bootstrap', boot_support))\n",
    "                comparison['confidence_intervals']['bootstrap'] = {\n",
    "                    'lower': results['extended_bootstrap'][metric]['iid_bootstrap']['ci_lower'],\n",
    "                    'upper': results['extended_bootstrap'][metric]['iid_bootstrap']['ci_upper']\n",
    "                }\n",
    "            \n",
    "            if metric in results['bayesian_analysis'] and 'hypothesis_interpretation' in results['bayesian_analysis'][metric]:\n",
    "                bayes_support = results['bayesian_analysis'][metric]['hypothesis_interpretation']\n",
    "                methods_support.append(('Bayesian', bayes_support))\n",
    "                comparison['p_values']['bayesian'] = results['bayesian_analysis'][metric]['bayesian_p_value']\n",
    "                comparison['confidence_intervals']['bayesian_credible'] = results['bayesian_analysis'][metric]['credible_interval_95']\n",
    "            \n",
    "            # Consensus analysis\n",
    "            support_votes = sum(1 for _, support in methods_support if support == 'SUPPORTS')\n",
    "            contradict_votes = sum(1 for _, support in methods_support if support == 'CONTRADICTS')\n",
    "            inconclusive_votes = sum(1 for _, support in methods_support if support == 'INCONCLUSIVE')\n",
    "            \n",
    "            if support_votes > contradict_votes and support_votes > inconclusive_votes:\n",
    "                consensus = 'STRONG_SUPPORT'\n",
    "            elif contradict_votes > support_votes and contradict_votes > inconclusive_votes:\n",
    "                consensus = 'STRONG_CONTRADICTION'\n",
    "            elif support_votes == contradict_votes and support_votes > 0:\n",
    "                consensus = 'MIXED_EVIDENCE'\n",
    "            else:\n",
    "                consensus = 'INSUFFICIENT_EVIDENCE'\n",
    "            \n",
    "            comparison['hypothesis_support_summary'] = {\n",
    "                'methods_tested': len(methods_support),\n",
    "                'support_votes': support_votes,\n",
    "                'contradict_votes': contradict_votes,\n",
    "                'inconclusive_votes': inconclusive_votes,\n",
    "                'consensus': consensus,\n",
    "                'method_results': methods_support\n",
    "            }\n",
    "            \n",
    "            results['method_comparison'][metric] = comparison\n",
    "    \n",
    "    # =================================================================\n",
    "    # 5. OVERALL HYPOTHESIS CONCLUSION\n",
    "    # =================================================================\n",
    "    all_consensuses = [comp['hypothesis_support_summary']['consensus'] \n",
    "                      for comp in results['method_comparison'].values()]\n",
    "    \n",
    "    strong_support_count = all_consensuses.count('STRONG_SUPPORT')\n",
    "    strong_contradiction_count = all_consensuses.count('STRONG_CONTRADICTION')\n",
    "    mixed_count = all_consensuses.count('MIXED_EVIDENCE')\n",
    "    insufficient_count = all_consensuses.count('INSUFFICIENT_EVIDENCE')\n",
    "    \n",
    "    if strong_support_count > strong_contradiction_count and strong_support_count > 0:\n",
    "        overall_conclusion = 'HYPOTHESIS_STRONGLY_SUPPORTED'\n",
    "    elif strong_contradiction_count > strong_support_count and strong_contradiction_count > 0:\n",
    "        overall_conclusion = 'HYPOTHESIS_STRONGLY_CONTRADICTED'\n",
    "    elif mixed_count > 0:\n",
    "        overall_conclusion = 'MIXED_EVIDENCE_ACROSS_METRICS'\n",
    "    else:\n",
    "        overall_conclusion = 'INSUFFICIENT_EVIDENCE'\n",
    "    \n",
    "    results['hypothesis_conclusion'] = {\n",
    "        'overall_result': overall_conclusion,\n",
    "        'metrics_tested': len(all_consensuses),\n",
    "        'strong_support_count': strong_support_count,\n",
    "        'strong_contradiction_count': strong_contradiction_count,\n",
    "        'mixed_evidence_count': mixed_count,\n",
    "        'insufficient_evidence_count': insufficient_count,\n",
    "        'confidence_level': 'HIGH' if (strong_support_count + strong_contradiction_count) >= len(all_consensuses) * 0.75 else 'MODERATE' if (strong_support_count + strong_contradiction_count) > 0 else 'LOW'\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute the advanced statistical methods analysis\n",
    "print(\"ADVANCED STATISTICAL METHODS FOR HYPOTHESIS 2:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Applying cutting-edge statistical methods:\")\n",
    "print(\"1. PERMUTATION TESTS - Exact p-values without distributional assumptions\")\n",
    "print(\"2. EXTENDED BOOTSTRAP - BCa intervals and multiple bootstrap types\")\n",
    "print(\"3. BAYESIAN ANALYSIS - Credible intervals and Bayes factors\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Execute the analysis\n",
    "advanced_results = advanced_statistical_methods_analysis(\n",
    "    n_permutations=10000,\n",
    "    n_bootstrap=5000,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Store results for further analysis\n",
    "globals()['advanced_statistical_results'] = advanced_results\n",
    "\n",
    "if advanced_results:\n",
    "    print(\"✅ ADVANCED STATISTICAL ANALYSIS COMPLETE!\")\n",
    "    print(f\"Results generated for {len(advanced_results.get('method_comparison', {}))} metrics\")\n",
    "    print(\"Results stored in 'advanced_statistical_results' for display in next cell.\")\n",
    "else:\n",
    "    print(\"❌ No advanced statistical results generated. Check data and error logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Advanced Statistical Methods: Results Display and Interpretation\n",
    "# Purpose: Display and interpret results from advanced statistical analysis of Hypothesis 2\n",
    "# Dependencies: Results from cell 11 (advanced_statistical_results)\n",
    "# Breadcrumbs: Advanced Methods -> Results Display -> Comprehensive Interpretation\n",
    "\n",
    "# Display comprehensive results from advanced statistical analysis\n",
    "try:\n",
    "    # Check if results are available from previous cell\n",
    "    if 'advanced_statistical_results' not in globals() or not advanced_statistical_results:\n",
    "        print(\"⚠️ No advanced statistical results found.\")\n",
    "        print(\"Please run Cell [11] first to generate the results.\")\n",
    "    else:\n",
    "        advanced_results = advanced_statistical_results\n",
    "        \n",
    "        print(\"COMPREHENSIVE ADVANCED STATISTICAL RESULTS FOR HYPOTHESIS 2:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"HYPOTHESIS: 'Hallucinations in LLM outputs significantly correlate with\")\n",
    "        print(\"            lower quality SiFP estimates in new product development'\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Display Permutation Test Results\n",
    "        if 'permutation_tests' in advanced_results and advanced_results['permutation_tests']:\n",
    "            print(\"1. PERMUTATION TEST RESULTS:\")\n",
    "            print(\"   (Exact p-values without distributional assumptions)\")\n",
    "            print(\"-\" * 50)\n",
    "            for metric, perm_data in advanced_results['permutation_tests'].items():\n",
    "                if 'error' in perm_data:\n",
    "                    print(f\"  {metric}: Error - {perm_data['error']}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  📊 {metric_name}:\")\n",
    "                print(f\"    Observed Difference (TP - Hallucinations): {perm_data['observed_difference']:.4f}\")\n",
    "                print(f\"    Two-sided p-value: {perm_data['two_sided']['p_value']:.6f}\")\n",
    "                print(f\"    One-sided p-value (TP < Hallucinations): {perm_data['one_sided_less']['p_value']:.6f}\")\n",
    "                print(f\"    Permutations: {perm_data['n_permutations']:,}\")\n",
    "                print(f\"    Hypothesis Support: {perm_data['hypothesis_interpretation']}\")\n",
    "                \n",
    "                # Interpretation with visual indicators\n",
    "                if perm_data['hypothesis_interpretation'] == 'SUPPORTS':\n",
    "                    print(f\"    ✅ SUPPORTS: TP links have significantly lower {metric_name.lower()}\")\n",
    "                    print(f\"       → Hallucinations correlate with HIGHER estimation errors\")\n",
    "                elif perm_data['hypothesis_interpretation'] == 'CONTRADICTS':\n",
    "                    print(f\"    ❌ CONTRADICTS: TP links have significantly higher {metric_name.lower()}\")\n",
    "                    print(f\"       → Hallucinations correlate with LOWER estimation errors\")\n",
    "                else:\n",
    "                    print(f\"    ❓ INCONCLUSIVE: No significant difference detected\")\n",
    "                    print(f\"       → Cannot conclude hallucinations affect estimation quality\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"1. PERMUTATION TESTS: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Extended Bootstrap Results\n",
    "        if 'extended_bootstrap' in advanced_results and advanced_results['extended_bootstrap']:\n",
    "            print(\"2. EXTENDED BOOTSTRAP RESULTS:\")\n",
    "            print(\"   (Robust confidence intervals and multiple bootstrap methods)\")\n",
    "            print(\"-\" * 60)\n",
    "            for metric, boot_data in advanced_results['extended_bootstrap'].items():\n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  📈 {metric_name}:\")\n",
    "                \n",
    "                # IID Bootstrap Results\n",
    "                if 'iid_bootstrap' in boot_data and 'error' not in boot_data['iid_bootstrap']:\n",
    "                    iid = boot_data['iid_bootstrap']\n",
    "                    print(f\"    IID Bootstrap (n={iid['n_bootstrap']:,}):\")\n",
    "                    print(f\"      95% CI: [{iid['ci_lower']:.4f}, {iid['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Bootstrap p-value: {iid['bootstrap_p_value']:.6f}\")\n",
    "                    print(f\"      Contains zero: {'Yes' if iid['contains_zero'] else 'No'}\")\n",
    "                    print(f\"      Hypothesis Support: {iid['hypothesis_interpretation']}\")\n",
    "                    \n",
    "                    # Visual interpretation\n",
    "                    if iid['hypothesis_interpretation'] == 'SUPPORTS':\n",
    "                        print(f\"      ✅ Bootstrap CI supports hypothesis\")\n",
    "                    elif iid['hypothesis_interpretation'] == 'CONTRADICTS':\n",
    "                        print(f\"      ❌ Bootstrap CI contradicts hypothesis\")\n",
    "                    else:\n",
    "                        print(f\"      ❓ Bootstrap CI is inconclusive\")\n",
    "                \n",
    "                # BCa Bootstrap Results\n",
    "                if 'bca_intervals' in boot_data and 'error' not in boot_data['bca_intervals']:\n",
    "                    bca = boot_data['bca_intervals']\n",
    "                    print(f\"    BCa Bootstrap (Bias-Corrected & Accelerated):\")\n",
    "                    print(f\"      95% CI: [{bca['ci_lower']:.4f}, {bca['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Bias correction: {bca['bias_correction']:.4f}\")\n",
    "                    print(f\"      Acceleration: {bca['acceleration']:.4f}\")\n",
    "                    print(f\"      Contains zero: {'Yes' if bca['contains_zero'] else 'No'}\")\n",
    "                \n",
    "                # Circular Block Bootstrap Results\n",
    "                if 'circular_block_bootstrap' in boot_data and 'error' not in boot_data['circular_block_bootstrap']:\n",
    "                    circular = boot_data['circular_block_bootstrap']\n",
    "                    print(f\"    Circular Block Bootstrap (n={circular['n_bootstrap']}):\")\n",
    "                    print(f\"      95% CI: [{circular['ci_lower']:.4f}, {circular['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Hypothesis Support: {circular['hypothesis_interpretation']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"2. EXTENDED BOOTSTRAP: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Bayesian Analysis Results\n",
    "        if 'bayesian_analysis' in advanced_results and advanced_results['bayesian_analysis']:\n",
    "            print(\"3. BAYESIAN ANALYSIS RESULTS:\")\n",
    "            print(\"   (Probabilistic inference and model comparison)\")\n",
    "            print(\"-\" * 55)\n",
    "            for metric, bayes_data in advanced_results['bayesian_analysis'].items():\n",
    "                if 'error' in bayes_data:\n",
    "                    print(f\"  {metric}: Error - {bayes_data['error']}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  🔮 {metric_name}:\")\n",
    "                \n",
    "                # Posterior results\n",
    "                print(f\"    Posterior Mean Difference: {bayes_data['posterior_mean_difference']:.4f}\")\n",
    "                print(f\"    Posterior Std Difference: {bayes_data['posterior_std_difference']:.4f}\")\n",
    "                \n",
    "                # Credible intervals\n",
    "                ci = bayes_data['credible_interval_95']\n",
    "                print(f\"    95% Credible Interval: [{ci['lower']:.4f}, {ci['upper']:.4f}]\")\n",
    "                print(f\"    Contains zero: {'Yes' if ci['contains_zero'] else 'No'}\")\n",
    "                \n",
    "                # Effect size\n",
    "                effect = bayes_data['effect_size']\n",
    "                print(f\"    Effect Size (mean): {effect['mean']:.4f}\")\n",
    "                print(f\"    Effect Size 95% CI: [{effect['credible_interval']['lower']:.4f}, {effect['credible_interval']['upper']:.4f}]\")\n",
    "                \n",
    "                # Probability assessments\n",
    "                print(f\"    Probability Supporting Hypothesis: {bayes_data['prob_supports_hypothesis']:.3f}\")\n",
    "                print(f\"    Bayesian p-value: {bayes_data['bayesian_p_value']:.6f}\")\n",
    "                \n",
    "                # Bayes Factor\n",
    "                print(f\"    Bayes Factor (BF₁₀): {bayes_data['bayes_factor']:.2f}\")\n",
    "                print(f\"    BF Interpretation: {bayes_data['bayes_factor_interpretation']}\")\n",
    "                print(f\"    Hypothesis Support: {bayes_data['hypothesis_interpretation']}\")\n",
    "                \n",
    "                # Visual interpretation\n",
    "                prob = bayes_data['prob_supports_hypothesis']\n",
    "                if prob > 0.95:\n",
    "                    print(f\"    ✅ STRONG BAYESIAN SUPPORT: {prob:.1%} probability TP has lower errors\")\n",
    "                elif prob < 0.05:\n",
    "                    print(f\"    ❌ STRONG BAYESIAN CONTRADICTION: {1-prob:.1%} probability TP has higher errors\")\n",
    "                else:\n",
    "                    print(f\"    ❓ BAYESIAN UNCERTAINTY: {prob:.1%} probability supporting hypothesis\")\n",
    "                \n",
    "                # Model info\n",
    "                model_info = bayes_data['model_summary']\n",
    "                print(f\"    Model: {model_info['n_samples']:,} samples, TP={model_info['n_tp']}, Halluc={model_info['n_hallucinations']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"3. BAYESIAN ANALYSIS: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Cross-Method Comparison\n",
    "        if 'method_comparison' in advanced_results and advanced_results['method_comparison']:\n",
    "            print(\"4. CROSS-METHOD COMPARISON:\")\n",
    "            print(\"   (Consensus across statistical approaches)\")\n",
    "            print(\"-\" * 50)\n",
    "            for metric, comp_data in advanced_results['method_comparison'].items():\n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                summary = comp_data['hypothesis_support_summary']\n",
    "                \n",
    "                print(f\"  🔍 {metric_name}:\")\n",
    "                print(f\"    Methods Tested: {summary['methods_tested']}\")\n",
    "                print(f\"    Support Votes: {summary['support_votes']} ✅\")\n",
    "                print(f\"    Contradict Votes: {summary['contradict_votes']} ❌\")\n",
    "                print(f\"    Inconclusive Votes: {summary['inconclusive_votes']} ❓\")\n",
    "                print(f\"    Consensus: {summary['consensus']}\")\n",
    "                \n",
    "                # Visual consensus interpretation\n",
    "                consensus = summary['consensus']\n",
    "                if consensus == 'STRONG_SUPPORT':\n",
    "                    print(f\"    🎯 STRONG CONSENSUS: Methods agree hypothesis is SUPPORTED\")\n",
    "                elif consensus == 'STRONG_CONTRADICTION':\n",
    "                    print(f\"    🎯 STRONG CONSENSUS: Methods agree hypothesis is CONTRADICTED\")\n",
    "                elif consensus == 'MIXED_EVIDENCE':\n",
    "                    print(f\"    ⚖️ MIXED EVIDENCE: Methods disagree on hypothesis\")\n",
    "                else:\n",
    "                    print(f\"    ❓ INSUFFICIENT EVIDENCE: No clear consensus\")\n",
    "                \n",
    "                print(f\"    Method Details:\")\n",
    "                for method, result in summary['method_results']:\n",
    "                    emoji = \"✅\" if result == \"SUPPORTS\" else \"❌\" if result == \"CONTRADICTS\" else \"❓\"\n",
    "                    print(f\"      {emoji} {method}: {result}\")\n",
    "                \n",
    "                # P-values summary\n",
    "                if 'p_values' in comp_data:\n",
    "                    print(f\"    P-values:\")\n",
    "                    for test_name, p_val in comp_data['p_values'].items():\n",
    "                        significance = \"significant\" if p_val < 0.05 else \"not significant\"\n",
    "                        print(f\"      {test_name}: {p_val:.6f} ({significance})\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"4. CROSS-METHOD COMPARISON: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Overall Hypothesis Conclusion\n",
    "        if 'hypothesis_conclusion' in advanced_results:\n",
    "            conclusion = advanced_results['hypothesis_conclusion']\n",
    "            print(\"5. 🎯 COMPREHENSIVE HYPOTHESIS 2 CONCLUSION:\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Overall Result: {conclusion['overall_result']}\")\n",
    "            print(f\"Confidence Level: {conclusion['confidence_level']}\")\n",
    "            print(f\"Metrics Tested: {conclusion['metrics_tested']}\")\n",
    "            print(f\"Strong Support: {conclusion['strong_support_count']} metric(s)\")\n",
    "            print(f\"Strong Contradiction: {conclusion['strong_contradiction_count']} metric(s)\")\n",
    "            print(f\"Mixed Evidence: {conclusion['mixed_evidence_count']} metric(s)\")\n",
    "            print(f\"Insufficient Evidence: {conclusion['insufficient_evidence_count']} metric(s)\")\n",
    "            print()\n",
    "            \n",
    "            # Final comprehensive interpretation with recommendations\n",
    "            result = conclusion['overall_result']\n",
    "            confidence = conclusion['confidence_level']\n",
    "            \n",
    "            print(\"📋 FINAL INTERPRETATION & RECOMMENDATIONS:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            if result == 'HYPOTHESIS_STRONGLY_SUPPORTED':\n",
    "                print(\"🏆 CONCLUSION: HYPOTHESIS 2 IS STRONGLY SUPPORTED\")\n",
    "                print()\n",
    "                print(\"📊 EVIDENCE SUMMARY:\")\n",
    "                print(\"   ✅ Multiple advanced statistical methods converge on the conclusion that\")\n",
    "                print(\"      hallucinations in LLM outputs (both FP over-identification and FN\")\n",
    "                print(\"      under-identification) significantly correlate with lower quality\")\n",
    "                print(\"      SiFP estimates in new product development.\")\n",
    "                print()\n",
    "                print(\"🔬 STATISTICAL RIGOR:\")\n",
    "                print(\"   • Permutation tests: Exact p-values without distributional assumptions\")\n",
    "                print(\"   • Extended bootstrap: Robust confidence intervals (IID, BCa, Circular Block)\")\n",
    "                print(\"   • Bayesian analysis: Probabilistic evidence with credible intervals & Bayes factors\")\n",
    "                print(\"   • Cross-method consensus: Multiple approaches agree on the conclusion\")\n",
    "                print()\n",
    "                print(\"💡 PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Hallucinated traceability links are associated with WORSE SiFP estimates\")\n",
    "                print(\"   2. Focus on improving LLM accuracy to enhance estimation quality\")\n",
    "                print(\"   3. Implement hallucination detection mechanisms in traceability systems\")\n",
    "                print(\"   4. Use estimation quality as a feedback signal for LLM performance\")\n",
    "                print()\n",
    "                print(\"🎯 RECOMMENDATIONS:\")\n",
    "                print(\"   • Prioritize reducing both FP (over-identification) and FN (missed features)\")\n",
    "                print(\"   • Implement confidence scoring for traceability predictions\")\n",
    "                print(\"   • Use estimation accuracy as a validation metric for traceability quality\")\n",
    "                print(\"   • Consider ensemble methods to reduce hallucination rates\")\n",
    "                \n",
    "            elif result == 'HYPOTHESIS_STRONGLY_CONTRADICTED':\n",
    "                print(\"🚫 CONCLUSION: HYPOTHESIS 2 IS STRONGLY CONTRADICTED\")\n",
    "                print()\n",
    "                print(\"📊 EVIDENCE SUMMARY:\")\n",
    "                print(\"   ❌ Multiple advanced statistical methods converge on the conclusion that\")\n",
    "                print(\"      hallucinations do NOT significantly correlate with lower quality\")\n",
    "                print(\"      SiFP estimates. In fact, the evidence suggests hallucinations may\")\n",
    "                print(\"      be associated with equal or even better estimation quality.\")\n",
    "                print()\n",
    "                print(\"🤔 UNEXPECTED FINDINGS:\")\n",
    "                print(\"   • Hallucinated links may not negatively impact estimation accuracy\")\n",
    "                print(\"   • Current LLM hallucinations might be 'beneficial' or neutral for SiFP\")\n",
    "                print(\"   • The relationship between traceability and estimation may be more complex\")\n",
    "                print()\n",
    "                print(\"💡 PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Hallucination reduction may not improve SiFP estimation quality\")\n",
    "                print(\"   2. Focus effort on other factors affecting estimation accuracy\")\n",
    "                print(\"   3. Re-examine the assumed relationship between traceability and estimation\")\n",
    "                print(\"   4. Consider whether current 'hallucinations' capture useful information\")\n",
    "                print()\n",
    "                print(\"🎯 RECOMMENDATIONS:\")\n",
    "                print(\"   • Investigate why hallucinations don't correlate with worse estimates\")\n",
    "                print(\"   • Examine other factors that might influence SiFP estimation quality\")\n",
    "                print(\"   • Consider revising the definition of 'hallucination' in this context\")\n",
    "                print(\"   • Focus optimization efforts on different aspects of the system\")\n",
    "                \n",
    "            elif result == 'MIXED_EVIDENCE_ACROSS_METRICS':\n",
    "                print(\"⚖️ CONCLUSION: MIXED EVIDENCE FOR HYPOTHESIS 2\")\n",
    "                print()\n",
    "                print(\"📊 EVIDENCE SUMMARY:\")\n",
    "                print(\"   🔀 Different error metrics show different patterns. The relationship\")\n",
    "                print(\"      between hallucinations and estimation quality appears to be more\")\n",
    "                print(\"      complex than hypothesized, potentially varying by error type,\")\n",
    "                print(\"      context, or other moderating factors.\")\n",
    "                print()\n",
    "                print(\"🧩 COMPLEXITY INDICATORS:\")\n",
    "                print(\"   • Some metrics support the hypothesis while others contradict it\")\n",
    "                print(\"   • Effect may depend on the type of estimation error measured\")\n",
    "                print(\"   • Relationship may be moderated by other variables\")\n",
    "                print()\n",
    "                print(\"💡 PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Simple binary relationship between hallucinations and quality may not exist\")\n",
    "                print(\"   2. Different types of estimation errors may be affected differently\")\n",
    "                print(\"   3. Context-dependent effects require more nuanced analysis\")\n",
    "                print()\n",
    "                print(\"🎯 RECOMMENDATIONS:\")\n",
    "                print(\"   • Conduct subgroup analyses to identify moderating factors\")\n",
    "                print(\"   • Examine different types of hallucinations separately\")\n",
    "                print(\"   • Consider interaction effects with other variables\")\n",
    "                print(\"   • Develop more sophisticated models of the relationship\")\n",
    "                \n",
    "            else:\n",
    "                print(\"❓ CONCLUSION: INSUFFICIENT EVIDENCE FOR HYPOTHESIS 2\")\n",
    "                print()\n",
    "                print(\"📊 EVIDENCE SUMMARY:\")\n",
    "                print(\"   ⚠️ The available data and methods do not provide sufficient evidence\")\n",
    "                print(\"      to draw strong conclusions about the relationship between\")\n",
    "                print(\"      hallucinations and SiFP estimation quality.\")\n",
    "                print()\n",
    "                print(\"🔍 LIMITATIONS:\")\n",
    "                print(\"   • Sample size may be too small for reliable detection\")\n",
    "                print(\"   • Effect size may be smaller than detectable with current data\")\n",
    "                print(\"   • Measurement noise may obscure true relationships\")\n",
    "                print()\n",
    "                print(\"🎯 RECOMMENDATIONS:\")\n",
    "                print(\"   • Collect additional data to increase statistical power\")\n",
    "                print(\"   • Improve measurement precision of key variables\")\n",
    "                print(\"   • Consider alternative analytical approaches\")\n",
    "                print(\"   • Replicate analysis with different datasets\")\n",
    "            \n",
    "            print()\n",
    "            print(\"🏁 ANALYSIS COMPLETE!\")\n",
    "            print(f\"Confidence in conclusion: {confidence}\")\n",
    "            print(\"Results available in 'advanced_statistical_results' for further analysis.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ADVANCED STATISTICAL HYPOTHESIS 2 TESTING COMPLETE!\")\n",
    "        print(\"All results stored in 'advanced_statistical_results' global variable.\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error displaying advanced statistical results: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2: Hallucination Impact on Estimation Quality\n",
    "**Hallucinations in LLM outputs significantly correlate with lower quality estimates in new product feature development.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Dependencies\n",
    "# Purpose: Import necessary libraries and configure the environment for traceability and SiFP analysis\n",
    "# Dependencies: All packages assumed to be installed\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Database connectivity for Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, balanced_accuracy_score, \n",
    "    cohen_kappa_score, matthews_corrcoef, fbeta_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Enhanced statistical analysis\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.power import ttest_power, tt_solve_power\n",
    "\n",
    "# Advanced statistical testing imports\n",
    "# 1. Permutation Testing\n",
    "from scipy.stats import permutation_test, bootstrap as scipy_bootstrap\n",
    "\n",
    "# 2. Extended Bootstrap Analysis\n",
    "from arch.bootstrap import IIDBootstrap, CircularBlockBootstrap, StationaryBootstrap\n",
    "\n",
    "# 3. Bayesian Analysis\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import bayesian_testing as bt\n",
    "\n",
    "# Suppress warnings for cleaner notebook output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging with shorter format for better PDF wrapping\n",
    "import textwrap\n",
    "\n",
    "class PDFLoggingFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter that wraps long log messages for better PDF display\"\"\"\n",
    "    def format(self, record):\n",
    "        # Format the basic log record\n",
    "        formatted = super().format(record)\n",
    "        \n",
    "        # Wrap long lines at 120 characters with proper indentation\n",
    "        if len(formatted) > 120:\n",
    "            lines = textwrap.wrap(formatted, width=120, \n",
    "                                subsequent_indent='    ')  # Indent continuation lines\n",
    "            formatted = '\\n'.join(lines)\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "# Create custom formatter\n",
    "pdf_formatter = PDFLoggingFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom formatter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # Override any existing handlers\n",
    ")\n",
    "\n",
    "# Apply custom formatter to all handlers\n",
    "logger = logging.getLogger(__name__)\n",
    "for handler in logging.getLogger().handlers:\n",
    "    handler.setFormatter(pdf_formatter)\n",
    "\n",
    "# Configure pandas display settings for legal landscape format\n",
    "pd.set_option('display.width', 130)           # Set width threshold for legal landscape\n",
    "pd.set_option('display.max_columns', 25)     # Reasonable number of columns\n",
    "pd.set_option('display.max_colwidth', 25)    # Compact column width\n",
    "pd.set_option('display.precision', 2)        # Only 2 decimal places to save space\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  # Consistent float formatting\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "# Note: Removed expand_frame_repr=False to allow natural wrapping at 130 chars\n",
    "\n",
    "# Configure matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'seaborn')\n",
    "\n",
    "# Custom color palettes for TP/FP/FN visualization\n",
    "# TP (True Positive) = Light Blue, FP (False Positive) = Red, FN (False Negative) = Orange\n",
    "tp_fp_fn_colors = [\"#4d98da\", \"#e74c3c\", \"#f39c12\"]  # Light blue for TP, Red for FP, Orange for FN\n",
    "hallucination_colors = [\"#4d98da\", \"#e74c3c\", \"#f39c12\"]  # Same as above but conceptually for TP vs Hallucinations (FP+FN)\n",
    "\n",
    "# Statistical testing configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Advanced statistical methods availability flags (all True since packages are installed)\n",
    "advanced_methods_available = {\n",
    "    'permutation_tests': True,\n",
    "    'extended_bootstrap': True,\n",
    "    'bayesian_analysis': True,\n",
    "    'bayesian_testing': True,\n",
    "    'monte_carlo': True,\n",
    "    'statsmodels': True,\n",
    "    'scipy_bootstrap': True,\n",
    "    'arch_bootstrap': True\n",
    "}\n",
    "\n",
    "# Set statsmodels_available flag for backward compatibility\n",
    "statsmodels_available = True\n",
    "\n",
    "# Optimization metric for threshold evaluation\n",
    "OPTIMIZATION_METRIC = 'F2'\n",
    "\n",
    "print(\"Notebook Environment Setup Complete âœ“\")\n",
    "print(\"HYPOTHESIS 2 FRAMEWORK:\")\n",
    "print(\"========================\")\n",
    "print(\"Hallucinations in LLM outputs = BOTH False Positive (FP) AND False Negative (FN) traceability links:\")\n",
    "print(\"- FP Hallucinations: Predicted as traceable but NOT verified by ground truth (over-identification)\")\n",
    "print(\"- FN Hallucinations: Predicted as non-traceable but ARE verified by ground truth (missed features/under-identification)\")\n",
    "print(\"- TP Links: Correctly identified traceable links (verified predictions)\")\n",
    "print(\"\")\n",
    "print(\"RESEARCH QUESTION: Do hallucinated traceability links (both FP over-identification and FN missed features)\")\n",
    "print(\"exhibit significantly higher SiFP estimation errors compared to TP links in new product development?\")\n",
    "print(\"\")\n",
    "print(\"ADVANCED STATISTICAL METHODS AVAILABLE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ“ Permutation Tests (exact p-values, distribution-free)\")\n",
    "print(\"âœ“ Extended Bootstrap Methods (IID, Circular Block, Stationary Bootstrap)\")\n",
    "print(\"âœ“ Bayesian Analysis (credible intervals, posterior distributions)\")\n",
    "print(\"âœ“ Bayesian Hypothesis Testing (Bayes factors, model comparison)\")\n",
    "print(\"âœ“ Monte Carlo Simulation Methods (manual implementation)\")\n",
    "print(\"âœ“ Enhanced Statistical Modeling (statsmodels)\")\n",
    "print(\"âœ“ Multiple Testing Corrections and Power Analysis\")\n",
    "print(\"\")\n",
    "print(\"This notebook analyzes the relationship between BOTH types of LLM hallucinations and SiFP estimation quality\")\n",
    "print(\"using comprehensive statistical validation including:\")\n",
    "print(\"- Traditional parametric and non-parametric tests\")\n",
    "print(\"- Permutation-based exact tests (distribution-free)\")\n",
    "print(\"- Extended bootstrap methods (IID, Circular Block, Stationary Bootstrap)\")\n",
    "print(\"- Bayesian hypothesis testing (credible intervals, Bayes factors)\")\n",
    "print(\"- Monte Carlo simulation methods (manual implementation)\")\n",
    "print(\"- Multiple testing corrections and power analysis\")\n",
    "print(\"\")\n",
    "print(\"ðŸŽ¯ READY FOR COMPREHENSIVE HYPOTHESIS 2 TESTING WITH ADVANCED STATISTICAL METHODS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Environment Configuration\n",
    "# Purpose: Load environment variables and configure analysis settings from .env file\n",
    "# Dependencies: os, dotenv\n",
    "# Breadcrumbs: Setup -> Environment Configuration -> Model Selection\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j credentials from environment variables\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_PROJECT_NAME = os.getenv('NEO4J_PROJECT_NAME')\n",
    "\n",
    "# Get current model from environment\n",
    "CURRENT_MODEL_KEY = os.getenv('CURRENT_MODEL')\n",
    "CURRENT_MODEL = os.getenv(os.getenv('CURRENT_MODEL', ''))\n",
    "\n",
    "# Configuration for analysis\n",
    "SHOW_VISUALIZATION = os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true'\n",
    "MIN_TRACEABILITY_THRESHOLD = int(os.getenv('MIN_TRACEABILITY_THRESHOLD', '3'))\n",
    "SIFP_ESTIMATION_REQUIREMENT = os.getenv('SIFP_ESTIMATION_REQUIREMENT', 'SOURCE')\n",
    "\n",
    "# Get models to include in analysis from environment\n",
    "ANALYSIS_MODEL_IDS = os.getenv('RESULTS_ANALYSIS_MODEL_IDS', '')\n",
    "if ANALYSIS_MODEL_IDS:\n",
    "    # Split by comma and strip whitespace to get variable names\n",
    "    model_var_names = [var_name.strip() for var_name in ANALYSIS_MODEL_IDS.split(',')]\n",
    "    \n",
    "    # Look up each variable in the environment to get actual model IDs\n",
    "    selected_models = []\n",
    "    for var_name in model_var_names:\n",
    "        model_id = os.getenv(var_name, '')\n",
    "        if model_id:\n",
    "            selected_models.append(model_id)\n",
    "else:\n",
    "    selected_models = [CURRENT_MODEL] if CURRENT_MODEL else []\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Environment Configuration:\")\n",
    "print(f\"==========================\")\n",
    "print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "print(f\"Current model: {CURRENT_MODEL}\")\n",
    "print(f\"Selected models for analysis: {selected_models}\")\n",
    "print(f\"Visualization: {'Enabled' if SHOW_VISUALIZATION else 'Disabled'}\")\n",
    "print(f\"Traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "print(f\"SIFP requirement type: {SIFP_ESTIMATION_REQUIREMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Neo4j Connection Setup\n",
    "# Purpose: Create and test connection to Neo4j database for data retrieval\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection -> Neo4j Client\n",
    "\n",
    "def create_neo4j_driver(uri=None, user=None, password=None):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance\n",
    "    \n",
    "    Parameters:\n",
    "        uri (str, optional): Neo4j connection URI. If None, uses NEO4J_URI from environment.\n",
    "        user (str, optional): Neo4j username. If None, uses NEO4J_USER from environment.\n",
    "        password (str, optional): Neo4j password. If None, uses NEO4J_PASSWORD from environment.\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use parameters if provided, otherwise use environment variables\n",
    "        uri = uri or NEO4J_URI\n",
    "        user = user or NEO4J_USER  \n",
    "        password = password or NEO4J_PASSWORD\n",
    "        \n",
    "        # Verify that all required connection parameters are available\n",
    "        if not all([uri, user, password]):\n",
    "            missing = [param for param, value in \n",
    "                      zip(['uri', 'user', 'password'], [uri, user, password]) \n",
    "                      if not value]\n",
    "            \n",
    "            error_msg = f\"Missing Neo4j connection parameters: {', '.join(missing)}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Create the driver\n",
    "        driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "        # Verify connection with a simple query\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 1 as test\").single()\n",
    "            if result and result[\"test\"] == 1:\n",
    "                logger.info(\"Successfully connected to Neo4j database\")\n",
    "                logger.info(f\"Connected to Neo4j at {uri}\")\n",
    "                return driver\n",
    "            else:\n",
    "                raise ConnectionError(\"Could not verify Neo4j connection\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver\n",
    "try:\n",
    "    driver = create_neo4j_driver()\n",
    "    print(f\"Neo4j Connection Status:\")\n",
    "    print(f\"======================\")\n",
    "    print(f\"Connected to: {NEO4J_URI}\")\n",
    "    print(f\"Project name: {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Connection successful âœ“\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Neo4j database:\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"Please check your environment variables and database connection settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query Meta Judge Links (Traceability Data)\n",
    "# Purpose: Retrieve traceability data including TP/FP classifications from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Database Connection -> Data Retrieval -> Traceability Links\n",
    "\n",
    "def query_meta_judge_links(driver, project_name=None, models=None, target_ids=None):\n",
    "    \"\"\"\n",
    "    Query LLM_RESULT_META_JUDGE links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        models (list, optional): List of models to query. If None, uses selected_models from environment.\n",
    "        target_ids (list, optional): List of target IDs to filter by. If None, retrieves all target IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing meta judge links with traceability data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        models = models or selected_models\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for meta judge query\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        if not models:\n",
    "            logger.warning(\"No models specified for meta judge query, using all available models\")\n",
    "        \n",
    "        # Build model filter clause if models are specified\n",
    "        model_filter = \"\"\n",
    "        if models:\n",
    "            model_list = \"', '\".join(models)\n",
    "            model_filter = f\"AND r.model IN ['{model_list}']\"\n",
    "        \n",
    "        # Build target ID filter clause if target_ids are specified\n",
    "        target_id_filter = \"\"\n",
    "        if target_ids and len(target_ids) > 0:\n",
    "            # Convert all IDs to strings and wrap in quotes\n",
    "            quoted_ids = [\"'\" + str(id).replace(\"'\", \"\\\\'\") + \"'\" for id in target_ids]\n",
    "            id_list = \", \".join(quoted_ids)\n",
    "            target_id_filter = f\"AND target.id IN [{id_list}]\"\n",
    "            logger.info(f\"Filtering meta judge query to {len(target_ids)} specific target IDs\")\n",
    "        \n",
    "        # Query for meta-judge links\n",
    "        meta_judge_query = f\"\"\"\n",
    "        MATCH (p:Project {{name: $project_name}})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_META_JUDGE]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' {model_filter} {target_id_filter}\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            source.description as source_description,\n",
    "            target.id as target_id,\n",
    "            target.description as target_description,\n",
    "            r.is_traceable as is_traceable,\n",
    "            r.judge_score as judge_score,\n",
    "            r.semantic_alignment as semantic_alignment,\n",
    "            r.non_functional_coverage as non_functional_coverage,\n",
    "            r.final_score as final_score,\n",
    "            r.actor_score as actor_score,\n",
    "            r.functional_completeness as functional_completeness,\n",
    "            r.model as model\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing meta judge query for project: {project_name}\")\n",
    "            results = session.run(meta_judge_query, project_name=project_name).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No meta judge links found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            meta_judge_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Convert boolean columns to proper boolean type\n",
    "            if 'is_traceable' in meta_judge_df.columns:\n",
    "                meta_judge_df['is_traceable'] = meta_judge_df['is_traceable'].map(\n",
    "                    lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                )\n",
    "            \n",
    "            # Convert numeric columns to float\n",
    "            numeric_cols = [\n",
    "                'judge_score', 'semantic_alignment', 'non_functional_coverage',\n",
    "                'final_score', 'actor_score', 'functional_completeness'\n",
    "            ]\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in meta_judge_df.columns:\n",
    "                    meta_judge_df[col] = pd.to_numeric(meta_judge_df[col], errors='coerce')\n",
    "            \n",
    "            # Calculate total score as judge_score + actor_score\n",
    "            if 'judge_score' in meta_judge_df.columns and 'actor_score' in meta_judge_df.columns:\n",
    "                meta_judge_df['total_score'] = meta_judge_df['judge_score'] + meta_judge_df['actor_score']\n",
    "            \n",
    "            # Add meta_judge_threshold column based on MIN_TRACEABILITY_THRESHOLD\n",
    "            if 'total_score' in meta_judge_df.columns:\n",
    "                meta_judge_df['meta_judge_threshold'] = meta_judge_df['total_score'] >= MIN_TRACEABILITY_THRESHOLD\n",
    "            \n",
    "            # Count metrics for logging\n",
    "            logger.info(f\"Retrieved {len(meta_judge_df)} meta judge links\")\n",
    "            if 'is_traceable' in meta_judge_df.columns:\n",
    "                traceable_count = meta_judge_df['is_traceable'].sum()\n",
    "                logger.info(f\"Traceable links: {traceable_count} ({traceable_count/len(meta_judge_df)*100:.2f}%)\")\n",
    "            \n",
    "            return meta_judge_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for meta judge links: {str(e)}\") \n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    # Wait until we have both ground truth data and SIFP data\n",
    "    sifp_target_ids = []\n",
    "    ground_truth_target_ids = []\n",
    "    \n",
    "    # Get target IDs from SIFP data if already loaded\n",
    "    if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "        if 'sifp_requirement_id' in globals()['sifp_results_df'].columns:\n",
    "            sifp_target_ids = globals()['sifp_results_df']['sifp_requirement_id'].unique().tolist()\n",
    "        print(f\"Found {len(sifp_target_ids)} unique target IDs with SIFP data\")\n",
    "    \n",
    "    # Get target IDs from ground truth data if already loaded\n",
    "    if 'ground_truth_df' in globals() and not globals()['ground_truth_df'].empty:\n",
    "        ground_truth_target_ids = globals()['ground_truth_df']['target_id'].unique().tolist()\n",
    "        print(f\"Found {len(ground_truth_target_ids)} unique target IDs in ground truth data\")\n",
    "    \n",
    "    # Only filter if we have data from both sources\n",
    "    filter_target_ids = None\n",
    "    if sifp_target_ids and ground_truth_target_ids:\n",
    "        # Find the intersection of target IDs (those with both SIFP data and ground truth)\n",
    "        filter_target_ids = list(set(sifp_target_ids).intersection(set(ground_truth_target_ids)))\n",
    "        print(f\"Filtering meta judge query to {len(filter_target_ids)} target IDs that have both SIFP data and ground truth\")\n",
    "    \n",
    "    # Query meta judge data with filtered target IDs\n",
    "    meta_judge_df = query_meta_judge_links(driver, target_ids=filter_target_ids)\n",
    "    \n",
    "    print(f\"Meta Judge Traceability Results:\")\n",
    "    print(f\"================================\")\n",
    "    \n",
    "    if not meta_judge_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total links analyzed: {len(meta_judge_df)}\")\n",
    "        \n",
    "        if 'model' in meta_judge_df.columns:\n",
    "            model_counts = meta_judge_df['model'].value_counts()\n",
    "            print(f\"Links by model:\")\n",
    "            for model, count in model_counts.items():\n",
    "                print(f\"  {model}: {count}\")\n",
    "        \n",
    "        if 'is_traceable' in meta_judge_df.columns:\n",
    "            traceable_count = meta_judge_df['is_traceable'].sum()\n",
    "            print(f\"Traceability Distribution:\")\n",
    "            print(f\"  Traceable links: {traceable_count} ({traceable_count/len(meta_judge_df)*100:.2f}%)\")\n",
    "            print(f\"  Non-traceable links: {len(meta_judge_df) - traceable_count} ({(len(meta_judge_df) - traceable_count)/len(meta_judge_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of meta judge data:\")\n",
    "        display_cols = ['source_id', 'target_id', 'model', 'is_traceable', 'judge_score', 'actor_score', 'total_score']\n",
    "        display_cols = [col for col in display_cols if col in meta_judge_df.columns]\n",
    "        print(meta_judge_df[display_cols].head())\n",
    "    else:\n",
    "        print(\"No meta judge data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving meta judge data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Query Ground Truth Links\n",
    "# Purpose: Retrieve ground truth traceability links for validation and baseline comparison\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Retrieval -> Ground Truth -> Validation Data\n",
    "\n",
    "def query_ground_truth_links(driver, project_name=None):\n",
    "    \"\"\"\n",
    "    Query ground truth traceability links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If project_name isn't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for ground truth query\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Query for ground truth links\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' AND target.type = 'TARGET'\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            source.description as source_description,\n",
    "            target.id as target_id,\n",
    "            target.description as target_description,\n",
    "            1 as ground_truth,\n",
    "            d.id as document_id\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing ground truth query for project: {project_name}\")\n",
    "            results = session.run(ground_truth_query, project_name=project_name).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No ground truth links found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            ground_truth_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Add pair_id column for merging (source_id + \"_\" + target_id)\n",
    "            if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "                ground_truth_df['pair_id'] = ground_truth_df['source_id'] + \"_\" + ground_truth_df['target_id']\n",
    "            \n",
    "            # Count metrics for logging\n",
    "            logger.info(f\"Retrieved {len(ground_truth_df)} ground truth links\")\n",
    "            source_count = ground_truth_df['source_id'].nunique() if 'source_id' in ground_truth_df.columns else 0\n",
    "            target_count = ground_truth_df['target_id'].nunique() if 'target_id' in ground_truth_df.columns else 0\n",
    "            logger.info(f\"Unique source requirements: {source_count}\")\n",
    "            logger.info(f\"Unique target requirements: {target_count}\")\n",
    "            \n",
    "            return ground_truth_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for ground truth links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    ground_truth_df = query_ground_truth_links(driver)\n",
    "    print(f\"Ground Truth Links:\")\n",
    "    print(f\"==================\")\n",
    "    \n",
    "    if not ground_truth_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total ground truth links: {len(ground_truth_df)}\")\n",
    "        \n",
    "        # Count unique source and target requirements\n",
    "        source_count = ground_truth_df['source_id'].nunique()\n",
    "        target_count = ground_truth_df['target_id'].nunique()\n",
    "        \n",
    "        print(f\"Unique source requirements: {source_count}\")\n",
    "        print(f\"Unique target requirements: {target_count}\")\n",
    "        \n",
    "        # Calculate link density\n",
    "        if source_count > 0 and target_count > 0:\n",
    "            link_density = len(ground_truth_df) / (source_count * target_count)\n",
    "            print(f\"Link density: {link_density:.4f}\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of ground truth data:\")\n",
    "        display_cols = ['source_id', 'target_id', 'ground_truth']\n",
    "        display_cols = [col for col in display_cols if col in ground_truth_df.columns]\n",
    "        print(ground_truth_df[display_cols].head())\n",
    "        \n",
    "        # Create a set of ground truth link pair IDs for faster lookups\n",
    "        if 'pair_id' in ground_truth_df.columns:\n",
    "            ground_truth_links = set(ground_truth_df['pair_id'])\n",
    "            print(f\"Created fast lookup set with {len(ground_truth_links)} ground truth link pairs\")\n",
    "    else:\n",
    "        print(\"No ground truth data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving ground truth data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Query SIFP Estimation Data\n",
    "# Purpose: Retrieve SiFP estimations and actual metrics from requirements analysis\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Retrieval -> SIFP Estimations -> Requirements Metrics\n",
    "\n",
    "def query_sifp_estimations(driver, project_name=None, models=None, requirement_type=None, target_ids=None):\n",
    "    \"\"\"\n",
    "    Query SIFP estimation data from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        models (list, optional): List of models to query. If None, uses selected_models from environment.\n",
    "        requirement_type (str, optional): Requirement type to query. If None, uses SIFP_ESTIMATION_REQUIREMENT from environment.\n",
    "        target_ids (list, optional): List of target IDs to filter by. If None, retrieves all target IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing SIFP estimation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, use globals\n",
    "        project_name = project_name or NEO4J_PROJECT_NAME\n",
    "        models = models or selected_models\n",
    "        requirement_type = requirement_type or SIFP_ESTIMATION_REQUIREMENT\n",
    "        \n",
    "        if not project_name:\n",
    "            logger.error(\"No project name provided for SIFP estimation query\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Build model filter clause if models are specified\n",
    "        model_filter = \"\"\n",
    "        if models:\n",
    "            model_list = \"', '\".join(models)\n",
    "            model_filter = f\"AND s.model IN ['{model_list}']\"\n",
    "        \n",
    "        # Build target ID filter clause if target_ids are specified\n",
    "        target_id_filter = \"\"\n",
    "        if target_ids and len(target_ids) > 0:\n",
    "            # Convert all IDs to strings and wrap in quotes\n",
    "            quoted_ids = [\"'\" + str(id).replace(\"'\", \"\\\\'\") + \"'\" for id in target_ids]\n",
    "            id_list = \", \".join(quoted_ids)\n",
    "            target_id_filter = f\"AND r.id IN [{id_list}]\"\n",
    "            logger.info(f\"Filtering SIFP query to {len(target_ids)} specific target IDs\")\n",
    "        \n",
    "        # Query for SIFP estimations\n",
    "        sifp_query = f\"\"\"\n",
    "        MATCH (p:Project {{name: $project_name}})\n",
    "        MATCH (p)-[:CONTAINS*]->(n)\n",
    "        MATCH (r:Requirement)-[s:SIFP_ESTIMATION]->(e)\n",
    "        WHERE r = n\n",
    "        AND r.type = $requirement_type\n",
    "        {model_filter}\n",
    "        {target_id_filter}\n",
    "        WITH r, s\n",
    "        WHERE s.actor_analysis IS NOT NULL\n",
    "        AND s.judge_evaluation IS NOT NULL\n",
    "        AND s.final_estimation IS NOT NULL\n",
    "        WITH r, s,\n",
    "            CASE\n",
    "                WHEN s.actor_analysis STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.actor_analysis)\n",
    "                ELSE NULL\n",
    "            END as actor_analysis,\n",
    "            CASE\n",
    "                WHEN s.judge_evaluation STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.judge_evaluation)\n",
    "                ELSE NULL\n",
    "            END as judge_eval,\n",
    "            CASE\n",
    "                WHEN s.final_estimation STARTS WITH '{{'\n",
    "                THEN apoc.convert.fromJsonMap(s.final_estimation)\n",
    "                ELSE NULL\n",
    "            END as final_est\n",
    "        WHERE actor_analysis IS NOT NULL\n",
    "        AND final_est IS NOT NULL\n",
    "        WITH r.id as sifp_requirement_id,\n",
    "            s.is_valid as sifp_is_valid,\n",
    "            s.model as sifp_model,\n",
    "            s.judge_score as sifp_judge_score,\n",
    "            s.judge_confidence as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            actor_analysis.confidence as sifp_actor_confidence,\n",
    "            actor_analysis.sifp_points.total as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            judge_eval.ugep_accuracy as sifp_judge_ugep_accuracy,\n",
    "            judge_eval.ugdg_accuracy as sifp_judge_ugdg_accuracy,\n",
    "            judge_eval.calculation_accuracy as sifp_judge_calculation_accuracy,\n",
    "            judge_eval.component_classification_accuracy as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            final_est.sifp_points.total as sifp_final_total\n",
    "        WITH sifp_requirement_id, sifp_model,\n",
    "            COLLECT([sifp_is_valid, sifp_judge_score, sifp_judge_confidence,\n",
    "            sifp_actor_confidence, sifp_actor_total, sifp_judge_ugep_accuracy,\n",
    "            sifp_judge_ugdg_accuracy, sifp_judge_calculation_accuracy,\n",
    "            sifp_judge_classification_accuracy, sifp_final_total])[0] as fields\n",
    "        RETURN \n",
    "            sifp_requirement_id,\n",
    "            sifp_model,\n",
    "            fields[0] as sifp_is_valid,\n",
    "            fields[1] as sifp_judge_score,\n",
    "            fields[2] as sifp_judge_confidence,\n",
    "            // Actor Analysis values\n",
    "            fields[3] as sifp_actor_confidence,\n",
    "            fields[4] as sifp_actor_total,\n",
    "            // Judge Evaluation values\n",
    "            fields[5] as sifp_judge_ugep_accuracy,\n",
    "            fields[6] as sifp_judge_ugdg_accuracy,\n",
    "            fields[7] as sifp_judge_calculation_accuracy,\n",
    "            fields[8] as sifp_judge_classification_accuracy,\n",
    "            // Final Estimation values\n",
    "            fields[9] as sifp_final_total\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            logger.info(f\"Executing SIFP estimation query for project: {project_name}\")\n",
    "            results = session.run(sifp_query, \n",
    "                                 project_name=project_name,\n",
    "                                 requirement_type=requirement_type).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(f\"No SIFP estimation results found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            sifp_results_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Convert boolean columns to proper boolean type\n",
    "            if 'sifp_is_valid' in sifp_results_df.columns:\n",
    "                sifp_results_df['sifp_is_valid'] = sifp_results_df['sifp_is_valid'].map(\n",
    "                    lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                )\n",
    "            \n",
    "            # Convert numeric columns to float\n",
    "            numeric_cols = [\n",
    "                'sifp_judge_score', 'sifp_judge_confidence', 'sifp_actor_confidence', 'sifp_actor_total',\n",
    "                'sifp_judge_ugep_accuracy', 'sifp_judge_ugdg_accuracy', 'sifp_judge_calculation_accuracy',\n",
    "                'sifp_judge_classification_accuracy', 'sifp_final_total'\n",
    "            ]\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in sifp_results_df.columns:\n",
    "                    sifp_results_df[col] = pd.to_numeric(sifp_results_df[col], errors='coerce')\n",
    "            \n",
    "            # Add columns to help with analysis\n",
    "            if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "                # Calculate difference between actor and final estimations\n",
    "                sifp_results_df['sifp_difference'] = sifp_results_df['sifp_final_total'] - sifp_results_df['sifp_actor_total']\n",
    "                \n",
    "                # Calculate percentage difference\n",
    "                nonzero_mask = sifp_results_df['sifp_actor_total'] != 0\n",
    "                sifp_results_df['sifp_pct_difference'] = np.nan\n",
    "                sifp_results_df.loc[nonzero_mask, 'sifp_pct_difference'] = (\n",
    "                    (sifp_results_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                     sifp_results_df.loc[nonzero_mask, 'sifp_actor_total']) / \n",
    "                    sifp_results_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "                )\n",
    "                \n",
    "                # Calculate absolute percentage difference\n",
    "                sifp_results_df['sifp_abs_pct_difference'] = sifp_results_df['sifp_pct_difference'].abs()\n",
    "            \n",
    "            # Log summary statistics\n",
    "            logger.info(f\"Retrieved {len(sifp_results_df)} SIFP estimation results\")\n",
    "            \n",
    "            if 'sifp_model' in sifp_results_df.columns:\n",
    "                logger.info(f\"Models in SIFP data: {sifp_results_df['sifp_model'].unique()}\")\n",
    "            \n",
    "            if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "                logger.info(f\"Average actor total: {sifp_results_df['sifp_actor_total'].mean():.2f}\")\n",
    "                logger.info(f\"Average final total: {sifp_results_df['sifp_final_total'].mean():.2f}\")\n",
    "            \n",
    "            return sifp_results_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Neo4j for SIFP estimations: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute query and get results\n",
    "try:\n",
    "    # First, extract target IDs from ground truth data\n",
    "    ground_truth_target_ids = []\n",
    "    if 'ground_truth_df' in globals() and not globals()['ground_truth_df'].empty:\n",
    "        ground_truth_target_ids = globals()['ground_truth_df']['target_id'].unique().tolist()\n",
    "        print(f\"Found {len(ground_truth_target_ids)} unique target IDs in ground truth data\")\n",
    "    \n",
    "    # Query SIFP data, filtered by target IDs from ground truth\n",
    "    sifp_results_df = query_sifp_estimations(driver, target_ids=ground_truth_target_ids)\n",
    "    \n",
    "    print(f\"SIFP Estimation Results:\")\n",
    "    print(f\"========================\")\n",
    "    \n",
    "    if not sifp_results_df.empty:\n",
    "        # Display general info about the dataset\n",
    "        print(f\"Total SIFP estimations: {len(sifp_results_df)}\")\n",
    "        \n",
    "        if 'sifp_model' in sifp_results_df.columns:\n",
    "            model_counts = sifp_results_df['sifp_model'].value_counts()\n",
    "            print(f\"Estimations by model:\")\n",
    "            for model, count in model_counts.items():\n",
    "                print(f\"  {model}: {count}\")\n",
    "        \n",
    "        # Display statistics on SIFP estimations\n",
    "        if 'sifp_actor_total' in sifp_results_df.columns and 'sifp_final_total' in sifp_results_df.columns:\n",
    "            print(f\"SIFP Estimation Statistics:\")\n",
    "            print(f\"  Average actor estimation: {sifp_results_df['sifp_actor_total'].mean():.2f} points\")\n",
    "            print(f\"  Average judge estimation: {sifp_results_df['sifp_final_total'].mean():.2f} points\")\n",
    "            print(f\"  Average improvement: {sifp_results_df['sifp_difference'].mean():.2f} points\")\n",
    "            print(f\"  Average percentage change: {sifp_results_df['sifp_pct_difference'].mean():.2f}%\")\n",
    "        \n",
    "        # Display sample of data\n",
    "        print(\"Sample of SIFP estimation data:\")\n",
    "        display_cols = ['sifp_requirement_id', 'sifp_model', 'sifp_actor_total', 'sifp_final_total', 'sifp_difference', 'sifp_pct_difference']\n",
    "        display_cols = [col for col in display_cols if col in sifp_results_df.columns]\n",
    "        print(sifp_results_df[display_cols].head())\n",
    "        \n",
    "        # Display summary statistics for key metrics\n",
    "        print(\"Summary Statistics for Key Metrics:\")\n",
    "        key_metrics = ['sifp_actor_total', 'sifp_final_total', 'sifp_difference', 'sifp_abs_pct_difference']\n",
    "        key_metrics = [col for col in key_metrics if col in sifp_results_df.columns]\n",
    "        \n",
    "        if key_metrics:\n",
    "            print(sifp_results_df[key_metrics].describe())\n",
    "    else:\n",
    "        print(\"No SIFP estimation data found. Please check your database connection and project name.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving SIFP estimation data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Model Evaluation and Threshold Optimization\n",
    "# Purpose: Evaluate traceability links and classify as TP/FP/FN/TN based on F2 optimization\n",
    "# Dependencies: pandas, numpy, sklearn.metrics\n",
    "# Breadcrumbs: Data Analysis -> Model Evaluation -> Classification Optimization\n",
    "\n",
    "def evaluate_model_thresholds(df, model_name, score_column='total_score', \n",
    "                             ground_truth_column='ground_truth_traceable', \n",
    "                             optimize_for='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance across different thresholds\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing model predictions and ground truth\n",
    "        model_name: Name of the model to evaluate\n",
    "        score_column: Column containing score values\n",
    "        ground_truth_column: Column containing ground truth values\n",
    "        optimize_for: Metric to optimize for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this model\n",
    "        model_df = df[df['model'] == model_name].copy()\n",
    "        \n",
    "        if model_df.empty:\n",
    "            print(f\"No data available for model: {model_name}\")\n",
    "            return {}\n",
    "            \n",
    "        if ground_truth_column not in model_df.columns:\n",
    "            print(f\"Ground truth column '{ground_truth_column}' not found for model: {model_name}\")\n",
    "            return {}\n",
    "        \n",
    "        # Get ground truth and scores\n",
    "        y_true = model_df[ground_truth_column].astype(int).values\n",
    "        \n",
    "        # Check for and handle None/NaN values in score column\n",
    "        if model_df[score_column].isna().any():\n",
    "            print(f\"Found NaN values in {score_column} for model {model_name}. Filling with 0.\")\n",
    "            model_df[score_column] = model_df[score_column].fillna(0)\n",
    "        \n",
    "        # Ensure scores are numeric\n",
    "        if model_df[score_column].dtype == object:\n",
    "            try:\n",
    "                model_df[score_column] = pd.to_numeric(model_df[score_column])\n",
    "                print(f\"Converted {score_column} to numeric for model {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {score_column} to numeric: {str(e)}\")\n",
    "                # Default to zeros if conversion fails\n",
    "                model_df[score_column] = 0\n",
    "        \n",
    "        scores = model_df[score_column].values\n",
    "        \n",
    "        # Print debug information\n",
    "        print(f\"  - Total data points: {len(model_df)}\")\n",
    "        print(f\"  - Positive examples: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Negative examples: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Score range: {scores.min():.4f} to {scores.max():.4f}\")\n",
    "        \n",
    "        # If all ground truth values are the same, we can't calculate meaningful metrics\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            print(f\"Insufficient ground truth variety for model {model_name} - all values are {np.unique(y_true)[0]}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum())\n",
    "            }\n",
    "        \n",
    "        # Generate possible thresholds from the data\n",
    "        unique_scores = np.unique(scores)\n",
    "        # Add some intermediate thresholds for a more fine-grained evaluation\n",
    "        thresholds = np.sort(np.concatenate([\n",
    "            unique_scores,\n",
    "            np.linspace(scores.min(), scores.max(), 20)\n",
    "        ]))\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Convert scores to binary predictions using this threshold\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            \n",
    "            # Only calculate if we have at least one prediction of each class\n",
    "            if np.unique(y_pred).size < 2:\n",
    "                continue\n",
    "                \n",
    "            # Confusion matrix components\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            # Handle division by zero\n",
    "            if tp + fp == 0:  # No positive predictions\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = tp / (tp + fp)\n",
    "                \n",
    "            if tp + fn == 0:  # No positive ground truth\n",
    "                rec = 0\n",
    "            else:\n",
    "                rec = tp / (tp + fn)\n",
    "            \n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "            \n",
    "            # Additional metrics\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity/True Negative Rate\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate/False Negative Rate\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'tnr': tnr,  # specificity\n",
    "                'fnr': fnr,  # miss rate\n",
    "                'f1_score': f1,\n",
    "                'f2_score': f2,\n",
    "                'mcc': mcc  # Matthews Correlation Coefficient\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if results_df.empty:\n",
    "            print(f\"No valid thresholds found for model {model_name}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "                'error': \"No valid thresholds found with current data\"\n",
    "            }\n",
    "        \n",
    "        # Find best threshold based on optimization metric\n",
    "        if optimize_for == 'F1':\n",
    "            best_idx = results_df['f1_score'].idxmax()\n",
    "            best_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_idx = results_df['f2_score'].idxmax()\n",
    "            best_metric = 'f2_score'\n",
    "            \n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        # Return comprehensive results with the original dataframe for further use\n",
    "        result_dict = {\n",
    "            'model_name': model_name,\n",
    "            'score_column': score_column,\n",
    "            'data_points': len(model_df),\n",
    "            'ground_truth_positive': int(y_true.sum()),\n",
    "            'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "            'best_threshold': best_result['threshold'],\n",
    "            'best_precision': best_result['precision'],\n",
    "            'best_recall': best_result['recall'],\n",
    "            'best_accuracy': best_result['accuracy'],\n",
    "            'best_balanced_accuracy': best_result['balanced_accuracy'],\n",
    "            'best_f1': best_result['f1_score'],\n",
    "            'best_f2': best_result['f2_score'],\n",
    "            'best_tnr': best_result['tnr'],\n",
    "            'best_fnr': best_result['fnr'],\n",
    "            'best_mcc': best_result['mcc'],\n",
    "            'best_tp': best_result['tp'],\n",
    "            'best_fp': best_result['fp'],\n",
    "            'best_fn': best_result['fn'],\n",
    "            'best_tn': best_result['tn'],\n",
    "            'optimization_metric': optimize_for,\n",
    "            'threshold_results': results_df,\n",
    "            'model_df': model_df\n",
    "        }\n",
    "        \n",
    "        return result_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df) if 'model_df' in locals() else 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def apply_classification_to_dataframe(df, evaluation_results):\n",
    "    \"\"\"\n",
    "    Apply classification (TP/FP/FN/TN) to a dataframe based on evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame to apply classification to\n",
    "        evaluation_results: Dictionary with evaluation results including best threshold\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added classification columns\n",
    "    \"\"\"\n",
    "    if not evaluation_results or 'error' in evaluation_results:\n",
    "        return df\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Extract key information from evaluation results\n",
    "    model_name = evaluation_results['model_name']\n",
    "    score_column = evaluation_results['score_column']\n",
    "    threshold = evaluation_results['best_threshold']\n",
    "    \n",
    "    # Add columns for this specific model and score type\n",
    "    col_prefix = f\"{model_name}_{score_column}\"\n",
    "    prediction_col = f\"{col_prefix}_prediction\"\n",
    "    classification_col = f\"{col_prefix}_classification\"\n",
    "    \n",
    "    # Filter for rows with this model\n",
    "    if 'model' in result_df.columns:\n",
    "        model_mask = (result_df['model'] == model_name)\n",
    "    else:\n",
    "        model_mask = pd.Series(True, index=result_df.index)\n",
    "    \n",
    "    # Create prediction column (True/False) based on threshold\n",
    "    result_df.loc[model_mask, prediction_col] = False\n",
    "    score_mask = model_mask & result_df[score_column].notna()\n",
    "    result_df.loc[score_mask, prediction_col] = result_df.loc[score_mask, score_column] >= threshold\n",
    "    \n",
    "    # Create classification column (TP/FP/FN/TN) based on prediction and ground truth\n",
    "    result_df.loc[model_mask, classification_col] = 'NA'\n",
    "    \n",
    "    # Apply classification logic\n",
    "    if 'ground_truth_traceable' in result_df.columns:\n",
    "        # TP: ground truth positive AND prediction positive\n",
    "        tp_mask = model_mask & result_df['ground_truth_traceable'] & result_df[prediction_col]\n",
    "        result_df.loc[tp_mask, classification_col] = 'TP'\n",
    "        \n",
    "        # FP: ground truth negative BUT prediction positive\n",
    "        # Make sure to handle potential float values correctly by ensuring boolean conversion\n",
    "        fp_mask = model_mask & ~result_df['ground_truth_traceable'] & result_df[prediction_col]\n",
    "        result_df.loc[fp_mask, classification_col] = 'FP'\n",
    "        \n",
    "        # FN: ground truth positive BUT prediction negative\n",
    "        # This is where the error occurred - we need to ensure we're treating prediction_col as boolean\n",
    "        prediction_bool = result_df[prediction_col].astype(bool)\n",
    "        fn_mask = model_mask & result_df['ground_truth_traceable'] & ~prediction_bool\n",
    "        result_df.loc[fn_mask, classification_col] = 'FN'\n",
    "        \n",
    "        # TN: ground truth negative AND prediction negative\n",
    "        tn_mask = model_mask & ~result_df['ground_truth_traceable'] & ~prediction_bool\n",
    "        result_df.loc[tn_mask, classification_col] = 'TN'\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_combined_dataset(meta_judge_df, ground_truth_df, sifp_results_df):\n",
    "    \"\"\"\n",
    "    Combine meta judge data with traceability classifications and SIFP estimates\n",
    "    \n",
    "    Parameters:\n",
    "        meta_judge_df: DataFrame with meta judge assessments\n",
    "        ground_truth_df: DataFrame with ground truth traceability data\n",
    "        sifp_results_df: DataFrame with SIFP estimation data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset linking classifications with SIFP data\n",
    "    \"\"\"\n",
    "    # Validate input data\n",
    "    if meta_judge_df is None or meta_judge_df.empty:\n",
    "        logger.error(\"Meta judge data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if ground_truth_df is None or ground_truth_df.empty:\n",
    "        logger.error(\"Ground truth data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if sifp_results_df is None or sifp_results_df.empty:\n",
    "        logger.error(\"SIFP results data is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"Creating combined dataset from {len(meta_judge_df)} meta judge links, \" \n",
    "                f\"{len(ground_truth_df)} ground truth links, and {len(sifp_results_df)} SIFP estimates\")\n",
    "    \n",
    "    # Create a set of ground truth link pairs for fast lookup\n",
    "    if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "        # Convert all IDs to strings for consistent comparison\n",
    "        ground_truth_df['source_id'] = ground_truth_df['source_id'].astype(str)\n",
    "        ground_truth_df['target_id'] = ground_truth_df['target_id'].astype(str)\n",
    "        \n",
    "        ground_truth_pairs = set(zip(ground_truth_df['source_id'], ground_truth_df['target_id']))\n",
    "        logger.info(f\"Created ground truth lookup set with {len(ground_truth_pairs)} link pairs\")\n",
    "        \n",
    "        # Create a copy to work with\n",
    "        result_df = meta_judge_df.copy()\n",
    "        \n",
    "        # Ensure IDs are strings\n",
    "        result_df['source_id'] = result_df['source_id'].astype(str)\n",
    "        result_df['target_id'] = result_df['target_id'].astype(str)\n",
    "        \n",
    "        # Add ground_truth_traceable column\n",
    "        result_df['ground_truth_traceable'] = result_df.apply(\n",
    "            lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Add a pair_id for easier merging with SIFP data\n",
    "        result_df['pair_id'] = result_df['source_id'] + \"_\" + result_df['target_id']\n",
    "        \n",
    "        # Now merge with SIFP data using target_id and model\n",
    "        # Ensure SIFP data has consistent string IDs\n",
    "        sifp_df = sifp_results_df.copy()\n",
    "        sifp_df['sifp_requirement_id'] = sifp_df['sifp_requirement_id'].astype(str)\n",
    "        \n",
    "        # Create merge keys that include both target_id and model for SIFP data\n",
    "        if 'model' in result_df.columns and 'sifp_model' in sifp_df.columns:\n",
    "            logger.info(\"Creating merge keys using both target_id and model\")\n",
    "            result_df['merge_key'] = result_df['target_id'] + '_' + result_df['model']\n",
    "            sifp_df['merge_key'] = sifp_df['sifp_requirement_id'] + '_' + sifp_df['sifp_model']\n",
    "            \n",
    "            # Check for duplicates in merge keys\n",
    "            if sifp_df['merge_key'].duplicated().any():\n",
    "                logger.warning(f\"Found {sifp_df['merge_key'].duplicated().sum()} duplicate merge keys in SIFP estimates. Using first occurrence.\")\n",
    "                sifp_df = sifp_df.drop_duplicates(subset=['merge_key'], keep='first')\n",
    "            \n",
    "            # Perform the merge\n",
    "            combined_df = result_df.merge(\n",
    "                sifp_df,\n",
    "                on='merge_key',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "            logger.info(f\"Merged dataframe has {len(combined_df)} rows with {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "            \n",
    "            # If the merge didn't work well, try with just target_id as fallback\n",
    "            if merge_pct < 50:\n",
    "                logger.warning(\"Poor merge results with target_id+model. Trying with just target_id as fallback.\")\n",
    "                \n",
    "                # Fallback to just target_id\n",
    "                combined_df = result_df.merge(\n",
    "                    sifp_df.drop(columns=['merge_key']),\n",
    "                    left_on='target_id',\n",
    "                    right_on='sifp_requirement_id',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_sifp')\n",
    "                )\n",
    "                \n",
    "                # Check fallback merge result\n",
    "                merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "                merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "                logger.info(f\"Fallback merge with target_id: {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "        else:\n",
    "            # If we don't have model columns in both dataframes, merge on target_id directly\n",
    "            logger.info(\"Missing model column in one or both DataFrames, merging on target_id only\")\n",
    "            combined_df = result_df.merge(\n",
    "                sifp_df,\n",
    "                left_on='target_id',\n",
    "                right_on='sifp_requirement_id',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            merge_success = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            merge_pct = merge_success / len(combined_df) * 100 if len(combined_df) > 0 else 0\n",
    "            logger.info(f\"Merged on target_id: {merge_success} valid SIFP links ({merge_pct:.1f}%)\")\n",
    "        \n",
    "        # Add error metrics if they don't exist\n",
    "        if 'sifp_abs_error' not in combined_df.columns:\n",
    "            combined_df['sifp_abs_error'] = (combined_df['sifp_final_total'] - combined_df['sifp_actor_total']).abs()\n",
    "        \n",
    "        if 'sifp_pct_error' not in combined_df.columns:\n",
    "            # Calculate percentage error, handling division by zero\n",
    "            combined_df['sifp_pct_error'] = np.nan\n",
    "            nonzero_mask = (combined_df['sifp_actor_total'] != 0) & combined_df['sifp_actor_total'].notna()\n",
    "            \n",
    "            combined_df.loc[nonzero_mask, 'sifp_pct_error'] = (\n",
    "                (combined_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                 combined_df.loc[nonzero_mask, 'sifp_actor_total']).abs() / \n",
    "                combined_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "            )\n",
    "        \n",
    "        # Log statistics by classification (after classification is applied)\n",
    "        if 'classification' in combined_df.columns:\n",
    "            logger.info(\"Calculating error metrics by classification:\")\n",
    "            for classification in combined_df['classification'].unique():\n",
    "                if pd.isna(classification):\n",
    "                    continue\n",
    "                    \n",
    "                class_df = combined_df[combined_df['classification'] == classification]\n",
    "                if len(class_df) > 0:\n",
    "                    logger.info(f\"  {classification}: {len(class_df)} links\")\n",
    "                    \n",
    "                    # Compute stats on valid data\n",
    "                    valid_data = class_df[class_df['sifp_abs_error'].notna()]\n",
    "                    if len(valid_data) > 0:\n",
    "                        logger.info(f\"    Mean Absolute Error: {valid_data['sifp_abs_error'].mean():.2f}\")\n",
    "                        \n",
    "                        pct_data = valid_data[valid_data['sifp_pct_error'].notna()]\n",
    "                        if len(pct_data) > 0:\n",
    "                            logger.info(f\"    Mean Percentage Error: {pct_data['sifp_pct_error'].mean():.2f}%\")\n",
    "                    else:\n",
    "                        logger.info(f\"    No valid SIFP data for {classification} links\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        logger.error(\"Required source_id and target_id columns missing from ground truth data\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the actual model name\n",
    "current_model_var = os.environ.get('CURRENT_MODEL', '')\n",
    "actual_model_name = os.environ.get(current_model_var, current_model_var)\n",
    "\n",
    "# Initialize best_thresholds_df as an empty DataFrame (will be filled later)\n",
    "best_thresholds_df = pd.DataFrame()\n",
    "\n",
    "# First, make sure meta_judge_df exists and has required data\n",
    "if 'meta_judge_df' not in globals() or globals()['meta_judge_df'] is None or globals()['meta_judge_df'].empty:\n",
    "    print(\"No meta judge data available. Please run previous cells first.\")\n",
    "else:\n",
    "    meta_judge_df = globals()['meta_judge_df']\n",
    "    \n",
    "    # Make sure ground_truth_df exists and has required data\n",
    "    if 'ground_truth_df' not in globals() or globals()['ground_truth_df'] is None or globals()['ground_truth_df'].empty:\n",
    "        print(\"No ground truth data available. Please run previous cells first.\")\n",
    "    else:\n",
    "        # Create a dataset for evaluation\n",
    "        ground_truth_df = globals()['ground_truth_df']\n",
    "        \n",
    "        print(f\"Preparing data for threshold evaluation:\")\n",
    "        print(f\"======================================\")\n",
    "        \n",
    "        # Create a set of ground truth link pairs for fast lookup\n",
    "        if 'source_id' in ground_truth_df.columns and 'target_id' in ground_truth_df.columns:\n",
    "            # Convert IDs to strings for consistent comparison\n",
    "            ground_truth_df['source_id'] = ground_truth_df['source_id'].astype(str)\n",
    "            ground_truth_df['target_id'] = ground_truth_df['target_id'].astype(str)\n",
    "            \n",
    "            ground_truth_pairs = set(zip(ground_truth_df['source_id'], ground_truth_df['target_id']))\n",
    "            print(f\"Created ground truth lookup set with {len(ground_truth_pairs)} link pairs\")\n",
    "            \n",
    "            # Convert source_id and target_id to strings in meta_judge_df too\n",
    "            meta_judge_df['source_id'] = meta_judge_df['source_id'].astype(str)\n",
    "            meta_judge_df['target_id'] = meta_judge_df['target_id'].astype(str)\n",
    "            \n",
    "            # Add ground_truth_traceable column to meta_judge_df\n",
    "            meta_judge_df['ground_truth_traceable'] = meta_judge_df.apply(\n",
    "                lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Add derived score columns if not already present\n",
    "            if 'judge_score' in meta_judge_df.columns and 'actor_score' in meta_judge_df.columns:\n",
    "                if 'total_score' not in meta_judge_df.columns:\n",
    "                    meta_judge_df['total_score'] = meta_judge_df['judge_score'] + meta_judge_df['actor_score']\n",
    "                \n",
    "                # Add alternative total scores for comparison\n",
    "                if 'final_score' in meta_judge_df.columns:\n",
    "                    if 'total_score_with_final' not in meta_judge_df.columns:\n",
    "                        meta_judge_df['total_score_with_final'] = meta_judge_df['actor_score'] + meta_judge_df['final_score']\n",
    "                    \n",
    "                    if 'total_score_all' not in meta_judge_df.columns:\n",
    "                        meta_judge_df['total_score_all'] = meta_judge_df['actor_score'] + meta_judge_df['judge_score'] + meta_judge_df['final_score']\n",
    "            \n",
    "            # Get list of all models\n",
    "            all_models = meta_judge_df['model'].unique()\n",
    "            \n",
    "            # Set OPTIMIZATION_METRIC if not already in globals\n",
    "            if 'OPTIMIZATION_METRIC' not in globals():\n",
    "                OPTIMIZATION_METRIC = 'F2'\n",
    "            else:\n",
    "                OPTIMIZATION_METRIC = globals()['OPTIMIZATION_METRIC']\n",
    "            \n",
    "            print(f\"Evaluating {len(all_models)} models using meta judge data\")\n",
    "            print(f\"Optimizing for {OPTIMIZATION_METRIC} score\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Define score columns to evaluate\n",
    "            score_columns_to_evaluate = [\n",
    "                'is_traceable',         # Boolean indicator\n",
    "                'actor_score',          # Individual score\n",
    "                'judge_score',          # Individual score\n",
    "                'final_score',          # Individual score\n",
    "                'total_score',          # judge_score + actor_score\n",
    "                'total_score_with_final',  # actor_score + final_score\n",
    "                'total_score_all'       # actor_score + judge_score + final_score\n",
    "            ]\n",
    "            \n",
    "            # Filter to only columns that exist\n",
    "            score_columns_to_evaluate = [\n",
    "                col for col in score_columns_to_evaluate \n",
    "                if col in meta_judge_df.columns and not meta_judge_df[col].isna().all()\n",
    "            ]\n",
    "            \n",
    "            # Evaluate models and score columns\n",
    "            evaluation_results = []\n",
    "            classified_df = meta_judge_df.copy()\n",
    "            \n",
    "            for model in all_models:\n",
    "                print(f\"Evaluating model: {model}\")\n",
    "                \n",
    "                # For each model, evaluate using different score columns\n",
    "                model_results = []\n",
    "                for score_column in score_columns_to_evaluate:\n",
    "                    print(f\"  Evaluating using {score_column}:\")\n",
    "                    result = evaluate_model_thresholds(\n",
    "                        meta_judge_df, \n",
    "                        model, \n",
    "                        score_column=score_column,\n",
    "                        optimize_for=OPTIMIZATION_METRIC\n",
    "                    )\n",
    "                    \n",
    "                    if result and 'best_threshold' in result:\n",
    "                        # Add score column to result\n",
    "                        result['score_column'] = score_column\n",
    "                        model_results.append(result)\n",
    "                        \n",
    "                        # Print key metrics\n",
    "                        print(f\"    - Best threshold: {result['best_threshold']:.3f}\")\n",
    "                        print(f\"    - Precision: {result['best_precision']:.3f}\")\n",
    "                        print(f\"    - Recall: {result['best_recall']:.3f}\")\n",
    "                        print(f\"    - F1: {result['best_f1']:.3f}\")\n",
    "                        print(f\"    - F2: {result['best_f2']:.3f}\")\n",
    "                        print(f\"    - MCC: {result['best_mcc']:.3f}\")\n",
    "                        \n",
    "                        # Apply classification to the dataframe\n",
    "                        classified_df = apply_classification_to_dataframe(classified_df, result)\n",
    "                \n",
    "                # Find best score column for this model based on optimization metric\n",
    "                if model_results:\n",
    "                    # Sort by the chosen optimization metric\n",
    "                    if OPTIMIZATION_METRIC == 'F1':\n",
    "                        model_results.sort(key=lambda x: x['best_f1'], reverse=True)\n",
    "                    else:  # F2\n",
    "                        model_results.sort(key=lambda x: x['best_f2'], reverse=True)\n",
    "                        \n",
    "                    best_result = model_results[0]\n",
    "                    print(f\"  Best performing score for {model}: {best_result['score_column']}\")\n",
    "                    print(f\"    - {OPTIMIZATION_METRIC} Score: {best_result['best_f2' if OPTIMIZATION_METRIC == 'F2' else 'best_f1']:.3f}\")\n",
    "                    \n",
    "                    # Add a 'best_model_score' classification column\n",
    "                    best_col_prefix = f\"{model}_{best_result['score_column']}\"\n",
    "                    best_classification_col = f\"{best_col_prefix}_classification\"\n",
    "                    classified_df[f\"{model}_best_classification\"] = classified_df[best_classification_col]\n",
    "                    \n",
    "                    evaluation_results.extend(model_results)\n",
    "            \n",
    "            # Create DataFrame of best thresholds with all metrics\n",
    "            if evaluation_results:\n",
    "                best_thresholds_df = pd.DataFrame([\n",
    "                    {\n",
    "                        'model_name': r['model_name'],\n",
    "                        'score_column': r['score_column'],\n",
    "                        'best_threshold': r['best_threshold'],\n",
    "                        'accuracy': r['best_accuracy'],\n",
    "                        'balanced_accuracy': r['best_balanced_accuracy'],\n",
    "                        'precision': r['best_precision'],\n",
    "                        'recall': r['best_recall'],\n",
    "                        'specificity': r['best_tnr'],\n",
    "                        'miss_rate': r['best_fnr'],\n",
    "                        'f1_score': r['best_f1'],\n",
    "                        'f2_score': r['best_f2'],\n",
    "                        'matthews_corr': r['best_mcc'],\n",
    "                        'true_positives': r['best_tp'],\n",
    "                        'false_positives': r['best_fp'],\n",
    "                        'false_negatives': r['best_fn'],\n",
    "                        'true_negatives': r['best_tn'],\n",
    "                        'data_points': r['data_points'],\n",
    "                        'ground_truth_positive': r['ground_truth_positive'],\n",
    "                        'ground_truth_negative': r['ground_truth_negative'],\n",
    "                    }\n",
    "                    for r in evaluation_results if 'best_threshold' in r\n",
    "                ])\n",
    "                \n",
    "                # Sort by the appropriate metric\n",
    "                sort_col = 'f1_score' if OPTIMIZATION_METRIC == 'F1' else 'f2_score'\n",
    "                best_thresholds_df = best_thresholds_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "                \n",
    "                print(\"Best Thresholds by Model and Score Column:\")\n",
    "                print(\"-\" * 80)\n",
    "                print(best_thresholds_df)\n",
    "                \n",
    "                # Add a simple 'classification' column that uses the best model's best score column\n",
    "                best_model_row = best_thresholds_df.iloc[0]\n",
    "                best_model = best_model_row['model_name']\n",
    "                best_score_col = best_model_row['score_column']\n",
    "                best_classification_col = f\"{best_model}_{best_score_col}_classification\"\n",
    "                \n",
    "                # Create the unified classification column\n",
    "                if best_classification_col in classified_df.columns:\n",
    "                    classified_df['classification'] = classified_df[best_classification_col]\n",
    "                    print(f\"Added 'classification' column using best model ({best_model}) and score column ({best_score_col})\")\n",
    "                    \n",
    "                    # Count by classification\n",
    "                    classification_counts = classified_df['classification'].value_counts()\n",
    "                    print(\"Classification Distribution:\")\n",
    "                    for cls, count in classification_counts.items():\n",
    "                        print(f\"  {cls}: {count} ({count/len(classified_df)*100:.2f}%)\")\n",
    "                \n",
    "                # Store the classified DataFrame for use in the next cell\n",
    "                globals()['classified_df'] = classified_df\n",
    "                globals()['best_thresholds_df'] = best_thresholds_df\n",
    "                \n",
    "                # If there's SIFP data available, create combined dataset\n",
    "                if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "                    print(\"Creating combined dataset with SIFP estimation data...\")\n",
    "                    combined_df = create_combined_dataset(\n",
    "                        classified_df, \n",
    "                        ground_truth_df, \n",
    "                        globals()['sifp_results_df']\n",
    "                    )\n",
    "                    \n",
    "                    if not combined_df.empty:\n",
    "                        globals()['combined_df'] = combined_df\n",
    "                        print(f\"Successfully created combined dataset with {len(combined_df)} rows\")\n",
    "                    else:\n",
    "                        print(\"Failed to create combined dataset\")\n",
    "            else:\n",
    "                print(\"No valid threshold results found.\")\n",
    "        else:\n",
    "            print(\"Required columns missing from ground truth data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Combine Classified Data with SIFP Estimates\n",
    "# Purpose: Merge TP/FP/FN classified links with SIFP estimation data for hallucination analysis\n",
    "# Dependencies: pandas, numpy, logging\n",
    "# Breadcrumbs: Model Evaluation -> Data Integration -> SIFP Mapping\n",
    "\n",
    "def combine_classification_with_sifp(classified_df=None, sifp_results_df=None, include_fn_as_hallucination=True):\n",
    "    \"\"\"\n",
    "    Combine classified traceability links (TP/FP/FN/TN) with SIFP estimates\n",
    "    \n",
    "    Parameters:\n",
    "        classified_df (pd.DataFrame, optional): DataFrame with classification results\n",
    "        sifp_results_df (pd.DataFrame, optional): DataFrame with SIFP estimates\n",
    "        include_fn_as_hallucination (bool): Whether to include FN as hallucination type (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset focusing on TP and hallucination links (FP+FN) with SIFP data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get required dataframes from globals if not provided\n",
    "        if classified_df is None or classified_df.empty:\n",
    "            if 'classified_df' in globals() and not globals()['classified_df'].empty:\n",
    "                classified_df = globals()['classified_df']\n",
    "            else:\n",
    "                logger.error(\"No classified data available\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        if sifp_results_df is None or sifp_results_df.empty:\n",
    "            if 'sifp_results_df' in globals() and not globals()['sifp_results_df'].empty:\n",
    "                sifp_results_df = globals()['sifp_results_df']\n",
    "            else:\n",
    "                logger.error(\"No SIFP estimates data available\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Create a copy of the classified dataframe\n",
    "        combined_df = classified_df.copy()\n",
    "        \n",
    "        # Ensure we have a classification column\n",
    "        if 'classification' not in combined_df.columns:\n",
    "            logger.error(\"Missing classification column in data\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter to TP, FP, and optionally FN links for hallucination analysis\n",
    "        if include_fn_as_hallucination:\n",
    "            # Include FN as a type of hallucination (missed features)\n",
    "            analysis_classifications = ['TP', 'FP', 'FN']\n",
    "            logger.info(\"Including FN (False Negatives) as hallucination type alongside FP\")\n",
    "        else:\n",
    "            # Original approach - only FP as hallucination\n",
    "            analysis_classifications = ['TP', 'FP']\n",
    "            logger.info(\"Using only FP (False Positives) as hallucination type\")\n",
    "        \n",
    "        tp_fp_fn_df = combined_df[combined_df['classification'].isin(analysis_classifications)].copy()\n",
    "        logger.info(f\"Filtered to {analysis_classifications} classifications: {len(tp_fp_fn_df)} rows from {len(combined_df)} total\")\n",
    "        \n",
    "        # Add hallucination indicator column\n",
    "        # TP = Non-hallucination (correct identification)\n",
    "        # FP = Hallucination (over-identification)  \n",
    "        # FN = Hallucination (missed features/under-identification)\n",
    "        tp_fp_fn_df['is_hallucination'] = tp_fp_fn_df['classification'].isin(['FP', 'FN'])\n",
    "        tp_fp_fn_df['hallucination_type'] = tp_fp_fn_df['classification'].apply(\n",
    "            lambda x: 'Correct_Identification' if x == 'TP' \n",
    "                     else 'Over_Identification' if x == 'FP'\n",
    "                     else 'Under_Identification' if x == 'FN'\n",
    "                     else 'Unknown'\n",
    "        )\n",
    "        \n",
    "        # Ensure target_id and model are strings for consistent comparison\n",
    "        tp_fp_fn_df['target_id'] = tp_fp_fn_df['target_id'].astype(str)\n",
    "        sifp_results_df['sifp_requirement_id'] = sifp_results_df['sifp_requirement_id'].astype(str)\n",
    "        \n",
    "        if 'model' in tp_fp_fn_df.columns and 'sifp_model' in sifp_results_df.columns:\n",
    "            tp_fp_fn_df['model'] = tp_fp_fn_df['model'].astype(str)\n",
    "            sifp_results_df['sifp_model'] = sifp_results_df['sifp_model'].astype(str)\n",
    "            \n",
    "            # Create merge keys that include both target_id and model\n",
    "            logger.info(\"Creating merge keys using both target_id and model\")\n",
    "            tp_fp_fn_df['merge_key'] = tp_fp_fn_df['target_id'] + '_' + tp_fp_fn_df['model']\n",
    "            sifp_results_df['merge_key'] = sifp_results_df['sifp_requirement_id'] + '_' + sifp_results_df['sifp_model']\n",
    "            \n",
    "            # Check for duplicates in merge keys\n",
    "            if sifp_results_df['merge_key'].duplicated().any():\n",
    "                logger.warning(f\"Found {sifp_results_df['merge_key'].duplicated().sum()} duplicate merge keys in SIFP estimates. Using first occurrence.\")\n",
    "                sifp_results_df = sifp_results_df.drop_duplicates(subset=['merge_key'], keep='first')\n",
    "            \n",
    "            # Perform the merge\n",
    "            merged_df = tp_fp_fn_df.merge(\n",
    "                sifp_results_df,\n",
    "                on='merge_key',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            # Check merge result\n",
    "            logger.info(f\"Merged dataframe has {len(merged_df)} rows\")\n",
    "            merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            logger.info(f\"Found {merge_success} rows with valid SIFP data after merge ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "            \n",
    "            # If the merge didn't work well, try with just target_id\n",
    "            if merge_success < len(merged_df) * 0.5:\n",
    "                logger.warning(\"Poor merge results with target_id+model. Trying with just target_id.\")\n",
    "                \n",
    "                # Fallback to just target_id\n",
    "                merged_df = tp_fp_fn_df.merge(\n",
    "                    sifp_results_df.drop(columns=['merge_key']),\n",
    "                    left_on='target_id',\n",
    "                    right_on='sifp_requirement_id',\n",
    "                    how='left',\n",
    "                    suffixes=('', '_sifp')\n",
    "                )\n",
    "                \n",
    "                merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "                logger.info(f\"Fallback merge with target_id: {merge_success} rows with valid SIFP data ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "        else:\n",
    "            # If we don't have model columns in both dataframes, merge on target_id directly\n",
    "            logger.info(\"Missing model column in one or both DataFrames, merging on target_id only\")\n",
    "            merged_df = tp_fp_fn_df.merge(\n",
    "                sifp_results_df,\n",
    "                left_on='target_id',\n",
    "                right_on='sifp_requirement_id',\n",
    "                how='left',\n",
    "                suffixes=('', '_sifp')\n",
    "            )\n",
    "            \n",
    "            merge_success = merged_df[['sifp_actor_total', 'sifp_final_total']].notna().any(axis=1).sum()\n",
    "            logger.info(f\"Merged on target_id: {merge_success} rows with valid SIFP data ({merge_success/len(merged_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Add error metrics if they don't exist\n",
    "        if 'sifp_abs_error' not in merged_df.columns:\n",
    "            merged_df['sifp_abs_error'] = (merged_df['sifp_final_total'] - merged_df['sifp_actor_total']).abs()\n",
    "        \n",
    "        if 'sifp_pct_error' not in merged_df.columns:\n",
    "            # Calculate percentage error, handling division by zero\n",
    "            merged_df['sifp_pct_error'] = np.nan\n",
    "            nonzero_mask = (merged_df['sifp_actor_total'] != 0) & merged_df['sifp_actor_total'].notna()\n",
    "            \n",
    "            merged_df.loc[nonzero_mask, 'sifp_pct_error'] = (\n",
    "                (merged_df.loc[nonzero_mask, 'sifp_final_total'] - \n",
    "                 merged_df.loc[nonzero_mask, 'sifp_actor_total']).abs() / \n",
    "                merged_df.loc[nonzero_mask, 'sifp_actor_total'] * 100\n",
    "            )\n",
    "        \n",
    "        # Calculate aggregated error metrics by classification and hallucination type\n",
    "        if merge_success > 0:\n",
    "            logger.info(\"Calculating error metrics by classification and hallucination type:\")\n",
    "            \n",
    "            # By individual classification\n",
    "            for classification in analysis_classifications:\n",
    "                class_df = merged_df[merged_df['classification'] == classification]\n",
    "                if len(class_df) > 0:\n",
    "                    abs_error_mean = class_df['sifp_abs_error'].mean()\n",
    "                    pct_error_mean = class_df.loc[class_df['sifp_pct_error'].notna(), 'sifp_pct_error'].mean()\n",
    "                    \n",
    "                    logger.info(f\"  {classification}: {len(class_df)} links\")\n",
    "                    logger.info(f\"    Mean Absolute Error: {abs_error_mean:.2f}\")\n",
    "                    logger.info(f\"    Mean Percentage Error: {pct_error_mean:.2f}%\")\n",
    "            \n",
    "            # By hallucination status (TP vs All Hallucinations)\n",
    "            hallucination_groups = merged_df.groupby('is_hallucination')\n",
    "            for is_halluc, group_df in hallucination_groups:\n",
    "                group_name = \"Hallucinations (FP+FN)\" if is_halluc else \"Correct Identification (TP)\"\n",
    "                abs_error_mean = group_df['sifp_abs_error'].mean()\n",
    "                pct_error_mean = group_df.loc[group_df['sifp_pct_error'].notna(), 'sifp_pct_error'].mean()\n",
    "                \n",
    "                logger.info(f\"  {group_name}: {len(group_df)} links\")\n",
    "                logger.info(f\"    Mean Absolute Error: {abs_error_mean:.2f}\")\n",
    "                logger.info(f\"    Mean Percentage Error: {pct_error_mean:.2f}%\")\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining classified data with SIFP estimates: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run the combination process\n",
    "print(f\"Combining Classified Links with SIFP Estimates (Including FN as Hallucination):\") \n",
    "print(f\"===============================================================================\")\n",
    "\n",
    "combined_df = combine_classification_with_sifp(include_fn_as_hallucination=True)\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Display summary statistics\n",
    "    print(f\"Successfully created combined dataset with {len(combined_df)} rows!\")\n",
    "    \n",
    "    # Count rows by classification\n",
    "    if 'classification' in combined_df.columns:\n",
    "        classification_counts = combined_df['classification'].value_counts()\n",
    "        print(\"Distribution by classification:\")\n",
    "        for cls, count in classification_counts.items():\n",
    "            print(f\"  {cls}: {count} ({count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows by hallucination status\n",
    "    if 'is_hallucination' in combined_df.columns:\n",
    "        hallucination_counts = combined_df['is_hallucination'].value_counts()\n",
    "        print(\"Distribution by hallucination status:\")\n",
    "        print(f\"  Correct Identification (TP): {hallucination_counts.get(False, 0)} ({hallucination_counts.get(False, 0)/len(combined_df)*100:.1f}%)\")\n",
    "        print(f\"  Hallucinations (FP+FN): {hallucination_counts.get(True, 0)} ({hallucination_counts.get(True, 0)/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows by hallucination type\n",
    "    if 'hallucination_type' in combined_df.columns:\n",
    "        type_counts = combined_df['hallucination_type'].value_counts()\n",
    "        print(\"Distribution by hallucination type:\")\n",
    "        for htype, count in type_counts.items():\n",
    "            print(f\"  {htype}: {count} ({count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Count rows with SIFP data\n",
    "    sifp_count = combined_df[['sifp_actor_total', 'sifp_final_total']].notna().all(axis=1).sum()\n",
    "    print(f\"Rows with complete SIFP data: {sifp_count} ({sifp_count/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Show error metrics by classification\n",
    "    if sifp_count > 0 and 'classification' in combined_df.columns:\n",
    "        print(\"SIFP Error Metrics by Classification:\")\n",
    "        for classification in ['TP', 'FP', 'FN']:\n",
    "            if classification in combined_df['classification'].values:\n",
    "                class_df = combined_df[\n",
    "                    (combined_df['classification'] == classification) & \n",
    "                    combined_df[['sifp_abs_error', 'sifp_actor_total']].notna().all(axis=1)\n",
    "                ]\n",
    "                \n",
    "                if len(class_df) > 0:\n",
    "                    print(f\"  {classification} Links ({len(class_df)} rows):\")\n",
    "                    print(f\"    Mean SIFP Actor Total: {class_df['sifp_actor_total'].mean():.2f}\")\n",
    "                    print(f\"    Mean SIFP Final Total: {class_df['sifp_final_total'].mean():.2f}\")\n",
    "                    print(f\"    Mean Absolute Error: {class_df['sifp_abs_error'].mean():.2f}\")\n",
    "                    \n",
    "                    pct_error_df = class_df[class_df['sifp_pct_error'].notna()]\n",
    "                    if len(pct_error_df) > 0:\n",
    "                        print(f\"    Mean Percentage Error: {pct_error_df['sifp_pct_error'].mean():.2f}%\")\n",
    "        \n",
    "        # Show error metrics by hallucination status\n",
    "        print(\"SIFP Error Metrics by Hallucination Status:\")\n",
    "        for is_halluc, group_name in [(False, \"Correct Identification (TP)\"), (True, \"Hallucinations (FP+FN)\")]:\n",
    "            group_df = combined_df[\n",
    "                (combined_df['is_hallucination'] == is_halluc) & \n",
    "                combined_df[['sifp_abs_error', 'sifp_actor_total']].notna().all(axis=1)\n",
    "            ]\n",
    "            \n",
    "            if len(group_df) > 0:\n",
    "                print(f\"  {group_name} ({len(group_df)} rows):\")\n",
    "                print(f\"    Mean SIFP Actor Total: {group_df['sifp_actor_total'].mean():.2f}\")\n",
    "                print(f\"    Mean SIFP Final Total: {group_df['sifp_final_total'].mean():.2f}\")\n",
    "                print(f\"    Mean Absolute Error: {group_df['sifp_abs_error'].mean():.2f}\")\n",
    "                \n",
    "                pct_error_df = group_df[group_df['sifp_pct_error'].notna()]\n",
    "                if len(pct_error_df) > 0:\n",
    "                    print(f\"    Mean Percentage Error: {pct_error_df['sifp_pct_error'].mean():.2f}%\")\n",
    "    \n",
    "    # Display sample of the combined dataset\n",
    "    print(f\"Sample of combined dataset ({min(5, len(combined_df))} rows):\")\n",
    "    display_cols = ['classification', 'is_hallucination', 'hallucination_type', 'source_id', 'target_id', 'model', \n",
    "                    'sifp_actor_total', 'sifp_final_total', 'sifp_abs_error', 'sifp_pct_error']\n",
    "    display_cols = [col for col in display_cols if col in combined_df.columns]\n",
    "    print(combined_df[display_cols].head())\n",
    "    \n",
    "    # Store in global namespace for next cells\n",
    "    globals()['combined_df'] = combined_df\n",
    "else:\n",
    "    print(\"No combined dataset created. Check error logs.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Calculate Estimation Accuracy Metrics\n",
    "# Purpose: Compute accuracy metrics for SiFP estimations by classification type including FN hallucinations\n",
    "# Dependencies: pandas, numpy, sklearn.metrics\n",
    "# Breadcrumbs: Data Integration -> Accuracy Analysis -> SiFP Metrics\n",
    "\n",
    "def calculate_sifp_accuracy_metrics(df=None, group_by_classification=True, include_hallucination_analysis=True):\n",
    "    \"\"\"\n",
    "    Calculate accuracy metrics for SiFP estimations including hallucination analysis\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        group_by_classification (bool): Whether to calculate metrics by TP/FP/FN classification\n",
    "        include_hallucination_analysis (bool): Whether to include hallucination vs non-hallucination analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy metrics DataFrames and analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global combined_df if not provided\n",
    "        df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(\"No data available for calculating accuracy metrics\")\n",
    "            return {}\n",
    "        \n",
    "        # Filter to rows that have SIFP data\n",
    "        sifp_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total'])\n",
    "        \n",
    "        if sifp_df.empty:\n",
    "            logger.error(\"No SIFP data available for calculating accuracy metrics\")\n",
    "            return {}\n",
    "        \n",
    "        logger.info(f\"Calculating accuracy metrics on {len(sifp_df)} links with SIFP data\")\n",
    "        \n",
    "        # Define metric calculation function\n",
    "        def calculate_metrics(data):\n",
    "            if len(data) == 0:\n",
    "                return pd.Series({\n",
    "                    'count': 0,\n",
    "                    'mae': np.nan,\n",
    "                    'mse': np.nan,\n",
    "                    'rmse': np.nan,\n",
    "                    'mape': np.nan,\n",
    "                    'r2': np.nan,\n",
    "                    'pearson_r': np.nan,\n",
    "                    'pearson_p': np.nan,\n",
    "                    'mean_abs_error': np.nan,\n",
    "                    'mean_pct_error': np.nan,\n",
    "                    'std_abs_error': np.nan,\n",
    "                    'std_pct_error': np.nan,\n",
    "                    'mean_actor_total': np.nan,\n",
    "                    'mean_final_total': np.nan,\n",
    "                    'median_actor_total': np.nan,\n",
    "                    'median_final_total': np.nan\n",
    "                })\n",
    "            \n",
    "            # Basic metrics\n",
    "            y_true = data['sifp_actor_total']\n",
    "            y_pred = data['sifp_final_total']\n",
    "            \n",
    "            # Calculate standard ML metrics\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "            # Filter out zeros to avoid division by zero\n",
    "            nonzero_mask = y_true != 0\n",
    "            if nonzero_mask.sum() > 0:\n",
    "                mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            # R-squared\n",
    "            r2 = r2_score(y_true, y_pred) if len(data) > 1 else np.nan\n",
    "            \n",
    "            # Pearson correlation\n",
    "            if len(data) > 1:\n",
    "                pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n",
    "            else:\n",
    "                pearson_r, pearson_p = np.nan, np.nan\n",
    "            \n",
    "            # Custom metrics calculated earlier\n",
    "            abs_error = data['sifp_abs_error'].mean() if 'sifp_abs_error' in data.columns else np.nan\n",
    "            pct_error = data['sifp_pct_error'].mean() if 'sifp_pct_error' in data.columns else np.nan\n",
    "            \n",
    "            std_abs_error = data['sifp_abs_error'].std() if 'sifp_abs_error' in data.columns else np.nan\n",
    "            std_pct_error = data['sifp_pct_error'].std() if 'sifp_pct_error' in data.columns else np.nan\n",
    "            \n",
    "            # Basic statistics\n",
    "            mean_actor = y_true.mean()\n",
    "            mean_final = y_pred.mean()\n",
    "            median_actor = y_true.median()\n",
    "            median_final = y_pred.median()\n",
    "            \n",
    "            return pd.Series({\n",
    "                'count': len(data),\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape,\n",
    "                'r2': r2,\n",
    "                'pearson_r': pearson_r,\n",
    "                'pearson_p': pearson_p,\n",
    "                'mean_abs_error': abs_error,\n",
    "                'mean_pct_error': pct_error,\n",
    "                'std_abs_error': std_abs_error,\n",
    "                'std_pct_error': std_pct_error,\n",
    "                'mean_actor_total': mean_actor,\n",
    "                'mean_final_total': mean_final,\n",
    "                'median_actor_total': median_actor,\n",
    "                'median_final_total': median_final\n",
    "            })\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Calculate global metrics\n",
    "        global_metrics = calculate_metrics(sifp_df)\n",
    "        results['overall'] = pd.DataFrame([global_metrics], index=['Overall'])\n",
    "        \n",
    "        # Calculate metrics by individual classification if requested\n",
    "        if group_by_classification and 'classification' in sifp_df.columns:\n",
    "            # Group data by classification and calculate metrics for each group\n",
    "            classification_groups = ['TP', 'FP', 'FN']\n",
    "            available_classifications = [cls for cls in classification_groups if cls in sifp_df['classification'].values]\n",
    "            \n",
    "            if available_classifications:\n",
    "                grouped_metrics = sifp_df[sifp_df['classification'].isin(available_classifications)].groupby('classification').apply(calculate_metrics)\n",
    "                \n",
    "                # Create a DataFrame with all metrics\n",
    "                classification_metrics_df = pd.DataFrame([global_metrics]).T.rename(columns={0: 'Overall'})\n",
    "                \n",
    "                # For each classification group, add metrics to the DataFrame\n",
    "                for cls in grouped_metrics.index:\n",
    "                    classification_metrics_df[cls] = grouped_metrics.loc[cls]\n",
    "                \n",
    "                # Convert to a more readable format\n",
    "                classification_metrics_df = classification_metrics_df.T\n",
    "                \n",
    "                # Reorder index to put Overall first if it exists\n",
    "                if 'Overall' in classification_metrics_df.index:\n",
    "                    new_index = ['Overall'] + [idx for idx in classification_metrics_df.index if idx != 'Overall']\n",
    "                    classification_metrics_df = classification_metrics_df.reindex(new_index)\n",
    "                \n",
    "                results['by_classification'] = classification_metrics_df\n",
    "                logger.info(f\"Calculated accuracy metrics for {len(classification_metrics_df)} classification groups\")\n",
    "        \n",
    "        # Calculate metrics by hallucination status if requested\n",
    "        if include_hallucination_analysis and 'is_hallucination' in sifp_df.columns:\n",
    "            hallucination_metrics = sifp_df.groupby('is_hallucination').apply(calculate_metrics)\n",
    "            \n",
    "            # Create DataFrame with hallucination analysis\n",
    "            hallucination_metrics_df = pd.DataFrame()\n",
    "            \n",
    "            # Add metrics for each group\n",
    "            for is_halluc in hallucination_metrics.index:\n",
    "                group_name = \"Hallucinations_FP_FN\" if is_halluc else \"Correct_Identification_TP\"\n",
    "                hallucination_metrics_df[group_name] = hallucination_metrics.loc[is_halluc]\n",
    "            \n",
    "            # Add overall metrics\n",
    "            hallucination_metrics_df['Overall'] = global_metrics\n",
    "            \n",
    "            # Convert to readable format\n",
    "            hallucination_metrics_df = hallucination_metrics_df.T\n",
    "            \n",
    "            # Reorder to put Overall first\n",
    "            if 'Overall' in hallucination_metrics_df.index:\n",
    "                new_index = ['Overall'] + [idx for idx in hallucination_metrics_df.index if idx != 'Overall']\n",
    "                hallucination_metrics_df = hallucination_metrics_df.reindex(new_index)\n",
    "            \n",
    "            results['by_hallucination_status'] = hallucination_metrics_df\n",
    "            logger.info(f\"Calculated hallucination analysis metrics\")\n",
    "        \n",
    "        # Calculate metrics by hallucination type if available\n",
    "        if include_hallucination_analysis and 'hallucination_type' in sifp_df.columns:\n",
    "            hallucination_type_metrics = sifp_df.groupby('hallucination_type').apply(calculate_metrics)\n",
    "            \n",
    "            # Create DataFrame with hallucination type analysis\n",
    "            hallucination_type_metrics_df = pd.DataFrame()\n",
    "            \n",
    "            # Add metrics for each group\n",
    "            for htype in hallucination_type_metrics.index:\n",
    "                hallucination_type_metrics_df[htype] = hallucination_type_metrics.loc[htype]\n",
    "            \n",
    "            # Add overall metrics\n",
    "            hallucination_type_metrics_df['Overall'] = global_metrics\n",
    "            \n",
    "            # Convert to readable format\n",
    "            hallucination_type_metrics_df = hallucination_type_metrics_df.T\n",
    "            \n",
    "            # Reorder to put Overall first\n",
    "            if 'Overall' in hallucination_type_metrics_df.index:\n",
    "                new_index = ['Overall'] + [idx for idx in hallucination_type_metrics_df.index if idx != 'Overall']\n",
    "                hallucination_type_metrics_df = hallucination_type_metrics_df.reindex(new_index)\n",
    "            \n",
    "            results['by_hallucination_type'] = hallucination_type_metrics_df\n",
    "            logger.info(f\"Calculated hallucination type analysis metrics\")\n",
    "        \n",
    "        return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating accuracy metrics: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "try:\n",
    "    accuracy_results = calculate_sifp_accuracy_metrics(include_hallucination_analysis=True)\n",
    "    print(f\"SIFP Estimation Accuracy Metrics with Hallucination Analysis:\")\n",
    "    print(f\"============================================================\")\n",
    "    \n",
    "    if accuracy_results:\n",
    "        # Display overall metrics\n",
    "        if 'overall' in accuracy_results:\n",
    "            print(\"Overall Metrics:\")\n",
    "            print(accuracy_results['overall'].round(2))\n",
    "            print()\n",
    "        \n",
    "        # Display metrics by individual classification (TP, FP, FN)\n",
    "        if 'by_classification' in accuracy_results:\n",
    "            print(\"Metrics by Individual Classification:\")\n",
    "            display_metrics = accuracy_results['by_classification'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = display_metrics.select_dtypes(include=[np.number]).columns\n",
    "            display_metrics[numeric_cols] = display_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(display_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Compare TP vs FP vs FN metrics\n",
    "            available_classes = [cls for cls in ['TP', 'FP', 'FN'] if cls in display_metrics.index]\n",
    "            if len(available_classes) >= 2:\n",
    "                print(\"Classification Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape', 'r2']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in display_metrics.columns]\n",
    "                \n",
    "                class_comparison = display_metrics.loc[available_classes, comparison_metrics]\n",
    "                print(class_comparison)\n",
    "                print()\n",
    "        \n",
    "        # Display metrics by hallucination status (TP vs FP+FN)\n",
    "        if 'by_hallucination_status' in accuracy_results:\n",
    "            print(\"Metrics by Hallucination Status (KEY ANALYSIS):\")\n",
    "            halluc_metrics = accuracy_results['by_hallucination_status'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = halluc_metrics.select_dtypes(include=[np.number]).columns\n",
    "            halluc_metrics[numeric_cols] = halluc_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(halluc_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Calculate differences between TP and Hallucinations\n",
    "            if 'Correct_Identification_TP' in halluc_metrics.index and 'Hallucinations_FP_FN' in halluc_metrics.index:\n",
    "                print(\"TP vs Hallucinations (FP+FN) Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape', 'r2']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in halluc_metrics.columns]\n",
    "                \n",
    "                tp_halluc_comparison = halluc_metrics.loc[['Correct_Identification_TP', 'Hallucinations_FP_FN'], comparison_metrics]\n",
    "                \n",
    "                # Calculate the difference and percentage difference\n",
    "                if not tp_halluc_comparison.empty:\n",
    "                    diff = tp_halluc_comparison.loc['Correct_Identification_TP'] - tp_halluc_comparison.loc['Hallucinations_FP_FN']\n",
    "                    pct_diff = ((tp_halluc_comparison.loc['Correct_Identification_TP'] - tp_halluc_comparison.loc['Hallucinations_FP_FN']) / \n",
    "                               tp_halluc_comparison.loc['Hallucinations_FP_FN'] * 100).round(1)\n",
    "                    \n",
    "                    # Skip count column for percentage difference\n",
    "                    if 'count' in pct_diff.index:\n",
    "                        pct_diff['count'] = np.nan\n",
    "                    \n",
    "                    # Add comparison rows to the DataFrame\n",
    "                    tp_halluc_comparison.loc['Difference (TP - Hallucinations)'] = diff\n",
    "                    tp_halluc_comparison.loc['% Difference'] = pct_diff\n",
    "                    \n",
    "                    # Format and display\n",
    "                    print(tp_halluc_comparison.round(2))\n",
    "                    print()\n",
    "                    \n",
    "                    # Highlight key insights for hypothesis testing\n",
    "                    print(\"Key Insights for Hypothesis 2:\")\n",
    "                    for metric in ['mean_abs_error', 'mean_pct_error', 'rmse', 'mape']:\n",
    "                        if metric in diff.index and not np.isnan(diff[metric]):\n",
    "                            if diff[metric] < 0:\n",
    "                                print(f\"- TP links have {abs(diff[metric]):.2f} LOWER {metric} than Hallucination links (FP+FN) - {abs(pct_diff[metric]):.1f}% better\")\n",
    "                                print(f\"  â†’ SUPPORTS hypothesis: Hallucinations correlate with HIGHER estimation errors\")\n",
    "                            elif diff[metric] > 0:\n",
    "                                print(f\"- TP links have {diff[metric]:.2f} HIGHER {metric} than Hallucination links (FP+FN) - {pct_diff[metric]:.1f}% worse\")\n",
    "                                print(f\"  â†’ CONTRADICTS hypothesis: Hallucinations correlate with LOWER estimation errors\")\n",
    "                            else:\n",
    "                                print(f\"- TP and Hallucination links show NO difference in {metric}\")\n",
    "        \n",
    "        # Display metrics by hallucination type (Over vs Under identification)\n",
    "        if 'by_hallucination_type' in accuracy_results:\n",
    "            print(\"Metrics by Hallucination Type:\")\n",
    "            type_metrics = accuracy_results['by_hallucination_type'].copy()\n",
    "            \n",
    "            # Round numeric columns to 2 decimal places for display\n",
    "            numeric_cols = type_metrics.select_dtypes(include=[np.number]).columns\n",
    "            type_metrics[numeric_cols] = type_metrics[numeric_cols].round(2)\n",
    "            \n",
    "            print(type_metrics)\n",
    "            print()\n",
    "            \n",
    "            # Compare Over vs Under identification if both exist\n",
    "            if 'Over_Identification' in type_metrics.index and 'Under_Identification' in type_metrics.index:\n",
    "                print(\"Over-Identification (FP) vs Under-Identification (FN) Comparison:\")\n",
    "                comparison_metrics = ['count', 'mean_abs_error', 'mean_pct_error', 'rmse', 'mape']\n",
    "                comparison_metrics = [m for m in comparison_metrics if m in type_metrics.columns]\n",
    "                \n",
    "                over_under_comparison = type_metrics.loc[['Over_Identification', 'Under_Identification'], comparison_metrics]\n",
    "                print(over_under_comparison)\n",
    "                print()\n",
    "        \n",
    "        # Store results in global namespace for next cells\n",
    "        globals()['accuracy_results'] = accuracy_results\n",
    "        \n",
    "        # Also store the main metrics DataFrame for backward compatibility\n",
    "        if 'by_classification' in accuracy_results:\n",
    "            globals()['accuracy_metrics'] = accuracy_results['by_classification']\n",
    "        elif 'overall' in accuracy_results:\n",
    "            globals()['accuracy_metrics'] = accuracy_results['overall']\n",
    "    else:\n",
    "        print(\"No accuracy metrics could be calculated. Check logs for details.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during accuracy metrics calculation: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Compare TP vs Hallucinations (FP+FN) Estimation Accuracy\n",
    "# Purpose: Analyze and compare SiFP accuracy between TP links and Hallucination links (FP+FN) with statistical tests\n",
    "# Dependencies: pandas, numpy, scipy.stats\n",
    "# Breadcrumbs: Accuracy Analysis -> Statistical Comparison -> Hypothesis 2 Testing\n",
    "\n",
    "def compare_tp_vs_hallucinations_accuracy(df=None, accuracy_results=None):\n",
    "    \"\"\"\n",
    "    Perform detailed comparison of SiFP accuracy between TP links and Hallucination links (FP+FN)\n",
    "    Core analysis for Hypothesis 2: \"Hallucinations significantly correlate with lower quality estimates\"\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        accuracy_results (dict, optional): Accuracy results from previous cell\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing comprehensive comparison results and statistical tests\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global variables if not provided\n",
    "        df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(\"No combined data available for TP vs Hallucinations comparison\")\n",
    "            return {}\n",
    "            \n",
    "        # Filter for rows with SIFP data and classification information\n",
    "        valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "        valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "        \n",
    "        if len(valid_df) == 0:\n",
    "            logger.error(\"No valid TP, FP, or FN data with SIFP estimations found\")\n",
    "            return {}\n",
    "            \n",
    "        # Split into TP (non-hallucination) and Hallucination (FP+FN) datasets\n",
    "        tp_df = valid_df[valid_df['is_hallucination'] == False]  # TP links\n",
    "        hallucination_df = valid_df[valid_df['is_hallucination'] == True]  # FP + FN links\n",
    "        \n",
    "        logger.info(f\"Comparing {len(tp_df)} TP links with {len(hallucination_df)} Hallucination links (FP+FN)\")\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            'hypothesis': \"Hallucinations in LLM outputs significantly correlate with lower quality estimates\",\n",
    "            'comparison_type': 'TP_vs_Hallucinations_FP_FN',\n",
    "            'counts': {\n",
    "                'TP': len(tp_df),\n",
    "                'Hallucinations_FP_FN': len(hallucination_df),\n",
    "                'FP_only': len(valid_df[valid_df['classification'] == 'FP']),\n",
    "                'FN_only': len(valid_df[valid_df['classification'] == 'FN']),\n",
    "                'total': len(valid_df)\n",
    "            },\n",
    "            'metrics': {},\n",
    "            'statistical_tests': {},\n",
    "            'effect_sizes': {},\n",
    "            'hypothesis_conclusion': {}\n",
    "        }\n",
    "        \n",
    "        # Check if we have enough data for meaningful comparison\n",
    "        if len(tp_df) < 5 or len(hallucination_df) < 5:\n",
    "            logger.warning(\"Sample size too small for reliable statistical comparison\")\n",
    "            results['warning'] = \"Sample size too small for reliable statistical comparison\"\n",
    "            return results\n",
    "            \n",
    "        # Extract metrics for comparison\n",
    "        metrics_to_compare = ['sifp_abs_error', 'sifp_pct_error']\n",
    "        \n",
    "        for metric in metrics_to_compare:\n",
    "            if metric in tp_df.columns and metric in hallucination_df.columns:\n",
    "                # Get the data\n",
    "                tp_values = tp_df[metric].dropna()\n",
    "                halluc_values = hallucination_df[metric].dropna()\n",
    "                \n",
    "                if len(tp_values) == 0 or len(halluc_values) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Basic descriptive statistics\n",
    "                results['metrics'][metric] = {\n",
    "                    'TP': {\n",
    "                        'mean': tp_values.mean(),\n",
    "                        'median': tp_values.median(),\n",
    "                        'std': tp_values.std(),\n",
    "                        'count': len(tp_values)\n",
    "                    },\n",
    "                    'Hallucinations_FP_FN': {\n",
    "                        'mean': halluc_values.mean(),\n",
    "                        'median': halluc_values.median(),\n",
    "                        'std': halluc_values.std(),\n",
    "                        'count': len(halluc_values)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Calculate difference and hypothesis direction\n",
    "                mean_diff = tp_values.mean() - halluc_values.mean()\n",
    "                pct_diff = (mean_diff / halluc_values.mean()) * 100 if halluc_values.mean() != 0 else np.nan\n",
    "                    \n",
    "                results['metrics'][metric]['difference'] = {\n",
    "                    'absolute': mean_diff,\n",
    "                    'percent': pct_diff\n",
    "                }\n",
    "                \n",
    "                # Hypothesis support check: mean_diff < 0 means TP has lower error â†’ SUPPORTS hypothesis\n",
    "                hypothesis_support = mean_diff < 0\n",
    "                results['metrics'][metric]['hypothesis_support'] = {\n",
    "                    'supports_hypothesis': hypothesis_support,\n",
    "                    'interpretation': f\"TP links have {'LOWER' if hypothesis_support else 'HIGHER'} {metric} than Hallucinations\"\n",
    "                }\n",
    "                \n",
    "                # Statistical tests\n",
    "                results['statistical_tests'][metric] = {}\n",
    "                \n",
    "                # Check for normality with Shapiro-Wilk test\n",
    "                if len(tp_values) <= 5000 and len(halluc_values) <= 5000:\n",
    "                    try:\n",
    "                        _, tp_norm_p = stats.shapiro(tp_values)\n",
    "                        _, halluc_norm_p = stats.shapiro(halluc_values)\n",
    "                        tp_is_normal = tp_norm_p > 0.05\n",
    "                        halluc_is_normal = halluc_norm_p > 0.05\n",
    "                        both_normal = tp_is_normal and halluc_is_normal\n",
    "                    except Exception:\n",
    "                        both_normal = False\n",
    "                else:\n",
    "                    both_normal = False\n",
    "                \n",
    "                results['statistical_tests'][metric]['normality'] = {\n",
    "                    'both_normal': both_normal\n",
    "                }\n",
    "                \n",
    "                # Main comparison tests\n",
    "                if both_normal:\n",
    "                    # Parametric: t-test\n",
    "                    try:\n",
    "                        _, levene_p = stats.levene(tp_values, halluc_values)\n",
    "                        equal_var = levene_p > 0.05\n",
    "                        t_stat, t_p = stats.ttest_ind(tp_values, halluc_values, equal_var=equal_var)\n",
    "                        \n",
    "                        results['statistical_tests'][metric]['parametric'] = {\n",
    "                            'test': 't-test',\n",
    "                            'statistic': t_stat,\n",
    "                            'p_value': t_p,\n",
    "                            'significant': t_p < 0.05,\n",
    "                            'hypothesis_result': 'SUPPORTS' if (t_p < 0.05 and hypothesis_support) else 'CONTRADICTS' if (t_p < 0.05 and not hypothesis_support) else 'INCONCLUSIVE'\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"t-test failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                # Non-parametric: Mann-Whitney U test\n",
    "                try:\n",
    "                    u_stat, u_p = stats.mannwhitneyu(tp_values, halluc_values, alternative='two-sided')\n",
    "                    \n",
    "                    results['statistical_tests'][metric]['non_parametric'] = {\n",
    "                        'test': 'Mann-Whitney U',\n",
    "                        'statistic': u_stat,\n",
    "                        'p_value': u_p,\n",
    "                        'significant': u_p < 0.05,\n",
    "                        'hypothesis_result': 'SUPPORTS' if (u_p < 0.05 and hypothesis_support) else 'CONTRADICTS' if (u_p < 0.05 and not hypothesis_support) else 'INCONCLUSIVE'\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Mann-Whitney U test failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                # Effect sizes\n",
    "                effect_sizes = {}\n",
    "                \n",
    "                # Cohen's d\n",
    "                try:\n",
    "                    pooled_std = np.sqrt(((len(tp_values) - 1) * tp_values.std()**2 + \n",
    "                                         (len(halluc_values) - 1) * halluc_values.std()**2) / \n",
    "                                         (len(tp_values) + len(halluc_values) - 2))\n",
    "                    if pooled_std > 0:\n",
    "                        cohen_d = mean_diff / pooled_std\n",
    "                        \n",
    "                        if abs(cohen_d) < 0.2:\n",
    "                            effect_interpretation = \"Negligible\"\n",
    "                        elif abs(cohen_d) < 0.5:\n",
    "                            effect_interpretation = \"Small\"\n",
    "                        elif abs(cohen_d) < 0.8:\n",
    "                            effect_interpretation = \"Medium\"\n",
    "                        else:\n",
    "                            effect_interpretation = \"Large\"\n",
    "                        \n",
    "                        effect_sizes['cohen_d'] = {\n",
    "                            'value': cohen_d,\n",
    "                            'interpretation': effect_interpretation\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Cohen's d calculation failed for {metric}: {str(e)}\")\n",
    "                \n",
    "                results['effect_sizes'][metric] = effect_sizes\n",
    "        \n",
    "        # Overall hypothesis conclusion\n",
    "        significant_results = []\n",
    "        \n",
    "        for metric, tests in results['statistical_tests'].items():\n",
    "            for test_type in ['parametric', 'non_parametric']:\n",
    "                if test_type in tests and tests[test_type]['significant']:\n",
    "                    significant_results.append({\n",
    "                        'metric': metric,\n",
    "                        'test': tests[test_type]['test'],\n",
    "                        'p_value': tests[test_type]['p_value'],\n",
    "                        'hypothesis_result': tests[test_type]['hypothesis_result']\n",
    "                    })\n",
    "        \n",
    "        # Determine overall conclusion\n",
    "        support_count = len([r for r in significant_results if r['hypothesis_result'] == 'SUPPORTS'])\n",
    "        contradict_count = len([r for r in significant_results if r['hypothesis_result'] == 'CONTRADICTS'])\n",
    "        \n",
    "        if support_count > contradict_count and support_count > 0:\n",
    "            overall_conclusion = \"HYPOTHESIS SUPPORTED\"\n",
    "        elif contradict_count > support_count and contradict_count > 0:\n",
    "            overall_conclusion = \"HYPOTHESIS CONTRADICTED\" \n",
    "        elif support_count == contradict_count and support_count > 0:\n",
    "            overall_conclusion = \"MIXED EVIDENCE\"\n",
    "        else:\n",
    "            overall_conclusion = \"INSUFFICIENT EVIDENCE\"\n",
    "        \n",
    "        results['hypothesis_conclusion'] = {\n",
    "            'overall_result': overall_conclusion,\n",
    "            'significant_supporting_tests': support_count,\n",
    "            'significant_contradicting_tests': contradict_count,\n",
    "            'total_significant_tests': len(significant_results),\n",
    "            'significant_results': significant_results\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error comparing TP vs Hallucinations accuracy: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Run the comprehensive comparison for Hypothesis 2\n",
    "try:\n",
    "    tp_vs_hallucinations_results = compare_tp_vs_hallucinations_accuracy()\n",
    "    print(f\"HYPOTHESIS 2 TESTING: TP vs Hallucinations (FP+FN) Accuracy Comparison\")\n",
    "    print(f\"======================================================================\")\n",
    "    \n",
    "    if tp_vs_hallucinations_results:\n",
    "        # Display basic counts\n",
    "        if 'counts' in tp_vs_hallucinations_results:\n",
    "            counts = tp_vs_hallucinations_results['counts']\n",
    "            print(f\"Sample Sizes:\")\n",
    "            print(f\"- TP (Correct Identification): {counts['TP']} links\")\n",
    "            print(f\"- Hallucinations (FP+FN): {counts['Hallucinations_FP_FN']} links\")\n",
    "            print(f\"  â””â”€ FP (Over-identification): {counts['FP_only']} links\")\n",
    "            print(f\"  â””â”€ FN (Under-identification): {counts['FN_only']} links\")\n",
    "            print()\n",
    "        \n",
    "        # Display descriptive statistics comparison\n",
    "        if 'metrics' in tp_vs_hallucinations_results:\n",
    "            print(\"Descriptive Statistics Comparison:\")\n",
    "            for metric, data in tp_vs_hallucinations_results['metrics'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                print(f\"    TP Links:         {data['TP']['mean']:.3f} Â± {data['TP']['std']:.3f}\")\n",
    "                print(f\"    Hallucinations:   {data['Hallucinations_FP_FN']['mean']:.3f} Â± {data['Hallucinations_FP_FN']['std']:.3f}\")\n",
    "                \n",
    "                diff = data['difference']\n",
    "                hypothesis_support = data['hypothesis_support']['supports_hypothesis']\n",
    "                direction = \"LOWER\" if diff['absolute'] < 0 else \"HIGHER\"\n",
    "                \n",
    "                print(f\"    Difference:       TP has {abs(diff['absolute']):.3f} {direction} error\")\n",
    "                print(f\"    Hypothesis:       {'âœ“ SUPPORTS' if hypothesis_support else 'âœ— CONTRADICTS'}\")\n",
    "                print()\n",
    "        \n",
    "        # Display statistical tests\n",
    "        if 'statistical_tests' in tp_vs_hallucinations_results:\n",
    "            print(\"Statistical Significance Tests:\")\n",
    "            for metric, tests in tp_vs_hallucinations_results['statistical_tests'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                \n",
    "                for test_type in ['parametric', 'non_parametric']:\n",
    "                    if test_type in tests:\n",
    "                        test_info = tests[test_type]\n",
    "                        significance = \"significant\" if test_info['significant'] else \"not significant\"\n",
    "                        \n",
    "                        print(f\"    {test_info['test']}: p={test_info['p_value']:.6f} ({significance})\")\n",
    "                        print(f\"      â†’ {test_info['hypothesis_result']}\")\n",
    "                print()\n",
    "        \n",
    "        # Display effect sizes\n",
    "        if 'effect_sizes' in tp_vs_hallucinations_results:\n",
    "            print(\"Effect Size Analysis:\")\n",
    "            for metric, effects in tp_vs_hallucinations_results['effect_sizes'].items():\n",
    "                print(f\"  {metric.upper().replace('_', ' ')}:\")\n",
    "                if 'cohen_d' in effects:\n",
    "                    cohen = effects['cohen_d']\n",
    "                    print(f\"    Cohen's d: {cohen['value']:.3f} ({cohen['interpretation']} effect)\")\n",
    "                print()\n",
    "        \n",
    "        # Display hypothesis conclusion\n",
    "        if 'hypothesis_conclusion' in tp_vs_hallucinations_results:\n",
    "            conclusion = tp_vs_hallucinations_results['hypothesis_conclusion']\n",
    "            print(\"=\"*80)\n",
    "            print(\"HYPOTHESIS 2 CONCLUSION:\")\n",
    "            print(f\"Overall Result: {conclusion['overall_result']}\")\n",
    "            print(f\"Supporting Evidence: {conclusion['significant_supporting_tests']} significant test(s)\")\n",
    "            print(f\"Contradicting Evidence: {conclusion['significant_contradicting_tests']} significant test(s)\")\n",
    "            print()\n",
    "            \n",
    "            # Provide interpretation\n",
    "            if conclusion['overall_result'] == \"HYPOTHESIS SUPPORTED\":\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"âœ“ Evidence supports that hallucinations (both FP and FN) significantly\")\n",
    "                print(\"  correlate with lower quality SiFP estimates in new product development.\")\n",
    "            elif conclusion['overall_result'] == \"HYPOTHESIS CONTRADICTED\":\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"âœ— Evidence contradicts the hypothesis - hallucinations appear to correlate\")\n",
    "                print(\"  with BETTER or equivalent SiFP estimation quality.\")\n",
    "            else:\n",
    "                print(\"INTERPRETATION:\")\n",
    "                print(\"? Mixed or insufficient evidence to draw strong conclusions.\")\n",
    "        \n",
    "        # Store results for further analysis\n",
    "        globals()['tp_vs_hallucinations_results'] = tp_vs_hallucinations_results\n",
    "    else:\n",
    "        print(\"No comparison data available. Check logs for details.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during TP vs Hallucinations comparison: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Enhanced Statistical Validation for Hypothesis 2\n",
    "# Purpose: Add comprehensive statistical measures including confidence intervals, power analysis, and robust testing\n",
    "# Dependencies: pandas, numpy, scipy.stats, statsmodels\n",
    "# Breadcrumbs: Statistical Comparison -> Enhanced Validation -> Robust Hypothesis Testing\n",
    "\n",
    "def enhanced_statistical_validation(df=None, alpha=0.05, bootstrap_samples=1000):\n",
    "    \"\"\"\n",
    "    Perform enhanced statistical validation for Hypothesis 2 with comprehensive measures\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        alpha (float): Significance level (default: 0.05)\n",
    "        bootstrap_samples (int): Number of bootstrap samples for confidence intervals\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive statistical validation results\n",
    "    \"\"\"\n",
    "    # Use global variables if not provided\n",
    "    df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(\"No data available for enhanced statistical validation\")\n",
    "        return {}\n",
    "    \n",
    "    # Filter for valid data\n",
    "    valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "    valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        logger.error(\"No valid data for enhanced statistical validation\")\n",
    "        return {}\n",
    "    \n",
    "    # Split groups\n",
    "    tp_df = valid_df[valid_df['is_hallucination'] == False]\n",
    "    hallucination_df = valid_df[valid_df['is_hallucination'] == True]\n",
    "    \n",
    "    results = {\n",
    "        'confidence_intervals': {},\n",
    "        'power_analysis': {},\n",
    "        'bootstrap_results': {},\n",
    "        'correlation_analysis': {},\n",
    "        'additional_effect_sizes': {},\n",
    "        'assumption_testing': {},\n",
    "        'multiple_testing_correction': {},\n",
    "        'sample_adequacy': {}\n",
    "    }\n",
    "    \n",
    "    metrics_to_test = ['sifp_abs_error', 'sifp_pct_error']\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric not in tp_df.columns or metric not in hallucination_df.columns:\n",
    "            continue\n",
    "            \n",
    "        tp_values = tp_df[metric].dropna()\n",
    "        halluc_values = hallucination_df[metric].dropna()\n",
    "        \n",
    "        if len(tp_values) < 3 or len(halluc_values) < 3:\n",
    "            continue\n",
    "        \n",
    "        results[metric] = {}\n",
    "        \n",
    "        # 1. CONFIDENCE INTERVALS\n",
    "        # Bootstrap confidence intervals for mean difference\n",
    "        def bootstrap_mean_diff(tp_vals, halluc_vals, n_samples=bootstrap_samples):\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            diffs = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                tp_boot = np.random.choice(tp_vals, size=len(tp_vals), replace=True)\n",
    "                halluc_boot = np.random.choice(halluc_vals, size=len(halluc_vals), replace=True)\n",
    "                diffs.append(np.mean(tp_boot) - np.mean(halluc_boot))\n",
    "            \n",
    "            diffs = np.array(diffs)\n",
    "            ci_lower = np.percentile(diffs, (alpha/2) * 100)\n",
    "            ci_upper = np.percentile(diffs, (1 - alpha/2) * 100)\n",
    "            \n",
    "            return {\n",
    "                'mean_difference': np.mean(diffs),\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'contains_zero': ci_lower <= 0 <= ci_upper,\n",
    "                'confidence_level': (1 - alpha) * 100\n",
    "            }\n",
    "        \n",
    "        bootstrap_ci = bootstrap_mean_diff(tp_values, halluc_values)\n",
    "        results['confidence_intervals'][metric] = bootstrap_ci\n",
    "        \n",
    "        # Parametric CI for mean difference\n",
    "        mean_diff = tp_values.mean() - halluc_values.mean()\n",
    "        se_diff = np.sqrt(tp_values.var()/len(tp_values) + halluc_values.var()/len(halluc_values))\n",
    "        \n",
    "        if len(tp_values) >= 30 and len(halluc_values) >= 30:\n",
    "            # Large sample: use z-distribution\n",
    "            z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "            margin_error = z_critical * se_diff\n",
    "            method = 'Large_sample_z'\n",
    "        else:\n",
    "            # Small sample: use t-distribution (Welch's)\n",
    "            df_welch = (tp_values.var()/len(tp_values) + halluc_values.var()/len(halluc_values))**2 / \\\n",
    "                      ((tp_values.var()/len(tp_values))**2/(len(tp_values)-1) + \n",
    "                       (halluc_values.var()/len(halluc_values))**2/(len(halluc_values)-1))\n",
    "            t_critical = stats.t.ppf(1 - alpha/2, df_welch)\n",
    "            margin_error = t_critical * se_diff\n",
    "            method = 'Welch_t'\n",
    "        \n",
    "        parametric_ci = {\n",
    "            'mean_difference': mean_diff,\n",
    "            'ci_lower': mean_diff - margin_error,\n",
    "            'ci_upper': mean_diff + margin_error,\n",
    "            'contains_zero': (mean_diff - margin_error) <= 0 <= (mean_diff + margin_error),\n",
    "            'method': method\n",
    "        }\n",
    "        \n",
    "        results['confidence_intervals'][metric]['parametric'] = parametric_ci\n",
    "        \n",
    "        # 2. POWER ANALYSIS (Fixed implementation)\n",
    "        # Calculate observed effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(tp_values) - 1) * tp_values.var() + \n",
    "                             (len(halluc_values) - 1) * halluc_values.var()) / \n",
    "                             (len(tp_values) + len(halluc_values) - 2))\n",
    "        \n",
    "        if pooled_std > 0:\n",
    "            observed_effect_size = abs(tp_values.mean() - halluc_values.mean()) / pooled_std\n",
    "            total_n = len(tp_values) + len(halluc_values)\n",
    "            \n",
    "            # Calculate achieved power using ttest_power\n",
    "            achieved_power = ttest_power(effect_size=observed_effect_size,\n",
    "                                       nobs=total_n,\n",
    "                                       alpha=alpha,\n",
    "                                       alternative='two-sided')\n",
    "            \n",
    "            # Calculate required sample size for 80% power using simplified approach\n",
    "            # For two-sample t-test with equal sample sizes\n",
    "            n_groups = 2\n",
    "            power_target = 0.8\n",
    "            \n",
    "            # Use iterative approach to find required sample size\n",
    "            required_total_n = total_n\n",
    "            for test_n in range(10, 1000, 5):\n",
    "                test_power = ttest_power(effect_size=observed_effect_size,\n",
    "                                       nobs=test_n,\n",
    "                                       alpha=alpha,\n",
    "                                       alternative='two-sided')\n",
    "                if test_power >= power_target:\n",
    "                    required_total_n = test_n\n",
    "                    break\n",
    "            \n",
    "            # Calculate required sample size per group (assuming equal allocation)\n",
    "            required_n_per_group = required_total_n / 2\n",
    "            \n",
    "            results['power_analysis'][metric] = {\n",
    "                'observed_effect_size': observed_effect_size,\n",
    "                'achieved_power': achieved_power,\n",
    "                'required_total_n_for_80_power': required_total_n,\n",
    "                'required_n_per_group_for_80_power': required_n_per_group,\n",
    "                'current_n_tp': len(tp_values),\n",
    "                'current_n_halluc': len(halluc_values),\n",
    "                'current_total_n': total_n,\n",
    "                'adequate_power': achieved_power >= 0.8\n",
    "            }\n",
    "        \n",
    "        # 3. ADDITIONAL EFFECT SIZES\n",
    "        additional_effects = {}\n",
    "        \n",
    "        # Cliff's Delta (robust, non-parametric effect size)\n",
    "        cliff_delta_sum = 0\n",
    "        for tp_val in tp_values:\n",
    "            for halluc_val in halluc_values:\n",
    "                if tp_val > halluc_val:\n",
    "                    cliff_delta_sum += 1\n",
    "                elif tp_val < halluc_val:\n",
    "                    cliff_delta_sum -= 1\n",
    "        \n",
    "        cliff_delta = cliff_delta_sum / (len(tp_values) * len(halluc_values))\n",
    "        \n",
    "        if abs(cliff_delta) < 0.147:\n",
    "            cliff_interpretation = \"Negligible\"\n",
    "        elif abs(cliff_delta) < 0.33:\n",
    "            cliff_interpretation = \"Small\"\n",
    "        elif abs(cliff_delta) < 0.474:\n",
    "            cliff_interpretation = \"Medium\"\n",
    "        else:\n",
    "            cliff_interpretation = \"Large\"\n",
    "        \n",
    "        additional_effects['cliff_delta'] = {\n",
    "            'value': cliff_delta,\n",
    "            'interpretation': cliff_interpretation\n",
    "        }\n",
    "        \n",
    "        # Glass's Delta\n",
    "        if halluc_values.std() > 0:\n",
    "            glass_delta = (tp_values.mean() - halluc_values.mean()) / halluc_values.std()\n",
    "            additional_effects['glass_delta'] = {\n",
    "                'value': glass_delta,\n",
    "                'interpretation': \"Control group (Hallucinations) as reference\"\n",
    "            }\n",
    "        \n",
    "        results['additional_effect_sizes'][metric] = additional_effects\n",
    "        \n",
    "        # 4. CORRELATION ANALYSIS\n",
    "        # Point-biserial correlation\n",
    "        combined_values = np.concatenate([tp_values, halluc_values])\n",
    "        binary_halluc = np.concatenate([np.zeros(len(tp_values)), np.ones(len(halluc_values))])\n",
    "        \n",
    "        correlation_coef, correlation_p = stats.pearsonr(binary_halluc, combined_values)\n",
    "        \n",
    "        results['correlation_analysis'][metric] = {\n",
    "            'point_biserial_r': correlation_coef,\n",
    "            'p_value': correlation_p,\n",
    "            'significant': correlation_p < alpha,\n",
    "            'interpretation': f\"{'Positive' if correlation_coef > 0 else 'Negative'} correlation between hallucination status and {metric}\"\n",
    "        }\n",
    "    \n",
    "    # 5. MULTIPLE TESTING CORRECTION\n",
    "    all_p_values = []\n",
    "    test_descriptions = []\n",
    "    \n",
    "    # Get p-values from previous analysis if available\n",
    "    if 'tp_vs_hallucinations_results' in globals():\n",
    "        prev_results = globals()['tp_vs_hallucinations_results']\n",
    "        if 'statistical_tests' in prev_results:\n",
    "            for metric, tests in prev_results['statistical_tests'].items():\n",
    "                for test_type in ['parametric', 'non_parametric']:\n",
    "                    if test_type in tests and 'p_value' in tests[test_type]:\n",
    "                        all_p_values.append(tests[test_type]['p_value'])\n",
    "                        test_descriptions.append(f\"{metric}_{test_type}\")\n",
    "    \n",
    "    # Add correlation p-values\n",
    "    for metric in results['correlation_analysis']:\n",
    "        if 'p_value' in results['correlation_analysis'][metric]:\n",
    "            all_p_values.append(results['correlation_analysis'][metric]['p_value'])\n",
    "            test_descriptions.append(f\"{metric}_correlation\")\n",
    "    \n",
    "    if all_p_values:\n",
    "        # Bonferroni correction\n",
    "        bonferroni_alpha = alpha / len(all_p_values)\n",
    "        bonferroni_significant = [p < bonferroni_alpha for p in all_p_values]\n",
    "        \n",
    "        # False Discovery Rate (Benjamini-Hochberg)\n",
    "        fdr_rejected, fdr_p_corrected, _, _ = multipletests(all_p_values, alpha=alpha, method='fdr_bh')\n",
    "        \n",
    "        results['multiple_testing_correction'] = {\n",
    "            'original_alpha': alpha,\n",
    "            'bonferroni_alpha': bonferroni_alpha,\n",
    "            'bonferroni_significant': bonferroni_significant,\n",
    "            'fdr_rejected': fdr_rejected.tolist(),\n",
    "            'fdr_corrected_p': fdr_p_corrected.tolist(),\n",
    "            'test_descriptions': test_descriptions,\n",
    "            'original_p_values': all_p_values\n",
    "        }\n",
    "    \n",
    "    # 6. SAMPLE ADEQUACY ASSESSMENT\n",
    "    total_n = len(tp_df) + len(hallucination_df)\n",
    "    tp_n = len(tp_df)\n",
    "    halluc_n = len(hallucination_df)\n",
    "    \n",
    "    sample_adequacy = {\n",
    "        'total_sample_size': total_n,\n",
    "        'tp_sample_size': tp_n,\n",
    "        'hallucination_sample_size': halluc_n,\n",
    "        'minimum_recommended': 30,\n",
    "        'adequate_total_size': total_n >= 30,\n",
    "        'adequate_group_sizes': tp_n >= 15 and halluc_n >= 15,\n",
    "        'balanced_groups': abs(tp_n - halluc_n) / max(tp_n, halluc_n) < 0.5,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    if not sample_adequacy['adequate_total_size']:\n",
    "        sample_adequacy['recommendations'].append(\"Increase total sample size to at least 30\")\n",
    "    if not sample_adequacy['adequate_group_sizes']:\n",
    "        sample_adequacy['recommendations'].append(\"Ensure each group has at least 15 observations\")\n",
    "    if not sample_adequacy['balanced_groups']:\n",
    "        sample_adequacy['recommendations'].append(\"Consider balancing group sizes for better statistical power\")\n",
    "    \n",
    "    results['sample_adequacy'] = sample_adequacy\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run enhanced statistical validation with improved interpretation\n",
    "print(f\"ENHANCED STATISTICAL VALIDATION FOR HYPOTHESIS 2:\")\n",
    "print(f\"================================================\")\n",
    "print(f\"HYPOTHESIS: 'Hallucinations in LLM outputs significantly correlate with lower quality SiFP estimates'\")\n",
    "print(f\"OPERATIONALIZATION:\")\n",
    "print(f\"- Hallucinations = Both FP (over-identification) and FN (under-identification) traceability links\")\n",
    "print(f\"- Lower quality = Higher absolute and percentage errors in SiFP estimations\")\n",
    "print(f\"- Significant = p < 0.05 with meaningful effect sizes\")\n",
    "print()\n",
    "\n",
    "enhanced_results = enhanced_statistical_validation()\n",
    "\n",
    "if enhanced_results:\n",
    "    # Display confidence intervals with hypothesis interpretation\n",
    "    if 'confidence_intervals' in enhanced_results:\n",
    "        print(\"1. CONFIDENCE INTERVALS FOR MEAN DIFFERENCES:\")\n",
    "        print(\"   (Testing if TP links have different error rates than Hallucinations)\")\n",
    "        print(\"-\" * 70)\n",
    "        for metric, ci_data in enhanced_results['confidence_intervals'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            \n",
    "            # Bootstrap CI\n",
    "            bootstrap_ci = ci_data\n",
    "            ci_level = bootstrap_ci.get('confidence_level', 95)\n",
    "            print(f\"    Bootstrap {ci_level}% CI: [{bootstrap_ci['ci_lower']:.3f}, {bootstrap_ci['ci_upper']:.3f}]\")\n",
    "            print(f\"    Mean Difference (TP - Hallucinations): {bootstrap_ci['mean_difference']:.3f}\")\n",
    "            \n",
    "            # Hypothesis interpretation\n",
    "            if not bootstrap_ci['contains_zero']:\n",
    "                if bootstrap_ci['ci_upper'] < 0:\n",
    "                    print(f\"    âœ“ SUPPORTS HYPOTHESIS: TP links have significantly LOWER {metric_name.lower()}\")\n",
    "                    print(f\"      â†’ Hallucinations correlate with HIGHER estimation errors\")\n",
    "                else:\n",
    "                    print(f\"    âœ— CONTRADICTS HYPOTHESIS: TP links have significantly HIGHER {metric_name.lower()}\")\n",
    "                    print(f\"      â†’ Hallucinations correlate with LOWER estimation errors\")\n",
    "            else:\n",
    "                print(f\"    ? INCONCLUSIVE: No significant difference detected\")\n",
    "                print(f\"      â†’ Cannot conclude hallucinations affect estimation quality\")\n",
    "            \n",
    "            # Parametric CI if available\n",
    "            if 'parametric' in ci_data:\n",
    "                param_ci = ci_data['parametric']\n",
    "                print(f\"    Parametric CI ({param_ci['method']}): [{param_ci['ci_lower']:.3f}, {param_ci['ci_upper']:.3f}]\")\n",
    "            print()\n",
    "    \n",
    "    # Display power analysis with interpretation\n",
    "    if 'power_analysis' in enhanced_results:\n",
    "        print(\"2. STATISTICAL POWER ANALYSIS:\")\n",
    "        print(\"   (Assessing reliability of our conclusions)\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric, power_data in enhanced_results['power_analysis'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            print(f\"    Observed Effect Size (Cohen's d): {power_data['observed_effect_size']:.3f}\")\n",
    "            print(f\"    Achieved Statistical Power: {power_data['achieved_power']:.3f} ({power_data['achieved_power']*100:.1f}%)\")\n",
    "            print(f\"    Required Total N for 80% Power: {power_data['required_total_n_for_80_power']:.0f}\")\n",
    "            print(f\"    Required N per Group for 80% Power: {power_data['required_n_per_group_for_80_power']:.0f}\")\n",
    "            print(f\"    Current Sample: TP={power_data['current_n_tp']}, Hallucinations={power_data['current_n_halluc']}\")\n",
    "            \n",
    "            # Power interpretation\n",
    "            if power_data['adequate_power']:\n",
    "                print(f\"    âœ“ ADEQUATE POWER: High confidence in detecting true effects\")\n",
    "            else:\n",
    "                print(f\"    âš  LOW POWER: May miss true effects (Type II error risk)\")\n",
    "                print(f\"      â†’ Consider increasing sample size for more reliable conclusions\")\n",
    "            print()\n",
    "    \n",
    "    # Display additional effect sizes with interpretation\n",
    "    if 'additional_effect_sizes' in enhanced_results:\n",
    "        print(\"3. ADDITIONAL EFFECT SIZE MEASURES:\")\n",
    "        print(\"   (Assessing practical significance beyond statistical significance)\")\n",
    "        print(\"-\" * 65)\n",
    "        for metric, effects in enhanced_results['additional_effect_sizes'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            if 'cliff_delta' in effects:\n",
    "                cliff = effects['cliff_delta']\n",
    "                print(f\"    Cliff's Delta: {cliff['value']:.3f} ({cliff['interpretation']} effect)\")\n",
    "                \n",
    "                # Interpretation for hypothesis\n",
    "                if cliff['value'] < 0:\n",
    "                    print(f\"      â†’ TP links tend to have lower {metric_name.lower()} than Hallucinations\")\n",
    "                    print(f\"      â†’ SUPPORTS hypothesis about hallucination impact\")\n",
    "                elif cliff['value'] > 0:\n",
    "                    print(f\"      â†’ TP links tend to have higher {metric_name.lower()} than Hallucinations\")\n",
    "                    print(f\"      â†’ CONTRADICTS hypothesis about hallucination impact\")\n",
    "                else:\n",
    "                    print(f\"      â†’ No systematic difference between groups\")\n",
    "                    \n",
    "            if 'glass_delta' in effects:\n",
    "                glass = effects['glass_delta']\n",
    "                print(f\"    Glass's Delta: {glass['value']:.3f}\")\n",
    "                print(f\"      â†’ Standardized difference using Hallucination group variability\")\n",
    "            print()\n",
    "    \n",
    "    # Display correlation analysis with hypothesis context\n",
    "    if 'correlation_analysis' in enhanced_results:\n",
    "        print(\"4. CORRELATION ANALYSIS:\")\n",
    "        print(\"   (Direct test of correlation between hallucinations and estimation errors)\")\n",
    "        print(\"-\" * 75)\n",
    "        for metric, corr_data in enhanced_results['correlation_analysis'].items():\n",
    "            metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "            print(f\"  {metric_name}:\")\n",
    "            print(f\"    Point-biserial correlation (r): {corr_data['point_biserial_r']:.3f}\")\n",
    "            print(f\"    p-value: {corr_data['p_value']:.6f}\")\n",
    "            print(f\"    Statistically significant: {'Yes' if corr_data['significant'] else 'No'}\")\n",
    "            \n",
    "            # Hypothesis interpretation\n",
    "            if corr_data['significant']:\n",
    "                if corr_data['point_biserial_r'] > 0:\n",
    "                    print(f\"    âœ“ SUPPORTS HYPOTHESIS: Positive correlation means hallucinations\")\n",
    "                    print(f\"      are associated with HIGHER {metric_name.lower()}\")\n",
    "                else:\n",
    "                    print(f\"    âœ— CONTRADICTS HYPOTHESIS: Negative correlation means hallucinations\")\n",
    "                    print(f\"      are associated with LOWER {metric_name.lower()}\")\n",
    "            else:\n",
    "                print(f\"    ? INCONCLUSIVE: No significant correlation detected\")\n",
    "            \n",
    "            # Strength interpretation\n",
    "            r_abs = abs(corr_data['point_biserial_r'])\n",
    "            if r_abs < 0.1:\n",
    "                strength = \"negligible\"\n",
    "            elif r_abs < 0.3:\n",
    "                strength = \"small\"\n",
    "            elif r_abs < 0.5:\n",
    "                strength = \"medium\"\n",
    "            else:\n",
    "                strength = \"large\"\n",
    "            print(f\"    Correlation strength: {strength} ({r_abs:.3f})\")\n",
    "            print()\n",
    "    \n",
    "    # Display multiple testing correction\n",
    "    if 'multiple_testing_correction' in enhanced_results:\n",
    "        print(\"5. MULTIPLE TESTING CORRECTION:\")\n",
    "        print(\"   (Controlling for false discoveries when testing multiple metrics)\")\n",
    "        print(\"-\" * 65)\n",
    "        correction_data = enhanced_results['multiple_testing_correction']\n",
    "        print(f\"    Original significance level (Î±): {correction_data['original_alpha']}\")\n",
    "        print(f\"    Bonferroni-corrected Î±: {correction_data['bonferroni_alpha']:.6f}\")\n",
    "        print(f\"    Number of statistical tests: {len(correction_data['original_p_values'])}\")\n",
    "        \n",
    "        significant_after_correction = sum(correction_data.get('bonferroni_significant', []))\n",
    "        print(f\"    Significant after Bonferroni correction: {significant_after_correction}\")\n",
    "        \n",
    "        if 'fdr_rejected' in correction_data:\n",
    "            fdr_significant = sum(correction_data['fdr_rejected'])\n",
    "            print(f\"    Significant after FDR correction: {fdr_significant}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if significant_after_correction > 0:\n",
    "            print(f\"    âœ“ ROBUST FINDINGS: Results survive correction for multiple testing\")\n",
    "            print(f\"      â†’ High confidence that observed effects are not due to chance\")\n",
    "        else:\n",
    "            print(f\"    âš  REDUCED CONFIDENCE: No tests significant after correction\")\n",
    "            print(f\"      â†’ Observed effects may be due to multiple testing artifacts\")\n",
    "        print()\n",
    "    \n",
    "    # Display sample adequacy\n",
    "    if 'sample_adequacy' in enhanced_results:\n",
    "        print(\"6. SAMPLE SIZE ADEQUACY:\")\n",
    "        print(\"   (Assessing whether we have sufficient data for reliable conclusions)\")\n",
    "        print(\"-\" * 70)\n",
    "        adequacy = enhanced_results['sample_adequacy']\n",
    "        print(f\"    Total Sample Size: {adequacy['total_sample_size']}\")\n",
    "        print(f\"    TP Group Size: {adequacy['tp_sample_size']}\")\n",
    "        print(f\"    Hallucination Group Size: {adequacy['hallucination_sample_size']}\")\n",
    "        print(f\"    Adequate for statistical tests: {'âœ“ Yes' if adequacy['adequate_total_size'] else 'âœ— No'}\")\n",
    "        print(f\"    Adequate group sizes: {'âœ“ Yes' if adequacy['adequate_group_sizes'] else 'âœ— No'}\")\n",
    "        print(f\"    Reasonably balanced groups: {'âœ“ Yes' if adequacy['balanced_groups'] else 'âœ— No'}\")\n",
    "        \n",
    "        if adequacy['recommendations']:\n",
    "            print(\"    Recommendations for improvement:\")\n",
    "            for rec in adequacy['recommendations']:\n",
    "                print(f\"      - {rec}\")\n",
    "        print()\n",
    "    \n",
    "    # Store enhanced results\n",
    "    globals()['enhanced_validation_results'] = enhanced_results\n",
    "    \n",
    "    print(\"OVERALL INTERPRETATION FOR HYPOTHESIS 2:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Your statistical analysis now provides multiple lines of evidence:\")\n",
    "    print()\n",
    "    print(\"âœ“ CONFIDENCE INTERVALS: Show the range of plausible differences\")\n",
    "    print(\"âœ“ STATISTICAL POWER: Assesses reliability of conclusions\")\n",
    "    print(\"âœ“ EFFECT SIZES: Measure practical significance beyond p-values\")\n",
    "    print(\"âœ“ CORRELATIONS: Direct test of the hypothesized relationship\")\n",
    "    print(\"âœ“ MULTIPLE TESTING: Controls for false discovery rates\")\n",
    "    print(\"âœ“ SAMPLE ADEQUACY: Confirms sufficient data for conclusions\")\n",
    "    print()\n",
    "    print(\"This comprehensive approach provides much stronger evidence\")\n",
    "    print(\"for or against your hypothesis than traditional p-value testing alone!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No enhanced validation results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Advanced Statistical Methods: Function Definitions and Execution\n",
    "# Purpose: Define and execute cutting-edge statistical methods for robust Hypothesis 2 testing\n",
    "# Dependencies: scipy, arch, pymc, arviz, bayesian_testing\n",
    "# Breadcrumbs: Enhanced Validation -> Advanced Methods -> Function Definition\n",
    "\n",
    "def advanced_statistical_methods_analysis(df=None, alpha=0.05, n_permutations=10000, n_bootstrap=5000):\n",
    "    \"\"\"\n",
    "    Comprehensive advanced statistical analysis for Hypothesis 2 using:\n",
    "    1. Permutation Tests (exact p-values, distribution-free)\n",
    "    2. Extended Bootstrap Methods (BCa intervals, multiple bootstrap types)\n",
    "    3. Bayesian Analysis (credible intervals, Bayes factors)\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame, optional): Combined dataset with traceability and SIFP data\n",
    "        alpha (float): Significance level (default: 0.05)\n",
    "        n_permutations (int): Number of permutation samples (default: 10000)\n",
    "        n_bootstrap (int): Number of bootstrap samples (default: 5000)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive results from all advanced statistical methods\n",
    "    \"\"\"\n",
    "    # Use global variables if not provided\n",
    "    df = df if df is not None else globals().get('combined_df', pd.DataFrame())\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(\"No data available for advanced statistical analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Filter for valid data\n",
    "    valid_df = df.dropna(subset=['sifp_actor_total', 'sifp_final_total', 'classification', 'is_hallucination'])\n",
    "    valid_df = valid_df[valid_df['classification'].isin(['TP', 'FP', 'FN'])]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        logger.error(\"No valid data for advanced statistical analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Split groups\n",
    "    tp_df = valid_df[valid_df['is_hallucination'] == False]\n",
    "    hallucination_df = valid_df[valid_df['is_hallucination'] == True]\n",
    "    \n",
    "    logger.info(f\"Advanced analysis: {len(tp_df)} TP links vs {len(hallucination_df)} Hallucination links\")\n",
    "    \n",
    "    results = {\n",
    "        'permutation_tests': {},\n",
    "        'extended_bootstrap': {},\n",
    "        'bayesian_analysis': {},\n",
    "        'method_comparison': {},\n",
    "        'hypothesis_conclusion': {}\n",
    "    }\n",
    "    \n",
    "    metrics_to_test = ['sifp_abs_error', 'sifp_pct_error']\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric not in tp_df.columns or metric not in hallucination_df.columns:\n",
    "            continue\n",
    "            \n",
    "        tp_values = tp_df[metric].dropna()\n",
    "        halluc_values = hallucination_df[metric].dropna()\n",
    "        \n",
    "        if len(tp_values) < 5 or len(halluc_values) < 5:\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Analyzing {metric} with advanced methods...\")\n",
    "        \n",
    "        # =================================================================\n",
    "        # 1. PERMUTATION TESTS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running permutation tests for {metric}...\")\n",
    "        \n",
    "        def mean_difference_statistic(x, y, axis=0):\n",
    "            \"\"\"Statistic function for permutation test\"\"\"\n",
    "            return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
    "        \n",
    "        # Two-sample permutation test for mean difference\n",
    "        perm_result = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='two-sided',\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # One-sided tests for hypothesis direction\n",
    "        perm_result_less = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='less',  # TP < Hallucinations (supports hypothesis)\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        perm_result_greater = permutation_test(\n",
    "            (tp_values, halluc_values),\n",
    "            mean_difference_statistic,\n",
    "            n_resamples=n_permutations,\n",
    "            alternative='greater',  # TP > Hallucinations (contradicts hypothesis)\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # Calculate observed effect\n",
    "        observed_diff = tp_values.mean() - halluc_values.mean()\n",
    "        \n",
    "        results['permutation_tests'][metric] = {\n",
    "            'observed_difference': observed_diff,\n",
    "            'two_sided': {\n",
    "                'statistic': perm_result.statistic,\n",
    "                'p_value': perm_result.pvalue,\n",
    "                'significant': perm_result.pvalue < alpha\n",
    "            },\n",
    "            'one_sided_less': {\n",
    "                'statistic': perm_result_less.statistic,\n",
    "                'p_value': perm_result_less.pvalue,\n",
    "                'significant': perm_result_less.pvalue < alpha,\n",
    "                'supports_hypothesis': perm_result_less.pvalue < alpha and observed_diff < 0\n",
    "            },\n",
    "            'one_sided_greater': {\n",
    "                'statistic': perm_result_greater.statistic,\n",
    "                'p_value': perm_result_greater.pvalue,\n",
    "                'significant': perm_result_greater.pvalue < alpha,\n",
    "                'contradicts_hypothesis': perm_result_greater.pvalue < alpha and observed_diff > 0\n",
    "            },\n",
    "            'n_permutations': n_permutations,\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if (perm_result_less.pvalue < alpha and observed_diff < 0) else 'CONTRADICTS' if (perm_result_greater.pvalue < alpha and observed_diff > 0) else 'INCONCLUSIVE'\n",
    "        }\n",
    "        \n",
    "        # =================================================================\n",
    "        # 2. EXTENDED BOOTSTRAP METHODS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running extended bootstrap methods for {metric}...\")\n",
    "        \n",
    "        bootstrap_results = {}\n",
    "        \n",
    "        # IID Bootstrap (standard) - Fixed implementation\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        \n",
    "        # Simple bootstrap implementation without arch library issues\n",
    "        bootstrap_diffs = []\n",
    "        for i in range(n_bootstrap):\n",
    "            # Bootstrap samples\n",
    "            tp_bootstrap = np.random.choice(tp_values, size=len(tp_values), replace=True)\n",
    "            halluc_bootstrap = np.random.choice(halluc_values, size=len(halluc_values), replace=True)\n",
    "            \n",
    "            # Calculate difference\n",
    "            diff = np.mean(tp_bootstrap) - np.mean(halluc_bootstrap)\n",
    "            bootstrap_diffs.append(diff)\n",
    "        \n",
    "        bootstrap_diffs = np.array(bootstrap_diffs)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        ci_lower = np.percentile(bootstrap_diffs, (alpha/2) * 100)\n",
    "        ci_upper = np.percentile(bootstrap_diffs, (1 - alpha/2) * 100)\n",
    "        \n",
    "        # BCa confidence intervals (bias-corrected and accelerated)\n",
    "        # Estimate bias correction\n",
    "        n_tp = len(tp_values)\n",
    "        n_halluc = len(halluc_values)\n",
    "        \n",
    "        # Jackknife for acceleration\n",
    "        jackknife_diffs = []\n",
    "        for i in range(n_tp):\n",
    "            tp_jack = np.delete(tp_values, i)\n",
    "            jackknife_diffs.append(tp_jack.mean() - halluc_values.mean())\n",
    "        for i in range(n_halluc):\n",
    "            halluc_jack = np.delete(halluc_values, i)\n",
    "            jackknife_diffs.append(tp_values.mean() - halluc_jack.mean())\n",
    "        \n",
    "        jackknife_diffs = np.array(jackknife_diffs)\n",
    "        jackknife_mean = np.mean(jackknife_diffs)\n",
    "        \n",
    "        # Acceleration parameter\n",
    "        acceleration = np.sum((jackknife_mean - jackknife_diffs)**3) / (6 * (np.sum((jackknife_mean - jackknife_diffs)**2))**(3/2))\n",
    "        if np.isnan(acceleration):\n",
    "            acceleration = 0\n",
    "        \n",
    "        # Bias correction\n",
    "        observed_diff = tp_values.mean() - halluc_values.mean()\n",
    "        bias_correction = stats.norm.ppf((bootstrap_diffs < observed_diff).mean())\n",
    "        if np.isnan(bias_correction):\n",
    "            bias_correction = 0\n",
    "        \n",
    "        # BCa confidence intervals\n",
    "        z_alpha_2 = stats.norm.ppf(alpha/2)\n",
    "        z_1_alpha_2 = stats.norm.ppf(1 - alpha/2)\n",
    "        \n",
    "        alpha_1 = stats.norm.cdf(bias_correction + (bias_correction + z_alpha_2)/(1 - acceleration * (bias_correction + z_alpha_2)))\n",
    "        alpha_2 = stats.norm.cdf(bias_correction + (bias_correction + z_1_alpha_2)/(1 - acceleration * (bias_correction + z_1_alpha_2)))\n",
    "        \n",
    "        # Ensure valid percentiles\n",
    "        alpha_1 = max(0.001, min(0.999, alpha_1))\n",
    "        alpha_2 = max(0.001, min(0.999, alpha_2))\n",
    "        \n",
    "        bca_ci_lower = np.percentile(bootstrap_diffs, alpha_1 * 100)\n",
    "        bca_ci_upper = np.percentile(bootstrap_diffs, alpha_2 * 100)\n",
    "        \n",
    "        bootstrap_results['bca_intervals'] = {\n",
    "            'ci_lower': bca_ci_lower,\n",
    "            'ci_upper': bca_ci_upper,\n",
    "            'bias_correction': bias_correction,\n",
    "            'acceleration': acceleration,\n",
    "            'contains_zero': bca_ci_lower <= 0 <= bca_ci_upper\n",
    "        }\n",
    "        \n",
    "        bootstrap_results['iid_bootstrap'] = {\n",
    "            'mean_difference': np.mean(bootstrap_diffs),\n",
    "            'std_difference': np.std(bootstrap_diffs),\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'contains_zero': ci_lower <= 0 <= ci_upper,\n",
    "            'n_bootstrap': n_bootstrap,\n",
    "            'bootstrap_p_value': 2 * min((bootstrap_diffs <= 0).mean(), (bootstrap_diffs >= 0).mean()),\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if ci_upper < 0 else 'CONTRADICTS' if ci_lower > 0 else 'INCONCLUSIVE'\n",
    "        }\n",
    "        \n",
    "        # Circular Block Bootstrap (simplified implementation)\n",
    "        # Simple block bootstrap without arch library complications\n",
    "        block_size = min(5, len(tp_values) // 5, len(halluc_values) // 5)\n",
    "        if block_size >= 2:\n",
    "            circular_diffs = []\n",
    "            combined_data = np.concatenate([tp_values, halluc_values])\n",
    "            combined_labels = np.concatenate([np.zeros(len(tp_values)), np.ones(len(halluc_values))])\n",
    "            \n",
    "            for i in range(min(1000, n_bootstrap)):  # Limit for computational efficiency\n",
    "                # Simple circular block sampling\n",
    "                n_blocks = len(combined_data) // block_size\n",
    "                block_starts = np.random.choice(len(combined_data), size=n_blocks, replace=True)\n",
    "                \n",
    "                boot_data = []\n",
    "                boot_labels = []\n",
    "                for start in block_starts:\n",
    "                    for j in range(block_size):\n",
    "                        idx = (start + j) % len(combined_data)\n",
    "                        boot_data.append(combined_data[idx])\n",
    "                        boot_labels.append(combined_labels[idx])\n",
    "                \n",
    "                boot_data = np.array(boot_data)\n",
    "                boot_labels = np.array(boot_labels)\n",
    "                \n",
    "                tp_boot = boot_data[boot_labels == 0]\n",
    "                halluc_boot = boot_data[boot_labels == 1]\n",
    "                \n",
    "                if len(tp_boot) > 0 and len(halluc_boot) > 0:\n",
    "                    circular_diffs.append(tp_boot.mean() - halluc_boot.mean())\n",
    "            \n",
    "            if circular_diffs:\n",
    "                circular_diffs = np.array(circular_diffs)\n",
    "                circular_ci_lower = np.percentile(circular_diffs, (alpha/2) * 100)\n",
    "                circular_ci_upper = np.percentile(circular_diffs, (1 - alpha/2) * 100)\n",
    "                \n",
    "                bootstrap_results['circular_block_bootstrap'] = {\n",
    "                    'mean_difference': np.mean(circular_diffs),\n",
    "                    'std_difference': np.std(circular_diffs),\n",
    "                    'ci_lower': circular_ci_lower,\n",
    "                    'ci_upper': circular_ci_upper,\n",
    "                    'contains_zero': circular_ci_lower <= 0 <= circular_ci_upper,\n",
    "                    'n_bootstrap': len(circular_diffs),\n",
    "                    'block_size': block_size,\n",
    "                    'hypothesis_interpretation': 'SUPPORTS' if circular_ci_upper < 0 else 'CONTRADICTS' if circular_ci_lower > 0 else 'INCONCLUSIVE'\n",
    "                }\n",
    "        \n",
    "        results['extended_bootstrap'][metric] = bootstrap_results\n",
    "        \n",
    "        # =================================================================\n",
    "        # 3. BAYESIAN ANALYSIS\n",
    "        # =================================================================\n",
    "        logger.info(f\"  Running Bayesian analysis for {metric}...\")\n",
    "        \n",
    "        # Bayesian two-sample t-test using PyMC\n",
    "        with pm.Model() as bayesian_model:\n",
    "            # Priors for group means\n",
    "            mu_tp = pm.Normal('mu_tp', mu=tp_values.mean(), sigma=tp_values.std()*2)\n",
    "            mu_halluc = pm.Normal('mu_halluc', mu=halluc_values.mean(), sigma=halluc_values.std()*2)\n",
    "            \n",
    "            # Priors for group standard deviations\n",
    "            sigma_tp = pm.HalfNormal('sigma_tp', sigma=tp_values.std()*2)\n",
    "            sigma_halluc = pm.HalfNormal('sigma_halluc', sigma=halluc_values.std()*2)\n",
    "            \n",
    "            # Likelihood for observations\n",
    "            tp_obs = pm.Normal('tp_obs', mu=mu_tp, sigma=sigma_tp, observed=tp_values)\n",
    "            halluc_obs = pm.Normal('halluc_obs', mu=mu_halluc, sigma=sigma_halluc, observed=halluc_values)\n",
    "            \n",
    "            # Derived quantities\n",
    "            diff_means = pm.Deterministic('diff_means', mu_tp - mu_halluc)\n",
    "            effect_size = pm.Deterministic('effect_size', diff_means / pm.math.sqrt((sigma_tp**2 + sigma_halluc**2) / 2))\n",
    "            \n",
    "            # Sample from posterior\n",
    "            trace = pm.sample(2000, tune=1000, cores=1, random_seed=RANDOM_SEED, \n",
    "                            progressbar=False, return_inferencedata=True)\n",
    "        \n",
    "        # Extract posterior samples\n",
    "        posterior_diff = trace.posterior['diff_means'].values.flatten()\n",
    "        posterior_effect_size = trace.posterior['effect_size'].values.flatten()\n",
    "        \n",
    "        # Calculate credible intervals\n",
    "        diff_ci = np.percentile(posterior_diff, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "        effect_ci = np.percentile(posterior_effect_size, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "        \n",
    "        # Bayesian p-value (probability that difference is in wrong direction)\n",
    "        bayesian_p_value = (posterior_diff > 0).mean() if posterior_diff.mean() < 0 else (posterior_diff < 0).mean()\n",
    "        \n",
    "        # Probability that TP has lower error (supports hypothesis)\n",
    "        prob_supports_hypothesis = (posterior_diff < 0).mean()\n",
    "        \n",
    "        # Bayes factor approximation using Savage-Dickey density ratio\n",
    "        # Compare H1: diff != 0 vs H0: diff = 0\n",
    "        from scipy.stats import gaussian_kde\n",
    "        posterior_kde = gaussian_kde(posterior_diff)\n",
    "        \n",
    "        # Prior density at 0 (assuming normal prior centered at 0)\n",
    "        prior_at_zero = stats.norm.pdf(0, 0, tp_values.std() + halluc_values.std())\n",
    "        posterior_at_zero = posterior_kde.evaluate([0])[0]\n",
    "        \n",
    "        # Bayes factor (BF10 = evidence for H1 vs H0)\n",
    "        bayes_factor = prior_at_zero / posterior_at_zero if posterior_at_zero > 0 else np.inf\n",
    "        \n",
    "        # Interpret Bayes factor\n",
    "        if bayes_factor < 1/3:\n",
    "            bf_interpretation = \"Strong evidence for H0 (no difference)\"\n",
    "        elif bayes_factor < 1:\n",
    "            bf_interpretation = \"Moderate evidence for H0\"\n",
    "        elif bayes_factor < 3:\n",
    "            bf_interpretation = \"Weak evidence for H1\"\n",
    "        elif bayes_factor < 10:\n",
    "            bf_interpretation = \"Moderate evidence for H1 (difference exists)\"\n",
    "        else:\n",
    "            bf_interpretation = \"Strong evidence for H1\"\n",
    "        \n",
    "        results['bayesian_analysis'][metric] = {\n",
    "            'posterior_mean_difference': np.mean(posterior_diff),\n",
    "            'posterior_std_difference': np.std(posterior_diff),\n",
    "            'credible_interval_95': {\n",
    "                'lower': diff_ci[0],\n",
    "                'upper': diff_ci[1],\n",
    "                'contains_zero': diff_ci[0] <= 0 <= diff_ci[1]\n",
    "            },\n",
    "            'effect_size': {\n",
    "                'mean': np.mean(posterior_effect_size),\n",
    "                'credible_interval': {'lower': effect_ci[0], 'upper': effect_ci[1]}\n",
    "            },\n",
    "            'bayesian_p_value': bayesian_p_value,\n",
    "            'prob_supports_hypothesis': prob_supports_hypothesis,\n",
    "            'bayes_factor': bayes_factor,\n",
    "            'bayes_factor_interpretation': bf_interpretation,\n",
    "            'hypothesis_interpretation': 'SUPPORTS' if prob_supports_hypothesis > 0.95 else 'CONTRADICTS' if prob_supports_hypothesis < 0.05 else 'INCONCLUSIVE',\n",
    "            'model_summary': {\n",
    "                'n_samples': len(posterior_diff),\n",
    "                'n_tp': len(tp_values),\n",
    "                'n_hallucinations': len(halluc_values)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # =================================================================\n",
    "    # 4. METHOD COMPARISON AND SYNTHESIS\n",
    "    # =================================================================\n",
    "    logger.info(\"Synthesizing results across all advanced methods...\")\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric in results['permutation_tests'] or metric in results['extended_bootstrap'] or metric in results['bayesian_analysis']:\n",
    "            comparison = {\n",
    "                'metric': metric,\n",
    "                'hypothesis_support_summary': {},\n",
    "                'p_values': {},\n",
    "                'confidence_intervals': {},\n",
    "                'effect_evidence': {}\n",
    "            }\n",
    "            \n",
    "            # Collect hypothesis support from each method\n",
    "            methods_support = []\n",
    "            \n",
    "            if metric in results['permutation_tests'] and 'hypothesis_interpretation' in results['permutation_tests'][metric]:\n",
    "                perm_support = results['permutation_tests'][metric]['hypothesis_interpretation']\n",
    "                methods_support.append(('Permutation', perm_support))\n",
    "                comparison['p_values']['permutation_two_sided'] = results['permutation_tests'][metric]['two_sided']['p_value']\n",
    "                comparison['p_values']['permutation_one_sided'] = results['permutation_tests'][metric]['one_sided_less']['p_value']\n",
    "            \n",
    "            if metric in results['extended_bootstrap'] and 'iid_bootstrap' in results['extended_bootstrap'][metric]:\n",
    "                boot_support = results['extended_bootstrap'][metric]['iid_bootstrap']['hypothesis_interpretation']\n",
    "                methods_support.append(('Bootstrap', boot_support))\n",
    "                comparison['confidence_intervals']['bootstrap'] = {\n",
    "                    'lower': results['extended_bootstrap'][metric]['iid_bootstrap']['ci_lower'],\n",
    "                    'upper': results['extended_bootstrap'][metric]['iid_bootstrap']['ci_upper']\n",
    "                }\n",
    "            \n",
    "            if metric in results['bayesian_analysis'] and 'hypothesis_interpretation' in results['bayesian_analysis'][metric]:\n",
    "                bayes_support = results['bayesian_analysis'][metric]['hypothesis_interpretation']\n",
    "                methods_support.append(('Bayesian', bayes_support))\n",
    "                comparison['p_values']['bayesian'] = results['bayesian_analysis'][metric]['bayesian_p_value']\n",
    "                comparison['confidence_intervals']['bayesian_credible'] = results['bayesian_analysis'][metric]['credible_interval_95']\n",
    "            \n",
    "            # Consensus analysis\n",
    "            support_votes = sum(1 for _, support in methods_support if support == 'SUPPORTS')\n",
    "            contradict_votes = sum(1 for _, support in methods_support if support == 'CONTRADICTS')\n",
    "            inconclusive_votes = sum(1 for _, support in methods_support if support == 'INCONCLUSIVE')\n",
    "            \n",
    "            if support_votes > contradict_votes and support_votes > inconclusive_votes:\n",
    "                consensus = 'STRONG_SUPPORT'\n",
    "            elif contradict_votes > support_votes and contradict_votes > inconclusive_votes:\n",
    "                consensus = 'STRONG_CONTRADICTION'\n",
    "            elif support_votes == contradict_votes and support_votes > 0:\n",
    "                consensus = 'MIXED_EVIDENCE'\n",
    "            else:\n",
    "                consensus = 'INSUFFICIENT_EVIDENCE'\n",
    "            \n",
    "            comparison['hypothesis_support_summary'] = {\n",
    "                'methods_tested': len(methods_support),\n",
    "                'support_votes': support_votes,\n",
    "                'contradict_votes': contradict_votes,\n",
    "                'inconclusive_votes': inconclusive_votes,\n",
    "                'consensus': consensus,\n",
    "                'method_results': methods_support\n",
    "            }\n",
    "            \n",
    "            results['method_comparison'][metric] = comparison\n",
    "    \n",
    "    # =================================================================\n",
    "    # 5. OVERALL HYPOTHESIS CONCLUSION\n",
    "    # =================================================================\n",
    "    all_consensuses = [comp['hypothesis_support_summary']['consensus'] \n",
    "                      for comp in results['method_comparison'].values()]\n",
    "    \n",
    "    strong_support_count = all_consensuses.count('STRONG_SUPPORT')\n",
    "    strong_contradiction_count = all_consensuses.count('STRONG_CONTRADICTION')\n",
    "    mixed_count = all_consensuses.count('MIXED_EVIDENCE')\n",
    "    insufficient_count = all_consensuses.count('INSUFFICIENT_EVIDENCE')\n",
    "    \n",
    "    if strong_support_count > strong_contradiction_count and strong_support_count > 0:\n",
    "        overall_conclusion = 'HYPOTHESIS_STRONGLY_SUPPORTED'\n",
    "    elif strong_contradiction_count > strong_support_count and strong_contradiction_count > 0:\n",
    "        overall_conclusion = 'HYPOTHESIS_STRONGLY_CONTRADICTED'\n",
    "    elif mixed_count > 0:\n",
    "        overall_conclusion = 'MIXED_EVIDENCE_ACROSS_METRICS'\n",
    "    else:\n",
    "        overall_conclusion = 'INSUFFICIENT_EVIDENCE'\n",
    "    \n",
    "    results['hypothesis_conclusion'] = {\n",
    "        'overall_result': overall_conclusion,\n",
    "        'metrics_tested': len(all_consensuses),\n",
    "        'strong_support_count': strong_support_count,\n",
    "        'strong_contradiction_count': strong_contradiction_count,\n",
    "        'mixed_evidence_count': mixed_count,\n",
    "        'insufficient_evidence_count': insufficient_count,\n",
    "        'confidence_level': 'HIGH' if (strong_support_count + strong_contradiction_count) >= len(all_consensuses) * 0.75 else 'MODERATE' if (strong_support_count + strong_contradiction_count) > 0 else 'LOW'\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute the advanced statistical methods analysis\n",
    "print(\"ADVANCED STATISTICAL METHODS FOR HYPOTHESIS 2:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Applying cutting-edge statistical methods:\")\n",
    "print(\"1. PERMUTATION TESTS - Exact p-values without distributional assumptions\")\n",
    "print(\"2. EXTENDED BOOTSTRAP - BCa intervals and multiple bootstrap types\")\n",
    "print(\"3. BAYESIAN ANALYSIS - Credible intervals and Bayes factors\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Execute the analysis\n",
    "advanced_results = advanced_statistical_methods_analysis(\n",
    "    n_permutations=10000,\n",
    "    n_bootstrap=5000,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Store results for further analysis\n",
    "globals()['advanced_statistical_results'] = advanced_results\n",
    "\n",
    "if advanced_results:\n",
    "    print(\"âœ… ADVANCED STATISTICAL ANALYSIS COMPLETE!\")\n",
    "    print(f\"Results generated for {len(advanced_results.get('method_comparison', {}))} metrics\")\n",
    "    print(\"Results stored in 'advanced_statistical_results' for display in next cell.\")\n",
    "else:\n",
    "    print(\"âŒ No advanced statistical results generated. Check data and error logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Advanced Statistical Methods: Results Display and Interpretation\n",
    "# Purpose: Display and interpret results from advanced statistical analysis of Hypothesis 2\n",
    "# Dependencies: Results from cell 11 (advanced_statistical_results)\n",
    "# Breadcrumbs: Advanced Methods -> Results Display -> Comprehensive Interpretation\n",
    "\n",
    "# Display comprehensive results from advanced statistical analysis\n",
    "try:\n",
    "    # Check if results are available from previous cell\n",
    "    if 'advanced_statistical_results' not in globals() or not advanced_statistical_results:\n",
    "        print(\"âš ï¸ No advanced statistical results found.\")\n",
    "        print(\"Please run Cell [11] first to generate the results.\")\n",
    "    else:\n",
    "        advanced_results = advanced_statistical_results\n",
    "        \n",
    "        print(\"COMPREHENSIVE ADVANCED STATISTICAL RESULTS FOR HYPOTHESIS 2:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"HYPOTHESIS: 'Hallucinations in LLM outputs significantly correlate with\")\n",
    "        print(\"            lower quality SiFP estimates in new product development'\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Display Permutation Test Results\n",
    "        if 'permutation_tests' in advanced_results and advanced_results['permutation_tests']:\n",
    "            print(\"1. PERMUTATION TEST RESULTS:\")\n",
    "            print(\"   (Exact p-values without distributional assumptions)\")\n",
    "            print(\"-\" * 50)\n",
    "            for metric, perm_data in advanced_results['permutation_tests'].items():\n",
    "                if 'error' in perm_data:\n",
    "                    print(f\"  {metric}: Error - {perm_data['error']}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  ðŸ“Š {metric_name}:\")\n",
    "                print(f\"    Observed Difference (TP - Hallucinations): {perm_data['observed_difference']:.4f}\")\n",
    "                print(f\"    Two-sided p-value: {perm_data['two_sided']['p_value']:.6f}\")\n",
    "                print(f\"    One-sided p-value (TP < Hallucinations): {perm_data['one_sided_less']['p_value']:.6f}\")\n",
    "                print(f\"    Permutations: {perm_data['n_permutations']:,}\")\n",
    "                print(f\"    Hypothesis Support: {perm_data['hypothesis_interpretation']}\")\n",
    "                \n",
    "                # Interpretation with visual indicators\n",
    "                if perm_data['hypothesis_interpretation'] == 'SUPPORTS':\n",
    "                    print(f\"    âœ… SUPPORTS: TP links have significantly lower {metric_name.lower()}\")\n",
    "                    print(f\"       â†’ Hallucinations correlate with HIGHER estimation errors\")\n",
    "                elif perm_data['hypothesis_interpretation'] == 'CONTRADICTS':\n",
    "                    print(f\"    âŒ CONTRADICTS: TP links have significantly higher {metric_name.lower()}\")\n",
    "                    print(f\"       â†’ Hallucinations correlate with LOWER estimation errors\")\n",
    "                else:\n",
    "                    print(f\"    â“ INCONCLUSIVE: No significant difference detected\")\n",
    "                    print(f\"       â†’ Cannot conclude hallucinations affect estimation quality\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"1. PERMUTATION TESTS: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Extended Bootstrap Results\n",
    "        if 'extended_bootstrap' in advanced_results and advanced_results['extended_bootstrap']:\n",
    "            print(\"2. EXTENDED BOOTSTRAP RESULTS:\")\n",
    "            print(\"   (Robust confidence intervals and multiple bootstrap methods)\")\n",
    "            print(\"-\" * 60)\n",
    "            for metric, boot_data in advanced_results['extended_bootstrap'].items():\n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  ðŸ“ˆ {metric_name}:\")\n",
    "                \n",
    "                # IID Bootstrap Results\n",
    "                if 'iid_bootstrap' in boot_data and 'error' not in boot_data['iid_bootstrap']:\n",
    "                    iid = boot_data['iid_bootstrap']\n",
    "                    print(f\"    IID Bootstrap (n={iid['n_bootstrap']:,}):\")\n",
    "                    print(f\"      95% CI: [{iid['ci_lower']:.4f}, {iid['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Bootstrap p-value: {iid['bootstrap_p_value']:.6f}\")\n",
    "                    print(f\"      Contains zero: {'Yes' if iid['contains_zero'] else 'No'}\")\n",
    "                    print(f\"      Hypothesis Support: {iid['hypothesis_interpretation']}\")\n",
    "                    \n",
    "                    # Visual interpretation\n",
    "                    if iid['hypothesis_interpretation'] == 'SUPPORTS':\n",
    "                        print(f\"      âœ… Bootstrap CI supports hypothesis\")\n",
    "                    elif iid['hypothesis_interpretation'] == 'CONTRADICTS':\n",
    "                        print(f\"      âŒ Bootstrap CI contradicts hypothesis\")\n",
    "                    else:\n",
    "                        print(f\"      â“ Bootstrap CI is inconclusive\")\n",
    "                \n",
    "                # BCa Bootstrap Results\n",
    "                if 'bca_intervals' in boot_data and 'error' not in boot_data['bca_intervals']:\n",
    "                    bca = boot_data['bca_intervals']\n",
    "                    print(f\"    BCa Bootstrap (Bias-Corrected & Accelerated):\")\n",
    "                    print(f\"      95% CI: [{bca['ci_lower']:.4f}, {bca['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Bias correction: {bca['bias_correction']:.4f}\")\n",
    "                    print(f\"      Acceleration: {bca['acceleration']:.4f}\")\n",
    "                    print(f\"      Contains zero: {'Yes' if bca['contains_zero'] else 'No'}\")\n",
    "                \n",
    "                # Circular Block Bootstrap Results\n",
    "                if 'circular_block_bootstrap' in boot_data and 'error' not in boot_data['circular_block_bootstrap']:\n",
    "                    circular = boot_data['circular_block_bootstrap']\n",
    "                    print(f\"    Circular Block Bootstrap (n={circular['n_bootstrap']}):\")\n",
    "                    print(f\"      95% CI: [{circular['ci_lower']:.4f}, {circular['ci_upper']:.4f}]\")\n",
    "                    print(f\"      Hypothesis Support: {circular['hypothesis_interpretation']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"2. EXTENDED BOOTSTRAP: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Bayesian Analysis Results\n",
    "        if 'bayesian_analysis' in advanced_results and advanced_results['bayesian_analysis']:\n",
    "            print(\"3. BAYESIAN ANALYSIS RESULTS:\")\n",
    "            print(\"   (Probabilistic inference and model comparison)\")\n",
    "            print(\"-\" * 55)\n",
    "            for metric, bayes_data in advanced_results['bayesian_analysis'].items():\n",
    "                if 'error' in bayes_data:\n",
    "                    print(f\"  {metric}: Error - {bayes_data['error']}\")\n",
    "                    continue\n",
    "                    \n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                print(f\"  ðŸ”® {metric_name}:\")\n",
    "                \n",
    "                # Posterior results\n",
    "                print(f\"    Posterior Mean Difference: {bayes_data['posterior_mean_difference']:.4f}\")\n",
    "                print(f\"    Posterior Std Difference: {bayes_data['posterior_std_difference']:.4f}\")\n",
    "                \n",
    "                # Credible intervals\n",
    "                ci = bayes_data['credible_interval_95']\n",
    "                print(f\"    95% Credible Interval: [{ci['lower']:.4f}, {ci['upper']:.4f}]\")\n",
    "                print(f\"    Contains zero: {'Yes' if ci['contains_zero'] else 'No'}\")\n",
    "                \n",
    "                # Effect size\n",
    "                effect = bayes_data['effect_size']\n",
    "                print(f\"    Effect Size (mean): {effect['mean']:.4f}\")\n",
    "                print(f\"    Effect Size 95% CI: [{effect['credible_interval']['lower']:.4f}, {effect['credible_interval']['upper']:.4f}]\")\n",
    "                \n",
    "                # Probability assessments\n",
    "                print(f\"    Probability Supporting Hypothesis: {bayes_data['prob_supports_hypothesis']:.3f}\")\n",
    "                print(f\"    Bayesian p-value: {bayes_data['bayesian_p_value']:.6f}\")\n",
    "                \n",
    "                # Bayes Factor\n",
    "                print(f\"    Bayes Factor (BFâ‚â‚€): {bayes_data['bayes_factor']:.2f}\")\n",
    "                print(f\"    BF Interpretation: {bayes_data['bayes_factor_interpretation']}\")\n",
    "                print(f\"    Hypothesis Support: {bayes_data['hypothesis_interpretation']}\")\n",
    "                \n",
    "                # Visual interpretation\n",
    "                prob = bayes_data['prob_supports_hypothesis']\n",
    "                if prob > 0.95:\n",
    "                    print(f\"    âœ… STRONG BAYESIAN SUPPORT: {prob:.1%} probability TP has lower errors\")\n",
    "                elif prob < 0.05:\n",
    "                    print(f\"    âŒ STRONG BAYESIAN CONTRADICTION: {1-prob:.1%} probability TP has higher errors\")\n",
    "                else:\n",
    "                    print(f\"    â“ BAYESIAN UNCERTAINTY: {prob:.1%} probability supporting hypothesis\")\n",
    "                \n",
    "                # Model info\n",
    "                model_info = bayes_data['model_summary']\n",
    "                print(f\"    Model: {model_info['n_samples']:,} samples, TP={model_info['n_tp']}, Halluc={model_info['n_hallucinations']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"3. BAYESIAN ANALYSIS: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Cross-Method Comparison\n",
    "        if 'method_comparison' in advanced_results and advanced_results['method_comparison']:\n",
    "            print(\"4. CROSS-METHOD COMPARISON:\")\n",
    "            print(\"   (Consensus across statistical approaches)\")\n",
    "            print(\"-\" * 50)\n",
    "            for metric, comp_data in advanced_results['method_comparison'].items():\n",
    "                metric_name = \"Absolute Error\" if \"abs\" in metric else \"Percentage Error\"\n",
    "                summary = comp_data['hypothesis_support_summary']\n",
    "                \n",
    "                print(f\"  ðŸ” {metric_name}:\")\n",
    "                print(f\"    Methods Tested: {summary['methods_tested']}\")\n",
    "                print(f\"    Support Votes: {summary['support_votes']} âœ…\")\n",
    "                print(f\"    Contradict Votes: {summary['contradict_votes']} âŒ\")\n",
    "                print(f\"    Inconclusive Votes: {summary['inconclusive_votes']} â“\")\n",
    "                print(f\"    Consensus: {summary['consensus']}\")\n",
    "                \n",
    "                # Visual consensus interpretation\n",
    "                consensus = summary['consensus']\n",
    "                if consensus == 'STRONG_SUPPORT':\n",
    "                    print(f\"    ðŸŽ¯ STRONG CONSENSUS: Methods agree hypothesis is SUPPORTED\")\n",
    "                elif consensus == 'STRONG_CONTRADICTION':\n",
    "                    print(f\"    ðŸŽ¯ STRONG CONSENSUS: Methods agree hypothesis is CONTRADICTED\")\n",
    "                elif consensus == 'MIXED_EVIDENCE':\n",
    "                    print(f\"    âš–ï¸ MIXED EVIDENCE: Methods disagree on hypothesis\")\n",
    "                else:\n",
    "                    print(f\"    â“ INSUFFICIENT EVIDENCE: No clear consensus\")\n",
    "                \n",
    "                print(f\"    Method Details:\")\n",
    "                for method, result in summary['method_results']:\n",
    "                    emoji = \"âœ…\" if result == \"SUPPORTS\" else \"âŒ\" if result == \"CONTRADICTS\" else \"â“\"\n",
    "                    print(f\"      {emoji} {method}: {result}\")\n",
    "                \n",
    "                # P-values summary\n",
    "                if 'p_values' in comp_data:\n",
    "                    print(f\"    P-values:\")\n",
    "                    for test_name, p_val in comp_data['p_values'].items():\n",
    "                        significance = \"significant\" if p_val < 0.05 else \"not significant\"\n",
    "                        print(f\"      {test_name}: {p_val:.6f} ({significance})\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"4. CROSS-METHOD COMPARISON: No results available\")\n",
    "            print()\n",
    "        \n",
    "        # Display Overall Hypothesis Conclusion\n",
    "        if 'hypothesis_conclusion' in advanced_results:\n",
    "            conclusion = advanced_results['hypothesis_conclusion']\n",
    "            print(\"5. ðŸŽ¯ COMPREHENSIVE HYPOTHESIS 2 CONCLUSION:\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Overall Result: {conclusion['overall_result']}\")\n",
    "            print(f\"Confidence Level: {conclusion['confidence_level']}\")\n",
    "            print(f\"Metrics Tested: {conclusion['metrics_tested']}\")\n",
    "            print(f\"Strong Support: {conclusion['strong_support_count']} metric(s)\")\n",
    "            print(f\"Strong Contradiction: {conclusion['strong_contradiction_count']} metric(s)\")\n",
    "            print(f\"Mixed Evidence: {conclusion['mixed_evidence_count']} metric(s)\")\n",
    "            print(f\"Insufficient Evidence: {conclusion['insufficient_evidence_count']} metric(s)\")\n",
    "            print()\n",
    "            \n",
    "            # Final comprehensive interpretation with recommendations\n",
    "            result = conclusion['overall_result']\n",
    "            confidence = conclusion['confidence_level']\n",
    "            \n",
    "            print(\"ðŸ“‹ FINAL INTERPRETATION & RECOMMENDATIONS:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            if result == 'HYPOTHESIS_STRONGLY_SUPPORTED':\n",
    "                print(\"ðŸ† CONCLUSION: HYPOTHESIS 2 IS STRONGLY SUPPORTED\")\n",
    "                print()\n",
    "                print(\"ðŸ“Š EVIDENCE SUMMARY:\")\n",
    "                print(\"   âœ… Multiple advanced statistical methods converge on the conclusion that\")\n",
    "                print(\"      hallucinations in LLM outputs (both FP over-identification and FN\")\n",
    "                print(\"      under-identification) significantly correlate with lower quality\")\n",
    "                print(\"      SiFP estimates in new product development.\")\n",
    "                print()\n",
    "                print(\"ðŸ”¬ STATISTICAL RIGOR:\")\n",
    "                print(\"   â€¢ Permutation tests: Exact p-values without distributional assumptions\")\n",
    "                print(\"   â€¢ Extended bootstrap: Robust confidence intervals (IID, BCa, Circular Block)\")\n",
    "                print(\"   â€¢ Bayesian analysis: Probabilistic evidence with credible intervals & Bayes factors\")\n",
    "                print(\"   â€¢ Cross-method consensus: Multiple approaches agree on the conclusion\")\n",
    "                print()\n",
    "                print(\"ðŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Hallucinated traceability links are associated with WORSE SiFP estimates\")\n",
    "                print(\"   2. Focus on improving LLM accuracy to enhance estimation quality\")\n",
    "                print(\"   3. Implement hallucination detection mechanisms in traceability systems\")\n",
    "                print(\"   4. Use estimation quality as a feedback signal for LLM performance\")\n",
    "                print()\n",
    "                print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "                print(\"   â€¢ Prioritize reducing both FP (over-identification) and FN (missed features)\")\n",
    "                print(\"   â€¢ Implement confidence scoring for traceability predictions\")\n",
    "                print(\"   â€¢ Use estimation accuracy as a validation metric for traceability quality\")\n",
    "                print(\"   â€¢ Consider ensemble methods to reduce hallucination rates\")\n",
    "                \n",
    "            elif result == 'HYPOTHESIS_STRONGLY_CONTRADICTED':\n",
    "                print(\"ðŸš« CONCLUSION: HYPOTHESIS 2 IS STRONGLY CONTRADICTED\")\n",
    "                print()\n",
    "                print(\"ðŸ“Š EVIDENCE SUMMARY:\")\n",
    "                print(\"   âŒ Multiple advanced statistical methods converge on the conclusion that\")\n",
    "                print(\"      hallucinations do NOT significantly correlate with lower quality\")\n",
    "                print(\"      SiFP estimates. In fact, the evidence suggests hallucinations may\")\n",
    "                print(\"      be associated with equal or even better estimation quality.\")\n",
    "                print()\n",
    "                print(\"ðŸ¤” UNEXPECTED FINDINGS:\")\n",
    "                print(\"   â€¢ Hallucinated links may not negatively impact estimation accuracy\")\n",
    "                print(\"   â€¢ Current LLM hallucinations might be 'beneficial' or neutral for SiFP\")\n",
    "                print(\"   â€¢ The relationship between traceability and estimation may be more complex\")\n",
    "                print()\n",
    "                print(\"ðŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Hallucination reduction may not improve SiFP estimation quality\")\n",
    "                print(\"   2. Focus effort on other factors affecting estimation accuracy\")\n",
    "                print(\"   3. Re-examine the assumed relationship between traceability and estimation\")\n",
    "                print(\"   4. Consider whether current 'hallucinations' capture useful information\")\n",
    "                print()\n",
    "                print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "                print(\"   â€¢ Investigate why hallucinations don't correlate with worse estimates\")\n",
    "                print(\"   â€¢ Examine other factors that might influence SiFP estimation quality\")\n",
    "                print(\"   â€¢ Consider revising the definition of 'hallucination' in this context\")\n",
    "                print(\"   â€¢ Focus optimization efforts on different aspects of the system\")\n",
    "                \n",
    "            elif result == 'MIXED_EVIDENCE_ACROSS_METRICS':\n",
    "                print(\"âš–ï¸ CONCLUSION: MIXED EVIDENCE FOR HYPOTHESIS 2\")\n",
    "                print()\n",
    "                print(\"ðŸ“Š EVIDENCE SUMMARY:\")\n",
    "                print(\"   ðŸ”€ Different error metrics show different patterns. The relationship\")\n",
    "                print(\"      between hallucinations and estimation quality appears to be more\")\n",
    "                print(\"      complex than hypothesized, potentially varying by error type,\")\n",
    "                print(\"      context, or other moderating factors.\")\n",
    "                print()\n",
    "                print(\"ðŸ§© COMPLEXITY INDICATORS:\")\n",
    "                print(\"   â€¢ Some metrics support the hypothesis while others contradict it\")\n",
    "                print(\"   â€¢ Effect may depend on the type of estimation error measured\")\n",
    "                print(\"   â€¢ Relationship may be moderated by other variables\")\n",
    "                print()\n",
    "                print(\"ðŸ’¡ PRACTICAL IMPLICATIONS:\")\n",
    "                print(\"   1. Simple binary relationship between hallucinations and quality may not exist\")\n",
    "                print(\"   2. Different types of estimation errors may be affected differently\")\n",
    "                print(\"   3. Context-dependent effects require more nuanced analysis\")\n",
    "                print()\n",
    "                print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "                print(\"   â€¢ Conduct subgroup analyses to identify moderating factors\")\n",
    "                print(\"   â€¢ Examine different types of hallucinations separately\")\n",
    "                print(\"   â€¢ Consider interaction effects with other variables\")\n",
    "                print(\"   â€¢ Develop more sophisticated models of the relationship\")\n",
    "                \n",
    "            else:\n",
    "                print(\"â“ CONCLUSION: INSUFFICIENT EVIDENCE FOR HYPOTHESIS 2\")\n",
    "                print()\n",
    "                print(\"ðŸ“Š EVIDENCE SUMMARY:\")\n",
    "                print(\"   âš ï¸ The available data and methods do not provide sufficient evidence\")\n",
    "                print(\"      to draw strong conclusions about the relationship between\")\n",
    "                print(\"      hallucinations and SiFP estimation quality.\")\n",
    "                print()\n",
    "                print(\"ðŸ” LIMITATIONS:\")\n",
    "                print(\"   â€¢ Sample size may be too small for reliable detection\")\n",
    "                print(\"   â€¢ Effect size may be smaller than detectable with current data\")\n",
    "                print(\"   â€¢ Measurement noise may obscure true relationships\")\n",
    "                print()\n",
    "                print(\"ðŸŽ¯ RECOMMENDATIONS:\")\n",
    "                print(\"   â€¢ Collect additional data to increase statistical power\")\n",
    "                print(\"   â€¢ Improve measurement precision of key variables\")\n",
    "                print(\"   â€¢ Consider alternative analytical approaches\")\n",
    "                print(\"   â€¢ Replicate analysis with different datasets\")\n",
    "            \n",
    "            print()\n",
    "            print(\"ðŸ ANALYSIS COMPLETE!\")\n",
    "            print(f\"Confidence in conclusion: {confidence}\")\n",
    "            print(\"Results available in 'advanced_statistical_results' for further analysis.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ADVANCED STATISTICAL HYPOTHESIS 2 TESTING COMPLETE!\")\n",
    "        print(\"All results stored in 'advanced_statistical_results' global variable.\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error displaying advanced statistical results: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

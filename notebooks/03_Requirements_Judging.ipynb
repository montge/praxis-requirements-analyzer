{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements Judging Workflow\n",
    "**LLM-based requirements traceability evaluation with actor-judge pattern, batch processing, Redis vector search, and Neo4j result storage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Requirements Judging\n",
    "# Dependencies: os, sys, logging, asyncio, dotenv, pathlib, datetime, redis, numpy, json, src modules\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration -> Logging Setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List\n",
    "from redis.asyncio import Redis\n",
    "from redisvl.index import AsyncSearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configure Python path, logging, and load environment variables for Requirements Judging\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters including model settings and paths\n",
    "    \"\"\"\n",
    "    # Get the absolute path to the project root directory (parent of notebooks)\n",
    "    notebook_dir = Path(os.getcwd())\n",
    "    project_root = notebook_dir.parent\n",
    "    src_path = project_root / 'src'\n",
    "    \n",
    "    # Add the project root to Python path if not already there\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.append(str(project_root))\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create logs directory structure if it doesn't exist\n",
    "    logs_dir = project_root / \"logs\"\n",
    "    logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get model configuration\n",
    "    current_model = os.getenv('CURRENT_MODEL', 'CLAUDE_3_5_MODEL_ID')\n",
    "    model_name = os.getenv(current_model, 'claude-2.1')\n",
    "    \n",
    "    # Create a safe directory name from model name\n",
    "    model_dir_name = model_name.lower().replace('-', '_').replace('.', '_')\n",
    "    model_dir = logs_dir / model_dir_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create log filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = model_dir / f\"{model_name}_{timestamp}.log\"\n",
    "    \n",
    "    # Configuration from environment variables\n",
    "    config = {\n",
    "        'PROJECT_ROOT': project_root,\n",
    "        'SRC_PATH': src_path,\n",
    "        'LOGS_DIR': logs_dir,\n",
    "        'LOG_FILENAME': log_filename,\n",
    "        'MODEL_NAME': model_name,\n",
    "        'CURRENT_MODEL': current_model,\n",
    "        'TEST_MODE': os.getenv('TEST_MODE', 'False').lower() == 'true',\n",
    "        'LOG_LEVEL': os.getenv('LOG_LEVEL', 'INFO'),\n",
    "        'MAX_CONCURRENT_JOBS': int(os.getenv('MAX_CONCURRENT_JOBS', 5)),\n",
    "        'BATCH_SIZE': int(os.getenv('BATCH_SIZE', 5)),\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_DATABASE': os.getenv('NEO4J_DATABASE', 'neo4j'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME', 'eANCI'),\n",
    "        'REDIS_HOST': os.getenv('REDIS_HOST', 'localhost'),\n",
    "        'REDIS_PORT': os.getenv('REDIS_PORT', 6379),\n",
    "        'REDIS_PASSWORD': os.getenv('REDIS_PASSWORD')\n",
    "    }\n",
    "    \n",
    "    print(f\"Project root added to path: {project_root}\")\n",
    "    print(f\"Model: {config['MODEL_NAME']}\")\n",
    "    print(f\"Test Mode: {config['TEST_MODE']}\")\n",
    "    print(f\"Log Level: {config['LOG_LEVEL']}\")\n",
    "    print(f\"Logs will be written to: {config['LOG_FILENAME']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Execute setup when imported\n",
    "CONFIG = setup_environment()\n",
    "\n",
    "# Import project modules after path setup\n",
    "from praxis_requirements_analyzer.utils.logger import ColoredFormatter, setup_logger\n",
    "from praxis_requirements_analyzer.neo4j.neo4j_client import Neo4jClient\n",
    "from praxis_requirements_analyzer.redis.redis_client import RedisClient\n",
    "from praxis_requirements_analyzer.llm.manager import LLMManager\n",
    "from praxis_requirements_analyzer.requirements_analyzer.requirements_workflow import RequirementsWorkflow\n",
    "from praxis_requirements_analyzer.requirements_analyzer.requirements_prompt_manager import RequirementsPromptManager\n",
    "from praxis_requirements_analyzer.llm.models.huggingface.hf_embeddings_client import HuggingFaceEmbeddingsClient\n",
    "from praxis_requirements_analyzer.neo4j.requirements_client import RequirementsClient\n",
    "from praxis_requirements_analyzer.models.requirement import Requirement\n",
    "\n",
    "print(\"Setup completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Configure Logging System\n",
    "# Purpose: Set up comprehensive logging with file and console handlers for requirements judging\n",
    "# Dependencies: ColoredFormatter, setup_logger from Cell 0\n",
    "# Breadcrumbs: Setup -> Logging Configuration -> Handler Setup\n",
    "\n",
    "def get_logging_level(level_str: str) -> int:\n",
    "    \"\"\"Convert string log level to logging constant\"\"\"\n",
    "    level_map = {\n",
    "        'DEBUG': logging.DEBUG,\n",
    "        'INFO': logging.INFO,\n",
    "        'WARNING': logging.WARNING,\n",
    "        'ERROR': logging.ERROR,\n",
    "        'CRITICAL': logging.CRITICAL\n",
    "    }\n",
    "    return level_map.get(level_str.upper(), logging.INFO)  # Default to INFO if not specified\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"\n",
    "    Configure comprehensive logging system for requirements judging workflow\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (file_handler, console_handler) for additional logger configuration\n",
    "    \"\"\"\n",
    "    # Get logging level from configuration\n",
    "    logging_level = get_logging_level(CONFIG['LOG_LEVEL'])\n",
    "    \n",
    "    # Set root logger to environment-specified level\n",
    "    logging.getLogger().setLevel(logging_level)\n",
    "    \n",
    "    print(f\"Configuring Logging System\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Log Level: {CONFIG['LOG_LEVEL']} ({logging_level})\")\n",
    "    print(f\"Test Mode: {CONFIG['TEST_MODE']}\")\n",
    "    print(f\"Model: {CONFIG['MODEL_NAME']}\")\n",
    "    print(f\"Log File: {CONFIG['LOG_FILENAME']}\")\n",
    "    \n",
    "    try:\n",
    "        # Create file handler with DEBUG level for complete logging\n",
    "        file_handler = logging.FileHandler(CONFIG['LOG_FILENAME'])\n",
    "        file_handler.setLevel(logging.DEBUG)  # Keep DEBUG level for file logging\n",
    "        file_handler.setFormatter(logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        ))\n",
    "\n",
    "        # Create console handler with appropriate level based on test mode\n",
    "        console_handler = logging.StreamHandler()\n",
    "        # Set DEBUG level if in test mode, otherwise use INFO\n",
    "        console_handler.setLevel(logging.DEBUG if CONFIG['TEST_MODE'] else logging.INFO)\n",
    "        console_handler.setFormatter(ColoredFormatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        ))\n",
    "\n",
    "        # Define all loggers with their levels\n",
    "        loggers = {\n",
    "            # Requirements analysis loggers\n",
    "            \"src.requirements_analyzer.requirements_workflow\": logging_level,\n",
    "            \"src.requirements_analyzer.requirements_prompt_manager\": logging_level,\n",
    "            \n",
    "            # LLM-related loggers\n",
    "            \"src.llm.manager\": logging_level,\n",
    "            \"src.llm.manager.llm_manager\": logging_level,\n",
    "            \"src.llm.models.anthropic.claude_client\": logging_level,\n",
    "            \"src.llm.models.openai.openai_client\": logging_level,\n",
    "            \"src.llm.models.huggingface.hf_client\": logging_level,\n",
    "            \"src.llm.models.huggingface.hf_embeddings_client\": logging_level,\n",
    "            \n",
    "            # Database loggers\n",
    "            \"src.redis.redis_client\": logging_level,\n",
    "            \"redisvl\": logging_level,\n",
    "            \n",
    "            # Other loggers\n",
    "            \"initialization\": logging_level,\n",
    "            \"redis\": logging_level\n",
    "        }\n",
    "\n",
    "        # Remove all existing handlers from root logger\n",
    "        root_logger = logging.getLogger()\n",
    "        root_logger.handlers = []\n",
    "        \n",
    "        # Add our handlers to root logger\n",
    "        root_logger.addHandler(file_handler)\n",
    "        root_logger.addHandler(console_handler)\n",
    "\n",
    "        # Configure all loggers\n",
    "        for logger_name, level in loggers.items():\n",
    "            logger = setup_logger(logger_name, level)\n",
    "            logger.propagate = False  # Prevent duplicate logging\n",
    "            \n",
    "            # Clear existing handlers and add our configured handlers\n",
    "            logger.handlers = []\n",
    "            logger.addHandler(file_handler)\n",
    "            logger.addHandler(console_handler)\n",
    "        \n",
    "        # Log initial setup information\n",
    "        root_logger.info(f\"Logging system initialized\")\n",
    "        root_logger.info(f\"Log file: {CONFIG['LOG_FILENAME']}\")\n",
    "        root_logger.info(f\"Model: {CONFIG['MODEL_NAME']}\")\n",
    "        root_logger.info(f\"Log level: {CONFIG['LOG_LEVEL']}\")\n",
    "        \n",
    "        if CONFIG['TEST_MODE']:\n",
    "            root_logger.info(f\"Running in TEST MODE - Console shows DEBUG messages\")\n",
    "        \n",
    "        if logging_level <= logging.DEBUG:\n",
    "            root_logger.debug(\"Debug logging is enabled\")\n",
    "\n",
    "        # Test logger setup with all levels\n",
    "        test_logger = logging.getLogger(\"test_logger\")\n",
    "        test_logger.setLevel(logging_level)\n",
    "        test_logger.debug(\"Test debug message\")\n",
    "        test_logger.info(\"Test info message\")\n",
    "        test_logger.warning(\"Test warning message\")\n",
    "        test_logger.error(\"Test error message\")\n",
    "        \n",
    "        print(\"Logging system configured successfully!\")\n",
    "        return file_handler, console_handler\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR setting up logging: {str(e)}\")\n",
    "        print(f\"Attempted to create log file at: {CONFIG['LOG_FILENAME']}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Project root: {CONFIG['PROJECT_ROOT']}\")\n",
    "        raise\n",
    "\n",
    "# Execute logging setup\n",
    "file_handler, console_handler = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Initialize Clients and Workflow\n",
    "# Purpose: Initialize Neo4j, Redis, LLM components and Requirements Workflow\n",
    "# Dependencies: Client classes from Cell 0, logging configuration from Cell 1\n",
    "# Breadcrumbs: Setup -> Logging -> Client Initialization -> Workflow Setup\n",
    "\n",
    "async def initialize_clients():\n",
    "    \"\"\"\n",
    "    Initialize all clients and workflow components for requirements judging\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (neo4j_client, redis_client, workflow, requirements_client)\n",
    "    \"\"\"\n",
    "    init_logger = logging.getLogger(\"initialization\")\n",
    "    \n",
    "    try:\n",
    "        init_logger.info(\"Starting client initialization\")\n",
    "        print(f\"\\nInitializing Clients and Workflow\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Initialize Neo4j client with configuration\n",
    "        init_logger.debug(\"Initializing Neo4j client\")\n",
    "        print(f\"Connecting to Neo4j database...\")\n",
    "        neo4j_client = Neo4jClient(\n",
    "            uri=CONFIG['NEO4J_URI'],\n",
    "            user=CONFIG['NEO4J_USER'],\n",
    "            password=CONFIG['NEO4J_PASSWORD'],\n",
    "            database=CONFIG['NEO4J_DATABASE']\n",
    "        )\n",
    "        await neo4j_client.connect()\n",
    "        init_logger.info(\"Neo4j client connected successfully\")\n",
    "        print(f\"Neo4j connected to database: {CONFIG['NEO4J_DATABASE']}\")\n",
    "\n",
    "        # Initialize Redis components\n",
    "        init_logger.debug(\"Initializing Redis client\")\n",
    "        print(f\"Setting up Redis connection...\")\n",
    "        \n",
    "        # Build Redis URL\n",
    "        redis_url = f\"redis://{CONFIG['REDIS_HOST']}:{CONFIG['REDIS_PORT']}\"\n",
    "        if CONFIG['REDIS_PASSWORD']:\n",
    "            redis_url = f\"redis://:{CONFIG['REDIS_PASSWORD']}@{CONFIG['REDIS_HOST']}:{CONFIG['REDIS_PORT']}\"\n",
    "        \n",
    "        # Create Redis base client\n",
    "        redis_base = Redis.from_url(redis_url)\n",
    "        \n",
    "        # Verify Redis connection\n",
    "        try:\n",
    "            await redis_base.ping()\n",
    "            init_logger.info(\"Redis connection verified\")\n",
    "            print(f\"Redis connected at {CONFIG['REDIS_HOST']}:{CONFIG['REDIS_PORT']}\")\n",
    "        except Exception as e:\n",
    "            init_logger.error(f\"Redis connection failed: {str(e)}\", exc_info=True)\n",
    "            print(f\"Redis connection failed: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Initialize our Redis client wrapper\n",
    "        redis_client = RedisClient(redis_base)\n",
    "        \n",
    "        # Initialize indices in Redis client\n",
    "        try:\n",
    "            print(f\"Initializing Redis indices...\")\n",
    "            await redis_client.initialize_indices()\n",
    "            init_logger.info(\"Successfully created/updated Redis indices\")\n",
    "            print(f\"Redis indices initialized successfully\")\n",
    "        except Exception as e:\n",
    "            init_logger.error(f\"Failed to initialize Redis indices: {str(e)}\", exc_info=True)\n",
    "            print(f\"Failed to initialize Redis indices: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize LLM components\n",
    "        init_logger.debug(\"Initializing LLM components\")\n",
    "        print(f\"Setting up LLM Manager...\")\n",
    "        llm_manager = LLMManager()\n",
    "        \n",
    "        # Initialize the models\n",
    "        try:\n",
    "            await llm_manager.initialize_models()\n",
    "            init_logger.info(f\"LLM manager initialized with model: {CONFIG['MODEL_NAME']}\")\n",
    "            print(f\"LLM Manager initialized with model: {CONFIG['MODEL_NAME']}\")\n",
    "        except Exception as e:\n",
    "            init_logger.error(f\"Failed to initialize LLM models: {str(e)}\", exc_info=True)\n",
    "            print(f\"Failed to initialize LLM models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize additional components\n",
    "        init_logger.debug(\"Initializing prompt manager and embedding client\")\n",
    "        print(f\"Initializing supporting components...\")\n",
    "        prompt_manager = RequirementsPromptManager()\n",
    "        embedding_client = HuggingFaceEmbeddingsClient()\n",
    "\n",
    "        # Initialize requirements client with project name\n",
    "        init_logger.debug(\"Initializing requirements client\")\n",
    "        requirements_client = RequirementsClient(\n",
    "            neo4j_client=neo4j_client,\n",
    "            project_name=CONFIG['NEO4J_PROJECT_NAME']\n",
    "        )\n",
    "        print(f\"Requirements client initialized for project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "\n",
    "        # Initialize workflow with proper components\n",
    "        init_logger.debug(\"Initializing requirements workflow\")\n",
    "        print(f\"Setting up Requirements Workflow...\")\n",
    "        workflow = RequirementsWorkflow(\n",
    "            llm_manager=llm_manager,\n",
    "            prompt_manager=prompt_manager,\n",
    "            model_name=CONFIG['MODEL_NAME'],\n",
    "            redis_client=redis_client,\n",
    "            embedding_client=embedding_client\n",
    "        )\n",
    "        \n",
    "        # Explicitly set workflow logger level\n",
    "        workflow.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Initialize RedisVL indices\n",
    "        try:\n",
    "            print(f\"Initializing RedisVL indices...\")\n",
    "            await workflow.init_indices()\n",
    "            init_logger.info(\"Successfully created RedisVL indices\")\n",
    "            print(f\"RedisVL indices created successfully\")\n",
    "        except Exception as e:\n",
    "            init_logger.error(f\"Failed to create RedisVL indices: {str(e)}\", exc_info=True)\n",
    "            # Log the Redis client state for debugging\n",
    "            init_logger.debug(f\"Redis client state: {redis_client.client}\")\n",
    "            try:\n",
    "                ping_result = await redis_client.client.ping()\n",
    "                init_logger.debug(f\"Redis client ping: {ping_result}\")\n",
    "            except Exception as ping_error:\n",
    "                init_logger.debug(f\"Redis ping failed: {ping_error}\")\n",
    "            print(f\"Failed to create RedisVL indices: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        init_logger.info(f\"Initialization complete - all components ready\")\n",
    "        print(f\"\\nAll Components Initialized Successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Model: {CONFIG['MODEL_NAME']}\")\n",
    "        print(f\"Project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "        print(f\"Test Mode: {CONFIG['TEST_MODE']}\")\n",
    "        print(f\"Max Concurrent Jobs: {CONFIG['MAX_CONCURRENT_JOBS']}\")\n",
    "        print(f\"Batch Size: {CONFIG['BATCH_SIZE']}\")\n",
    "        \n",
    "        return neo4j_client, redis_client, workflow, requirements_client\n",
    "        \n",
    "    except Exception as e:\n",
    "        init_logger.critical(f\"Critical error during initialization: {str(e)}\", exc_info=True)\n",
    "        print(f\"Critical initialization error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute client initialization\n",
    "init_logger = logging.getLogger(\"initialization\")\n",
    "init_logger.info(\"Starting client initialization process\")\n",
    "\n",
    "try:\n",
    "    neo4j_client, redis_client, workflow, requirements_client = await initialize_clients()\n",
    "    init_logger.info(\"Client initialization completed successfully\")\n",
    "except Exception as e:\n",
    "    init_logger.critical(\"Failed to initialize clients\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Define Helper Functions\n",
    "# Purpose: Define the main requirements processing function with batch handling\n",
    "# Dependencies: Workflow and clients from Cell 2, CONFIG from Cell 0\n",
    "# Breadcrumbs: Client Initialization -> Helper Functions -> Batch Processing Logic\n",
    "\n",
    "async def process_requirements():\n",
    "    \"\"\"\n",
    "    Process requirements using the initialized workflow, handling requirements concurrently in batches.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of requirement matches with scores and reasoning\n",
    "    \"\"\"\n",
    "    requirements_logger = setup_logger(\"src.requirements_analyzer.requirements_workflow\", logging.DEBUG)\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nStarting Requirements Processing\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Fetch requirements from Neo4j\n",
    "        print(f\"Fetching requirements from Neo4j...\")\n",
    "        requirements = await requirements_client.get_requirements()\n",
    "        \n",
    "        # Apply test mode limitations if enabled\n",
    "        if CONFIG['TEST_MODE']:\n",
    "            original_source_count = len(requirements['source'])\n",
    "            original_target_count = len(requirements['target'])\n",
    "            requirements['source'] = requirements['source'][:2]\n",
    "            requirements['target'] = requirements['target'][:12]\n",
    "            print(f\"TEST MODE: Limited requirements\")\n",
    "            print(f\"   Source: {original_source_count} → {len(requirements['source'])}\")\n",
    "            print(f\"   Target: {original_target_count} → {len(requirements['target'])}\")\n",
    "        \n",
    "        total_source = len(requirements['source'])\n",
    "        total_target = len(requirements['target'])\n",
    "        \n",
    "        print(f\"Processing Configuration:\")\n",
    "        print(f\"   Source requirements: {total_source}\")\n",
    "        print(f\"   Target requirements: {total_target}\")\n",
    "        print(f\"   Max concurrent jobs: {CONFIG['MAX_CONCURRENT_JOBS']}\")\n",
    "        print(f\"   Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "        \n",
    "        # Calculate batch information\n",
    "        total_batches = (total_target + CONFIG['BATCH_SIZE'] - 1) // CONFIG['BATCH_SIZE']  # Ceiling division\n",
    "        all_matches = []\n",
    "        \n",
    "        requirements_logger.info(f\"Starting processing of {total_source} source requirements against {total_target} target requirements\")\n",
    "        requirements_logger.info(f\"Will process in {total_batches} batches of up to {CONFIG['BATCH_SIZE']} requirements each\")\n",
    "        \n",
    "        print(f\"\\nProcessing {total_batches} batches...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Process target requirements in batches\n",
    "        for i in range(0, total_target, CONFIG['BATCH_SIZE']):\n",
    "            target_batch = requirements['target'][i:i + CONFIG['BATCH_SIZE']]\n",
    "            batch_num = i // CONFIG['BATCH_SIZE'] + 1\n",
    "            batch_size_actual = len(target_batch)\n",
    "            \n",
    "            print(f\"\\nBatch {batch_num}/{total_batches}: Processing requirements {i+1} to {i+batch_size_actual}\")\n",
    "            requirements_logger.info(f\"Processing target requirements batch {batch_num}/{total_batches} \"\n",
    "                                f\"(requirements {i+1} to {i+batch_size_actual})\")\n",
    "            requirements_logger.debug(f\"Batch {batch_num} contains {batch_size_actual} requirements\")\n",
    "            \n",
    "            try:\n",
    "                # Process the batch\n",
    "                batch_matches = await workflow.process_requirements_batch(\n",
    "                    source_requirements=requirements['source'],\n",
    "                    target_requirements=target_batch,\n",
    "                    max_concurrent=CONFIG['MAX_CONCURRENT_JOBS']\n",
    "                )\n",
    "                \n",
    "                if batch_matches:\n",
    "                    all_matches.extend(batch_matches)\n",
    "                    requirements_logger.info(f\"Batch {batch_num}/{total_batches} completed: found {len(batch_matches)} matches\")\n",
    "                    print(f\"   Found {len(batch_matches)} matches in this batch\")\n",
    "                else:\n",
    "                    requirements_logger.warning(f\"Batch {batch_num}/{total_batches} completed: found no matches\")\n",
    "                    print(f\"   No matches found in this batch\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                requirements_logger.error(f\"Error processing batch {batch_num}/{total_batches}: {str(e)}\", exc_info=True)\n",
    "                print(f\"   Error processing batch {batch_num}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\nProcessing Complete!\")\n",
    "        print(\"=\" * 80)\n",
    "        if all_matches:\n",
    "            print(f\"Found total of {len(all_matches)} requirement matches across all batches\")\n",
    "            \n",
    "            # Display some statistics\n",
    "            unique_sources = len(set(match['source_id'] for match in all_matches))\n",
    "            unique_targets = len(set(match['target_id'] for match in all_matches))\n",
    "            print(f\"Statistics:\")\n",
    "            print(f\"   Unique source requirements with matches: {unique_sources}\")\n",
    "            print(f\"   Unique target requirements with matches: {unique_targets}\")\n",
    "            print(f\"   Average matches per source: {len(all_matches) / unique_sources:.2f}\")\n",
    "        else:\n",
    "            print(\"WARNING: No requirement matches found in any batch\")\n",
    "            \n",
    "        return all_matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        requirements_logger.error(f\"Error in process_requirements: {str(e)}\", exc_info=True)\n",
    "        print(f\"Error in requirements processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Execute Requirements Processing\n",
    "# Purpose: Run the main requirements processing workflow and display detailed results\n",
    "# Dependencies: process_requirements function from Cell 3, logging configuration\n",
    "# Breadcrumbs: Helper Functions -> Main Processing -> Results Display\n",
    "\n",
    "def configure_logging_for_processing():\n",
    "    \"\"\"Configure logging levels for the processing phase\"\"\"\n",
    "    # Configure console handler based on TEST_MODE setting\n",
    "    for handler in logging.getLogger().handlers:\n",
    "        if isinstance(handler, logging.StreamHandler):\n",
    "            if not CONFIG['TEST_MODE']:  # Only adjust if not in test mode\n",
    "                handler.setLevel(logging.INFO)\n",
    "                print(\"Console logging level set to INFO to show evaluation progress\")\n",
    "            else:\n",
    "                print(\"TEST MODE: Console showing DEBUG level messages\")\n",
    "\n",
    "    # Ensure workflow logger respects the TEST_MODE setting\n",
    "    requirements_logger = logging.getLogger(\"src.requirements_analyzer.requirements_workflow\")\n",
    "    if not CONFIG['TEST_MODE']:\n",
    "        requirements_logger.setLevel(logging.INFO)\n",
    "    else:\n",
    "        requirements_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def display_detailed_results(results):\n",
    "    \"\"\"Display detailed results of the requirements processing\"\"\"\n",
    "    print(f\"\\nDetailed Results Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total matches processed: {len(results)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"WARNING: No matches to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nSample Results (showing first 3 matches):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, match in enumerate(results[:3], 1):\n",
    "        print(f\"\\nMatch {i}:\")\n",
    "        print(f\"   Source ID: {match['source_id']} → Target ID: {match['target_id']}\")\n",
    "        \n",
    "        # Scores and reasoning\n",
    "        print(f\"\\n   Actor Evaluation:\")\n",
    "        print(f\"      Score: {match.get('actor_score', 0)}\")\n",
    "        print(f\"      Reasoning: {match.get('actor_reasoning', 'No actor reasoning provided')[:200]}...\")\n",
    "        \n",
    "        print(f\"\\n   Judge Evaluation:\")\n",
    "        print(f\"      Score: {match.get('judge_score', 0)}\")\n",
    "        print(f\"      Reasoning: {match.get('judge_reasoning', 'No judge reasoning provided')[:200]}...\")\n",
    "        \n",
    "        # Match quality metrics\n",
    "        match_quality = match.get('match_quality', {})\n",
    "        print(f\"\\n   Match Quality Metrics:\")\n",
    "        print(f\"      Semantic Alignment: {match_quality.get('semantic_alignment', 0)}\")\n",
    "        print(f\"      Functional Completeness: {match_quality.get('functional_completeness', 0)}\")\n",
    "        print(f\"      Non-functional Coverage: {match_quality.get('non_functional_coverage', 0)}\")\n",
    "        \n",
    "        # Final results\n",
    "        print(f\"\\n   Final Results:\")\n",
    "        print(f\"      Final Score: {match.get('final_score', 0)}\")\n",
    "        print(f\"      Is Traceable: {'YES' if match.get('is_traceable', False) else 'NO'}\")\n",
    "        print(f\"      Meta-Judge Reasoning: {match.get('meta_judge_reasoning', 'No meta-judge reasoning provided')[:200]}...\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    if len(results) > 3:\n",
    "        print(f\"\\n... and {len(results) - 3} more matches\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    traceable_count = sum(1 for match in results if match.get('is_traceable', False))\n",
    "    avg_final_score = sum(match.get('final_score', 0) for match in results) / len(results) if results else 0\n",
    "    \n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"   Total matches: {len(results)}\")\n",
    "    print(f\"   Traceable matches: {traceable_count} ({traceable_count/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Average final score: {avg_final_score:.2f}\")\n",
    "\n",
    "# Configure logging for processing\n",
    "configure_logging_for_processing()\n",
    "\n",
    "# Execute the main requirements processing\n",
    "print(f\"\\nStarting Main Requirements Processing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    results = await process_requirements()\n",
    "    \n",
    "    # Display detailed results\n",
    "    display_detailed_results(results)\n",
    "    \n",
    "    print(f\"\\nRequirements processing completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during requirements processing: {str(e)}\")\n",
    "    logging.getLogger().error(\"Requirements processing failed\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Store Results in Neo4j Database\n",
    "# Purpose: Store the processed requirement matches and judgments in Neo4j for persistence\n",
    "# Dependencies: results from Cell 4, neo4j_client from Cell 2, CONFIG from Cell 0\n",
    "# Breadcrumbs: Results Processing -> Data Storage -> Neo4j Persistence\n",
    "\n",
    "async def store_results_in_neo4j(results):\n",
    "    \"\"\"\n",
    "    Store requirement matching results in Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        results: List of requirement matches with scores and reasoning\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"WARNING: No results to store\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nStoring Results in Neo4j Database\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {CONFIG['MODEL_NAME']}\")\n",
    "    print(f\"Results to store: {len(results)}\")\n",
    "    \n",
    "    storage_logger = logging.getLogger(\"neo4j_storage\")\n",
    "    successful_stores = 0\n",
    "    failed_stores = 0\n",
    "    \n",
    "    for i, match in enumerate(results, 1):\n",
    "        try:\n",
    "            await neo4j_client.store_judge_results(\n",
    "                source_id=match['source_id'],\n",
    "                target_id=match['target_id'],\n",
    "                model_name=CONFIG['MODEL_NAME'],\n",
    "                judgment_results=match\n",
    "            )\n",
    "            \n",
    "            storage_logger.info(f\"Stored match {match['source_id']}->{match['target_id']}\")\n",
    "            successful_stores += 1\n",
    "            \n",
    "            # Progress indicator for large datasets\n",
    "            if i % 10 == 0 or i == len(results):\n",
    "                print(f\"   Progress: {i}/{len(results)} matches stored\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            storage_logger.error(f\"Failed to store match {match['source_id']}->{match['target_id']}: {str(e)}\")\n",
    "            failed_stores += 1\n",
    "            continue\n",
    "    \n",
    "    # Storage summary\n",
    "    print(f\"\\nStorage Summary:\")\n",
    "    print(f\"   Successfully stored: {successful_stores}\")\n",
    "    print(f\"   Failed to store: {failed_stores}\")\n",
    "    print(f\"   Success rate: {successful_stores/(successful_stores+failed_stores)*100:.1f}%\")\n",
    "    \n",
    "    if failed_stores > 0:\n",
    "        print(f\"WARNING: Some results failed to store. Check logs for details.\")\n",
    "    else:\n",
    "        print(f\"All results stored successfully!\")\n",
    "\n",
    "# Execute storage operation\n",
    "try:\n",
    "    await store_results_in_neo4j(results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during storage operation: {str(e)}\")\n",
    "    logging.getLogger().error(\"Failed to store results in Neo4j\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Test Similarity Searches (Optional)\n",
    "# Purpose: Test vector similarity searches to verify Redis indices and embeddings\n",
    "# Dependencies: workflow indices from Cell 2, numpy and json from Cell 0\n",
    "# Breadcrumbs: Data Storage -> Similarity Testing -> Index Verification\n",
    "\n",
    "async def test_similarity_searches():\n",
    "    \"\"\"\n",
    "    Test both source-to-source and source-to-target similarity searches for verification\n",
    "    \n",
    "    This function helps verify that:\n",
    "    1. Redis indices are working correctly\n",
    "    2. Embeddings are stored properly\n",
    "    3. Vector similarity searches return expected results\n",
    "    \"\"\"\n",
    "    similarity_logger = logging.getLogger(\"similarity_testing\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nTesting Similarity Searches\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get all keys from Redis\n",
    "        all_keys = await redis_client.client.keys(\"*\")\n",
    "        print(f\"Total keys in Redis: {len(all_keys)}\")\n",
    "        \n",
    "        # Count requirements by type\n",
    "        src_keys = [k for k in all_keys if k.startswith(b'src_req:')]\n",
    "        tgt_keys = [k for k in all_keys if k.startswith(b'tgt_req:')]\n",
    "        \n",
    "        print(f\"Data Overview:\")\n",
    "        print(f\"   Source Requirements: {len(src_keys)}\")\n",
    "        print(f\"   Target Requirements: {len(tgt_keys)}\")\n",
    "        \n",
    "        if not src_keys:\n",
    "            print(\"WARNING: No source requirements found in Redis\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nTesting with first source requirement...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Test with the first source requirement only (for demonstration)\n",
    "        src_key = src_keys[0]\n",
    "        \n",
    "        try:\n",
    "            # Get source requirement details\n",
    "            src_content = await redis_client.client.hget(src_key, \"text\")\n",
    "            src_embedding = await redis_client.client.hget(src_key, \"embedding\")\n",
    "            src_metadata = await redis_client.client.hget(src_key, \"metadata\")\n",
    "            \n",
    "            if src_content and src_embedding and src_metadata:\n",
    "                print(f\"Analyzing: {src_key.decode()}\")\n",
    "                print(f\"Content preview: {src_content.decode()[:150]}...\")\n",
    "                \n",
    "                # Convert binary embedding to numpy array\n",
    "                src_embedding_vector = np.frombuffer(src_embedding, dtype=np.float32)\n",
    "                print(f\"Embedding dimension: {len(src_embedding_vector)}\")\n",
    "                \n",
    "                # Test source-to-source similarity\n",
    "                print(f\"\\nTesting Source-to-Source Similarity:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                src_query = VectorQuery(\n",
    "                    vector=src_embedding_vector.tolist(),\n",
    "                    vector_field_name=\"embedding\",\n",
    "                    return_fields=[\"text\", \"metadata\", \"vector_distance\"],\n",
    "                    num_results=5  # Limit to top 5 for demonstration\n",
    "                )\n",
    "                \n",
    "                src_results = await workflow.src_index.query(src_query)\n",
    "                \n",
    "                if not src_results:\n",
    "                    print(\"ERROR: No source-to-source results found\")\n",
    "                    index_info = await workflow.src_index.info()\n",
    "                    print(f\"Source index info: {index_info}\")\n",
    "                else:\n",
    "                    print(f\"Found {len(src_results)} source matches:\")\n",
    "                    for rank, result in enumerate(src_results[:3], 1):  # Show top 3\n",
    "                        try:\n",
    "                            metadata = json.loads(result.get(\"metadata\", \"{}\"))\n",
    "                            vector_distance = float(result.get(\"vector_distance\", 0.0))\n",
    "                            similarity_score = 1 - vector_distance\n",
    "                            print(f\"   {rank}. ID: {metadata.get('id')} | Score: {similarity_score:.4f}\")\n",
    "                        except Exception as e:\n",
    "                            similarity_logger.error(f\"Error processing source result: {str(e)}\")\n",
    "                \n",
    "                # Test source-to-target similarity\n",
    "                print(f\"\\nTesting Source-to-Target Similarity:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                tgt_query = VectorQuery(\n",
    "                    vector=src_embedding_vector.tolist(),\n",
    "                    vector_field_name=\"embedding\",\n",
    "                    return_fields=[\"text\", \"metadata\", \"vector_distance\"],\n",
    "                    num_results=5  # Limit to top 5 for demonstration\n",
    "                )\n",
    "                \n",
    "                tgt_results = await workflow.tgt_index.query(tgt_query)\n",
    "                \n",
    "                if not tgt_results:\n",
    "                    print(\"ERROR: No source-to-target results found\")\n",
    "                    index_info = await workflow.tgt_index.info()\n",
    "                    print(f\"Target index info: {index_info}\")\n",
    "                else:\n",
    "                    print(f\"Found {len(tgt_results)} target matches:\")\n",
    "                    for rank, result in enumerate(tgt_results[:3], 1):  # Show top 3\n",
    "                        try:\n",
    "                            metadata = json.loads(result.get(\"metadata\", \"{}\"))\n",
    "                            vector_distance = float(result.get(\"vector_distance\", 0.0))\n",
    "                            similarity_score = 1 - vector_distance\n",
    "                            print(f\"   {rank}. ID: {metadata.get('id')} | Score: {similarity_score:.4f}\")\n",
    "                        except Exception as e:\n",
    "                            similarity_logger.error(f\"Error processing target result: {str(e)}\")\n",
    "                \n",
    "                print(f\"\\nSimilarity testing completed successfully!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"ERROR: Missing data for source requirement: {src_key.decode()}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            similarity_logger.error(f\"Error processing requirement {src_key.decode()}: {str(e)}\", exc_info=True)\n",
    "            print(f\"ERROR processing requirement: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        similarity_logger.error(f\"Error in similarity testing: {str(e)}\", exc_info=True)\n",
    "        print(f\"Error in similarity testing: {str(e)}\")\n",
    "\n",
    "# Optionally run similarity tests (useful for debugging)\n",
    "run_similarity_tests = input(\"Run similarity tests for verification? (y/N): \").lower().startswith('y')\n",
    "\n",
    "if run_similarity_tests:\n",
    "    print(\"Starting similarity search tests...\")\n",
    "    await test_similarity_searches()\n",
    "else:\n",
    "    print(\"Skipping similarity tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Cleanup Connections and Finalize\n",
    "# Purpose: Properly close all database connections and finalize the workflow\n",
    "# Dependencies: Clients from Cell 2\n",
    "# Breadcrumbs: Processing Complete -> Connection Cleanup -> Finalization\n",
    "\n",
    "async def cleanup_connections():\n",
    "    \"\"\"\n",
    "    Properly close all database connections and clean up resources\n",
    "    \"\"\"\n",
    "    cleanup_logger = logging.getLogger(\"cleanup\")\n",
    "    cleanup_errors = []\n",
    "    \n",
    "    print(f\"\\nCleaning Up Connections\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Close Neo4j connection\n",
    "    try:\n",
    "        await neo4j_client.close()\n",
    "        cleanup_logger.info(\"Neo4j connection closed successfully\")\n",
    "        print(\"Neo4j connection closed successfully\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Neo4j connection cleanup failed: {str(e)}\"\n",
    "        cleanup_logger.error(f\"{error_msg}\")\n",
    "        print(f\"Error closing Neo4j connection: {str(e)}\")\n",
    "        cleanup_errors.append(error_msg)\n",
    "\n",
    "    # Close Redis connection\n",
    "    try:\n",
    "        await redis_client.client.aclose()\n",
    "        cleanup_logger.info(\"Redis connection closed successfully\")\n",
    "        print(\"Redis connection closed successfully\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Redis connection cleanup failed: {str(e)}\"\n",
    "        cleanup_logger.error(f\"{error_msg}\")\n",
    "        print(f\"Error closing Redis connection: {str(e)}\")\n",
    "        cleanup_errors.append(error_msg)\n",
    "    \n",
    "    # Final status\n",
    "    if cleanup_errors:\n",
    "        print(f\"\\nCleanup completed with {len(cleanup_errors)} errors\")\n",
    "        for error in cleanup_errors:\n",
    "            print(f\"   • {error}\")\n",
    "    else:\n",
    "        print(f\"\\nAll connections cleaned up successfully\")\n",
    "    \n",
    "    return len(cleanup_errors) == 0\n",
    "\n",
    "# Execute cleanup\n",
    "cleanup_success = await cleanup_connections()\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nRequirements Judging Workflow Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model Used: {CONFIG['MODEL_NAME']}\")\n",
    "print(f\"Project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "print(f\"Test Mode: {CONFIG['TEST_MODE']}\")\n",
    "print(f\"Results Processed: {len(results) if 'results' in locals() else 'N/A'}\")\n",
    "print(f\"Log File: {CONFIG['LOG_FILENAME']}\")\n",
    "print(f\"Cleanup Status: {'Success' if cleanup_success else 'With Warnings'}\")\n",
    "\n",
    "# Log final completion\n",
    "completion_logger = logging.getLogger(\"completion\")\n",
    "completion_logger.info(f\"Requirements judging workflow completed successfully\")\n",
    "completion_logger.info(f\"Model: {CONFIG['MODEL_NAME']}, Results: {len(results) if 'results' in locals() else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   • Check the log file for detailed processing information\")\n",
    "print(f\"   • Review stored results in Neo4j database\")\n",
    "print(f\"   • Analyze traceability metrics using the results\")\n",
    "print(f\"\\nWorkflow completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Analyze Unique Source Requirements\n",
    "# Purpose: Count unique source requirements that have matches\n",
    "# Dependencies: results from Cell 4\n",
    "# Breadcrumbs: Results Analysis -> Source Coverage Analysis\n",
    "\n",
    "unique_sources = len(set(match['source_id'] for match in results)) if 'results' in locals() else 0\n",
    "print(f\"Unique source requirements with matches: {unique_sources}\")\n",
    "unique_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Analyze Unique Target Requirements\n",
    "# Purpose: Count unique target requirements that have matches\n",
    "# Dependencies: results from Cell 4\n",
    "# Breadcrumbs: Results Analysis -> Target Coverage Analysis\n",
    "\n",
    "unique_targets = len(set(match['target_id'] for match in results)) if 'results' in locals() else 0\n",
    "print(f\"Unique target requirements with matches: {unique_targets}\")\n",
    "unique_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Total Results Count\n",
    "# Purpose: Display total number of requirement matches found\n",
    "# Dependencies: results from Cell 4\n",
    "# Breadcrumbs: Results Analysis -> Total Count\n",
    "\n",
    "total_results = len(results) if 'results' in locals() else 0\n",
    "print(f\"Total requirement matches: {total_results}\")\n",
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Display Complete Results\n",
    "# Purpose: Show the complete results dataset for detailed examination\n",
    "# Dependencies: results from Cell 4\n",
    "# Breadcrumbs: Results Analysis -> Complete Dataset View\n",
    "\n",
    "if 'results' in locals():\n",
    "    print(f\"Complete Results Dataset ({len(results)} matches):\")\n",
    "    print(\"=\" * 60)\n",
    "    if results:\n",
    "        print(\"Results available for examination\")\n",
    "        print(f\"Sample keys: {list(results[0].keys()) if results else 'N/A'}\")\n",
    "    else:\n",
    "        print(\"WARNING: Results list is empty\")\n",
    "    results\n",
    "else:\n",
    "    print(\"ERROR: Results variable not found. Please run Cell 4 first.\")\n",
    "    None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

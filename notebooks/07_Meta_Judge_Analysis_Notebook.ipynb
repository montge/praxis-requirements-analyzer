{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Meta Judge Requirements Traceability Analysis\n",
    "**Statistical evaluation and comparison of LLM meta judge scoring methods with confidence intervals, threshold optimization, performance metrics, and Neo4j integration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings\n",
    "# Dependencies: pandas, numpy, neo4j, sklearn, matplotlib, seaborn, scipy, statsmodels\n",
    "# Breadcrumbs: Setup -> Imports\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score,\n",
    "    cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Statistical testing imports\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare, kruskal, mannwhitneyu\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to initialize environment variables and configuration\n",
    "def initialize_environment():\n",
    "    \"\"\"Initialize environment variables and configuration settings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration dictionary with all settings\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Neo4j credentials from environment variables\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        \n",
    "        # Get current model from environment\n",
    "        'CURRENT_MODEL_KEY': os.getenv('CURRENT_MODEL'),\n",
    "        'CURRENT_MODEL': os.getenv(os.getenv('CURRENT_MODEL')),\n",
    "        \n",
    "        # Get optimization metric from environment\n",
    "        'OPTIMIZATION_METRIC': os.getenv('OPTIMIZATION_METRIC', 'F2').upper(),\n",
    "        \n",
    "        # Check if visualizations should be shown\n",
    "        'SHOW_VISUALIZATION': os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true',\n",
    "        \n",
    "        # Set minimum traceability threshold\n",
    "        'MIN_TRACEABILITY_THRESHOLD': int(os.getenv('MIN_TRACEABILITY_THRESHOLD', '3'))\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Only execute this code when run directly (not when imported)\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize environment\n",
    "    config = initialize_environment()\n",
    "    \n",
    "    # Extract variables from config to global namespace for notebook use\n",
    "    NEO4J_URI = config['NEO4J_URI']\n",
    "    NEO4J_USER = config['NEO4J_USER']\n",
    "    NEO4J_PASSWORD = config['NEO4J_PASSWORD']\n",
    "    NEO4J_PROJECT_NAME = config['NEO4J_PROJECT_NAME']\n",
    "    CURRENT_MODEL_KEY = config['CURRENT_MODEL_KEY']\n",
    "    CURRENT_MODEL = config['CURRENT_MODEL']\n",
    "    OPTIMIZATION_METRIC = config['OPTIMIZATION_METRIC']\n",
    "    SHOW_VISUALIZATION = config['SHOW_VISUALIZATION']\n",
    "    MIN_TRACEABILITY_THRESHOLD = config['MIN_TRACEABILITY_THRESHOLD']\n",
    "    \n",
    "    # Log configuration\n",
    "    logger.info(f\"Using {OPTIMIZATION_METRIC} score for threshold application in traceability analysis\")\n",
    "    logger.info(f\"Visualization display is set to: {SHOW_VISUALIZATION}\")\n",
    "    logger.info(f\"Current model: {CURRENT_MODEL}\")\n",
    "    logger.info(f\"Project name: {NEO4J_PROJECT_NAME}\")\n",
    "    logger.info(f\"Minimum traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "    \n",
    "    # Print configuration summary\n",
    "    print(f\"Visualization setting: {'Enabled' if SHOW_VISUALIZATION else 'Disabled'}\")\n",
    "    print(f\"Optimization metric: {OPTIMIZATION_METRIC}\")\n",
    "    print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Model: {CURRENT_MODEL}\")\n",
    "    print(f\"Minimum traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Connection Setup\n",
    "# Purpose: Create connection to Neo4j database containing traceability data\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection\n",
    "\n",
    "def create_neo4j_driver(uri=None, user=None, password=None):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance\n",
    "    \n",
    "    Parameters:\n",
    "        uri (str, optional): Neo4j connection URI. If None, uses NEO4J_URI from environment.\n",
    "        user (str, optional): Neo4j username. If None, uses NEO4J_USER from environment.\n",
    "        password (str, optional): Neo4j password. If None, uses NEO4J_PASSWORD from environment.\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, try to get them from globals (when run as main)\n",
    "        # or load from environment (when imported)\n",
    "        if uri is None:\n",
    "            if 'NEO4J_URI' in globals():\n",
    "                uri = NEO4J_URI\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                uri = config['NEO4J_URI']\n",
    "                \n",
    "        if user is None:\n",
    "            if 'NEO4J_USER' in globals():\n",
    "                user = NEO4J_USER\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment() if 'config' not in locals() else config\n",
    "                user = config['NEO4J_USER']\n",
    "                \n",
    "        if password is None:\n",
    "            if 'NEO4J_PASSWORD' in globals():\n",
    "                password = NEO4J_PASSWORD\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment() if 'config' not in locals() else config\n",
    "                password = config['NEO4J_PASSWORD']\n",
    "        \n",
    "        driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Only create the driver when running the notebook directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Create Neo4j driver\n",
    "    driver = create_neo4j_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Query Meta Judge Links\n",
    "# Purpose: Retrieve LLM_RESULT_META_JUDGE links from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Acquisition -> Meta Judge Links\n",
    "\n",
    "def query_meta_judge_links(driver, project_name=None, model=None):\n",
    "    \"\"\"\n",
    "    Query LLM_RESULT_META_JUDGE links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        model (str, optional): Model name to query. If None, uses CURRENT_MODEL from environment.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing meta judge links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, try to get them from globals or environment\n",
    "        if project_name is None:\n",
    "            if 'NEO4J_PROJECT_NAME' in globals():\n",
    "                project_name = NEO4J_PROJECT_NAME\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                project_name = config['NEO4J_PROJECT_NAME']\n",
    "                \n",
    "        if model is None:\n",
    "            if 'CURRENT_MODEL' in globals():\n",
    "                model = CURRENT_MODEL\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment() if 'config' not in locals() else config\n",
    "                model = config['CURRENT_MODEL']\n",
    "        \n",
    "        # Query for meta-judge links\n",
    "        meta_judge_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_META_JUDGE]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' AND r.model = $model\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_traceable as is_traceable,\n",
    "            r.judge_score as judge_score,\n",
    "            r.semantic_alignment as semantic_alignment,\n",
    "            r.non_functional_coverage as non_functional_coverage,\n",
    "            r.final_score as final_score,\n",
    "            r.actor_score as actor_score,\n",
    "            r.functional_completeness as functional_completeness,\n",
    "            r.model as model\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                # Execute the query with project name and model parameters\n",
    "                results = session.run(\n",
    "                    meta_judge_query, \n",
    "                    project_name=project_name,\n",
    "                    model=model\n",
    "                ).data()\n",
    "                \n",
    "                if results:\n",
    "                    logger.info(f\"Retrieved {len(results)} meta judge links\")\n",
    "                    meta_judge_df = pd.DataFrame(results)\n",
    "                    \n",
    "                    # Convert boolean columns to boolean type if they exist as strings\n",
    "                    if 'is_traceable' in meta_judge_df.columns:\n",
    "                        if meta_judge_df['is_traceable'].dtype == 'object':\n",
    "                            meta_judge_df['is_traceable'] = meta_judge_df['is_traceable'].map(\n",
    "                                lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                            )\n",
    "                    \n",
    "                    # Convert numeric columns to float\n",
    "                    numeric_columns = [\n",
    "                        'judge_score', 'semantic_alignment', 'non_functional_coverage',\n",
    "                        'final_score', 'actor_score', 'functional_completeness'\n",
    "                    ]\n",
    "                    \n",
    "                    for col in numeric_columns:\n",
    "                        if col in meta_judge_df.columns:\n",
    "                            meta_judge_df[col] = pd.to_numeric(meta_judge_df[col], errors='coerce')\n",
    "                    \n",
    "                    return meta_judge_df\n",
    "                else:\n",
    "                    logger.warning(f\"No meta judge links found for project: {project_name} and model: {model}\")\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error executing meta judge query: {str(e)}\")\n",
    "                logger.error(\"Exception details:\", exc_info=True)\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying meta judge links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Only execute this code when running the notebook directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the query and get results\n",
    "    meta_judge_df = query_meta_judge_links(driver)\n",
    "\n",
    "    # Display information about the retrieved data\n",
    "    if not meta_judge_df.empty:\n",
    "        print(\"\\nMeta Judge Links for Project:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"=\" * 80)\n",
    "        display(meta_judge_df.head())\n",
    "        \n",
    "        # Count unique source and target requirements\n",
    "        unique_sources = meta_judge_df['source_id'].nunique()\n",
    "        unique_targets = meta_judge_df['target_id'].nunique()\n",
    "        print(f\"\\nMeta Judge Dataset Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total meta judge links: {len(meta_judge_df)}\")\n",
    "        print(f\"Unique source requirements: {unique_sources}\")\n",
    "        print(f\"Unique target requirements: {unique_targets}\")\n",
    "        print(f\"Link density: {len(meta_judge_df) / (unique_sources * unique_targets):.4f}\")\n",
    "        \n",
    "        # Display distribution of is_traceable\n",
    "        if 'is_traceable' in meta_judge_df.columns:\n",
    "            traceable_count = meta_judge_df['is_traceable'].sum()\n",
    "            print(f\"\\nTraceability Distribution:\")\n",
    "            print(f\"Traceable links: {traceable_count} ({traceable_count/len(meta_judge_df)*100:.2f}%)\")\n",
    "            print(f\"Non-traceable links: {len(meta_judge_df) - traceable_count} ({(len(meta_judge_df) - traceable_count)/len(meta_judge_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Display score statistics\n",
    "        score_columns = ['judge_score', 'semantic_alignment', 'non_functional_coverage', \n",
    "                         'final_score', 'actor_score', 'functional_completeness']\n",
    "        \n",
    "        print(\"\\nScore Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        for col in score_columns:\n",
    "            if col in meta_judge_df.columns:\n",
    "                print(f\"{col.replace('_', ' ').title()}:\")\n",
    "                stats = meta_judge_df[col].describe()\n",
    "                print(f\"  Mean: {stats['mean']:.2f}\")\n",
    "                print(f\"  Median: {stats['50%']:.2f}\")\n",
    "                print(f\"  Min: {stats['min']:.2f}\")\n",
    "                print(f\"  Max: {stats['max']:.2f}\")\n",
    "                print(f\"  StdDev: {stats['std']:.2f}\")\n",
    "                print()\n",
    "    else:\n",
    "        print(\"\\nNo meta judge links found. Please check that:\")\n",
    "        print(\"  - The project name is correct\")\n",
    "        print(\"  - The model name matches what's in the database\")\n",
    "        print(\"  - LLM_RESULT_META_JUDGE relationships exist in the database\")\n",
    "        print(\"\\nProject:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"Model:\", CURRENT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings\n",
    "# Dependencies: pandas, numpy, neo4j, sklearn, matplotlib, seaborn, scipy, statsmodels\n",
    "# Breadcrumbs: Setup -> Imports\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score,\n",
    "    cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Statistical testing imports\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare, kruskal, mannwhitneyu\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to initialize environment variables and configuration\n",
    "def initialize_environment():\n",
    "    \"\"\"Initialize environment variables and configuration settings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration dictionary with all settings\n",
    "    \"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Neo4j credentials from environment variables\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        \n",
    "        # Get current model from environment\n",
    "        'CURRENT_MODEL_KEY': os.getenv('CURRENT_MODEL'),\n",
    "        'CURRENT_MODEL': os.getenv(os.getenv('CURRENT_MODEL')),\n",
    "        \n",
    "        # Get optimization metric from environment\n",
    "        'OPTIMIZATION_METRIC': os.getenv('OPTIMIZATION_METRIC', 'F2').upper(),\n",
    "        \n",
    "        # Check if visualizations should be shown\n",
    "        'SHOW_VISUALIZATION': os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true',\n",
    "        \n",
    "        # Set minimum traceability threshold\n",
    "        'MIN_TRACEABILITY_THRESHOLD': int(os.getenv('MIN_TRACEABILITY_THRESHOLD', '3'))\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Only execute this code when run directly (not when imported)\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize environment\n",
    "    config = initialize_environment()\n",
    "    \n",
    "    # Extract variables from config to global namespace for notebook use\n",
    "    NEO4J_URI = config['NEO4J_URI']\n",
    "    NEO4J_USER = config['NEO4J_USER']\n",
    "    NEO4J_PASSWORD = config['NEO4J_PASSWORD']\n",
    "    NEO4J_PROJECT_NAME = config['NEO4J_PROJECT_NAME']\n",
    "    CURRENT_MODEL_KEY = config['CURRENT_MODEL_KEY']\n",
    "    CURRENT_MODEL = config['CURRENT_MODEL']\n",
    "    OPTIMIZATION_METRIC = config['OPTIMIZATION_METRIC']\n",
    "    SHOW_VISUALIZATION = config['SHOW_VISUALIZATION']\n",
    "    MIN_TRACEABILITY_THRESHOLD = config['MIN_TRACEABILITY_THRESHOLD']\n",
    "    \n",
    "    # Log configuration\n",
    "    logger.info(f\"Using {OPTIMIZATION_METRIC} score for threshold application in traceability analysis\")\n",
    "    logger.info(f\"Visualization display is set to: {SHOW_VISUALIZATION}\")\n",
    "    logger.info(f\"Current model: {CURRENT_MODEL}\")\n",
    "    logger.info(f\"Project name: {NEO4J_PROJECT_NAME}\")\n",
    "    logger.info(f\"Minimum traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")\n",
    "    \n",
    "    # Print configuration summary\n",
    "    print(f\"Visualization setting: {'Enabled' if SHOW_VISUALIZATION else 'Disabled'}\")\n",
    "    print(f\"Optimization metric: {OPTIMIZATION_METRIC}\")\n",
    "    print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Model: {CURRENT_MODEL}\")\n",
    "    print(f\"Minimum traceability threshold: {MIN_TRACEABILITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query LLM Result Links\n",
    "# Purpose: Retrieve LLM_RESULT links from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Acquisition -> LLM Result Links\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "def query_llm_result_links(driver, project_name=None, model=None):\n",
    "    \"\"\"\n",
    "    Query LLM_RESULT links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "        model (str, optional): Model name to query. If None, uses CURRENT_MODEL from environment.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing LLM result links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If parameters aren't provided, try to get them from globals or environment\n",
    "        if project_name is None:\n",
    "            if 'NEO4J_PROJECT_NAME' in globals():\n",
    "                project_name = NEO4J_PROJECT_NAME\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                project_name = config['NEO4J_PROJECT_NAME']\n",
    "                \n",
    "        if model is None:\n",
    "            if 'CURRENT_MODEL' in globals():\n",
    "                model = CURRENT_MODEL\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment() if 'config' not in locals() else config\n",
    "                model = config['CURRENT_MODEL']\n",
    "        \n",
    "        # Query for LLM_RESULT links\n",
    "        llm_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' AND r.model = $model\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_associated as is_associated,\n",
    "            r.association_probability as association_probability,\n",
    "            r.model as model\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                # Execute the query with project name and model parameters\n",
    "                results = session.run(\n",
    "                    llm_query, \n",
    "                    project_name=project_name,\n",
    "                    model=model\n",
    "                ).data()\n",
    "                \n",
    "                if results:\n",
    "                    logger.info(f\"Retrieved {len(results)} LLM result links\")\n",
    "                    llm_result_df = pd.DataFrame(results)\n",
    "                    \n",
    "                    # Convert boolean columns to boolean type if they exist as strings\n",
    "                    if 'is_associated' in llm_result_df.columns:\n",
    "                        if llm_result_df['is_associated'].dtype == 'object':\n",
    "                            llm_result_df['is_associated'] = llm_result_df['is_associated'].map(\n",
    "                                lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                            )\n",
    "                    \n",
    "                    # Convert association_probability to float\n",
    "                    if 'association_probability' in llm_result_df.columns:\n",
    "                        llm_result_df['association_probability'] = pd.to_numeric(\n",
    "                            llm_result_df['association_probability'], errors='coerce'\n",
    "                        )\n",
    "                    \n",
    "                    return llm_result_df\n",
    "                else:\n",
    "                    logger.warning(f\"No LLM result links found for project: {project_name} and model: {model}\")\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error executing LLM result query: {str(e)}\")\n",
    "                logger.error(\"Exception details:\", exc_info=True)\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying LLM result links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Only execute this code when running the notebook directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the query and get results\n",
    "    llm_result_df = query_llm_result_links(driver)\n",
    "\n",
    "    # Display information about the retrieved data\n",
    "    if not llm_result_df.empty:\n",
    "        print(\"\\nLLM Result Links for Project:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"=\" * 80)\n",
    "        display(llm_result_df.head())\n",
    "        \n",
    "        # Count unique source and target requirements\n",
    "        unique_sources = llm_result_df['source_id'].nunique()\n",
    "        unique_targets = llm_result_df['target_id'].nunique()\n",
    "        print(f\"\\nLLM Result Dataset Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total LLM result links: {len(llm_result_df)}\")\n",
    "        print(f\"Unique source requirements: {unique_sources}\")\n",
    "        print(f\"Unique target requirements: {unique_targets}\")\n",
    "        print(f\"Link density: {len(llm_result_df) / (unique_sources * unique_targets):.4f}\")\n",
    "        \n",
    "        # Display distribution of is_associated\n",
    "        if 'is_associated' in llm_result_df.columns:\n",
    "            associated_count = llm_result_df['is_associated'].sum()\n",
    "            print(f\"\\nAssociation Distribution:\")\n",
    "            print(f\"Associated links: {associated_count} ({associated_count/len(llm_result_df)*100:.2f}%)\")\n",
    "            print(f\"Non-associated links: {len(llm_result_df) - associated_count} ({(len(llm_result_df) - associated_count)/len(llm_result_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Display probability statistics\n",
    "        if 'association_probability' in llm_result_df.columns:\n",
    "            print(\"\\nAssociation Probability Statistics:\")\n",
    "            print(\"-\" * 50)\n",
    "            stats = llm_result_df['association_probability'].describe()\n",
    "            print(f\"Mean: {stats['mean']:.2f}\")\n",
    "            print(f\"Median: {stats['50%']:.2f}\")\n",
    "            print(f\"Min: {stats['min']:.2f}\")\n",
    "            print(f\"Max: {stats['max']:.2f}\")\n",
    "            print(f\"StdDev: {stats['std']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo LLM result links found. Please check that:\")\n",
    "        print(\"  - The project name is correct\")\n",
    "        print(\"  - The model name matches what's in the database\")\n",
    "        print(\"  - LLM_RESULT relationships exist in the database\")\n",
    "        print(\"\\nProject:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"Model:\", CURRENT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Query Ground Truth Links\n",
    "# Purpose: Retrieve ground truth traceability links from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Acquisition -> Ground Truth\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "def query_ground_truth_links(driver, project_name=None):\n",
    "    \"\"\"\n",
    "    Query ground truth traceability links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name (str, optional): Project name to query. If None, uses NEO4J_PROJECT_NAME from environment.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If project_name isn't provided, try to get it from globals or environment\n",
    "        if project_name is None:\n",
    "            if 'NEO4J_PROJECT_NAME' in globals():\n",
    "                project_name = NEO4J_PROJECT_NAME\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                project_name = config['NEO4J_PROJECT_NAME']\n",
    "                \n",
    "        # Query for ground truth links\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            p.description as project_description,\n",
    "            d.id as document_id,\n",
    "            source.id as source_id,\n",
    "            source.type as source_type,\n",
    "            target.id as target_id,\n",
    "            target.type as target_type,\n",
    "            1 as ground_truth\n",
    "        ORDER BY source.id, target.id DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                # Execute the query with project name parameter\n",
    "                results = session.run(ground_truth_query, project_name=project_name).data()\n",
    "                \n",
    "                if results:\n",
    "                    logger.info(f\"Retrieved {len(results)} ground truth links using GROUND_TRUTH relationship\")\n",
    "                    df_ground_truth = pd.DataFrame(results)\n",
    "                    return df_ground_truth\n",
    "                else:\n",
    "                    logger.warning(f\"No ground truth links found for project: {project_name}\")\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error executing ground truth query: {str(e)}\")\n",
    "                logger.error(\"Exception details:\", exc_info=True)\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying ground truth links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Only execute the query and display results when running the notebook directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the query and get results\n",
    "    df_ground_truth = query_ground_truth_links(driver)\n",
    "\n",
    "    # Display information about the retrieved data\n",
    "    if not df_ground_truth.empty:\n",
    "        print(\"\\nGround Truth Links for Project:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"=\" * 80)\n",
    "        display(df_ground_truth.head())\n",
    "        \n",
    "        # Count source and target requirements\n",
    "        unique_sources = df_ground_truth['source_id'].nunique()\n",
    "        unique_targets = df_ground_truth['target_id'].nunique()\n",
    "        print(f\"\\nGround Truth Dataset Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total ground truth links: {len(df_ground_truth)}\")\n",
    "        print(f\"Unique source requirements: {unique_sources}\")\n",
    "        print(f\"Unique target requirements: {unique_targets}\")\n",
    "        print(f\"Link density: {len(df_ground_truth) / (unique_sources * unique_targets):.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo ground truth links found for\", NEO4J_PROJECT_NAME)\n",
    "        print(\"Continuing analysis without ground truth data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Create combined dataset for analysis\n",
    "# Purpose: Merge meta judge, LLM result, and ground truth data for analysis\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Data Preparation -> Combination\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "def create_combined_dataset(meta_judge_df=None, llm_result_df=None, df_ground_truth=None, min_traceability_threshold=None):\n",
    "    \"\"\"\n",
    "    Create a combined dataset with meta judge, LLM result, and ground truth information\n",
    "    \n",
    "    Parameters:\n",
    "        meta_judge_df (pd.DataFrame, optional): DataFrame with meta judge data\n",
    "        llm_result_df (pd.DataFrame, optional): DataFrame with LLM result data\n",
    "        df_ground_truth (pd.DataFrame, optional): DataFrame with ground truth data\n",
    "        min_traceability_threshold (int, optional): Threshold for traceability. If None, uses environment value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with all relevant information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if we have meta judge data\n",
    "        if meta_judge_df is None or meta_judge_df.empty:\n",
    "            if 'meta_judge_df' in globals() and not globals()['meta_judge_df'].empty:\n",
    "                meta_judge_df = globals()['meta_judge_df']\n",
    "            else:\n",
    "                logger.error(\"No meta judge data available to create combined dataset\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Get min_traceability_threshold if not provided\n",
    "        if min_traceability_threshold is None:\n",
    "            if 'MIN_TRACEABILITY_THRESHOLD' in globals():\n",
    "                min_traceability_threshold = globals()['MIN_TRACEABILITY_THRESHOLD']\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                min_traceability_threshold = config['MIN_TRACEABILITY_THRESHOLD']\n",
    "        \n",
    "        # Make sure we have ground truth data\n",
    "        if df_ground_truth is None or df_ground_truth.empty:\n",
    "            if 'df_ground_truth' in globals() and not globals()['df_ground_truth'].empty:\n",
    "                df_ground_truth = globals()['df_ground_truth']\n",
    "            else:\n",
    "                logger.warning(\"No ground truth data available for filtering relevant requirements\")\n",
    "                df_ground_truth = pd.DataFrame()\n",
    "        \n",
    "        # Filter for relevant source and target requirements if ground truth is available\n",
    "        if not df_ground_truth.empty:\n",
    "            # Extract unique source and target IDs with ground truth links\n",
    "            valid_source_ids = df_ground_truth['source_id'].unique()\n",
    "            valid_target_ids = df_ground_truth['target_id'].unique()\n",
    "            \n",
    "            # Filter meta_judge_df to only include valid source and target requirements\n",
    "            filtered_meta_judge_df = meta_judge_df[\n",
    "                (meta_judge_df['source_id'].isin(valid_source_ids)) & \n",
    "                (meta_judge_df['target_id'].isin(valid_target_ids))\n",
    "            ].copy()\n",
    "            \n",
    "            logger.info(f\"Filtered meta judge data from {len(meta_judge_df)} to {len(filtered_meta_judge_df)} rows\")\n",
    "            logger.info(f\"Using {len(valid_source_ids)} valid source requirements and {len(valid_target_ids)} valid target requirements\")\n",
    "            \n",
    "            # Use the filtered dataframe\n",
    "            combined_df = filtered_meta_judge_df\n",
    "        else:\n",
    "            # No ground truth available, use all meta judge data\n",
    "            combined_df = meta_judge_df.copy()\n",
    "            logger.warning(\"No ground truth data available for filtering. Using all meta judge data.\")\n",
    "            \n",
    "        # Add LLM result information if available\n",
    "        if llm_result_df is not None and not llm_result_df.empty:\n",
    "            # Create key for merging\n",
    "            combined_df['merge_key'] = combined_df['source_id'] + '_' + combined_df['target_id']\n",
    "            llm_temp = llm_result_df.copy()\n",
    "            llm_temp['merge_key'] = llm_temp['source_id'] + '_' + llm_temp['target_id']\n",
    "            \n",
    "            # Select columns to merge from LLM results\n",
    "            llm_cols = ['merge_key', 'is_associated', 'association_probability', 'explanation']\n",
    "            llm_cols = [c for c in llm_cols if c in llm_temp.columns]\n",
    "            \n",
    "            # Merge dataframes\n",
    "            combined_df = combined_df.merge(\n",
    "                llm_temp[llm_cols],\n",
    "                on='merge_key',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Remove merge key column\n",
    "            combined_df.drop('merge_key', axis=1, inplace=True)\n",
    "            \n",
    "            logger.info(f\"Added LLM result data to combined dataset\")\n",
    "        else:\n",
    "            if 'llm_result_df' in globals() and not globals()['llm_result_df'].empty:\n",
    "                # Use global if available\n",
    "                llm_temp = globals()['llm_result_df'].copy()\n",
    "                \n",
    "                # Create key for merging\n",
    "                combined_df['merge_key'] = combined_df['source_id'] + '_' + combined_df['target_id']\n",
    "                llm_temp['merge_key'] = llm_temp['source_id'] + '_' + llm_temp['target_id']\n",
    "                \n",
    "                # Select columns to merge from LLM results\n",
    "                llm_cols = ['merge_key', 'is_associated', 'association_probability', 'explanation']\n",
    "                llm_cols = [c for c in llm_cols if c in llm_temp.columns]\n",
    "                \n",
    "                # Merge dataframes\n",
    "                combined_df = combined_df.merge(\n",
    "                    llm_temp[llm_cols],\n",
    "                    on='merge_key',\n",
    "                    how='left'\n",
    "                )\n",
    "                \n",
    "                # Remove merge key column\n",
    "                combined_df.drop('merge_key', axis=1, inplace=True)\n",
    "                \n",
    "                logger.info(f\"Added LLM result data to combined dataset from global variable\")\n",
    "            else:\n",
    "                logger.warning(\"No LLM result data available for combined dataset\")\n",
    "            \n",
    "        # Add ground truth information if available\n",
    "        if not df_ground_truth.empty:\n",
    "            # Create a set of ground truth links for fast lookup\n",
    "            ground_truth_pairs = set(zip(df_ground_truth['source_id'], df_ground_truth['target_id']))\n",
    "            \n",
    "            # Add ground_truth_traceable column\n",
    "            combined_df['ground_truth_traceable'] = combined_df.apply(\n",
    "                lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Added ground truth data: {combined_df['ground_truth_traceable'].sum()} true links out of {len(combined_df)} total\")\n",
    "        else:\n",
    "            logger.warning(\"No ground truth data available for combined dataset\")\n",
    "        \n",
    "        # Add derived columns for analysis\n",
    "        if 'judge_score' in combined_df.columns and 'actor_score' in combined_df.columns:\n",
    "            # Original total_score: judge_score + actor_score\n",
    "            combined_df['total_score'] = combined_df['judge_score'] + combined_df['actor_score']\n",
    "            \n",
    "            # Add alternative total scores for comparison\n",
    "            if 'final_score' in combined_df.columns:\n",
    "                # Alternative 1: actor_score + final_score\n",
    "                combined_df['total_score_with_final'] = combined_df['actor_score'] + combined_df['final_score']\n",
    "                \n",
    "                # Alternative 2: actor_score + judge_score + final_score\n",
    "                combined_df['total_score_all'] = combined_df['actor_score'] + combined_df['judge_score'] + combined_df['final_score']\n",
    "            \n",
    "            # Create a new column for different threshold mechanisms\n",
    "            combined_df['is_traceable_threshold'] = combined_df['total_score'] >= min_traceability_threshold\n",
    "            \n",
    "            # Also create threshold columns for alternative total scores if they exist\n",
    "            if 'total_score_with_final' in combined_df.columns:\n",
    "                combined_df['is_traceable_threshold_with_final'] = combined_df['total_score_with_final'] >= min_traceability_threshold\n",
    "            \n",
    "            if 'total_score_all' in combined_df.columns:\n",
    "                combined_df['is_traceable_threshold_all'] = combined_df['total_score_all'] >= min_traceability_threshold * 1.5  # Adjust threshold for 3 scores\n",
    "            \n",
    "            logger.info(f\"Added derived total_score columns and is_traceable_threshold columns using threshold {min_traceability_threshold}\")\n",
    "            \n",
    "        return combined_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating combined dataset: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Model evaluation and threshold optimization with statistical analysis\n",
    "# Purpose: Evaluate meta judge models and find optimal thresholds with confidence intervals\n",
    "# Dependencies: pandas, numpy, sklearn.metrics, scipy.stats\n",
    "# Breadcrumbs: Analysis -> Evaluation -> Threshold Optimization -> Statistical Analysis\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def get_actual_model_name(model_key):\n",
    "    \"\"\"\n",
    "    Resolve the actual model name from environment variables\n",
    "    \n",
    "    If model_key is a reference to another environment variable,\n",
    "    resolve it to get the actual model name\n",
    "    \"\"\"\n",
    "    if model_key in os.environ:\n",
    "        return os.environ[model_key]\n",
    "    return model_key\n",
    "\n",
    "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_iterations=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval for a metric\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        metric_func: Function to calculate the metric\n",
    "        n_iterations: Number of bootstrap iterations\n",
    "        confidence_level: Confidence level (default 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains mean, std, ci_lower, ci_upper\n",
    "    \"\"\"\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Resample with replacement\n",
    "        indices = resample(range(len(y_true)), replace=True, n_samples=len(y_true))\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        \n",
    "        # Calculate metric\n",
    "        try:\n",
    "            score = metric_func(y_true_boot, y_pred_boot)\n",
    "            bootstrapped_scores.append(score)\n",
    "        except:\n",
    "            # Skip if metric calculation fails (e.g., no positive samples)\n",
    "            continue\n",
    "    \n",
    "    if not bootstrapped_scores:\n",
    "        return {'mean': 0, 'std': 0, 'ci_lower': 0, 'ci_upper': 0}\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower = np.percentile(bootstrapped_scores, (alpha/2) * 100)\n",
    "    upper = np.percentile(bootstrapped_scores, (1 - alpha/2) * 100)\n",
    "    mean = np.mean(bootstrapped_scores)\n",
    "    std = np.std(bootstrapped_scores)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'ci_lower': lower,\n",
    "        'ci_upper': upper\n",
    "    }\n",
    "\n",
    "def evaluate_model_thresholds(df, model_name, score_column='total_score', \n",
    "                             ground_truth_column='ground_truth_traceable', \n",
    "                             optimize_for='F2', calculate_ci=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance across different thresholds with confidence intervals\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing model predictions and ground truth\n",
    "        model_name: Name of the model to evaluate\n",
    "        score_column: Column containing score values\n",
    "        ground_truth_column: Column containing ground truth values\n",
    "        optimize_for: Metric to optimize for ('F1' or 'F2')\n",
    "        calculate_ci: Whether to calculate confidence intervals\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results with confidence intervals\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this model\n",
    "        model_df = df[df['model'] == model_name].copy()\n",
    "        \n",
    "        if model_df.empty:\n",
    "            print(f\"No data available for model: {model_name}\")\n",
    "            return {}\n",
    "            \n",
    "        if ground_truth_column not in model_df.columns:\n",
    "            print(f\"Ground truth column '{ground_truth_column}' not found for model: {model_name}\")\n",
    "            return {}\n",
    "        \n",
    "        # Get ground truth and scores\n",
    "        y_true = model_df[ground_truth_column].astype(int).values\n",
    "        \n",
    "        # Check for and handle None/NaN values in score column\n",
    "        if model_df[score_column].isna().any():\n",
    "            print(f\"Found NaN values in {score_column} for model {model_name}. Filling with 0.\")\n",
    "            model_df[score_column] = model_df[score_column].fillna(0)\n",
    "        \n",
    "        # Ensure scores are numeric\n",
    "        if model_df[score_column].dtype == object:\n",
    "            try:\n",
    "                model_df[score_column] = pd.to_numeric(model_df[score_column])\n",
    "                print(f\"Converted {score_column} to numeric for model {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {score_column} to numeric: {str(e)}\")\n",
    "                # Default to zeros if conversion fails\n",
    "                model_df[score_column] = 0\n",
    "        \n",
    "        scores = model_df[score_column].values\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"  - Total data points: {len(model_df)}\")\n",
    "        print(f\"  - Positive examples: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Negative examples: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Score range: {scores.min():.4f} to {scores.max():.4f}\")\n",
    "        \n",
    "        # If all ground truth values are the same, we can't calculate meaningful metrics\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            print(f\"Insufficient ground truth variety for model {model_name} - all values are {np.unique(y_true)[0]}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum())\n",
    "            }\n",
    "        \n",
    "        # Generate possible thresholds from the data\n",
    "        unique_scores = np.unique(scores)\n",
    "        # Add some intermediate thresholds to get a more fine-grained evaluation\n",
    "        thresholds = np.sort(np.concatenate([\n",
    "            unique_scores,\n",
    "            np.linspace(scores.min(), scores.max(), 20)\n",
    "        ]))\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Convert scores to binary predictions using this threshold\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            \n",
    "            # Only calculate if we have at least one prediction of each class\n",
    "            if np.unique(y_pred).size < 2:\n",
    "                continue\n",
    "                \n",
    "            # Confusion matrix components\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            # Handle division by zero\n",
    "            if tp + fp == 0:  # No positive predictions\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = tp / (tp + fp)\n",
    "                \n",
    "            if tp + fn == 0:  # No positive ground truth\n",
    "                rec = 0\n",
    "            else:\n",
    "                rec = tp / (tp + fn)\n",
    "            \n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "            \n",
    "            # Additional metrics\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity/True Negative Rate\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate/False Negative Rate\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'tnr': tnr,  # specificity\n",
    "                'fnr': fnr,  # miss rate\n",
    "                'f1_score': f1,\n",
    "                'f2_score': f2,\n",
    "                'mcc': mcc  # Matthews Correlation Coefficient\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if results_df.empty:\n",
    "            print(f\"No valid thresholds found for model {model_name}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "                'error': \"No valid thresholds found with current data\"\n",
    "            }\n",
    "        \n",
    "        # Find best threshold based on optimization metric\n",
    "        if optimize_for == 'F1':\n",
    "            best_idx = results_df['f1_score'].idxmax()\n",
    "            best_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_idx = results_df['f2_score'].idxmax()\n",
    "            best_metric = 'f2_score'\n",
    "            \n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        # Calculate confidence intervals for the best threshold if requested\n",
    "        confidence_intervals = {}\n",
    "        if calculate_ci:\n",
    "            best_threshold = best_result['threshold']\n",
    "            best_y_pred = (scores >= best_threshold).astype(int)\n",
    "            \n",
    "            print(f\"  Calculating confidence intervals...\")\n",
    "            \n",
    "            # Calculate CIs for key metrics\n",
    "            confidence_intervals['precision'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred, \n",
    "                lambda y_t, y_p: precision_score(y_t, y_p, zero_division=0)\n",
    "            )\n",
    "            \n",
    "            confidence_intervals['recall'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred,\n",
    "                lambda y_t, y_p: recall_score(y_t, y_p, zero_division=0)\n",
    "            )\n",
    "            \n",
    "            confidence_intervals['f1'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred,\n",
    "                lambda y_t, y_p: f1_score(y_t, y_p, zero_division=0)\n",
    "            )\n",
    "            \n",
    "            confidence_intervals['f2'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred,\n",
    "                lambda y_t, y_p: fbeta_score(y_t, y_p, beta=2, zero_division=0)\n",
    "            )\n",
    "            \n",
    "            confidence_intervals['accuracy'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred,\n",
    "                accuracy_score\n",
    "            )\n",
    "            \n",
    "            confidence_intervals['mcc'] = bootstrap_confidence_interval(\n",
    "                y_true, best_y_pred,\n",
    "                matthews_corrcoef\n",
    "            )\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df),\n",
    "            'ground_truth_positive': int(y_true.sum()),\n",
    "            'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "            'best_threshold': best_result['threshold'],\n",
    "            'best_precision': best_result['precision'],\n",
    "            'best_recall': best_result['recall'],\n",
    "            'best_accuracy': best_result['accuracy'],\n",
    "            'best_balanced_accuracy': best_result['balanced_accuracy'],\n",
    "            'best_f1': best_result['f1_score'],\n",
    "            'best_f2': best_result['f2_score'],\n",
    "            'best_tnr': best_result['tnr'],\n",
    "            'best_fnr': best_result['fnr'],\n",
    "            'best_mcc': best_result['mcc'],\n",
    "            'best_tp': best_result['tp'],\n",
    "            'best_fp': best_result['fp'],\n",
    "            'best_fn': best_result['fn'],\n",
    "            'best_tn': best_result['tn'],\n",
    "            'optimization_metric': optimize_for,\n",
    "            'threshold_results': results_df,\n",
    "            'confidence_intervals': confidence_intervals\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df) if 'model_df' in locals() else 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# This will be executed when the cell is run\n",
    "# Load environment variables for model name resolution\n",
    "load_dotenv()\n",
    "\n",
    "# Get the actual model name\n",
    "current_model_var = os.environ.get('CURRENT_MODEL', '')\n",
    "actual_model_name = os.environ.get(current_model_var, current_model_var)\n",
    "\n",
    "# Update the global variable if it exists\n",
    "if 'CURRENT_MODEL_KEY' in globals():\n",
    "    CURRENT_MODEL_KEY = actual_model_name\n",
    "\n",
    "# Initialize best_thresholds_df as an empty DataFrame (will be filled later)\n",
    "best_thresholds_df = pd.DataFrame()\n",
    "\n",
    "# First, make sure combined_df exists by creating it if necessary\n",
    "if 'combined_df' not in globals() or globals()['combined_df'] is None or globals()['combined_df'].empty:\n",
    "    print(\"Creating combined dataset from previous data...\")\n",
    "    try:\n",
    "        # Check if we have the required data frames from previous cells\n",
    "        if all(var in globals() for var in ['meta_judge_df', 'llm_result_df', 'df_ground_truth']):\n",
    "            # Create combined_df using the create_combined_dataset function from cell 5\n",
    "            if 'create_combined_dataset' in globals():\n",
    "                combined_df = create_combined_dataset(\n",
    "                    meta_judge_df=globals()['meta_judge_df'],\n",
    "                    llm_result_df=globals()['llm_result_df'],\n",
    "                    df_ground_truth=globals()['df_ground_truth']\n",
    "                )\n",
    "                print(f\"Successfully created combined dataset with {len(combined_df)} rows\")\n",
    "            else:\n",
    "                print(\"create_combined_dataset function not found. Please run cell 5 first.\")\n",
    "                combined_df = pd.DataFrame()\n",
    "        else:\n",
    "            print(\"Missing required data. Please run cells 0-5 first.\")\n",
    "            # Create empty DataFrame to prevent errors\n",
    "            combined_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating combined dataset: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Create empty DataFrame to prevent errors\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "# Only proceed with evaluation if we have data\n",
    "if not combined_df.empty and 'ground_truth_traceable' in combined_df.columns and 'model' in combined_df.columns:\n",
    "    # Get list of all models\n",
    "    all_models = combined_df['model'].unique()\n",
    "    \n",
    "    # Set OPTIMIZATION_METRIC if not already in globals\n",
    "    if 'OPTIMIZATION_METRIC' not in globals():\n",
    "        OPTIMIZATION_METRIC = 'F2'\n",
    "    else:\n",
    "        OPTIMIZATION_METRIC = globals()['OPTIMIZATION_METRIC']\n",
    "    \n",
    "    # Set SHOW_VISUALIZATION if not already in globals\n",
    "    if 'SHOW_VISUALIZATION' not in globals():\n",
    "        SHOW_VISUALIZATION = True\n",
    "    else:\n",
    "        SHOW_VISUALIZATION = globals()['SHOW_VISUALIZATION']\n",
    "    \n",
    "    # Evaluate each model\n",
    "    evaluation_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(all_models)} models using meta judge data\")\n",
    "    print(f\"Optimizing for {OPTIMIZATION_METRIC} score\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define score columns to evaluate - including the new score columns\n",
    "    score_columns_to_evaluate = [\n",
    "        'is_traceable',         # Boolean indicator\n",
    "        'actor_score',          # Individual score\n",
    "        'judge_score',          # Individual score\n",
    "        'final_score',          # Individual score\n",
    "        'total_score',          # judge_score + actor_score\n",
    "        'total_score_with_final',  # actor_score + final_score\n",
    "        'total_score_all'       # actor_score + judge_score + final_score\n",
    "    ]\n",
    "    \n",
    "    # Add a new total score column that combines all the requested metrics\n",
    "    try:\n",
    "        combined_df['total_combined_score'] = combined_df.apply(\n",
    "            lambda row: sum([\n",
    "                row.get('judge_score', 0) or 0,\n",
    "                row.get('semantic_alignment', 0) or 0, \n",
    "                row.get('non_functional_coverage', 0) or 0,\n",
    "                row.get('final_score', 0) or 0,\n",
    "                row.get('actor_score', 0) or 0,\n",
    "                row.get('functional_completeness', 0) or 0\n",
    "            ]), \n",
    "            axis=1\n",
    "        )\n",
    "        score_columns_to_evaluate.append('total_combined_score')\n",
    "        print(f\"Created new total_combined_score column combining all requested metrics\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating total_combined_score: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Filter to only columns that exist\n",
    "    score_columns_to_evaluate = [\n",
    "        col for col in score_columns_to_evaluate \n",
    "        if col in combined_df.columns and not combined_df[col].isna().all()\n",
    "    ]\n",
    "    \n",
    "    for model in all_models:\n",
    "        print(f\"Evaluating model: {model}\")\n",
    "        \n",
    "        # For each model, evaluate using different score columns\n",
    "        model_results = []\n",
    "        for score_column in score_columns_to_evaluate:\n",
    "            print(f\"  Evaluating using {score_column}:\")\n",
    "            result = evaluate_model_thresholds(\n",
    "                combined_df, \n",
    "                model, \n",
    "                score_column=score_column,\n",
    "                optimize_for=OPTIMIZATION_METRIC,\n",
    "                calculate_ci=True  # Enable confidence interval calculation\n",
    "            )\n",
    "            \n",
    "            if result and 'best_threshold' in result:\n",
    "                # Add score column to result\n",
    "                result['score_column'] = score_column\n",
    "                model_results.append(result)\n",
    "                \n",
    "                # Print key metrics with confidence intervals\n",
    "                print(f\"    - Best threshold: {result['best_threshold']:.3f}\")\n",
    "                \n",
    "                if 'confidence_intervals' in result and result['confidence_intervals']:\n",
    "                    ci = result['confidence_intervals']\n",
    "                    \n",
    "                    # Print metrics with CIs\n",
    "                    for metric in ['precision', 'recall', 'f1', 'f2']:\n",
    "                        if metric in ci:\n",
    "                            metric_ci = ci[metric]\n",
    "                            print(f\"    - {metric.capitalize()}: {result[f'best_{metric}']:.3f} \"\n",
    "                                  f\"(95% CI: [{metric_ci['ci_lower']:.3f}, {metric_ci['ci_upper']:.3f}])\")\n",
    "                    \n",
    "                    if 'mcc' in ci:\n",
    "                        mcc_ci = ci['mcc']\n",
    "                        print(f\"    - MCC: {result['best_mcc']:.3f} \"\n",
    "                              f\"(95% CI: [{mcc_ci['ci_lower']:.3f}, {mcc_ci['ci_upper']:.3f}])\")\n",
    "                else:\n",
    "                    # Print without CIs if not calculated\n",
    "                    print(f\"    - Precision: {result['best_precision']:.3f}\")\n",
    "                    print(f\"    - Recall: {result['best_recall']:.3f}\")\n",
    "                    print(f\"    - F1: {result['best_f1']:.3f}\")\n",
    "                    print(f\"    - F2: {result['best_f2']:.3f}\")\n",
    "                    print(f\"    - MCC: {result['best_mcc']:.3f}\")\n",
    "        \n",
    "        # Find best score column for this model based on optimization metric\n",
    "        if model_results:\n",
    "            # Sort by the chosen optimization metric\n",
    "            if OPTIMIZATION_METRIC == 'F1':\n",
    "                model_results.sort(key=lambda x: x['best_f1'], reverse=True)\n",
    "            else:  # F2\n",
    "                model_results.sort(key=lambda x: x['best_f2'], reverse=True)\n",
    "                \n",
    "            best_result = model_results[0]\n",
    "            print(f\"  Best performing score for {model}: {best_result['score_column']}\")\n",
    "            print(f\"    - {OPTIMIZATION_METRIC} Score: {best_result['best_f2' if OPTIMIZATION_METRIC == 'F2' else 'best_f1']:.3f}\")\n",
    "            \n",
    "            evaluation_results.extend(model_results)\n",
    "    \n",
    "    # Create DataFrame of best thresholds with all metrics\n",
    "    if evaluation_results:\n",
    "        best_thresholds_df = pd.DataFrame([\n",
    "            {\n",
    "                'model_name': r['model_name'],\n",
    "                'score_column': r['score_column'],\n",
    "                'best_threshold': r['best_threshold'],\n",
    "                'accuracy': r['best_accuracy'],\n",
    "                'balanced_accuracy': r['best_balanced_accuracy'],\n",
    "                'precision': r['best_precision'],\n",
    "                'recall': r['best_recall'],\n",
    "                'specificity': r['best_tnr'],\n",
    "                'miss_rate': r['best_fnr'],\n",
    "                'f1_score': r['best_f1'],\n",
    "                'f2_score': r['best_f2'],\n",
    "                'matthews_corr': r['best_mcc'],\n",
    "                'true_positives': r['best_tp'],\n",
    "                'false_positives': r['best_fp'],\n",
    "                'false_negatives': r['best_fn'],\n",
    "                'true_negatives': r['best_tn'],\n",
    "                'data_points': r['data_points'],\n",
    "                'ground_truth_positive': r['ground_truth_positive'],\n",
    "                'ground_truth_negative': r['ground_truth_negative'],\n",
    "                # Add confidence interval data\n",
    "                'precision_ci_lower': r['confidence_intervals']['precision']['ci_lower'] if 'confidence_intervals' in r and 'precision' in r['confidence_intervals'] else None,\n",
    "                'precision_ci_upper': r['confidence_intervals']['precision']['ci_upper'] if 'confidence_intervals' in r and 'precision' in r['confidence_intervals'] else None,\n",
    "                'recall_ci_lower': r['confidence_intervals']['recall']['ci_lower'] if 'confidence_intervals' in r and 'recall' in r['confidence_intervals'] else None,\n",
    "                'recall_ci_upper': r['confidence_intervals']['recall']['ci_upper'] if 'confidence_intervals' in r and 'recall' in r['confidence_intervals'] else None,\n",
    "                'f1_ci_lower': r['confidence_intervals']['f1']['ci_lower'] if 'confidence_intervals' in r and 'f1' in r['confidence_intervals'] else None,\n",
    "                'f1_ci_upper': r['confidence_intervals']['f1']['ci_upper'] if 'confidence_intervals' in r and 'f1' in r['confidence_intervals'] else None,\n",
    "                'f2_ci_lower': r['confidence_intervals']['f2']['ci_lower'] if 'confidence_intervals' in r and 'f2' in r['confidence_intervals'] else None,\n",
    "                'f2_ci_upper': r['confidence_intervals']['f2']['ci_upper'] if 'confidence_intervals' in r and 'f2' in r['confidence_intervals'] else None,\n",
    "            }\n",
    "            for r in evaluation_results if 'best_threshold' in r\n",
    "        ])\n",
    "        \n",
    "        # Check if we have any results\n",
    "        if not best_thresholds_df.empty:\n",
    "            # Sort by the appropriate metric\n",
    "            sort_col = 'f1_score' if OPTIMIZATION_METRIC == 'F1' else 'f2_score'\n",
    "            best_thresholds_df = best_thresholds_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            print(\"\\nBest Thresholds by Model and Score Column:\")\n",
    "            print(\"-\" * 80)\n",
    "            display(best_thresholds_df[['model_name', 'score_column', 'best_threshold', \n",
    "                                       'precision', 'recall', 'f1_score', 'f2_score', 'matthews_corr']])\n",
    "            \n",
    "            # Store best_thresholds_df in globals\n",
    "            globals()['best_thresholds_df'] = best_thresholds_df\n",
    "            \n",
    "            # Store evaluation_results for statistical comparisons\n",
    "            globals()['evaluation_results'] = evaluation_results\n",
    "            \n",
    "            # Create Metrics Comparison Heatmap like in Sentence Transformer notebook\n",
    "            print(\"\\nMetrics Comparison Heatmap:\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            # Select relevant metrics for comparison\n",
    "            metrics_to_compare = ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                                 'specificity', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "\n",
    "            # Filter to only include metrics that exist in the DataFrame\n",
    "            available_metrics = [metric for metric in metrics_to_compare if metric in best_thresholds_df.columns]\n",
    "\n",
    "            if len(available_metrics) > 0:\n",
    "                # Create a new dataframe with model+score_column as index and metrics as columns\n",
    "                best_thresholds_df['model_score'] = best_thresholds_df.apply(\n",
    "                    lambda row: f\"{row['model_name']} ({row['score_column']})\", axis=1)\n",
    "                \n",
    "                comparison_df = best_thresholds_df.set_index('model_score')[available_metrics]\n",
    "                \n",
    "                # Create heatmap\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.heatmap(comparison_df, annot=True, cmap='YlGnBu', fmt='.3f',\n",
    "                           linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "                plt.title(f'Project: {NEO4J_PROJECT_NAME} - Comparison of Models Across Metrics', fontsize=14)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"No metrics available for heatmap visualization.\")\n",
    "        else:\n",
    "            print(\"No valid threshold results found. Check that the data contains ground truth information.\")\n",
    "            # Initialize as empty DataFrame to prevent errors\n",
    "            globals()['best_thresholds_df'] = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"No evaluation results available. Check that the data contains model and ground truth information.\")\n",
    "        # Initialize as empty DataFrame to prevent errors\n",
    "        globals()['best_thresholds_df'] = pd.DataFrame()\n",
    "else:\n",
    "    print(\"\\nCannot evaluate model thresholds: missing ground truth data or model information\")\n",
    "    print(\"Make sure combined_df has the 'ground_truth_traceable' and 'model' columns\")\n",
    "    print(\"Please run cells 0-5 first to create the combined dataset.\")\n",
    "    \n",
    "    # Initialize as empty DataFrame to prevent errors in later cells\n",
    "    globals()['best_thresholds_df'] = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Statistical Comparison of Scoring Methods\n",
    "# Purpose: Compare different scoring methods using statistical tests\n",
    "# Dependencies: scipy.stats, pandas, numpy\n",
    "# Breadcrumbs: Analysis -> Statistical Tests -> Scoring Method Comparison\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "def compare_scoring_methods_statistically(df, model_name, ground_truth='ground_truth_traceable'):\n",
    "    \"\"\"\n",
    "    Statistically compare different scoring methods using paired tests\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing all data\n",
    "        model_name: Model to analyze\n",
    "        ground_truth: Ground truth column name\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistical test results\n",
    "    \"\"\"\n",
    "    from scipy.stats import wilcoxon, friedmanchisquare\n",
    "    import pandas as pd\n",
    "    \n",
    "    model_df = df[df['model'] == model_name].copy()\n",
    "    \n",
    "    if model_df.empty:\n",
    "        print(f\"No data for model {model_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Get all scoring columns\n",
    "    scoring_columns = [\n",
    "        'is_traceable', 'actor_score', 'judge_score', 'final_score',\n",
    "        'total_score', 'total_score_with_final', 'total_score_all', 'total_combined_score'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_columns = [col for col in scoring_columns if col in model_df.columns]\n",
    "    \n",
    "    print(f\"\\nStatistical Comparison of Scoring Methods for {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Comparing {len(available_columns)} scoring methods\")\n",
    "    \n",
    "    # Get predictions and F2 scores for each method\n",
    "    method_predictions = {}\n",
    "    method_f2_scores = []\n",
    "    method_names = []\n",
    "    \n",
    "    for col in available_columns:\n",
    "        # Find optimal threshold for this scoring method\n",
    "        scores = model_df[col].fillna(0).values\n",
    "        y_true = model_df[ground_truth].values\n",
    "        \n",
    "        # For boolean columns, use directly\n",
    "        if col == 'is_traceable':\n",
    "            best_pred = model_df[col].astype(int).values\n",
    "            best_threshold = 0.5\n",
    "        else:\n",
    "            # Find best threshold\n",
    "            thresholds = np.unique(scores)\n",
    "            best_f2 = 0\n",
    "            best_pred = None\n",
    "            best_threshold = 0\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                pred = (scores >= thresh).astype(int)\n",
    "                if len(np.unique(pred)) < 2:\n",
    "                    continue\n",
    "                f2 = fbeta_score(y_true, pred, beta=2, zero_division=0)\n",
    "                if f2 > best_f2:\n",
    "                    best_f2 = f2\n",
    "                    best_pred = pred\n",
    "                    best_threshold = thresh\n",
    "        \n",
    "        if best_pred is not None:\n",
    "            method_predictions[col] = best_pred\n",
    "            # Calculate per-sample correctness for Friedman test\n",
    "            correct = (best_pred == y_true).astype(int)\n",
    "            method_f2_scores.append(correct)\n",
    "            method_names.append(col)\n",
    "            \n",
    "            # Calculate overall metrics\n",
    "            f2 = fbeta_score(y_true, best_pred, beta=2, zero_division=0)\n",
    "            precision = precision_score(y_true, best_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, best_pred, zero_division=0)\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Threshold: {best_threshold:.3f}\")\n",
    "            print(f\"  F2 Score: {f2:.3f}\")\n",
    "            print(f\"  Precision: {precision:.3f}\")\n",
    "            print(f\"  Recall: {recall:.3f}\")\n",
    "    \n",
    "    # Perform statistical tests if we have multiple methods\n",
    "    if len(method_f2_scores) > 2:\n",
    "        # Friedman test for multiple related samples\n",
    "        try:\n",
    "            stat, p_value = friedmanchisquare(*method_f2_scores)\n",
    "            print(f\"\\nFriedman Test Results:\")\n",
    "            print(f\"  Chi-squared statistic: {stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (=0.05)\")\n",
    "            \n",
    "            # Post-hoc pairwise comparisons if significant\n",
    "            if p_value < 0.05:\n",
    "                print(\"\\nPost-hoc Pairwise Comparisons (Wilcoxon signed-rank test):\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                pairwise_results = []\n",
    "                for i in range(len(method_names)):\n",
    "                    for j in range(i+1, len(method_names)):\n",
    "                        try:\n",
    "                            # Wilcoxon signed-rank test\n",
    "                            stat, p = wilcoxon(method_f2_scores[i], method_f2_scores[j])\n",
    "                            \n",
    "                            # Calculate effect size (r = Z / sqrt(N))\n",
    "                            n = len(method_f2_scores[i])\n",
    "                            z = stat\n",
    "                            effect_size = z / np.sqrt(n)\n",
    "                            \n",
    "                            pairwise_results.append({\n",
    "                                'method1': method_names[i],\n",
    "                                'method2': method_names[j],\n",
    "                                'statistic': stat,\n",
    "                                'p_value': p,\n",
    "                                'effect_size': abs(effect_size),\n",
    "                                'significant': p < 0.05\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"{method_names[i]} vs {method_names[j]}:\")\n",
    "                            print(f\"  p-value: {p:.4f} {'*' if p < 0.05 else ''}\")\n",
    "                            print(f\"  Effect size (r): {abs(effect_size):.3f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not compare {method_names[i]} vs {method_names[j]}: {str(e)}\")\n",
    "                \n",
    "                # Apply multiple testing correction\n",
    "                if pairwise_results:\n",
    "                    p_values = [r['p_value'] for r in pairwise_results]\n",
    "                    from statsmodels.stats.multitest import multipletests\n",
    "                    reject, p_adjusted, _, _ = multipletests(p_values, method='bonferroni')\n",
    "                    \n",
    "                    print(\"\\nBonferroni-corrected p-values:\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(pairwise_results):\n",
    "                        print(f\"{result['method1']} vs {result['method2']}: \"\n",
    "                              f\"p_adj={p_adjusted[i]:.4f} {'*' if reject[i] else ''}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Friedman test: {str(e)}\")\n",
    "    \n",
    "    elif len(method_f2_scores) == 2:\n",
    "        # Just two methods - use Wilcoxon signed-rank test\n",
    "        try:\n",
    "            stat, p_value = wilcoxon(method_f2_scores[0], method_f2_scores[1])\n",
    "            print(f\"\\nWilcoxon Signed-Rank Test Results:\")\n",
    "            print(f\"  Comparing: {method_names[0]} vs {method_names[1]}\")\n",
    "            print(f\"  Test statistic: {stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (=0.05)\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Wilcoxon test: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'methods': method_names,\n",
    "        'predictions': method_predictions,\n",
    "        'f2_scores': method_f2_scores\n",
    "    }\n",
    "\n",
    "# Run statistical comparison for each model\n",
    "if 'combined_df' in globals() and not combined_df.empty:\n",
    "    models = combined_df['model'].unique()\n",
    "    \n",
    "    statistical_results = {}\n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        results = compare_scoring_methods_statistically(combined_df, model)\n",
    "        statistical_results[model] = results\n",
    "    \n",
    "    # Store results for later use\n",
    "    globals()['scoring_method_comparison'] = statistical_results\n",
    "else:\n",
    "    print(\"No data available for statistical comparison. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Traceability Analysis with Fixed Threshold\n",
    "# Purpose: Analyze traceability prediction using meta judge data with MIN_TRACEABILITY_THRESHOLD from .env\n",
    "# Dependencies: pandas, numpy, seaborn, matplotlib\n",
    "# Breadcrumbs: Analysis -> Traceability Evaluation -> Fixed Threshold\n",
    "\n",
    "def analyze_fixed_threshold(combined_df, min_traceability_threshold=None):\n",
    "    \"\"\"\n",
    "    Analyze traceability prediction using a fixed threshold value\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df (pd.DataFrame): Combined dataset with meta judge, LLM and ground truth data\n",
    "        min_traceability_threshold (int, optional): Fixed threshold value. If None, uses environment value.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy of the combined DataFrame for evaluation\n",
    "        combined_traced_eval_df = combined_df.copy()\n",
    "        \n",
    "        # Get min_traceability_threshold if not provided\n",
    "        if min_traceability_threshold is None:\n",
    "            if 'MIN_TRACEABILITY_THRESHOLD' in globals():\n",
    "                min_traceability_threshold = globals()['MIN_TRACEABILITY_THRESHOLD']\n",
    "            else:\n",
    "                # Load from environment if being imported\n",
    "                config = initialize_environment()\n",
    "                min_traceability_threshold = config['MIN_TRACEABILITY_THRESHOLD']\n",
    "        \n",
    "        # Define score columns of interest (including the new total score variants)\n",
    "        score_columns = [\n",
    "            'is_traceable', 'judge_score', 'actor_score', 'final_score', \n",
    "            'total_score', 'total_score_with_final', 'total_score_all'\n",
    "        ]\n",
    "        score_columns = [col for col in score_columns if col in combined_traced_eval_df.columns]\n",
    "        \n",
    "        # Check for ground truth data\n",
    "        if 'ground_truth_traceable' not in combined_traced_eval_df.columns:\n",
    "            logger.warning(\"No ground truth data available for evaluation\")\n",
    "            has_ground_truth = False\n",
    "        else:\n",
    "            has_ground_truth = True\n",
    "        \n",
    "        # Create a dictionary with fixed thresholds for each model and score type\n",
    "        model_thresholds = {}\n",
    "        for model_name in combined_traced_eval_df['model'].unique():\n",
    "            model_thresholds[model_name] = {\n",
    "                'is_traceable': True,  # Boolean\n",
    "                'judge_score': min_traceability_threshold,\n",
    "                'actor_score': min_traceability_threshold,\n",
    "                'final_score': min_traceability_threshold,\n",
    "                'total_score': min_traceability_threshold * 2,  # Double for combined judge+actor\n",
    "                'total_score_with_final': min_traceability_threshold * 2,  # Double for actor+final\n",
    "                'total_score_all': min_traceability_threshold * 3  # Triple for actor+judge+final\n",
    "            }\n",
    "        \n",
    "        # Make sure there are no None values in score columns\n",
    "        for col in score_columns:\n",
    "            if col in combined_traced_eval_df.columns:\n",
    "                null_count = combined_traced_eval_df[col].isna().sum()\n",
    "                if null_count > 0:\n",
    "                    logger.warning(f\"Found {null_count} null values in {col} column before applying thresholds. Replacing with 0 or False.\")\n",
    "                    \n",
    "                    if col == 'is_traceable':\n",
    "                        combined_traced_eval_df[col] = combined_traced_eval_df[col].fillna(False)\n",
    "                    else:\n",
    "                        combined_traced_eval_df[col] = combined_traced_eval_df[col].fillna(0)\n",
    "                \n",
    "                # Ensure score column is numeric (except for boolean is_traceable)\n",
    "                if col != 'is_traceable' and combined_traced_eval_df[col].dtype == object:\n",
    "                    try:\n",
    "                        combined_traced_eval_df[col] = pd.to_numeric(combined_traced_eval_df[col])\n",
    "                        logger.info(f\"Converted {col} to numeric type for threshold application\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error converting {col} to numeric: {str(e)}\")\n",
    "                        logger.error(\"Using zeros for score\")\n",
    "                        combined_traced_eval_df[col] = 0\n",
    "        \n",
    "        # Create prediction columns for each score type\n",
    "        for score_col in score_columns:\n",
    "            if score_col in combined_traced_eval_df.columns:\n",
    "                pred_col_name = f'predicted_{score_col}'\n",
    "                \n",
    "                # Apply appropriate threshold based on score column type\n",
    "                if score_col == 'is_traceable':\n",
    "                    # For boolean column, just use the value directly\n",
    "                    combined_traced_eval_df[pred_col_name] = combined_traced_eval_df[score_col]\n",
    "                else:\n",
    "                    # For numeric columns, apply fixed threshold\n",
    "                    def apply_threshold(row):\n",
    "                        model = row['model']\n",
    "                        if model in model_thresholds and score_col in model_thresholds[model]:\n",
    "                            threshold = model_thresholds[model][score_col]\n",
    "                        else:\n",
    "                            # Fallback thresholds if not found\n",
    "                            if score_col == 'total_score':\n",
    "                                threshold = min_traceability_threshold * 2  # Default for combined score\n",
    "                            elif score_col == 'total_score_with_final':\n",
    "                                threshold = min_traceability_threshold * 2  # Default for actor+final\n",
    "                            elif score_col == 'total_score_all':\n",
    "                                threshold = min_traceability_threshold * 3  # Default for actor+judge+final\n",
    "                            else:\n",
    "                                threshold = min_traceability_threshold  # Default for other scores\n",
    "                        \n",
    "                        return row[score_col] >= threshold\n",
    "                    \n",
    "                    combined_traced_eval_df[pred_col_name] = combined_traced_eval_df.apply(apply_threshold, axis=1)\n",
    "                \n",
    "                logger.info(f\"Created prediction column {pred_col_name} based on {score_col}\")\n",
    "        \n",
    "        # Add confusion matrix categories for each prediction type if we have ground truth\n",
    "        if has_ground_truth:\n",
    "            for score_col in score_columns:\n",
    "                pred_col = f'predicted_{score_col}'\n",
    "                if pred_col in combined_traced_eval_df.columns:\n",
    "                    conf_col = f'confusion_{score_col}'\n",
    "                    \n",
    "                    def get_confusion_category(row):\n",
    "                        if row['ground_truth_traceable'] and row[pred_col]:\n",
    "                            return 'TP'  # True Positive\n",
    "                        elif not row['ground_truth_traceable'] and row[pred_col]:\n",
    "                            return 'FP'  # False Positive\n",
    "                        elif row['ground_truth_traceable'] and not row[pred_col]:\n",
    "                            return 'FN'  # False Negative\n",
    "                        else:  # not ground_truth and not predicted\n",
    "                            return 'TN'  # True Negative\n",
    "                    \n",
    "                    combined_traced_eval_df[conf_col] = combined_traced_eval_df.apply(get_confusion_category, axis=1)\n",
    "                    logger.info(f\"Created confusion category column {conf_col} based on {pred_col}\")\n",
    "        \n",
    "        # Save fixed model thresholds to combined_traced_eval_df for use in other cells\n",
    "        combined_traced_eval_df.attrs['fixed_model_thresholds'] = model_thresholds\n",
    "        \n",
    "        return combined_traced_eval_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in fixed threshold evaluation: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Confusion Matrix and Performance Metrics\n",
    "# Purpose: Analyze confusion matrices and model performance for different score variants\n",
    "# Dependencies: pandas, numpy, matplotlib, seaborn\n",
    "# Breadcrumbs: Analysis -> Confusion Matrix -> Visualization\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "def get_actual_model_name(model_key):\n",
    "    \"\"\"\n",
    "    Resolve the actual model name from environment variables\n",
    "    \n",
    "    If model_key is a reference to another environment variable,\n",
    "    resolve it to get the actual model name\n",
    "    \"\"\"\n",
    "    if model_key in os.environ:\n",
    "        return os.environ[model_key]\n",
    "    return model_key\n",
    "\n",
    "def create_heatmap(df, title, ax=None, cmap='Blues', annot=False, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap visualization with absolute counts\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to visualize (with counts)\n",
    "        title (str): Title for the heatmap\n",
    "        ax (matplotlib.axes.Axes, optional): Axes to plot on\n",
    "        cmap (str): Colormap to use\n",
    "        annot (bool): Whether to use seaborn's annotations (we'll add our own)\n",
    "        vmin (float): Minimum value for colormap\n",
    "        vmax (float): Maximum value for colormap\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: The axes with the heatmap\n",
    "    \"\"\"\n",
    "    sns.heatmap(df, annot=annot, cmap=cmap, ax=ax, cbar=True, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def analyze_confusion_matrix(combined_results, score_columns=None, models=None):\n",
    "    \"\"\"\n",
    "    Analyze confusion matrix for different score columns and models\n",
    "    \n",
    "    Parameters:\n",
    "        combined_results (pd.DataFrame): Combined dataset with prediction results\n",
    "        score_columns (list, optional): List of score columns to analyze\n",
    "        models (list, optional): List of models to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make sure we have the necessary data\n",
    "        if combined_results is None or combined_results.empty:\n",
    "            print(\"No data available for confusion matrix analysis\")\n",
    "            return {}\n",
    "            \n",
    "        # Check if we have ground truth data\n",
    "        if 'ground_truth_traceable' not in combined_results.columns:\n",
    "            print(\"No ground truth data available for confusion matrix analysis\")\n",
    "            return {}\n",
    "            \n",
    "        # Use all score columns if not specified\n",
    "        if score_columns is None:\n",
    "            # Look for columns that start with 'predicted_'\n",
    "            pred_cols = [col for col in combined_results.columns if col.startswith('predicted_')]\n",
    "            score_columns = [col.replace('predicted_', '') for col in pred_cols]\n",
    "            \n",
    "        # Use all models if not specified\n",
    "        if models is None:\n",
    "            models = combined_results['model'].unique()\n",
    "            \n",
    "        # Ensure all columns exist\n",
    "        score_columns = [col for col in score_columns \n",
    "                        if f'predicted_{col}' in combined_results.columns]\n",
    "            \n",
    "        # Initialize results dictionary\n",
    "        results = {}\n",
    "        \n",
    "        # Calculate metrics for each model and score column\n",
    "        for model in models:\n",
    "            model_df = combined_results[combined_results['model'] == model]\n",
    "            \n",
    "            if model_df.empty:\n",
    "                continue\n",
    "                \n",
    "            model_results = {}\n",
    "            \n",
    "            for score_col in score_columns:\n",
    "                pred_col = f'predicted_{score_col}'\n",
    "                \n",
    "                if pred_col not in model_df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                # Create confusion matrix\n",
    "                y_true = model_df['ground_truth_traceable'].astype(int).values\n",
    "                y_pred = model_df[pred_col].astype(int).values\n",
    "                \n",
    "                # Confusion matrix components\n",
    "                try:\n",
    "                    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "                    \n",
    "                    # Handle division by zero for precision and recall\n",
    "                    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    \n",
    "                    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "                    f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "                    \n",
    "                    # Additional metrics\n",
    "                    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "                    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate\n",
    "                    \n",
    "                    model_results[score_col] = {\n",
    "                        'confusion_matrix': {\n",
    "                            'tn': tn,\n",
    "                            'fp': fp,\n",
    "                            'fn': fn,\n",
    "                            'tp': tp\n",
    "                        },\n",
    "                        'metrics': {\n",
    "                            'accuracy': accuracy,\n",
    "                            'balanced_accuracy': balanced_acc,\n",
    "                            'precision': prec,\n",
    "                            'recall': rec,\n",
    "                            'f1_score': f1,\n",
    "                            'f2_score': f2,\n",
    "                            'specificity': tnr,\n",
    "                            'miss_rate': fnr\n",
    "                        }\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating confusion matrix for {model}, {score_col}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            if model_results:\n",
    "                results[model] = model_results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in confusion matrix analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "# Run the fixed threshold analysis if combined_df exists\n",
    "print(\"Running fixed threshold analysis to create evaluation dataset...\")\n",
    "\n",
    "# Create total_combined_score if needed\n",
    "if 'combined_df' in globals() and not combined_df.empty:\n",
    "    # Only add it if it doesn't already exist\n",
    "    if 'total_combined_score' not in combined_df.columns:\n",
    "        combined_df['total_combined_score'] = combined_df.apply(\n",
    "            lambda row: sum([\n",
    "                row.get('judge_score', 0) or 0,\n",
    "                row.get('semantic_alignment', 0) or 0, \n",
    "                row.get('non_functional_coverage', 0) or 0,\n",
    "                row.get('final_score', 0) or 0,\n",
    "                row.get('actor_score', 0) or 0,\n",
    "                row.get('functional_completeness', 0) or 0\n",
    "            ]), \n",
    "            axis=1\n",
    "        )\n",
    "        print(\"Created total_combined_score column\")\n",
    "\n",
    "# Apply fixed threshold analysis\n",
    "if 'analyze_fixed_threshold' in globals() and 'combined_df' in globals() and not combined_df.empty:\n",
    "    # Get min_traceability_threshold from environment if available\n",
    "    if 'MIN_TRACEABILITY_THRESHOLD' in globals():\n",
    "        min_traceability_threshold = globals()['MIN_TRACEABILITY_THRESHOLD']\n",
    "    else:\n",
    "        config = initialize_environment() if 'initialize_environment' in globals() else {}\n",
    "        min_traceability_threshold = config.get('MIN_TRACEABILITY_THRESHOLD', 3)\n",
    "    \n",
    "    # Include the total_combined_score in the score_columns list\n",
    "    combined_traced_eval_df = analyze_fixed_threshold(combined_df, min_traceability_threshold)\n",
    "    \n",
    "    if not combined_traced_eval_df.empty:\n",
    "        print(f\"Created evaluation dataset with {len(combined_traced_eval_df)} rows\")\n",
    "        \n",
    "        # Make sure the total_combined_score column is included in evaluation\n",
    "        # Add predicted_total_combined_score column if not already exists\n",
    "        if 'total_combined_score' in combined_traced_eval_df.columns and 'predicted_total_combined_score' not in combined_traced_eval_df.columns:\n",
    "            # Calculate threshold as 6x the min_traceability_threshold (sum of 6 scores)\n",
    "            threshold = min_traceability_threshold * 6\n",
    "            combined_traced_eval_df['predicted_total_combined_score'] = combined_traced_eval_df['total_combined_score'] >= threshold\n",
    "            print(f\"Created prediction column for total_combined_score with threshold {threshold}\")\n",
    "            \n",
    "            # Add confusion matrix column if we have ground truth\n",
    "            if 'ground_truth_traceable' in combined_traced_eval_df.columns:\n",
    "                def get_confusion_category(row):\n",
    "                    if row['ground_truth_traceable'] and row['predicted_total_combined_score']:\n",
    "                        return 'TP'  # True Positive\n",
    "                    elif not row['ground_truth_traceable'] and row['predicted_total_combined_score']:\n",
    "                        return 'FP'  # False Positive\n",
    "                    elif row['ground_truth_traceable'] and not row['predicted_total_combined_score']:\n",
    "                        return 'FN'  # False Negative\n",
    "                    else:  # not ground_truth and not predicted\n",
    "                        return 'TN'  # True Negative\n",
    "                \n",
    "                combined_traced_eval_df['confusion_total_combined_score'] = combined_traced_eval_df.apply(get_confusion_category, axis=1)\n",
    "                print(\"Created confusion category column for total_combined_score\")\n",
    "        \n",
    "        # Analyze confusion matrices and create performance table\n",
    "        print(\"\\nAnalyzing confusion matrices and performance metrics\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Make sure to include total_combined_score in the list of score columns\n",
    "        all_score_columns = [\n",
    "            'is_traceable', 'actor_score', 'judge_score', 'final_score', \n",
    "            'total_score', 'total_score_with_final', 'total_score_all', 'total_combined_score'\n",
    "        ]\n",
    "        \n",
    "        # Filter to only columns that exist\n",
    "        score_columns_to_analyze = [\n",
    "            col for col in all_score_columns \n",
    "            if f'predicted_{col}' in combined_traced_eval_df.columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"Analyzing {len(score_columns_to_analyze)} score columns: {score_columns_to_analyze}\")\n",
    "        \n",
    "        # Get all models\n",
    "        all_models = combined_traced_eval_df['model'].unique()\n",
    "        \n",
    "        # Analyze confusion matrices\n",
    "        confusion_results = analyze_confusion_matrix(\n",
    "            combined_traced_eval_df, \n",
    "            score_columns=score_columns_to_analyze,\n",
    "            models=all_models\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated confusion matrices for {len(confusion_results)} models\")\n",
    "        \n",
    "        # Create a dataframe for performance comparison\n",
    "        performance_rows = []\n",
    "        \n",
    "        for model, model_results in confusion_results.items():\n",
    "            for score_col, result in model_results.items():\n",
    "                cm = result['confusion_matrix']\n",
    "                metrics = result['metrics']\n",
    "                \n",
    "                performance_rows.append({\n",
    "                    'model': model,\n",
    "                    'score_column': score_col,\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1_score': metrics['f1_score'],\n",
    "                    'f2_score': metrics['f2_score'],\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'balanced_accuracy': metrics['balanced_accuracy'],\n",
    "                    'specificity': metrics['specificity'],\n",
    "                    'miss_rate': metrics['miss_rate'],\n",
    "                    'tp': cm['tp'],\n",
    "                    'fp': cm['fp'],\n",
    "                    'fn': cm['fn'],\n",
    "                    'tn': cm['tn']\n",
    "                })\n",
    "        \n",
    "        # Create performance comparison DataFrame\n",
    "        if performance_rows:\n",
    "            performance_df = pd.DataFrame(performance_rows)\n",
    "            \n",
    "            # Sort by F2 score (or F1 if specified)\n",
    "            if 'OPTIMIZATION_METRIC' in globals() and globals()['OPTIMIZATION_METRIC'] == 'F1':\n",
    "                performance_df = performance_df.sort_values('f1_score', ascending=False).reset_index(drop=True)\n",
    "                sort_metric = 'F1'\n",
    "            else:\n",
    "                performance_df = performance_df.sort_values('f2_score', ascending=False).reset_index(drop=True)\n",
    "                sort_metric = 'F2'\n",
    "            \n",
    "            # Display performance comparison\n",
    "            print(f\"\\nPerformance Comparison of Score Columns (sorted by {sort_metric} score):\")\n",
    "            print(\"-\" * 80)\n",
    "            display(performance_df)\n",
    "            \n",
    "            # Find the best overall\n",
    "            best_row = performance_df.iloc[0]\n",
    "            print(f\"\\nBest performing score column: {best_row['score_column']} for model {best_row['model']}\")\n",
    "            print(f\"  - {sort_metric} Score: {best_row['f2_score' if sort_metric == 'F2' else 'f1_score']:.3f}\")\n",
    "            print(f\"  - Precision: {best_row['precision']:.3f}\")\n",
    "            print(f\"  - Recall: {best_row['recall']:.3f}\")\n",
    "            \n",
    "            # Show confusion matrix for best result\n",
    "            print(\"\\nConfusion matrix counts for best result:\")\n",
    "            print(f\"  - True Positives (TP): {best_row['tp']}\")\n",
    "            print(f\"  - False Positives (FP): {best_row['fp']}\")\n",
    "            print(f\"  - False Negatives (FN): {best_row['fn']}\")\n",
    "            print(f\"  - True Negatives (TN): {best_row['tn']}\")\n",
    "            \n",
    "            # Store results for later use\n",
    "            globals()['performance_comparison_df'] = performance_df\n",
    "            globals()['combined_traced_eval_df'] = combined_traced_eval_df\n",
    "            \n",
    "            # Check if visualization is enabled in environment\n",
    "            if 'SHOW_VISUALIZATION' in globals() and globals()['SHOW_VISUALIZATION']:\n",
    "                # Get project name from globals\n",
    "                project_name = globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "                \n",
    "                # Create confusion matrix visualization grid\n",
    "                print(\"\\nCreating confusion matrix visualizations...\")\n",
    "                \n",
    "                # Define color palette for consistency\n",
    "                color_palette = {\n",
    "                    'TP': '#1A85FF',  # Good - Blue\n",
    "                    'FP': '#FFC61A',  # Okay - Yellow/Gold\n",
    "                    'FN': '#D41159',  # Not Great - Magenta/Red\n",
    "                    'TN': '#CCCCCC'   # Neutral - Light Gray\n",
    "                }\n",
    "                \n",
    "                # Set up portrait layout - 2 columns, up to 4 rows\n",
    "                n_score_cols = len(score_columns_to_analyze)\n",
    "                n_cols = 4  # Fixed at 2 columns\n",
    "                n_rows = min(2, (n_score_cols + n_cols - 1) // n_cols)  # Up to 4 rows\n",
    "                \n",
    "                # Create figure with subplots\n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 4 * n_rows))\n",
    "                \n",
    "                # Flatten axes if we have multiple rows\n",
    "                if n_rows > 1:\n",
    "                    axes = axes.flatten()\n",
    "                else:\n",
    "                    # Make sure axes is always an array\n",
    "                    axes = np.array([axes]) if n_cols == 1 else axes\n",
    "                \n",
    "                # For each score column, create a confusion matrix visualization\n",
    "                for i, score_col in enumerate(score_columns_to_analyze):\n",
    "                    if i >= len(axes):\n",
    "                        break\n",
    "                        \n",
    "                    ax = axes[i]\n",
    "                    \n",
    "                    # Get metrics for the first model (assuming single model analysis)\n",
    "                    model = all_models[0]\n",
    "                    if model in confusion_results and score_col in confusion_results[model]:\n",
    "                        cm = confusion_results[model][score_col]['confusion_matrix']\n",
    "                        \n",
    "                        # Create confusion matrix as numpy array\n",
    "                        cm_array = np.array([\n",
    "                            [cm['tn'], cm['fp']],\n",
    "                            [cm['fn'], cm['tp']]\n",
    "                        ])\n",
    "                        \n",
    "                        # Create confusion matrix plot\n",
    "                        sns.heatmap(\n",
    "                            cm_array, \n",
    "                            annot=True, \n",
    "                            fmt='g', \n",
    "                            cmap='Blues',\n",
    "                            xticklabels=['Negative', 'Positive'],\n",
    "                            yticklabels=['Negative', 'Positive'], \n",
    "                            ax=ax,\n",
    "                            cbar=False  # No colorbar\n",
    "                        )\n",
    "                        \n",
    "                        # Set title with score column name\n",
    "                        ax.set_title(f\"{score_col}\", fontsize=10)\n",
    "                        ax.set_xlabel('Predicted', fontsize=8)\n",
    "                        ax.set_ylabel('Actual', fontsize=8)\n",
    "                    else:\n",
    "                        # Empty plot if no data\n",
    "                        ax.axis('off')\n",
    "                        ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "                \n",
    "                # Hide any unused subplots\n",
    "                for i in range(n_score_cols, len(axes)):\n",
    "                    axes[i].axis('off')\n",
    "                \n",
    "                # Add a common title with project name and model\n",
    "                plt.suptitle(f\"Project: {project_name}\\nConfusion Matrices for Model: {model}\", fontsize=14, y=1.05)\n",
    "                \n",
    "                # Adjust layout\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Show plot\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"No performance metrics calculated. Check that the dataset has ground truth and prediction data.\")\n",
    "    else:\n",
    "        print(\"Failed to create evaluation dataset\")\n",
    "else:\n",
    "    print(\"Missing required data or functions. Please run cells 0-6 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Statistical Comparison Between Models\n",
    "# Purpose: Compare performance between different models using statistical tests\n",
    "# Dependencies: scipy.stats, pandas, numpy\n",
    "# Breadcrumbs: Analysis -> Statistical Tests -> Model Comparison\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "def compare_models_statistically(performance_df, metric='f2_score'):\n",
    "    \"\"\"\n",
    "    Compare multiple models using statistical tests\n",
    "    \n",
    "    Parameters:\n",
    "        performance_df: DataFrame with model performance metrics\n",
    "        metric: Metric to use for comparison\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistical test results\n",
    "    \"\"\"\n",
    "    from scipy.stats import kruskal, mannwhitneyu\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "    print(f\"\\nStatistical Comparison of Models using {metric.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Group by model and get best score for each\n",
    "    model_scores = {}\n",
    "    \n",
    "    for model in performance_df['model'].unique():\n",
    "        model_data = performance_df[performance_df['model'] == model]\n",
    "        # Get the best score for this model across all scoring methods\n",
    "        best_score = model_data[metric].max()\n",
    "        # Get all scores for this model\n",
    "        all_scores = model_data[metric].values\n",
    "        model_scores[model] = {\n",
    "            'best': best_score,\n",
    "            'all': all_scores,\n",
    "            'mean': np.mean(all_scores),\n",
    "            'std': np.std(all_scores)\n",
    "        }\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    for model, scores in model_scores.items():\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  Best {metric}: {scores['best']:.3f}\")\n",
    "        print(f\"  Mean {metric}: {scores['mean']:.3f} ({scores['std']:.3f})\")\n",
    "    \n",
    "    # Kruskal-Wallis test if more than 2 models\n",
    "    if len(model_scores) > 2:\n",
    "        # Use all scores for each model\n",
    "        scores_lists = [scores['all'] for scores in model_scores.values()]\n",
    "        \n",
    "        try:\n",
    "            h_stat, p_value = kruskal(*scores_lists)\n",
    "            print(f\"\\nKruskal-Wallis H-test:\")\n",
    "            print(f\"  H-statistic: {h_stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (=0.05)\")\n",
    "            \n",
    "            # Post-hoc pairwise comparisons if significant\n",
    "            if p_value < 0.05:\n",
    "                print(\"\\nPost-hoc Pairwise Comparisons (Mann-Whitney U test):\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                model_names = list(model_scores.keys())\n",
    "                pairwise_results = []\n",
    "                \n",
    "                for i in range(len(model_names)):\n",
    "                    for j in range(i+1, len(model_names)):\n",
    "                        scores1 = model_scores[model_names[i]]['all']\n",
    "                        scores2 = model_scores[model_names[j]]['all']\n",
    "                        \n",
    "                        try:\n",
    "                            stat, p = mannwhitneyu(scores1, scores2, alternative='two-sided')\n",
    "                            \n",
    "                            # Calculate effect size (r = Z / sqrt(N))\n",
    "                            n1, n2 = len(scores1), len(scores2)\n",
    "                            z = stat\n",
    "                            effect_size = z / np.sqrt(n1 + n2)\n",
    "                            \n",
    "                            pairwise_results.append({\n",
    "                                'model1': model_names[i],\n",
    "                                'model2': model_names[j],\n",
    "                                'statistic': stat,\n",
    "                                'p_value': p,\n",
    "                                'effect_size': abs(effect_size)\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"{model_names[i]} vs {model_names[j]}:\")\n",
    "                            print(f\"  p-value: {p:.4f}\")\n",
    "                            print(f\"  Effect size (r): {abs(effect_size):.3f}\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Could not compare {model_names[i]} vs {model_names[j]}: {str(e)}\")\n",
    "                \n",
    "                # Apply multiple testing correction\n",
    "                if pairwise_results:\n",
    "                    p_values = [r['p_value'] for r in pairwise_results]\n",
    "                    reject, p_adjusted, _, _ = multipletests(p_values, method='bonferroni')\n",
    "                    \n",
    "                    print(\"\\nBonferroni-corrected p-values:\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(pairwise_results):\n",
    "                        print(f\"{result['model1']} vs {result['model2']}: \"\n",
    "                              f\"p_adj={p_adjusted[i]:.4f} {'*' if reject[i] else ''}\")\n",
    "                              \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Kruskal-Wallis test: {str(e)}\")\n",
    "    \n",
    "    elif len(model_scores) == 2:\n",
    "        # Mann-Whitney U test for two models\n",
    "        model_names = list(model_scores.keys())\n",
    "        scores1 = model_scores[model_names[0]]['all']\n",
    "        scores2 = model_scores[model_names[1]]['all']\n",
    "        \n",
    "        try:\n",
    "            stat, p_value = mannwhitneyu(scores1, scores2, alternative='two-sided')\n",
    "            print(f\"\\nMann-Whitney U Test Results:\")\n",
    "            print(f\"  Comparing: {model_names[0]} vs {model_names[1]}\")\n",
    "            print(f\"  U-statistic: {stat:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (=0.05)\")\n",
    "            \n",
    "            # Calculate effect size\n",
    "            n1, n2 = len(scores1), len(scores2)\n",
    "            effect_size = stat / (n1 * n2)\n",
    "            print(f\"  Effect size (r): {effect_size:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in Mann-Whitney U test: {str(e)}\")\n",
    "    \n",
    "    return model_scores\n",
    "\n",
    "# Run model comparison if we have performance data\n",
    "if 'performance_comparison_df' in globals() and not performance_comparison_df.empty:\n",
    "    # Use the optimization metric for comparison\n",
    "    metric_to_compare = 'f2_score' if OPTIMIZATION_METRIC == 'F2' else 'f1_score'\n",
    "    \n",
    "    model_comparison_results = compare_models_statistically(\n",
    "        performance_comparison_df, \n",
    "        metric=metric_to_compare\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    globals()['model_comparison_results'] = model_comparison_results\n",
    "    \n",
    "elif 'best_thresholds_df' in globals() and not best_thresholds_df.empty:\n",
    "    # Use best_thresholds_df if performance_comparison_df not available\n",
    "    metric_to_compare = 'f2_score' if OPTIMIZATION_METRIC == 'F2' else 'f1_score'\n",
    "    \n",
    "    model_comparison_results = compare_models_statistically(\n",
    "        best_thresholds_df, \n",
    "        metric=metric_to_compare\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    globals()['model_comparison_results'] = model_comparison_results\n",
    "else:\n",
    "    print(\"No performance data available for model comparison. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Analysis Summary and Conclusions with Model Name Resolution\n",
    "# Purpose: Summarize findings and draw conclusions about traceability prediction\n",
    "# Dependencies: pandas\n",
    "# Breadcrumbs: Analysis -> Summary\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_actual_model_name(model_key):\n",
    "    \"\"\"\n",
    "    Resolve the actual model name from environment variables\n",
    "    \n",
    "    If model_key is a reference to another environment variable,\n",
    "    resolve it to get the actual model name\n",
    "    \"\"\"\n",
    "    if model_key in os.environ:\n",
    "        return os.environ[model_key]\n",
    "    return model_key\n",
    "\n",
    "def summarize_analysis_results(best_thresholds_df=None, project_name=None, optimization_metric=None, show_visualization=False, model_key=None):\n",
    "    \"\"\"\n",
    "    Create a summary of model evaluation and traceability analysis\n",
    "    \n",
    "    Parameters:\n",
    "        best_thresholds_df (pd.DataFrame): DataFrame containing model evaluation results\n",
    "        project_name (str): Name of the project being analyzed\n",
    "        optimization_metric (str): Metric used for optimization ('F1' or 'F2')\n",
    "        show_visualization (bool): Whether to show visualizations\n",
    "        model_key (str): Model key/identifier\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing summary information\n",
    "    \"\"\"\n",
    "    # Get parameter values from globals if not provided\n",
    "    if best_thresholds_df is None and 'best_thresholds_df' in globals() and globals()['best_thresholds_df'] is not None and isinstance(globals()['best_thresholds_df'], pd.DataFrame) and not globals()['best_thresholds_df'].empty:\n",
    "        best_thresholds_df = globals()['best_thresholds_df']\n",
    "        \n",
    "    if project_name is None and 'NEO4J_PROJECT_NAME' in globals():\n",
    "        project_name = globals()['NEO4J_PROJECT_NAME']\n",
    "    else:\n",
    "        project_name = project_name or \"Unknown Project\"\n",
    "        \n",
    "    if optimization_metric is None and 'OPTIMIZATION_METRIC' in globals():\n",
    "        optimization_metric = globals()['OPTIMIZATION_METRIC']\n",
    "    else:\n",
    "        optimization_metric = optimization_metric or \"F2\"\n",
    "        \n",
    "    if show_visualization is None and 'SHOW_VISUALIZATION' in globals():\n",
    "        show_visualization = globals()['SHOW_VISUALIZATION']\n",
    "        \n",
    "    if model_key is None and 'CURRENT_MODEL_KEY' in globals():\n",
    "        model_key = globals()['CURRENT_MODEL_KEY']\n",
    "    else:\n",
    "        model_key = model_key or \"Unknown Model\"\n",
    "        \n",
    "    # Create empty result dictionary\n",
    "    summary = {\n",
    "        'project_name': project_name,\n",
    "        'optimization_metric': optimization_metric,\n",
    "        'has_model_evaluation': False,\n",
    "        'models_evaluated': 0,\n",
    "        'best_model': None,\n",
    "        'best_model_threshold': None,\n",
    "        'best_model_metrics': {},\n",
    "        'model_family_comparison': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # If we don't have threshold results, return basic summary\n",
    "    if best_thresholds_df is None or not isinstance(best_thresholds_df, pd.DataFrame) or best_thresholds_df.empty:\n",
    "        return summary\n",
    "        \n",
    "    # We have model evaluation results\n",
    "    summary['has_model_evaluation'] = True\n",
    "    summary['models_evaluated'] = len(best_thresholds_df['model_name'].unique())\n",
    "    \n",
    "    # Count score columns evaluated\n",
    "    score_columns = best_thresholds_df['score_column'].unique()\n",
    "    summary['score_columns_evaluated'] = len(score_columns)\n",
    "    summary['score_columns'] = list(score_columns)\n",
    "    \n",
    "    # Identify the best performing model and score column combination\n",
    "    if optimization_metric.upper() == 'F1':\n",
    "        best_idx = best_thresholds_df['f1_score'].idxmax()\n",
    "        sort_metric = 'f1_score'\n",
    "    else:\n",
    "        best_idx = best_thresholds_df['f2_score'].idxmax()\n",
    "        sort_metric = 'f2_score'\n",
    "        \n",
    "    best_row = best_thresholds_df.loc[best_idx]\n",
    "    summary['best_model'] = best_row['model_name']\n",
    "    summary['best_score_column'] = best_row['score_column']\n",
    "    summary['best_model_threshold'] = float(best_row['best_threshold'])\n",
    "    summary['sort_metric'] = sort_metric\n",
    "    \n",
    "    # Store best model metrics\n",
    "    summary['best_model_metrics'] = {\n",
    "        'threshold': float(best_row['best_threshold']),\n",
    "        'precision': float(best_row['precision']),\n",
    "        'recall': float(best_row['recall']),\n",
    "        'f1_score': float(best_row['f1_score']),\n",
    "        'f2_score': float(best_row['f2_score']),\n",
    "        'matthews_corr': float(best_row['matthews_corr']),\n",
    "        'balanced_accuracy': float(best_row['balanced_accuracy']),\n",
    "        'confusion_matrix': {\n",
    "            'tp': int(best_row['true_positives']),\n",
    "            'fp': int(best_row['false_positives']),\n",
    "            'fn': int(best_row['false_negatives']),\n",
    "            'tn': int(best_row['true_negatives'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get performance by score column type\n",
    "    score_column_performance = best_thresholds_df.groupby('score_column')[sort_metric].mean().sort_values(ascending=False)\n",
    "    summary['score_column_performance'] = {col: float(val) for col, val in score_column_performance.items()}\n",
    "    \n",
    "    # Get the best performing score column type\n",
    "    best_score_column = score_column_performance.index[0]\n",
    "    summary['best_score_column_type'] = best_score_column\n",
    "    summary['best_score_column_score'] = float(score_column_performance.iloc[0])\n",
    "    \n",
    "    # Get performance by model\n",
    "    model_performance = best_thresholds_df.groupby('model_name')[sort_metric].max().sort_values(ascending=False)\n",
    "    summary['model_performance'] = {model: float(val) for model, val in model_performance.items()}\n",
    "    \n",
    "    # Total data points analyzed\n",
    "    total_data_points = best_thresholds_df['data_points'].iloc[0]\n",
    "    summary['total_data_points'] = int(total_data_points)\n",
    "    summary['ground_truth_positive'] = int(best_row['ground_truth_positive'])\n",
    "    summary['ground_truth_negative'] = int(best_row['ground_truth_negative'])\n",
    "    summary['ground_truth_ratio'] = float(best_row['ground_truth_positive'] / total_data_points)\n",
    "    \n",
    "    # Recommendations based on analysis\n",
    "    recommendations = [\n",
    "        f\"Use {best_row['model_name']} model with {best_row['score_column']} score for best traceability results\",\n",
    "        f\"Apply a threshold of {best_row['best_threshold']:.3f} to {best_row['score_column']} scores\",\n",
    "        f\"Expected {optimization_metric} score: {best_row[sort_metric]:.3f}\"\n",
    "    ]\n",
    "    \n",
    "    # Check precision-recall balance\n",
    "    if abs(best_row['precision'] - best_row['recall']) > 0.1:\n",
    "        if best_row['precision'] > best_row['recall']:\n",
    "            recommendations.append(f\"Note that precision ({best_row['precision']:.3f}) is higher than recall ({best_row['recall']:.3f}), meaning the model finds fewer links but with higher confidence\")\n",
    "        else:\n",
    "            recommendations.append(f\"Note that recall ({best_row['recall']:.3f}) is higher than precision ({best_row['precision']:.3f}), meaning the model finds more links but with lower confidence\")\n",
    "    \n",
    "    summary['recommendations'] = recommendations\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Execute only when the notebook is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get the actual model name\n",
    "    current_model_var = os.environ.get('CURRENT_MODEL', '')\n",
    "    actual_model_name = os.environ.get(current_model_var, current_model_var)\n",
    "    \n",
    "    # Update the global variable if it exists\n",
    "    if 'CURRENT_MODEL_KEY' in globals():\n",
    "        CURRENT_MODEL_KEY = actual_model_name\n",
    "    \n",
    "    # Check if best_thresholds_df is available\n",
    "    if 'best_thresholds_df' in globals() and globals()['best_thresholds_df'] is not None and isinstance(globals()['best_thresholds_df'], pd.DataFrame) and not globals()['best_thresholds_df'].empty:\n",
    "        # Create a pivot table to compare different score columns across models\n",
    "        score_comparison_df = best_thresholds_df.pivot_table(\n",
    "            index='model_name',\n",
    "            columns='score_column',\n",
    "            values=['f1_score', 'f2_score', 'precision', 'recall', 'matthews_corr'],\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        # Define the score columns we want to compare\n",
    "        score_columns = best_thresholds_df['score_column'].unique()\n",
    "        \n",
    "        # Print summary of results\n",
    "        print(\"\\nMeta Judge Analysis Summary:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "        print(f\"Model: {actual_model_name}\")\n",
    "        print(f\"Optimization Metric: {OPTIMIZATION_METRIC}\")\n",
    "        print(f\"Score Columns Evaluated: {', '.join(score_columns)}\")\n",
    "        \n",
    "        # Identify best performing score column by optimization metric\n",
    "        if OPTIMIZATION_METRIC.upper() == 'F1':\n",
    "            best_score_col = best_thresholds_df.loc[best_thresholds_df['f1_score'].idxmax()]['score_column']\n",
    "            sort_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_score_col = best_thresholds_df.loc[best_thresholds_df['f2_score'].idxmax()]['score_column']\n",
    "            sort_metric = 'f2_score'\n",
    "            \n",
    "        print(f\"\\nBest Performing Score Column: {best_score_col} (based on {OPTIMIZATION_METRIC} score)\")\n",
    "        \n",
    "        # Get average performance by score column\n",
    "        avg_by_score_col = best_thresholds_df.groupby('score_column')[sort_metric].mean().sort_values(ascending=False)\n",
    "        print(\"\\nAverage Performance by Score Column Type:\")\n",
    "        for col, score in avg_by_score_col.items():\n",
    "            print(f\"  {col}: {score:.3f}\")\n",
    "        \n",
    "        # Display top 3 model and score column combinations\n",
    "        top_combinations = best_thresholds_df.sort_values(sort_metric, ascending=False).head(3)\n",
    "        print(\"\\nTop 3 Model and Score Column Combinations:\")\n",
    "        for i, (_, row) in enumerate(top_combinations.iterrows(), 1):\n",
    "            print(f\"  {i}. {row['model_name']} with {row['score_column']}: {row[sort_metric]:.3f} {OPTIMIZATION_METRIC}\")\n",
    "            print(f\"     - Threshold: {row['best_threshold']:.3f}\")\n",
    "            print(f\"     - Precision: {row['precision']:.3f}, Recall: {row['recall']:.3f}\")\n",
    "            print(f\"     - TP: {row['true_positives']}, FP: {row['false_positives']}, FN: {row['false_negatives']}, TN: {row['true_negatives']}\")\n",
    "        \n",
    "        # If show visualization is enabled, create visualizations\n",
    "        if SHOW_VISUALIZATION:\n",
    "            print(\"\\nCreating visualization for score column comparison...\")\n",
    "            \n",
    "            # Create a heatmap showing model performance by score column\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Access the specific level of the MultiIndex for the optimization metric\n",
    "            metric_values = score_comparison_df.loc[:, (sort_metric, slice(None))]\n",
    "            \n",
    "            # Flatten the MultiIndex columns\n",
    "            metric_values.columns = metric_values.columns.get_level_values(1)\n",
    "            \n",
    "            # Create the heatmap\n",
    "            sns.heatmap(metric_values, annot=True, cmap='YlGnBu', fmt='.3f',\n",
    "                        linewidths=0.5, cbar_kws={'label': OPTIMIZATION_METRIC})\n",
    "            \n",
    "            plt.title(f'Model Performance by Score Column ({OPTIMIZATION_METRIC} Score)\\nProject: {NEO4J_PROJECT_NAME}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create combined bar chart for top models\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Get the top 5 model/score combinations\n",
    "            top5 = best_thresholds_df.sort_values(sort_metric, ascending=False).head(5)\n",
    "            \n",
    "            # Create combined labels\n",
    "            labels = [f\"{row['model_name'].split('/')[-1]}\\n({row['score_column']})\" for _, row in top5.iterrows()]\n",
    "            \n",
    "            # Plot bars for key metrics\n",
    "            metrics = ['precision', 'recall', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "            metric_labels = ['Precision', 'Recall', 'F1 Score', 'F2 Score', 'Matthews Corr']\n",
    "            bar_width = 0.15\n",
    "            \n",
    "            x = np.arange(len(labels))\n",
    "            \n",
    "            for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "                plt.bar(x + (i - 2) * bar_width, top5[metric], width=bar_width, \n",
    "                        label=label, zorder=2)\n",
    "            \n",
    "            # Set chart properties\n",
    "            plt.xlabel('Model + Score Column', fontsize=12)\n",
    "            plt.ylabel('Score', fontsize=12)\n",
    "            plt.title(f'Top 5 Model + Score Column Combinations\\nProject: {NEO4J_PROJECT_NAME}', fontsize=14)\n",
    "            plt.xticks(x, labels, rotation=45, ha='right')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.3, zorder=1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo model evaluation results available. Please run Cell 6 first to perform threshold optimization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - LLM Meta Judge Metrics Comparison Whisker Chart with Confidence Intervals\n",
    "# Purpose: Create box plots with statistical heatmap and confidence intervals for metrics\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy\n",
    "# Breadcrumbs: Visualization -> LLM Metrics Distribution -> Confidence Intervals\n",
    "# notebooks/07_Meta_Judge_Analysis_Notebook.ipynb\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def extract_llm_name(model_metrics_df):\n",
    "    \"\"\"\n",
    "    Extract the LLM name from the model data\n",
    "    \n",
    "    Parameters:\n",
    "        model_metrics_df: DataFrame containing model metrics\n",
    "        \n",
    "    Returns:\n",
    "        str: Name of the LLM used in the analysis\n",
    "    \"\"\"\n",
    "    if model_metrics_df.empty:\n",
    "        return \"Unknown LLM\"\n",
    "    \n",
    "    # Try to extract LLM name from model_name column\n",
    "    if 'model_name' in model_metrics_df.columns:\n",
    "        # Look for common LLM names in the model_name column\n",
    "        model_names = model_metrics_df['model_name'].astype(str).tolist()\n",
    "        \n",
    "        # Try to detect the LLM from the model names\n",
    "        llm_indicators = {\n",
    "            'gpt-4o': 'gpt-4o',\n",
    "            'gpt-4': 'gpt-4',\n",
    "            'gpt-3': 'gpt-3.5',\n",
    "            'claude': 'claude',\n",
    "            'llama': 'llama',\n",
    "            'falcon': 'falcon',\n",
    "            'palm': 'palm',\n",
    "            'gemini': 'gemini',\n",
    "            'mistral': 'mistral'\n",
    "        }\n",
    "        \n",
    "        # Check if any of the indicators appear in the model names\n",
    "        for model_name in model_names:\n",
    "            for indicator, llm_name in llm_indicators.items():\n",
    "                if indicator.lower() in model_name.lower():\n",
    "                    return llm_name\n",
    "        \n",
    "        # If no specific LLM detected but model names exist, return the first one\n",
    "        if model_names:\n",
    "            # Extract first part of the model name if it contains a slash\n",
    "            first_model = model_names[0]\n",
    "            if '/' in first_model:\n",
    "                return first_model.split('/')[0]\n",
    "            return first_model\n",
    "    \n",
    "    # Check if LLM_NAME exists in globals\n",
    "    if 'LLM_NAME' in globals():\n",
    "        return globals()['LLM_NAME']\n",
    "    \n",
    "    # Look for llm column or property\n",
    "    for col in model_metrics_df.columns:\n",
    "        if 'llm' in col.lower():\n",
    "            unique_values = model_metrics_df[col].unique()\n",
    "            if len(unique_values) > 0 and pd.notna(unique_values[0]):\n",
    "                return str(unique_values[0])\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"gpt-4o\"\n",
    "\n",
    "def create_llm_metrics_whisker_plot_with_ci(model_metrics_df=None, project_name=None, show_visualization=True):\n",
    "    \"\"\"\n",
    "    Create whisker plots showing the distribution of metrics with confidence intervals\n",
    "    \n",
    "    Parameters:\n",
    "        model_metrics_df: DataFrame containing LLM model metrics (default: best_thresholds_df)\n",
    "        project_name: Name of the project for visualization titles\n",
    "        show_visualization: Whether to display visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing plotting data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global variables if parameters not provided\n",
    "        _model_metrics_df = model_metrics_df if model_metrics_df is not None else globals().get('best_thresholds_df', pd.DataFrame())\n",
    "        _project_name = project_name if project_name is not None else globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "        _show_visualization = show_visualization if show_visualization is not None else globals().get('SHOW_VISUALIZATION', True)\n",
    "        \n",
    "        if _model_metrics_df.empty:\n",
    "            print(\"No LLM meta judge metrics data available. Please run model evaluation first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Extract the LLM name from the data\n",
    "        llm_name = extract_llm_name(_model_metrics_df)\n",
    "        print(f\"Detected LLM: {llm_name}\")\n",
    "            \n",
    "        # Select the metrics we want to visualize\n",
    "        metrics_to_plot = [\n",
    "            'accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "            'f1_score', 'f2_score', 'matthews_corr'\n",
    "        ]\n",
    "        \n",
    "        # Define human-readable names for the metrics\n",
    "        metric_names = {\n",
    "            'accuracy': 'Accuracy',\n",
    "            'balanced_accuracy': 'Balanced Accuracy',\n",
    "            'precision': 'Precision',\n",
    "            'recall': 'Recall',\n",
    "            'f1_score': 'F1 Score',\n",
    "            'f2_score': 'F2 Score',\n",
    "            'matthews_corr': 'Matthews Correlation'\n",
    "        }\n",
    "        \n",
    "        # Reshape data from wide to long format for easier plotting with seaborn\n",
    "        plot_data = pd.melt(\n",
    "            _model_metrics_df, \n",
    "            id_vars=['model_name', 'score_column'], \n",
    "            value_vars=metrics_to_plot,\n",
    "            var_name='metric', \n",
    "            value_name='score'\n",
    "        )\n",
    "        \n",
    "        # Map metric names to their human-readable versions\n",
    "        plot_data['metric'] = plot_data['metric'].map(metric_names)\n",
    "        \n",
    "        # Add confidence interval data if available\n",
    "        ci_data = {}\n",
    "        for metric in metrics_to_plot:\n",
    "            metric_display = metric_names[metric]\n",
    "            ci_data[metric_display] = {\n",
    "                'lower': [],\n",
    "                'upper': [],\n",
    "                'has_ci': False\n",
    "            }\n",
    "            \n",
    "            # Check for CI columns\n",
    "            ci_lower_col = f\"{metric}_ci_lower\"\n",
    "            ci_upper_col = f\"{metric}_ci_upper\"\n",
    "            \n",
    "            if ci_lower_col in _model_metrics_df.columns and ci_upper_col in _model_metrics_df.columns:\n",
    "                ci_data[metric_display]['lower'] = _model_metrics_df[ci_lower_col].dropna().values\n",
    "                ci_data[metric_display]['upper'] = _model_metrics_df[ci_upper_col].dropna().values\n",
    "                ci_data[metric_display]['has_ci'] = len(ci_data[metric_display]['lower']) > 0\n",
    "        \n",
    "        # Calculate metric statistics for the heatmap\n",
    "        stats_list = ['min', '25%', 'mean', '50%', '75%', 'max', 'std']\n",
    "        stats_names = ['Min', 'Q1', 'Mean', 'Median', 'Q3', 'Max', 'Std Dev']\n",
    "        \n",
    "        # Create stats DataFrames\n",
    "        stats_values = {}\n",
    "        for metric in metrics_to_plot:\n",
    "            metric_display_name = metric_names[metric]\n",
    "            stats = _model_metrics_df[metric].describe()\n",
    "            stats_values[metric_display_name] = [stats[stat] for stat in stats_list]\n",
    "        \n",
    "        # Create numeric DataFrame for heatmap coloring\n",
    "        stats_df_numeric = pd.DataFrame(stats_values, index=stats_names)\n",
    "        \n",
    "        # Create formatted DataFrame\n",
    "        formatted_data = {}\n",
    "        for col in stats_df_numeric.columns:\n",
    "            formatted_data[col] = []\n",
    "            for idx in stats_names:\n",
    "                value = stats_df_numeric.loc[idx, col]\n",
    "                if idx == 'Std Dev':\n",
    "                    if value < 0.001:\n",
    "                        formatted_data[col].append(f\"{value:.2e}\")\n",
    "                    else:\n",
    "                        formatted_data[col].append(f\"{value:.3f}\")\n",
    "                else:\n",
    "                    formatted_data[col].append(f\"{value:.3f}\")\n",
    "        \n",
    "        stats_df_formatted = pd.DataFrame(formatted_data, index=stats_names)\n",
    "        \n",
    "        # Create a figure with two subplots\n",
    "        fig = plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Create grid for the plots\n",
    "        gs = plt.GridSpec(2, 1, height_ratios=[2, 1], hspace=0.3)\n",
    "        \n",
    "        # Box plot subplot\n",
    "        ax_box = fig.add_subplot(gs[0])\n",
    "        \n",
    "        # Create box plot\n",
    "        box_plot = sns.boxplot(\n",
    "            x='metric', \n",
    "            y='score', \n",
    "            data=plot_data, \n",
    "            ax=ax_box,\n",
    "            color='lightblue',\n",
    "            width=0.5\n",
    "        )\n",
    "        \n",
    "        # Add confidence intervals if available\n",
    "        for i, metric_display in enumerate(metric_names.values()):\n",
    "            if metric_display in ci_data and ci_data[metric_display]['has_ci']:\n",
    "                # Get CI bounds\n",
    "                lower_bounds = ci_data[metric_display]['lower']\n",
    "                upper_bounds = ci_data[metric_display]['upper']\n",
    "                \n",
    "                # Calculate mean CI width\n",
    "                if len(lower_bounds) > 0 and len(upper_bounds) > 0:\n",
    "                    mean_lower = np.mean(lower_bounds)\n",
    "                    mean_upper = np.mean(upper_bounds)\n",
    "                    \n",
    "                    # Add CI bar\n",
    "                    ax_box.plot([i, i], [mean_lower, mean_upper], \n",
    "                               color='darkred', linewidth=3, alpha=0.7, \n",
    "                               solid_capstyle='round')\n",
    "                    \n",
    "                    # Add CI caps\n",
    "                    cap_width = 0.1\n",
    "                    ax_box.plot([i-cap_width, i+cap_width], [mean_lower, mean_lower], \n",
    "                               color='darkred', linewidth=2, alpha=0.7)\n",
    "                    ax_box.plot([i-cap_width, i+cap_width], [mean_upper, mean_upper], \n",
    "                               color='darkred', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Add individual data points\n",
    "        sns.stripplot(\n",
    "            x='metric', \n",
    "            y='score', \n",
    "            data=plot_data, \n",
    "            jitter=True, \n",
    "            color='navy',\n",
    "            marker='o', \n",
    "            alpha=0.5,\n",
    "            size=4,\n",
    "            ax=ax_box\n",
    "        )\n",
    "        \n",
    "        # Customize the box plot\n",
    "        ax_box.set_title(f'Project: {_project_name} - Distribution of Meta Judge Performance Metrics', fontsize=14)\n",
    "        ax_box.set_xlabel('')\n",
    "        ax_box.set_ylabel('Score', fontsize=12)\n",
    "        ax_box.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax_box.set_ylim(0, 1.0)\n",
    "        \n",
    "        # Add a horizontal line at y=0.5 as a reference\n",
    "        ax_box.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='lightblue', edgecolor='navy', alpha=0.7, label=f'Meta Judge ({llm_name})'),\n",
    "            Line2D([0], [0], color='darkred', linewidth=3, label='95% Confidence Interval')\n",
    "        ]\n",
    "        ax_box.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        # Heatmap subplot\n",
    "        ax_heatmap = fig.add_subplot(gs[1])\n",
    "        \n",
    "        # Create the heatmap\n",
    "        mask = np.isnan(stats_df_numeric.values)\n",
    "        cmap = sns.light_palette(\"steelblue\", as_cmap=True)\n",
    "        sns.heatmap(\n",
    "            stats_df_numeric,\n",
    "            annot=stats_df_formatted.values,\n",
    "            fmt=\"\",\n",
    "            cmap=cmap,\n",
    "            linewidths=0.5,\n",
    "            linecolor='lightgray',\n",
    "            cbar=False,\n",
    "            ax=ax_heatmap,\n",
    "            mask=mask,\n",
    "            annot_kws={\"size\": 10, \"weight\": \"normal\"},\n",
    "            vmin=0,\n",
    "            vmax=1.0\n",
    "        )\n",
    "        \n",
    "        # Customize heatmap appearance\n",
    "        ax_heatmap.set_title(f'Statistical Summary of Meta Judge ({llm_name}) Metrics', fontsize=12)\n",
    "        ax_heatmap.set_xticklabels(ax_heatmap.get_xticklabels(), rotation=0, ha='center')\n",
    "        \n",
    "        # Adjust figure layout\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        \n",
    "        if _show_visualization:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        return {\n",
    "            'plot_data': plot_data,\n",
    "            'stats_df_numeric': stats_df_numeric,\n",
    "            'stats_df_formatted': stats_df_formatted,\n",
    "            'llm_name': llm_name,\n",
    "            'ci_data': ci_data\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating LLM metrics whisker plot: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return {}\n",
    "\n",
    "# Create and display metrics whisker plot with confidence intervals\n",
    "llm_metrics_viz_data = create_llm_metrics_whisker_plot_with_ci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [13] - Store Meta Judge Whisker Chart Data in Neo4j with Correct Model Name\n",
    "# Purpose: Store the metrics statistics data from whisker chart in Neo4j with the specific LLM model name\n",
    "# Dependencies: neo4j, pandas, logging, os, dotenv\n",
    "# Breadcrumbs: Data Storage -> Neo4j Persistence\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def store_meta_judge_whisker_data_in_neo4j(driver=None, project_name=None):\n",
    "    \"\"\"\n",
    "    Store the Meta Judge whisker chart data in Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name: Project name to attach the metrics data to\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load environment variables to get the current model name\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Get the current model variable reference (e.g., OPENAI_MODEL_ID)\n",
    "        model_var_name = os.getenv('CURRENT_MODEL')\n",
    "        \n",
    "        # Get the actual model name value from that variable (e.g., gpt-4o)\n",
    "        model_name = os.getenv(model_var_name)\n",
    "        \n",
    "        # Make sure we have a clean model name without any score columns appended\n",
    "        if model_name and '_' in model_name:\n",
    "            # Check if score column names are appended to the model name\n",
    "            score_keywords = ['total_score', 'is_traceable', 'actor_score', 'judge_score', 'final_score']\n",
    "            for keyword in score_keywords:\n",
    "                if keyword in model_name:\n",
    "                    # Extract the part before the score column\n",
    "                    model_name = model_name.split('_' + keyword)[0]\n",
    "                    break\n",
    "        \n",
    "        print(f\"Current model from environment: {model_name}\")\n",
    "        \n",
    "        # Use global variables if parameters not provided\n",
    "        _driver = driver if driver is not None else globals().get('driver')\n",
    "        _project_name = project_name if project_name is not None else globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "        \n",
    "        if not _driver:\n",
    "            print(\"No Neo4j driver available\")\n",
    "            return False\n",
    "        \n",
    "        # DEBUG: Print all global variables that might contain DataFrame or dict data\n",
    "        print(\"\\n=== DEBUG: Available Global Variables ===\")\n",
    "        potential_data_vars = []\n",
    "        for var_name, var_value in globals().items():\n",
    "            if isinstance(var_value, (dict, pd.DataFrame)) and not var_name.startswith('_'):\n",
    "                var_type = type(var_value).__name__\n",
    "                var_info = \"\"\n",
    "                if isinstance(var_value, pd.DataFrame):\n",
    "                    var_info = f\"DataFrame with shape {var_value.shape}\"\n",
    "                    if 'model_name' in var_value.columns:\n",
    "                        var_info += f\", contains 'model_name' column\"\n",
    "                    if 'metric' in var_value.columns:\n",
    "                        var_info += f\", contains 'metric' column\"\n",
    "                    if 'score_column' in var_value.columns:\n",
    "                        var_info += f\", contains 'score_column' column\"\n",
    "                elif isinstance(var_value, dict):\n",
    "                    var_info = f\"Dict with {len(var_value)} keys\"\n",
    "                    if 'stats_df_numeric' in var_value:\n",
    "                        var_info += f\", contains 'stats_df_numeric'\"\n",
    "                    if 'plot_data' in var_value:\n",
    "                        var_info += f\", contains 'plot_data'\"\n",
    "                \n",
    "                potential_data_vars.append((var_name, var_type, var_info))\n",
    "                print(f\"- {var_name}: {var_type} - {var_info}\")\n",
    "        \n",
    "        # Additional variable names to check beyond our previous list\n",
    "        viz_var_names = [\n",
    "            'metrics_viz_data', 'whisker_chart_data', 'judge_metrics_viz', 'viz_data', 'chart_data',\n",
    "            'results_viz', 'model_metrics_viz', 'model_viz_data', 'llm_metrics_viz', 'meta_judge_viz',\n",
    "            'metrics_data', 'model_metrics', 'meta_metrics', 'metrics_whisker',\n",
    "            # Additional names that might be more specific to this notebook\n",
    "            'judge_results', 'judge_data', 'performance_viz', 'evaluation_data',\n",
    "            # Try looking at variable names containing 'whisker'\n",
    "            *[name for name in globals() if 'whisker' in str(name).lower()],\n",
    "            # Try looking at variable names containing 'viz' or 'vis'\n",
    "            *[name for name in globals() if ('viz' in str(name).lower() or 'vis' in str(name).lower())]\n",
    "        ]\n",
    "        \n",
    "        # Look for whisker chart data from cell 12\n",
    "        metrics_viz_data = None\n",
    "        found_var_name = None\n",
    "        \n",
    "        for var_name in viz_var_names:\n",
    "            if var_name in globals() and globals()[var_name] is not None:\n",
    "                var_value = globals()[var_name]\n",
    "                \n",
    "                # If it's a dictionary, check if it has the expected structure\n",
    "                if isinstance(var_value, dict) and ('stats_df_numeric' in var_value or 'plot_data' in var_value):\n",
    "                    metrics_viz_data = var_value\n",
    "                    found_var_name = var_name\n",
    "                    print(f\"\\nFound promising whisker chart data in variable '{var_name}'\")\n",
    "                    print(f\"Keys: {list(var_value.keys())}\")\n",
    "                    break\n",
    "                \n",
    "                # If it's a DataFrame, it might be the actual stats data\n",
    "                elif isinstance(var_value, pd.DataFrame) and any(col for col in var_value.columns if 'score' in str(col).lower() or 'metric' in str(col).lower()):\n",
    "                    # Create a synthetic metrics_viz_data dict\n",
    "                    print(f\"\\nFound DataFrame in '{var_name}' that might contain metrics data\")\n",
    "                    print(f\"Columns: {list(var_value.columns)}\")\n",
    "                    \n",
    "                    # If this looks like the plot data\n",
    "                    if 'model_name' in var_value.columns and 'metric' in var_value.columns:\n",
    "                        stats_df = var_value.groupby('metric')['score'].agg(['mean', 'min', 'max', 'std']).reset_index()\n",
    "                        metrics_viz_data = {\n",
    "                            'plot_data': var_value,\n",
    "                            'stats_df_numeric': stats_df\n",
    "                        }\n",
    "                        found_var_name = var_name\n",
    "                        print(f\"Created metrics_viz_data from DataFrame in '{var_name}'\")\n",
    "                        break\n",
    "        \n",
    "        # If we still don't have data, look for the raw model metrics data\n",
    "        if metrics_viz_data is None:\n",
    "            # Look for any DataFrame that might contain model metrics\n",
    "            for var_name, var_value in globals().items():\n",
    "                if isinstance(var_value, pd.DataFrame) and not var_name.startswith('_'):\n",
    "                    # Look for common column patterns in model metrics data\n",
    "                    cols = list(var_value.columns)\n",
    "                    if ('model_name' in cols or 'model' in cols) and len(var_value) > 0:\n",
    "                        print(f\"\\nFound potential model metrics DataFrame in '{var_name}'\")\n",
    "                        print(f\"Columns: {cols}\")\n",
    "                        print(f\"Sample data:\\n{var_value.head(2)}\")\n",
    "                        \n",
    "                        # Use this as our data source\n",
    "                        if 'model_name' not in cols and 'model' in cols:\n",
    "                            var_value = var_value.rename(columns={'model': 'model_name'})\n",
    "                        \n",
    "                        # If we have numeric columns, we can create a stats DataFrame\n",
    "                        numeric_cols = var_value.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                        if numeric_cols and 'model_name' in var_value.columns:\n",
    "                            # Calculate stats (min, max, mean, etc.) for numeric columns\n",
    "                            stats_df = pd.DataFrame()\n",
    "                            for col in numeric_cols:\n",
    "                                if col != 'model_name':\n",
    "                                    stats = var_value[col].agg(['mean', 'min', 'max', 'std']).reset_index()\n",
    "                                    stats.columns = ['Statistic', col]\n",
    "                                    if stats_df.empty:\n",
    "                                        stats_df = stats\n",
    "                                    else:\n",
    "                                        stats_df = pd.merge(stats_df, stats, on='Statistic')\n",
    "                            \n",
    "                            if not stats_df.empty:\n",
    "                                stats_df = stats_df.set_index('Statistic')\n",
    "                                # Create our viz data structure\n",
    "                                metrics_viz_data = {\n",
    "                                    'plot_data': var_value,\n",
    "                                    'stats_df_numeric': stats_df\n",
    "                                }\n",
    "                                found_var_name = var_name\n",
    "                                print(f\"Created stats from numeric columns in '{var_name}'\")\n",
    "                                break\n",
    "        \n",
    "        if metrics_viz_data is None:\n",
    "            print(\"\\nCould not find whisker chart data in global variables\")\n",
    "            print(\"Available variables that might contain the data:\")\n",
    "            for var_name, var_type, var_info in potential_data_vars:\n",
    "                print(f\"- {var_name}: {var_type} - {var_info}\")\n",
    "            print(\"\\nTry running cell 11 first to generate the whisker chart data\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\nUsing data from '{found_var_name}'\")\n",
    "        \n",
    "        # Check if we have the stats data\n",
    "        if 'stats_df_numeric' not in metrics_viz_data or metrics_viz_data['stats_df_numeric'].empty:\n",
    "            print(\"No statistical metrics data found in the data structure\")\n",
    "            return False\n",
    "        \n",
    "        # Get the stats DataFrame\n",
    "        stats_df = metrics_viz_data['stats_df_numeric']\n",
    "        print(f\"\\nStats DataFrame shape: {stats_df.shape}\")\n",
    "        print(f\"Stats DataFrame columns: {list(stats_df.columns)}\")\n",
    "        print(f\"Stats DataFrame index: {list(stats_df.index)}\")\n",
    "        \n",
    "        # Prepare data for Neo4j storage - convert stats dataframe to dictionary\n",
    "        metrics_data = {}\n",
    "        for column in stats_df.columns:\n",
    "            # Each column is a metric\n",
    "            metric_stats = {}\n",
    "            for idx, value in stats_df[column].items():\n",
    "                # Each row is a statistic (min, max, etc.)\n",
    "                # Convert to standard lowercase keys without spaces\n",
    "                key = str(idx).lower().replace(' ', '_')\n",
    "                try:\n",
    "                    metric_stats[key] = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    metric_stats[key] = str(value)\n",
    "            \n",
    "            # Add metric stats to overall data\n",
    "            metrics_data[column.lower().replace(' ', '_')] = metric_stats\n",
    "        \n",
    "        # Also add the number of models analyzed\n",
    "        if 'plot_data' in metrics_viz_data and not metrics_viz_data['plot_data'].empty:\n",
    "            try:\n",
    "                model_col = None\n",
    "                for col in ['model_name', 'model', 'llm_model', 'name']:\n",
    "                    if col in metrics_viz_data['plot_data'].columns:\n",
    "                        model_col = col\n",
    "                        break\n",
    "                \n",
    "                if model_col:\n",
    "                    model_count = metrics_viz_data['plot_data'][model_col].nunique()\n",
    "                    metrics_data['model_count'] = model_count\n",
    "                    print(f\"Found {model_count} unique models in the data\")\n",
    "                else:\n",
    "                    print(\"Could not find model name column in plot data\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating model count: {str(e)}\")\n",
    "        \n",
    "        # Ensure we store only the clean model name without any score column appendages\n",
    "        clean_model_name = model_name\n",
    "        if '_' in clean_model_name:\n",
    "            # Additional safeguard to ensure no score metrics in model name\n",
    "            score_columns = ['total_score', 'total_score_with_final', 'total_combined_score', \n",
    "                            'is_traceable', 'actor_score', 'judge_score', 'final_score']\n",
    "            for score in score_columns:\n",
    "                if f\"_{score}\" in clean_model_name:\n",
    "                    clean_model_name = clean_model_name.split(f\"_{score}\")[0]\n",
    "        \n",
    "        # Add the clean model name to the metrics data\n",
    "        metrics_data['current_model'] = clean_model_name\n",
    "        \n",
    "        # Create results_data dictionary to store TP, FP, FN, TN for each model\n",
    "        results_data = {}\n",
    "        \n",
    "        # If we have best_thresholds_df, extract the confusion matrix data (TP, FP, FN, TN)\n",
    "        if 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "            best_df = globals()['best_thresholds_df']\n",
    "            print(f\"\\nExtracting confusion matrix data from best_thresholds_df with shape {best_df.shape}\")\n",
    "            \n",
    "            # Confusion matrix column names mapping - USING FULL NAMES instead of abbreviations\n",
    "            cm_columns = {\n",
    "                'true_positives': 'true_positives',\n",
    "                'false_positives': 'false_positives',\n",
    "                'false_negatives': 'false_negatives',\n",
    "                'true_negatives': 'true_negatives',\n",
    "                # Handle alternative column names by mapping them to full names\n",
    "                'tp': 'true_positives',\n",
    "                'fp': 'false_positives',\n",
    "                'fn': 'false_negatives',\n",
    "                'tn': 'true_negatives'\n",
    "            }\n",
    "            \n",
    "            # Check which confusion matrix columns are available\n",
    "            available_cm_cols = [col for col in cm_columns.keys() if col in best_df.columns]\n",
    "            \n",
    "            if available_cm_cols:\n",
    "                # For each model+score_column combination, store its confusion matrix data\n",
    "                for _, row in best_df.iterrows():\n",
    "                    # Create a unique key for this model+score combination\n",
    "                    if 'model_name' in row and 'score_column' in row:\n",
    "                        model_key = str(row['score_column'])  # Use only the score_column as the key\n",
    "                    elif 'score_column' in row:\n",
    "                        model_key = str(row['score_column'])\n",
    "                    elif 'model_name' in row:\n",
    "                        model_key = str(row['model_name'])\n",
    "                    elif 'model' in row:\n",
    "                        model_key = str(row['model'])\n",
    "                    else:\n",
    "                        # Default fallback key\n",
    "                        model_key = f\"model_{_}\"\n",
    "                    \n",
    "                    # Standardize key formatting\n",
    "                    model_key = model_key.replace('/', '_').replace(' ', '_')\n",
    "                    \n",
    "                    # Store confusion matrix data for this model\n",
    "                    results_data[model_key] = {}\n",
    "                    \n",
    "                    # Extract confusion matrix values\n",
    "                    for orig_col, result_col in cm_columns.items():\n",
    "                        if orig_col in row:\n",
    "                            val = row[orig_col]\n",
    "                            if pd.notna(val):\n",
    "                                results_data[model_key][result_col] = int(val)\n",
    "                            else:\n",
    "                                results_data[model_key][result_col] = 0\n",
    "                    \n",
    "                    # Add threshold if available\n",
    "                    if 'best_threshold' in row:\n",
    "                        results_data[model_key]['threshold'] = float(row['best_threshold']) if pd.notna(row['best_threshold']) else 0.0\n",
    "                    \n",
    "                    # Add score_column if available\n",
    "                    if 'score_column' in row:\n",
    "                        results_data[model_key]['score_type'] = str(row['score_column'])\n",
    "                \n",
    "                print(f\"  Added confusion matrix data for {len(results_data)} score methods\")\n",
    "                # Show a few examples\n",
    "                for model_name, data in list(results_data.items())[:3]:\n",
    "                    # Display using the full names for confusion matrix values\n",
    "                    confusion_info = \", \".join([f\"{k}={v}\" for k, v in data.items() \n",
    "                                              if k in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']])\n",
    "                    print(f\"    {model_name}: {confusion_info}\")\n",
    "                if len(results_data) > 3:\n",
    "                    print(f\"    ... and {len(results_data) - 3} more\")\n",
    "            else:\n",
    "                print(\"No confusion matrix columns found in best_thresholds_df\")\n",
    "        \n",
    "        # If we don't have confusion matrix data from best_thresholds_df, try to find it elsewhere\n",
    "        if not results_data:\n",
    "            # Try to find confusion matrix data in other potential sources\n",
    "            for var_name in ['performance_df', 'performance_comparison_df', 'top5', 'top_combinations']:\n",
    "                if var_name in globals() and isinstance(globals()[var_name], pd.DataFrame) and not globals()[var_name].empty:\n",
    "                    df = globals()[var_name]\n",
    "                    \n",
    "                    # Check for confusion matrix columns\n",
    "                    cm_columns = {\n",
    "                        'true_positives': 'true_positives',\n",
    "                        'false_positives': 'false_positives',\n",
    "                        'false_negatives': 'false_negatives',\n",
    "                        'true_negatives': 'true_negatives',\n",
    "                        # Handle alternative column names by mapping them to full names\n",
    "                        'tp': 'true_positives',\n",
    "                        'fp': 'false_positives',\n",
    "                        'fn': 'false_negatives',\n",
    "                        'tn': 'true_negatives'\n",
    "                    }\n",
    "                    \n",
    "                    available_cm_cols = [col for col in cm_columns.keys() if col in df.columns]\n",
    "                    \n",
    "                    if available_cm_cols:\n",
    "                        print(f\"\\nFound confusion matrix data in {var_name} with columns: {available_cm_cols}\")\n",
    "                        \n",
    "                        for _, row in df.iterrows():\n",
    "                            # Create a unique key for this model+score combination\n",
    "                            if 'score_column' in row:\n",
    "                                model_key = str(row['score_column'])  # Use only the score_column as the key\n",
    "                            elif 'model_name' in row:\n",
    "                                model_key = str(row['model_name'])\n",
    "                            elif 'model' in row:\n",
    "                                model_key = str(row['model'])\n",
    "                            else:\n",
    "                                # Default fallback key\n",
    "                                model_key = f\"model_{_}\"\n",
    "                            \n",
    "                            # Standardize key formatting\n",
    "                            model_key = model_key.replace('/', '_').replace(' ', '_')\n",
    "                            \n",
    "                            # Store confusion matrix data for this model\n",
    "                            results_data[model_key] = {}\n",
    "                            \n",
    "                            # Extract confusion matrix values\n",
    "                            for orig_col, result_col in cm_columns.items():\n",
    "                                if orig_col in row:\n",
    "                                    val = row[orig_col]\n",
    "                                    if pd.notna(val):\n",
    "                                        results_data[model_key][result_col] = int(val)\n",
    "                                    else:\n",
    "                                        results_data[model_key][result_col] = 0\n",
    "                            \n",
    "                            # Add threshold if available\n",
    "                            if 'best_threshold' in row:\n",
    "                                results_data[model_key]['threshold'] = float(row['best_threshold']) if pd.notna(row['best_threshold']) else 0.0\n",
    "                            elif 'threshold' in row:\n",
    "                                results_data[model_key]['threshold'] = float(row['threshold']) if pd.notna(row['threshold']) else 0.0\n",
    "                            \n",
    "                            # Add score_column if available\n",
    "                            if 'score_column' in row:\n",
    "                                results_data[model_key]['score_type'] = str(row['score_column'])\n",
    "                        \n",
    "                        print(f\"  Added confusion matrix data for {len(results_data)} score methods\")\n",
    "                        # Show a few examples\n",
    "                        for model_name, data in list(results_data.items())[:3]:\n",
    "                            # Display using the full names for confusion matrix values\n",
    "                            confusion_info = \", \".join([f\"{k}={v}\" for k, v in data.items() \n",
    "                                                      if k in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']])\n",
    "                            print(f\"    {model_name}: {confusion_info}\")\n",
    "                        if len(results_data) > 3:\n",
    "                            print(f\"    ... and {len(results_data) - 3} more\")\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "            if not results_data:\n",
    "                print(\"\\nNo confusion matrix data found in available dataframes.\")\n",
    "                # Creating a placeholder results_data to avoid empty field\n",
    "                results_data = {\"no_data\": {\"info\": \"No confusion matrix data available\"}}\n",
    "        \n",
    "        # Add model_data with the actual whisker chart data points\n",
    "        model_data = {}\n",
    "        \n",
    "        # Define the scoring method names we're looking for\n",
    "        scoring_methods = [\n",
    "            'is_traceable',\n",
    "            'actor_score',\n",
    "            'judge_score',\n",
    "            'final_score',\n",
    "            'total_score',\n",
    "            'total_score_with_final',\n",
    "            'total_score_all',\n",
    "            'total_combined_score'\n",
    "        ]\n",
    "        \n",
    "        # First try to find best_thresholds_df which should have the proper score names\n",
    "        if 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "            best_df = globals()['best_thresholds_df']\n",
    "            print(f\"\\nFound best_thresholds_df with shape {best_df.shape}\")\n",
    "            \n",
    "            # Check if it has the score_column we need\n",
    "            if 'score_column' in best_df.columns:\n",
    "                print(f\"Score columns found: {best_df['score_column'].unique()}\")\n",
    "                \n",
    "                # Metrics we want to capture\n",
    "                metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                           'f1_score', 'f2_score', 'matthews_corr']\n",
    "                \n",
    "                # Initialize the model_data structure\n",
    "                for metric in metrics:\n",
    "                    metric_key = metric.lower().replace(' ', '_')\n",
    "                    model_data[metric_key] = {}\n",
    "                \n",
    "                # For each scoring method, get the values for each metric\n",
    "                for _, row in best_df.iterrows():\n",
    "                    score_method = row['score_column']\n",
    "                    \n",
    "                    for metric in metrics:\n",
    "                        if metric in row:\n",
    "                            metric_key = metric.lower().replace(' ', '_')\n",
    "                            model_data[metric_key][score_method] = float(row[metric])\n",
    "                \n",
    "                print(f\"Extracted data for {len(best_df)} scoring methods across {len(metrics)} metrics\")\n",
    "            else:\n",
    "                print(\"best_thresholds_df doesn't have a score_column - looking for alternative data sources\")\n",
    "        \n",
    "        # If we couldn't find best_thresholds_df or it doesn't have the right structure,\n",
    "        # try looking for alternative data sources\n",
    "        if not model_data:\n",
    "            # Try to find performance_df or top5 which often has the right structure\n",
    "            for var_name in ['performance_df', 'top5', 'top_combinations', 'performance_comparison_df']:\n",
    "                if var_name in globals() and not globals()[var_name].empty:\n",
    "                    df = globals()[var_name]\n",
    "                    print(f\"\\nFound {var_name} with shape {df.shape}\")\n",
    "                    \n",
    "                    # Check if it has score_column\n",
    "                    if 'score_column' in df.columns:\n",
    "                        print(f\"Score columns found: {df['score_column'].unique()}\")\n",
    "                        \n",
    "                        # Metrics we want to capture\n",
    "                        metrics = [col for col in df.columns if col.lower() in \n",
    "                                  ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                                   'f1_score', 'f2_score', 'matthews_corr']]\n",
    "                        \n",
    "                        # Initialize the model_data structure\n",
    "                        for metric in metrics:\n",
    "                            metric_key = metric.lower().replace(' ', '_')\n",
    "                            model_data[metric_key] = {}\n",
    "                        \n",
    "                        # For each scoring method, get the values for each metric\n",
    "                        for _, row in df.iterrows():\n",
    "                            score_method = row['score_column']\n",
    "                            \n",
    "                            for metric in metrics:\n",
    "                                metric_key = metric.lower().replace(' ', '_')\n",
    "                                model_data[metric_key][score_method] = float(row[metric])\n",
    "                        \n",
    "                        print(f\"Extracted data for {len(df)} scoring methods across {len(metrics)} metrics\")\n",
    "                        break\n",
    "            \n",
    "            # If we still don't have data, try to use plot_data from the whisker chart\n",
    "            if not model_data and 'plot_data' in metrics_viz_data and not metrics_viz_data['plot_data'].empty:\n",
    "                plot_df = metrics_viz_data['plot_data']\n",
    "                print(f\"\\nExamining plot_data from whisker chart with shape {plot_df.shape}\")\n",
    "                \n",
    "                # If plot_data has both 'metric' and 'score_column'\n",
    "                if 'metric' in plot_df.columns and 'score_column' in plot_df.columns:\n",
    "                    print(f\"Metric and score columns found in plot_data\")\n",
    "                    \n",
    "                    metrics = plot_df['metric'].unique()\n",
    "                    for metric in metrics:\n",
    "                        metric_key = metric.lower().replace(' ', '_')\n",
    "                        model_data[metric_key] = {}\n",
    "                        \n",
    "                        # Get data for this metric\n",
    "                        metric_data = plot_df[plot_df['metric'] == metric]\n",
    "                        \n",
    "                        # Store scores by scoring method\n",
    "                        for _, row in metric_data.iterrows():\n",
    "                            score_method = row['score_column']\n",
    "                            model_data[metric_key][score_method] = float(row['score'])\n",
    "                    \n",
    "                    print(f\"Extracted data for {len(metrics)} metrics\")\n",
    "                \n",
    "                # If plot_data doesn't have score_column but has metric\n",
    "                elif 'metric' in plot_df.columns:\n",
    "                    print(f\"Only metric column found in plot_data - trying to match scores\")\n",
    "                    \n",
    "                    # We'll need to match the scores to methods using best_thresholds_df or similar\n",
    "                    metrics = plot_df['metric'].unique()\n",
    "                    for metric in metrics:\n",
    "                        metric_key = metric.lower().replace(' ', '_')\n",
    "                        model_data[metric_key] = {}\n",
    "                        \n",
    "                        # Get scores for this metric\n",
    "                        scores = plot_df[plot_df['metric'] == metric]['score'].values\n",
    "                        \n",
    "                        # Try to match scores to methods using other data sources\n",
    "                        if 'best_thresholds_df' in globals():\n",
    "                            best_df = globals()['best_thresholds_df']\n",
    "                            # If we have best_thresholds_df with this metric and score_column\n",
    "                            if metric_key in best_df.columns and 'score_column' in best_df.columns:\n",
    "                                # Match scores to methods\n",
    "                                for score in scores:\n",
    "                                    # Find the scoring method with the closest score\n",
    "                                    closest_row = best_df.iloc[(best_df[metric_key] - score).abs().argsort()[0]]\n",
    "                                    score_method = closest_row['score_column']\n",
    "                                    model_data[metric_key][score_method] = float(score)\n",
    "                            else:\n",
    "                                # Fallback to standard scoring method names if we can't match\n",
    "                                for i, score in enumerate(scores):\n",
    "                                    if i < len(scoring_methods):\n",
    "                                        model_data[metric_key][scoring_methods[i]] = float(score)\n",
    "                                    else:\n",
    "                                        model_data[metric_key][f\"score_{i}\"] = float(score)\n",
    "                        else:\n",
    "                            # Fallback to standard scoring method names if we can't match\n",
    "                            for i, score in enumerate(scores):\n",
    "                                if i < len(scoring_methods):\n",
    "                                    model_data[metric_key][scoring_methods[i]] = float(score)\n",
    "                                else:\n",
    "                                    model_data[metric_key][f\"score_{i}\"] = float(score)\n",
    "                    \n",
    "                    print(f\"Extracted data for {len(metrics)} metrics with estimated scoring method mapping\")\n",
    "        \n",
    "        # If we still don't have data, we need to fall back to a more direct approach\n",
    "        if not model_data:\n",
    "            print(\"\\nCould not find structured data for the whisker chart points. Using direct method.\")\n",
    "            \n",
    "            # Directly access potentially relevant datasets in a specific order\n",
    "            # Getting data directly from best_thresholds_df\n",
    "            if 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "                df = globals()['best_thresholds_df']\n",
    "                print(f\"Trying direct extraction from best_thresholds_df with shape {df.shape}\")\n",
    "                \n",
    "                # These are the metrics we want to extract\n",
    "                metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                          'f1_score', 'f2_score', 'matthews_corr']\n",
    "                \n",
    "                # Initialize each metric in model_data\n",
    "                for metric in metrics:\n",
    "                    metric_key = metric.lower().replace(' ', '_')\n",
    "                    model_data[metric_key] = {}\n",
    "                \n",
    "                # Try to use the proper scoring method names\n",
    "                # First see if we have score_column in the DataFrame\n",
    "                if 'score_column' in df.columns:\n",
    "                    # We can use the actual scoring method names\n",
    "                    for i, row in df.iterrows():\n",
    "                        score_method = row['score_column']\n",
    "                        \n",
    "                        for metric in metrics:\n",
    "                            if metric in df.columns:\n",
    "                                metric_key = metric.lower().replace(' ', '_')\n",
    "                                model_data[metric_key][score_method] = float(row[metric])\n",
    "                    \n",
    "                    print(f\"Extracted data using actual scoring method names\")\n",
    "                else:\n",
    "                    # We need to map rows to our scoring method names\n",
    "                    # Assuming rows are in the same order as our scoring_methods list\n",
    "                    for i, row in df.iterrows():\n",
    "                        if i < len(scoring_methods):\n",
    "                            score_method = scoring_methods[i]\n",
    "                        else:\n",
    "                            score_method = f\"score_{i}\"\n",
    "                            \n",
    "                        for metric in metrics:\n",
    "                            if metric in df.columns:\n",
    "                                metric_key = metric.lower().replace(' ', '_')\n",
    "                                model_data[metric_key][score_method] = float(row[metric])\n",
    "                    \n",
    "                    print(f\"Extracted data with mapped scoring method names\")\n",
    "        \n",
    "        # Print a summary of what we found\n",
    "        print(\"\\nModel data summary:\")\n",
    "        for metric, values in model_data.items():\n",
    "            print(f\"  {metric}: {len(values)} data points\")\n",
    "            for score_method, value in list(values.items())[:3]:  # Show first 3 as example\n",
    "                print(f\"    {score_method}: {value}\")\n",
    "            if len(values) > 3:\n",
    "                print(f\"    ... and {len(values) - 3} more\")\n",
    "        \n",
    "        # Print the model name we're going to use\n",
    "        print(f\"\\nUsing clean model name for Neo4j storage: '{clean_model_name}'\")\n",
    "        \n",
    "        # Add model_data to metrics_data\n",
    "        metrics_data['model_data'] = model_data\n",
    "        \n",
    "        # Serialize all data to JSON for Neo4j storage\n",
    "        metrics_json = json.dumps(metrics_data)\n",
    "        model_data_json = json.dumps(model_data)\n",
    "        results_json = json.dumps(results_data)\n",
    "        \n",
    "        # Current timestamp for the analysis record\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Cypher query to create metrics data connected to project\n",
    "        # Using stable identifiers to avoid duplication\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})\n",
    "        MERGE (m:MetricsAnalysis {project_name: $project_name, model_type: $model_type, analysis_type: 'whisker_chart'})\n",
    "        MERGE (p)-[r:HAS_METRICS_ANALYSIS {model_type: $model_type}]->(m)\n",
    "        SET m.metrics_data = $metrics_data,\n",
    "            m.model_count = $model_count,\n",
    "            m.model_data = $model_data,\n",
    "            m.results = $results_data,\n",
    "            m.created_at = CASE WHEN m.created_at IS NULL THEN $timestamp ELSE m.created_at END,\n",
    "            m.last_updated = $timestamp,\n",
    "            r.timestamp = $timestamp,\n",
    "            r.last_updated = $timestamp\n",
    "        RETURN p.name as project_name, m.created_at as created_at\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute query to store metrics data\n",
    "        with _driver.session() as session:\n",
    "            result = session.run(\n",
    "                query,\n",
    "                project_name=_project_name,\n",
    "                model_type=clean_model_name,  # Use the clean model name without score column appended\n",
    "                timestamp=timestamp,\n",
    "                metrics_data=metrics_json,\n",
    "                model_data=model_data_json,\n",
    "                results_data=results_json,\n",
    "                model_count=metrics_data.get('model_count', 0)\n",
    "            ).single()\n",
    "            \n",
    "            if result:\n",
    "                print(f\"Successfully stored whisker chart data for model {clean_model_name} in project: {result['project_name']}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"No result returned when storing whisker chart data for project: {_project_name}\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error storing whisker chart data in Neo4j: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# Store the whisker chart data in Neo4j\n",
    "print(\"Starting to store whisker chart data in Neo4j...\")\n",
    "success = store_meta_judge_whisker_data_in_neo4j()\n",
    "\n",
    "if success:\n",
    "    # Get the model name again for displaying\n",
    "    load_dotenv()\n",
    "    model_var_name = os.getenv('CURRENT_MODEL')\n",
    "    model_name = os.getenv(model_var_name)\n",
    "    \n",
    "    # Clean model name for display\n",
    "    if model_name and '_' in model_name:\n",
    "        # Check if score column names are appended to the model name\n",
    "        score_keywords = ['total_score', 'is_traceable', 'actor_score', 'judge_score', 'final_score']\n",
    "        for keyword in score_keywords:\n",
    "            if keyword in model_name:\n",
    "                # Extract the part before the score column\n",
    "                model_name = model_name.split('_' + keyword)[0]\n",
    "                break\n",
    "    \n",
    "    print(\"\\nWhisker chart data successfully stored in Neo4j\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "else:\n",
    "    print(\"\\nFailed to store whisker chart data.\")\n",
    "    print(\"See debugging information above for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

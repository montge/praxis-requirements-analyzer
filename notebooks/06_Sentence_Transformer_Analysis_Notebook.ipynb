{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer Requirements Traceability Analysis\n",
    "**Evaluation and comparison of sentence transformer models for requirements traceability with threshold optimization, performance metrics, statistical visualization, and Neo4j integration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings\n",
    "# Dependencies: pandas, numpy, neo4j, sklearn, matplotlib, seaborn\n",
    "# Breadcrumbs: Setup -> Imports\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score,\n",
    "    cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import json\n",
    "from datetime import datetime  # Added global import for datetime\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configure logging and load environment variables\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Neo4j credentials from environment variables\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        'OPTIMIZATION_METRIC': os.getenv('OPTIMIZATION_METRIC', 'F2').upper(),\n",
    "        'SHOW_VISUALIZATION': os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true',\n",
    "        'MATCH_DIRECTION': os.getenv('MATCH_DIRECTION', 'source_to_target')\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Using {config['OPTIMIZATION_METRIC']} score for threshold application in traceability analysis\")\n",
    "    logger.info(f\"Visualization display is set to: {config['SHOW_VISUALIZATION']}\")\n",
    "    print(f\"Visualization setting: {'Enabled' if config['SHOW_VISUALIZATION'] else 'Disabled'}\")\n",
    "    print(f\"Optimization metric: {config['OPTIMIZATION_METRIC']}\")\n",
    "    \n",
    "    return config, logger\n",
    "\n",
    "# Execute setup when imported\n",
    "CONFIG, logger = setup_environment()\n",
    "NEO4J_URI = CONFIG['NEO4J_URI']\n",
    "NEO4J_USER = CONFIG['NEO4J_USER']\n",
    "NEO4J_PASSWORD = CONFIG['NEO4J_PASSWORD']\n",
    "NEO4J_PROJECT_NAME = CONFIG['NEO4J_PROJECT_NAME']\n",
    "OPTIMIZATION_METRIC = CONFIG['OPTIMIZATION_METRIC']\n",
    "SHOW_VISUALIZATION = CONFIG['SHOW_VISUALIZATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Connection Setup\n",
    "# Purpose: Create connection to Neo4j database containing traceability data\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection\n",
    "\n",
    "def create_neo4j_driver(uri=None, user=None, password=None):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance\n",
    "    \n",
    "    Parameters:\n",
    "        uri (str, optional): Neo4j URI. Defaults to NEO4J_URI from environment.\n",
    "        user (str, optional): Neo4j username. Defaults to NEO4J_USER from environment.\n",
    "        password (str, optional): Neo4j password. Defaults to NEO4J_PASSWORD from environment.\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use parameters if provided, otherwise use globals from setup_environment\n",
    "        _uri = uri if uri is not None else NEO4J_URI\n",
    "        _user = user if user is not None else NEO4J_USER\n",
    "        _password = password if password is not None else NEO4J_PASSWORD\n",
    "        \n",
    "        driver = GraphDatabase.driver(_uri, auth=(_user, _password))\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver when this module is imported or run directly\n",
    "driver = create_neo4j_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Query SIMILAR_TO Links\n",
    "# Purpose: Retrieve sentence transformer similarity links from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Acquisition -> Similarity Links\n",
    "\n",
    "def query_similar_to_links(driver):\n",
    "    \"\"\"\n",
    "    Query SIMILAR_TO links from Neo4j, with direction specified by MATCH_DIRECTION\n",
    "    Only includes requirements that have GROUND_TRUTH links\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (combined_df, source_to_target_df, target_to_source_df)\n",
    "            - All three are pandas DataFrames containing similarity links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get match direction from environment\n",
    "        MATCH_DIRECTION = os.getenv('MATCH_DIRECTION', 'source_to_target')\n",
    "        logger.info(f\"Using match direction from .env: '{MATCH_DIRECTION}'\")\n",
    "        \n",
    "        # Validate MATCH_DIRECTION parameter\n",
    "        valid_directions = ['source_to_target', 'target_to_source', 'both']\n",
    "        if MATCH_DIRECTION not in valid_directions:\n",
    "            logger.warning(f\"Invalid MATCH_DIRECTION value: '{MATCH_DIRECTION}'. Using default: 'source_to_target'\")\n",
    "            MATCH_DIRECTION = 'source_to_target'\n",
    "        else:\n",
    "            logger.info(f\"Validating MATCH_DIRECTION parameter: '{MATCH_DIRECTION}'\")\n",
    "        \n",
    "        logger.info(f\"Using MATCH_DIRECTION={MATCH_DIRECTION} for analysis\")\n",
    "        print(f\"DEBUG - MATCH_DIRECTION value: '{MATCH_DIRECTION}'\")\n",
    "        print(f\"DEBUG - Valid directions: {valid_directions}\")\n",
    "        print(f\"MATCH_DIRECTION set to: '{MATCH_DIRECTION}'\")\n",
    "        \n",
    "        # Query for source-to-target links with GROUND_TRUTH filtering\n",
    "        if MATCH_DIRECTION in ['source_to_target', 'both']:\n",
    "            source_to_target_query = \"\"\"\n",
    "            MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source_req:Requirement)-[r:SIMILAR_TO]->(target_req:Requirement)\n",
    "            WHERE source_req.type = 'SOURCE' AND target_req.type = 'TARGET'\n",
    "            AND EXISTS { \n",
    "                MATCH (source_req)-[:GROUND_TRUTH]->() \n",
    "            } \n",
    "            AND EXISTS { \n",
    "                MATCH ()-[:GROUND_TRUTH]->(target_req) \n",
    "            }\n",
    "            RETURN \n",
    "                p.name as project_name,\n",
    "                source_req.id as source_id,\n",
    "                target_req.id as target_id,\n",
    "                r.model as sentence_transformer_model,\n",
    "                r.similarity as similarity_score,\n",
    "                p.name as model_project,\n",
    "                r.timestamp as timestamp,\n",
    "                'source_to_target' as direction\n",
    "            \"\"\"\n",
    "            \n",
    "            with driver.session() as session:\n",
    "                source_to_target_results = session.run(\n",
    "                    source_to_target_query, \n",
    "                    project_name=NEO4J_PROJECT_NAME\n",
    "                ).data()\n",
    "                \n",
    "                if source_to_target_results:\n",
    "                    source_to_target_df = pd.DataFrame(source_to_target_results)\n",
    "                    logger.info(f\"Retrieved {len(source_to_target_df)} SIMILAR_TO links for project: {NEO4J_PROJECT_NAME} in direction: source_to_target\")\n",
    "                else:\n",
    "                    source_to_target_df = pd.DataFrame()\n",
    "                    logger.warning(f\"No SIMILAR_TO links found for project: {NEO4J_PROJECT_NAME} in direction: source_to_target\")\n",
    "        else:\n",
    "            source_to_target_df = pd.DataFrame()\n",
    "        \n",
    "        # Query for target-to-source links with GROUND_TRUTH filtering\n",
    "        if MATCH_DIRECTION in ['target_to_source', 'both']:\n",
    "            target_to_source_query = \"\"\"\n",
    "            MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(target_req:Requirement)-[r:SIMILAR_TO]->(source_req:Requirement)\n",
    "            WHERE target_req.type = 'TARGET' AND source_req.type = 'SOURCE'\n",
    "            AND EXISTS { \n",
    "                MATCH (source_req)-[:GROUND_TRUTH]->() \n",
    "            } \n",
    "            AND EXISTS { \n",
    "                MATCH ()-[:GROUND_TRUTH]->(target_req) \n",
    "            }\n",
    "            RETURN \n",
    "                p.name as project_name,\n",
    "                source_req.id as source_id,\n",
    "                target_req.id as target_id,\n",
    "                r.model as sentence_transformer_model,\n",
    "                r.similarity as similarity_score,\n",
    "                p.name as model_project,\n",
    "                r.timestamp as timestamp,\n",
    "                'target_to_source' as direction\n",
    "            \"\"\"\n",
    "            \n",
    "            with driver.session() as session:\n",
    "                target_to_source_results = session.run(\n",
    "                    target_to_source_query, \n",
    "                    project_name=NEO4J_PROJECT_NAME\n",
    "                ).data()\n",
    "                \n",
    "                if target_to_source_results:\n",
    "                    target_to_source_df = pd.DataFrame(target_to_source_results)\n",
    "                    logger.info(f\"Retrieved {len(target_to_source_df)} SIMILAR_TO links for project: {NEO4J_PROJECT_NAME} in direction: target_to_source\")\n",
    "                else:\n",
    "                    target_to_source_df = pd.DataFrame()\n",
    "                    logger.warning(f\"No SIMILAR_TO links found for project: {NEO4J_PROJECT_NAME} in direction: target_to_source\")\n",
    "        else:\n",
    "            target_to_source_df = pd.DataFrame()\n",
    "            \n",
    "        # Combine results based on MATCH_DIRECTION\n",
    "        if MATCH_DIRECTION == 'source_to_target':\n",
    "            logger.info(\"Using only source_to_target direction as specified\")\n",
    "            similar_to_df = source_to_target_df\n",
    "            similar_to_src_to_tgt_df = source_to_target_df\n",
    "            similar_to_tgt_to_src_df = pd.DataFrame()\n",
    "        elif MATCH_DIRECTION == 'target_to_source':\n",
    "            logger.info(\"Using only target_to_source direction as specified\")\n",
    "            similar_to_df = target_to_source_df\n",
    "            similar_to_src_to_tgt_df = pd.DataFrame()\n",
    "            similar_to_tgt_to_src_df = target_to_source_df\n",
    "        else:  # both\n",
    "            logger.info(\"Using both source_to_target and target_to_source directions\")\n",
    "            similar_to_df = pd.concat([source_to_target_df, target_to_source_df], ignore_index=True)\n",
    "            similar_to_src_to_tgt_df = source_to_target_df\n",
    "            similar_to_tgt_to_src_df = target_to_source_df\n",
    "            \n",
    "        # Count unique models in the dataset\n",
    "        if not similar_to_src_to_tgt_df.empty:\n",
    "            src_to_tgt_models = similar_to_src_to_tgt_df['sentence_transformer_model'].value_counts().to_dict()\n",
    "            logger.info(f\"Sentence transformer models found in SIMILAR_TO links (source_to_target):\")\n",
    "            for model, count in src_to_tgt_models.items():\n",
    "                logger.info(f\"  - {model}: {count} links\")\n",
    "                \n",
    "        if not similar_to_tgt_to_src_df.empty:\n",
    "            tgt_to_src_models = similar_to_tgt_to_src_df['sentence_transformer_model'].value_counts().to_dict()\n",
    "            logger.info(f\"Sentence transformer models found in SIMILAR_TO links (target_to_source):\")\n",
    "            for model, count in tgt_to_src_models.items():\n",
    "                logger.info(f\"  - {model}: {count} links\")\n",
    "        \n",
    "        return similar_to_df, similar_to_src_to_tgt_df, similar_to_tgt_to_src_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying SIMILAR_TO links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Execute the query and get results when imported directly\n",
    "similar_to_df, similar_to_src_to_tgt_df, similar_to_tgt_to_src_df = query_similar_to_links(driver)\n",
    "\n",
    "# Function to display information about the retrieved data\n",
    "def display_similar_to_info():\n",
    "    \"\"\"Display information about the retrieved SIMILAR_TO links data\"\"\"\n",
    "    if not similar_to_df.empty:\n",
    "        print(f\"\\nUsing {'ONLY source_to_target' if similar_to_tgt_to_src_df.empty else 'BOTH'} direction for analysis\")\n",
    "        print(\"\\nDataset Information for Project:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"SIMILAR_TO links (source to target only): {len(similar_to_src_to_tgt_df)}\")\n",
    "        print(f\"SIMILAR_TO links (target to source only): {len(similar_to_tgt_to_src_df)}\")\n",
    "        print(f\"SIMILAR_TO links (using only MATCH_DIRECTION={os.getenv('MATCH_DIRECTION', 'source_to_target')}): {len(similar_to_df)}\")\n",
    "        \n",
    "        # Display models found in source_to_target links\n",
    "        if not similar_to_src_to_tgt_df.empty:\n",
    "            print(\"\\nSentence Transformer Models in SOURCE_TO_TARGET SIMILAR_TO links:\")\n",
    "            print(\"-\" * 50)\n",
    "            st_models = similar_to_src_to_tgt_df['sentence_transformer_model'].value_counts().to_dict()\n",
    "            for model, count in st_models.items():\n",
    "                print(f\"{model}: {count} links\")\n",
    "                \n",
    "        # Display head of DataFrame with data types info for debugging\n",
    "        print(\"\\nSample of SIMILAR_TO links:\")\n",
    "        print(\"-\" * 50)\n",
    "        display(similar_to_df.head())\n",
    "        \n",
    "        # Check for null values in similarity_score column\n",
    "        null_scores = similar_to_df['similarity_score'].isna().sum()\n",
    "        print(f\"\\nNull values in similarity_score column: {null_scores} ({null_scores/len(similar_to_df)*100:.2f}%)\")\n",
    "        \n",
    "        # Display data types of columns\n",
    "        print(\"\\nData types of columns:\")\n",
    "        print(similar_to_df.dtypes)\n",
    "\n",
    "# Run the display function when this cell is executed\n",
    "display_similar_to_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query Ground Truth Links\n",
    "# Purpose: Retrieve ground truth traceability links from Neo4j\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Acquisition -> Ground Truth\n",
    "\n",
    "def query_ground_truth_links(driver):\n",
    "    \"\"\"\n",
    "    Query ground truth traceability links from Neo4j database\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the exact query format that works in notebook 36\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            p.description as project_description,\n",
    "            d.id as document_id,\n",
    "            source.id as source_id,\n",
    "            source.type as source_type,\n",
    "            target.id as target_id,\n",
    "            target.type as target_type,\n",
    "            1 as ground_truth\n",
    "        ORDER BY source.id, target.id DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            try:\n",
    "                # Execute the query with project name parameter\n",
    "                results = session.run(ground_truth_query, project_name=NEO4J_PROJECT_NAME).data()\n",
    "                \n",
    "                if results:\n",
    "                    logger.info(f\"Retrieved {len(results)} ground truth links using GROUND_TRUTH relationship\")\n",
    "                    df_ground_truth = pd.DataFrame(results)\n",
    "                    return df_ground_truth\n",
    "                else:\n",
    "                    logger.warning(f\"No ground truth links found for project: {NEO4J_PROJECT_NAME}\")\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error executing ground truth query: {str(e)}\")\n",
    "                logger.error(\"Exception details:\", exc_info=True)\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying ground truth links: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute the query and get results\n",
    "df_ground_truth = query_ground_truth_links(driver)\n",
    "\n",
    "def display_ground_truth_info(df_ground_truth, similar_to_df=None):\n",
    "    \"\"\"\n",
    "    Display information about the retrieved ground truth links\n",
    "    \n",
    "    Parameters:\n",
    "        df_ground_truth: DataFrame containing ground truth links\n",
    "        similar_to_df: Optional DataFrame containing similarity links \n",
    "                      used for creating synthetic ground truth if needed\n",
    "    \"\"\"\n",
    "    if not df_ground_truth.empty:\n",
    "        print(\"\\nGround Truth Links for Project:\", NEO4J_PROJECT_NAME)\n",
    "        print(\"=\" * 80)\n",
    "        display(df_ground_truth.head())\n",
    "        \n",
    "        # Count source and target requirements\n",
    "        unique_sources = df_ground_truth['source_id'].nunique()\n",
    "        unique_targets = df_ground_truth['target_id'].nunique()\n",
    "        print(f\"\\nGround Truth Dataset Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total ground truth links: {len(df_ground_truth)}\")\n",
    "        print(f\"Unique source requirements: {unique_sources}\")\n",
    "        print(f\"Unique target requirements: {unique_targets}\")\n",
    "        print(f\"Link density: {len(df_ground_truth) / (unique_sources * unique_targets):.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo ground truth links found. Creating synthetic ground truth for testing:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # If we have similarity data but no ground truth, create synthetic ground truth \n",
    "        # using high similarity scores for testing purposes\n",
    "        if similar_to_df is not None and not similar_to_df.empty:\n",
    "            # Get the top 10% highest similarity scores for each model as synthetic ground truth\n",
    "            synthetic_gt = []\n",
    "            \n",
    "            for model in similar_to_df['sentence_transformer_model'].unique():\n",
    "                model_df = similar_to_df[similar_to_df['sentence_transformer_model'] == model]\n",
    "                threshold = model_df['similarity_score'].quantile(0.9)  # Top 10%\n",
    "                \n",
    "                high_sim_pairs = model_df[model_df['similarity_score'] >= threshold]\n",
    "                \n",
    "                if not high_sim_pairs.empty:\n",
    "                    for _, row in high_sim_pairs.iterrows():\n",
    "                        synthetic_gt.append({\n",
    "                            'project_name': row['project_name'],\n",
    "                            'project_description': 'Synthetic ground truth',\n",
    "                            'document_id': 'synthetic',\n",
    "                            'source_id': row['source_id'],\n",
    "                            'source_type': 'SOURCE',\n",
    "                            'target_id': row['target_id'], \n",
    "                            'target_type': 'TARGET',\n",
    "                            'ground_truth': 1\n",
    "                        })\n",
    "            \n",
    "            if synthetic_gt:\n",
    "                df_ground_truth = pd.DataFrame(synthetic_gt).drop_duplicates(subset=['source_id', 'target_id'])\n",
    "                print(f\"Created {len(df_ground_truth)} synthetic ground truth links from high similarity scores\")\n",
    "                \n",
    "                # Count source and target requirements\n",
    "                unique_sources = df_ground_truth['source_id'].nunique()\n",
    "                unique_targets = df_ground_truth['target_id'].nunique()\n",
    "                print(f\"\\nSynthetic Ground Truth Dataset Metrics:\")\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"Total ground truth links: {len(df_ground_truth)}\")\n",
    "                print(f\"Unique source requirements: {unique_sources}\")\n",
    "                print(f\"Unique target requirements: {unique_targets}\")\n",
    "                print(f\"Link density: {len(df_ground_truth) / (unique_sources * unique_targets):.4f}\")\n",
    "                print(\"\\nWARNING: Using synthetic ground truth for testing purposes only!\")\n",
    "                \n",
    "                return df_ground_truth\n",
    "\n",
    "# Display information about ground truth links\n",
    "display_ground_truth_info(df_ground_truth, similar_to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Create combined dataset for analysis\n",
    "# Purpose: Merge sentence transformer and ground truth data for analysis\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Data Preparation -> Combination\n",
    "\n",
    "# First, create a combined dataset with ground truth information\n",
    "def create_combined_dataset(similar_to_df=None, df_ground_truth=None):\n",
    "    \"\"\"\n",
    "    Create a combined dataset with sentence transformer results and ground truth\n",
    "    \n",
    "    Parameters:\n",
    "        similar_to_df: DataFrame containing sentence transformer similarity results\n",
    "        df_ground_truth: DataFrame containing ground truth traceability links\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with ground truth information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use provided dataframes or global variables if None\n",
    "        _similar_to_df = similar_to_df if similar_to_df is not None else globals().get('similar_to_df', pd.DataFrame())\n",
    "        _df_ground_truth = df_ground_truth if df_ground_truth is not None else globals().get('df_ground_truth', pd.DataFrame())\n",
    "        \n",
    "        # Check if we have the required data\n",
    "        if _similar_to_df.empty:\n",
    "            logger.error(\"No SIMILAR_TO links available to create combined dataset\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Start with similar_to_df as the base\n",
    "        combined_df = _similar_to_df.copy()\n",
    "        \n",
    "        # Add ground truth information if available\n",
    "        if not _df_ground_truth.empty:\n",
    "            # Create a set of ground truth links for fast lookup\n",
    "            ground_truth_pairs = set(zip(_df_ground_truth['source_id'], _df_ground_truth['target_id']))\n",
    "            \n",
    "            # Add ground_truth_traceable column\n",
    "            combined_df['ground_truth_traceable'] = combined_df.apply(\n",
    "                lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Added ground truth data: {combined_df['ground_truth_traceable'].sum()} true links out of {len(combined_df)} total\")\n",
    "        else:\n",
    "            logger.warning(\"No ground truth data available for combined dataset\")\n",
    "            \n",
    "        # Rename sentence_transformer_model to model for simplicity in later operations\n",
    "        if 'sentence_transformer_model' in combined_df.columns and 'model' not in combined_df.columns:\n",
    "            combined_df['model'] = combined_df['sentence_transformer_model']\n",
    "            \n",
    "        # IMPORTANT: Replace any None values in similarity_score with 0\n",
    "        if 'similarity_score' in combined_df.columns:\n",
    "            null_count = combined_df['similarity_score'].isna().sum()\n",
    "            if null_count > 0:\n",
    "                logger.warning(f\"Found {null_count} null values in similarity_score column. Replacing with 0.\")\n",
    "                combined_df['similarity_score'] = combined_df['similarity_score'].fillna(0)\n",
    "                \n",
    "        # Convert similarity_score to numeric if it's not already\n",
    "        if 'similarity_score' in combined_df.columns and combined_df['similarity_score'].dtype == 'object':\n",
    "            try:\n",
    "                combined_df['similarity_score'] = pd.to_numeric(combined_df['similarity_score'])\n",
    "                logger.info(\"Converted similarity_score to numeric type\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting similarity_score to numeric: {str(e)}\")\n",
    "                \n",
    "        return combined_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating combined dataset: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create the combined dataset\n",
    "combined_df = create_combined_dataset()\n",
    "\n",
    "# Function to display information about the combined dataset\n",
    "def display_combined_dataset_info(combined_df):\n",
    "    \"\"\"\n",
    "    Display information about the combined dataset for analysis\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df: Combined DataFrame with similarity and ground truth data\n",
    "    \"\"\"\n",
    "    if not combined_df.empty:\n",
    "        print(\"\\nCombined Dataset for Analysis\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Size of combined dataset: {len(combined_df)} records\")\n",
    "        print(f\"Contains sentence transformer data: {'model' in combined_df.columns}\")\n",
    "        print(f\"Contains ground truth data: {'ground_truth_traceable' in combined_df.columns}\")\n",
    "        \n",
    "        if 'model' in combined_df.columns:\n",
    "            st_models = combined_df['model'].value_counts().to_dict()\n",
    "            print(f\" - Sentence transformer records: {len(combined_df)} across {len(st_models)} models\")\n",
    "            \n",
    "        # Display head of combined dataset for debugging\n",
    "        print(\"\\nSample of combined dataset:\")\n",
    "        print(\"-\" * 50)\n",
    "        display(combined_df.head())\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        print(\"\\nNull values in key columns:\")\n",
    "        null_cols = {\n",
    "            'similarity_score': combined_df['similarity_score'].isna().sum(),\n",
    "            'model': combined_df['model'].isna().sum(),\n",
    "            'ground_truth_traceable': combined_df['ground_truth_traceable'].isna().sum() if 'ground_truth_traceable' in combined_df.columns else 'N/A'\n",
    "        }\n",
    "        for col, count in null_cols.items():\n",
    "            print(f\"  - {col}: {count}\")\n",
    "            \n",
    "        # Display min/max similarity scores and data types\n",
    "        if 'similarity_score' in combined_df.columns:\n",
    "            print(f\"\\nSimilarity score range: {combined_df['similarity_score'].min():.4f} to {combined_df['similarity_score'].max():.4f}\")\n",
    "            \n",
    "        # Display data types\n",
    "        print(\"\\nData types of columns:\")\n",
    "        print(combined_df.dtypes)\n",
    "\n",
    "# Display information about the combined dataset\n",
    "display_combined_dataset_info(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Model evaluation and threshold optimization\n",
    "# Purpose: Evaluate sentence transformer models and find optimal thresholds\n",
    "# Dependencies: pandas, numpy, sklearn.metrics\n",
    "# Breadcrumbs: Analysis -> Evaluation -> Threshold Optimization\n",
    "\n",
    "def evaluate_model_thresholds(df, model_name, score_column='similarity_score', \n",
    "                             ground_truth_column='ground_truth_traceable', \n",
    "                             optimize_for='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance across different thresholds\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing model predictions and ground truth\n",
    "        model_name: Name of the model to evaluate\n",
    "        score_column: Column containing similarity scores\n",
    "        ground_truth_column: Column containing ground truth values\n",
    "        optimize_for: Metric to optimize for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this model\n",
    "        model_df = df[df['model'] == model_name].copy()\n",
    "        \n",
    "        if model_df.empty:\n",
    "            logger.warning(f\"No data available for model: {model_name}\")\n",
    "            return {}\n",
    "            \n",
    "        if ground_truth_column not in model_df.columns:\n",
    "            logger.warning(f\"Ground truth column '{ground_truth_column}' not found for model: {model_name}\")\n",
    "            return {}\n",
    "        \n",
    "        # Get ground truth and scores\n",
    "        y_true = model_df[ground_truth_column].astype(int).values\n",
    "        \n",
    "        # Check for and handle None/NaN values in similarity scores\n",
    "        if model_df[score_column].isna().any():\n",
    "            logger.warning(f\"Found NaN values in {score_column} for model {model_name}. Filling with 0.\")\n",
    "            model_df[score_column] = model_df[score_column].fillna(0)\n",
    "        \n",
    "        # Ensure similarity scores are numeric\n",
    "        if model_df[score_column].dtype == object:\n",
    "            try:\n",
    "                model_df[score_column] = pd.to_numeric(model_df[score_column])\n",
    "                logger.info(f\"Converted {score_column} to numeric for model {model_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting {score_column} to numeric: {str(e)}\")\n",
    "                # Default to zeros if conversion fails\n",
    "                model_df[score_column] = 0\n",
    "        \n",
    "        scores = model_df[score_column].values\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"  - Data points: {len(model_df)}\")\n",
    "        print(f\"  - Positive examples: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Negative examples: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Score range: {scores.min():.4f} to {scores.max():.4f}\")\n",
    "        \n",
    "        # If all ground truth values are the same, we can't calculate meaningful metrics\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            logger.warning(f\"Insufficient ground truth variety for model {model_name} - all values are {np.unique(y_true)[0]}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum())\n",
    "            }\n",
    "        \n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "        \n",
    "        # Add a threshold of 1.0 to the end for completeness\n",
    "        thresholds = np.append(thresholds, 1.0)\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        results = []\n",
    "        \n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            # Convert scores to binary predictions using this threshold\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            \n",
    "            # Confusion matrix components\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "            prec = precision[min(i, len(precision)-1)]\n",
    "            rec = recall[min(i, len(recall)-1)]\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "            \n",
    "            # Additional metrics\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity/True Negative Rate\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate/False Negative Rate\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'tnr': tnr,  # specificity\n",
    "                'fnr': fnr,  # miss rate\n",
    "                'f1_score': f1,\n",
    "                'f2_score': f2,\n",
    "                'mcc': mcc  # Matthews Correlation Coefficient\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Find best threshold based on optimization metric\n",
    "        if optimize_for == 'F1':\n",
    "            best_idx = results_df['f1_score'].idxmax()\n",
    "            best_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_idx = results_df['f2_score'].idxmax()\n",
    "            best_metric = 'f2_score'\n",
    "            \n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df),\n",
    "            'ground_truth_positive': int(y_true.sum()),\n",
    "            'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "            'best_threshold': best_result['threshold'],\n",
    "            'best_precision': best_result['precision'],\n",
    "            'best_recall': best_result['recall'],\n",
    "            'best_accuracy': best_result['accuracy'],\n",
    "            'best_balanced_accuracy': best_result['balanced_accuracy'],\n",
    "            'best_f1': best_result['f1_score'],\n",
    "            'best_f2': best_result['f2_score'],\n",
    "            'best_tnr': best_result['tnr'],\n",
    "            'best_fnr': best_result['fnr'],\n",
    "            'best_mcc': best_result['mcc'],\n",
    "            'best_tp': best_result['tp'],\n",
    "            'best_fp': best_result['fp'],\n",
    "            'best_fn': best_result['fn'],\n",
    "            'best_tn': best_result['tn'],\n",
    "            'optimization_metric': optimize_for,\n",
    "            'threshold_results': results_df\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error evaluating model {model_name}: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df) if 'model_df' in locals() else 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def evaluate_all_models(combined_df, optimization_metric='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate all models in the combined dataset\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df: DataFrame containing model predictions and ground truth\n",
    "        optimization_metric: Metric to optimize thresholds for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (evaluation_results, best_thresholds_df)\n",
    "            - evaluation_results: List of dictionaries with evaluation results\n",
    "            - best_thresholds_df: DataFrame with best thresholds for each model\n",
    "    \"\"\"\n",
    "    # Check if we have the necessary data\n",
    "    if 'ground_truth_traceable' not in combined_df.columns or 'model' not in combined_df.columns:\n",
    "        logger.error(\"Cannot evaluate models: missing ground truth or model data\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Get list of all models\n",
    "    all_models = combined_df['model'].unique()\n",
    "    \n",
    "    # Evaluate each model\n",
    "    evaluation_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(all_models)} sentence transformer models\")\n",
    "    print(f\"Optimizing for {optimization_metric} score\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model in all_models:\n",
    "        print(f\"Evaluating model: {model}\")\n",
    "        result = evaluate_model_thresholds(combined_df, model, optimize_for=optimization_metric)\n",
    "        \n",
    "        if result:\n",
    "            evaluation_results.append(result)\n",
    "            print(f\"  - Data points: {result['data_points']}\")\n",
    "            \n",
    "            if 'ground_truth_positive' in result:\n",
    "                print(f\"  - Ground truth positive: {result['ground_truth_positive']} ({result['ground_truth_positive']/result['data_points']*100:.2f}%)\")\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"  - Error: {result['error']}\")\n",
    "                \n",
    "            if 'best_threshold' in result:\n",
    "                print(f\"  - Best threshold: {result['best_threshold']:.3f}\")\n",
    "                print(f\"  - Confusion Matrix (TP, FP, FN, TN): {result['best_tp']}, {result['best_fp']}, {result['best_fn']}, {result['best_tn']}\")\n",
    "                print(f\"  - Accuracy: {result['best_accuracy']:.3f}\")\n",
    "                print(f\"  - Balanced Accuracy: {result['best_balanced_accuracy']:.3f}\")\n",
    "                print(f\"  - Precision: {result['best_precision']:.3f}\")\n",
    "                print(f\"  - Recall/TPR: {result['best_recall']:.3f}\")\n",
    "                print(f\"  - Specificity/TNR: {result['best_tnr']:.3f}\")\n",
    "                print(f\"  - Miss Rate/FNR: {result['best_fnr']:.3f}\")\n",
    "                print(f\"  - F1: {result['best_f1']:.3f}\")\n",
    "                print(f\"  - F2: {result['best_f2']:.3f}\")\n",
    "                print(f\"  - Matthews Correlation Coefficient: {result['best_mcc']:.3f}\")\n",
    "    \n",
    "    # Create DataFrame of best thresholds with all metrics\n",
    "    if evaluation_results:\n",
    "        best_thresholds_df = pd.DataFrame([\n",
    "            {\n",
    "                'model_name': r['model_name'],\n",
    "                'best_threshold': r['best_threshold'] if 'best_threshold' in r else np.nan,\n",
    "                'accuracy': r['best_accuracy'] if 'best_accuracy' in r else np.nan,\n",
    "                'balanced_accuracy': r['best_balanced_accuracy'] if 'best_balanced_accuracy' in r else np.nan,\n",
    "                'precision': r['best_precision'] if 'best_precision' in r else np.nan,\n",
    "                'recall': r['best_recall'] if 'best_recall' in r else np.nan,\n",
    "                'specificity': r['best_tnr'] if 'best_tnr' in r else np.nan,\n",
    "                'miss_rate': r['best_fnr'] if 'best_fnr' in r else np.nan,\n",
    "                'f1_score': r['best_f1'] if 'best_f1' in r else np.nan,\n",
    "                'f2_score': r['best_f2'] if 'best_f2' in r else np.nan,\n",
    "                'matthews_corr': r['best_mcc'] if 'best_mcc' in r else np.nan,\n",
    "                'true_positives': r['best_tp'] if 'best_tp' in r else np.nan,\n",
    "                'false_positives': r['best_fp'] if 'best_fp' in r else np.nan,\n",
    "                'false_negatives': r['best_fn'] if 'best_fn' in r else np.nan,\n",
    "                'true_negatives': r['best_tn'] if 'best_tn' in r else np.nan,\n",
    "                'data_points': r['data_points'],\n",
    "                'ground_truth_positive': r['ground_truth_positive'] if 'ground_truth_positive' in r else 0,\n",
    "                'ground_truth_negative': r['ground_truth_negative'] if 'ground_truth_negative' in r else 0,\n",
    "            }\n",
    "            for r in evaluation_results if 'best_threshold' in r\n",
    "        ])\n",
    "        \n",
    "        # Sort by the appropriate metric\n",
    "        sort_col = 'f1_score' if optimization_metric == 'F1' else 'f2_score'\n",
    "        best_thresholds_df = best_thresholds_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\nBest Thresholds by Model:\")\n",
    "        print(\"-\" * 80)\n",
    "        display(best_thresholds_df)\n",
    "        \n",
    "        return evaluation_results, best_thresholds_df\n",
    "    else:\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# Run model evaluation if this cell is executed directly\n",
    "if 'combined_df' in globals() and not combined_df.empty:\n",
    "    evaluation_results, best_thresholds_df = evaluate_all_models(combined_df, OPTIMIZATION_METRIC)\n",
    "    \n",
    "    # Create visualizations if enabled\n",
    "    if SHOW_VISUALIZATION and not best_thresholds_df.empty:\n",
    "        # Define color palette for consistency\n",
    "        color_palette = {\n",
    "            'TP': '#1A85FF',  # Good - Blue\n",
    "            'FP': '#FFC61A',  # Okay - Yellow/Gold\n",
    "            'FN': '#D41159',  # Not Great - Magenta/Red\n",
    "            'TN': '#CCCCCC'   # Neutral - Light Gray\n",
    "        }\n",
    "        \n",
    "        # 1. Create confusion matrix for each model\n",
    "        print(\"\\nConfusion Matrices:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Set up a grid of subplots for confusion matrices - MODIFIED FOR 5x2 LAYOUT\n",
    "        n_models = len(best_thresholds_df)\n",
    "        n_cols = min(5, n_models)  # Maximum 5 columns\n",
    "        n_rows = min(2, (n_models + n_cols - 1) // n_cols)  # Maximum 2 rows\n",
    "        \n",
    "        # More compact figure size - reduced height\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 3.5, n_rows * 3))\n",
    "        \n",
    "        # Flatten axes array if it's multi-dimensional\n",
    "        if n_models > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (idx, row) in enumerate(best_thresholds_df.iterrows()):\n",
    "            # Skip if we have more models than subplot positions\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            # Extract confusion matrix components\n",
    "            model_cm = np.array([\n",
    "                [row['true_negatives'], row['false_positives']],\n",
    "                [row['false_negatives'], row['true_positives']]\n",
    "            ])\n",
    "            \n",
    "            # Create normalized confusion matrix (percentages)\n",
    "            row_sums = model_cm.sum(axis=1, keepdims=True)\n",
    "            norm_cm = model_cm / row_sums * 100 if row_sums.min() > 0 else model_cm * 0\n",
    "            \n",
    "            # Create confusion matrix plot with custom colors - removed colorbar\n",
    "            ax = axes[i]\n",
    "            sns.heatmap(model_cm, annot=True, fmt='g', cmap='Blues', \n",
    "                        xticklabels=['Negative', 'Positive'],\n",
    "                        yticklabels=['Negative', 'Positive'], ax=ax,\n",
    "                        cbar=False)  # Removed colorbar\n",
    "            \n",
    "            # Set title and labels\n",
    "            model_short_name = row['model_name'].split('/')[-1] if '/' in row['model_name'] else row['model_name']\n",
    "            ax.set_title(f\"Project: {NEO4J_PROJECT_NAME}\\n{model_short_name}\", fontsize=9)\n",
    "            ax.set_xlabel('Predicted', fontsize=8)\n",
    "            ax.set_ylabel('Actual', fontsize=8)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_models, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Create comparative heatmap of all metrics\n",
    "        print(\"\\nMetrics Comparison Heatmap:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Select relevant metrics for comparison\n",
    "        metrics_to_compare = ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                              'specificity', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "        \n",
    "        # Create a new dataframe with model names as index and metrics as columns\n",
    "        comparison_df = best_thresholds_df.set_index('model_name')[metrics_to_compare]\n",
    "        \n",
    "        # Use short names for models for better display\n",
    "        comparison_df.index = [name.split('/')[-1] if '/' in name else name \n",
    "                              for name in comparison_df.index]\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(comparison_df, annot=True, cmap='YlGnBu', fmt='.3f',\n",
    "                   linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "        plt.title(f'Project: {NEO4J_PROJECT_NAME} - Comparison of Models Across Metrics', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Create grouped bar chart for key metrics\n",
    "        print(\"\\nKey Metrics Comparison Chart:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Select key metrics for bar chart\n",
    "        key_metrics = ['precision', 'recall', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "        \n",
    "        # Create plot with rotated x-labels for better readability\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        x = np.arange(len(comparison_df.index))\n",
    "        width = 0.15\n",
    "        offsets = np.linspace(0, width*4, 5) - width*2\n",
    "        \n",
    "        # Custom colors for metrics\n",
    "        metric_colors = ['#1A85FF', '#FFC61A', '#00C9A7', '#845EC2', '#D41159']\n",
    "        \n",
    "        for i, metric in enumerate(key_metrics):\n",
    "            ax.bar(x + offsets[i], comparison_df[metric], width, \n",
    "                   label=metric.replace('_', ' ').title(), color=metric_colors[i])\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(comparison_df.index, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(f'Project: {NEO4J_PROJECT_NAME} - Comparison of Key Metrics Across Models', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add project name in the top right corner\n",
    "        plt.figtext(0.95, 0.95, f\"Project: {NEO4J_PROJECT_NAME}\", \n",
    "                  horizontalalignment='right', fontsize=10, \n",
    "                  bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nCannot evaluate model thresholds: missing ground truth or model data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Comprehensive traceability analysis with source-target matrix visualization\n",
    "# Purpose: Analyze traceability prediction using sentence transformers and visualize results\n",
    "# Dependencies: pandas, numpy, seaborn, matplotlib\n",
    "# Breadcrumbs: Analysis -> Traceability Evaluation -> Visualization\n",
    "\n",
    "def apply_thresholds_and_evaluate(combined_df, model_thresholds=None, default_threshold=None):\n",
    "    \"\"\"\n",
    "    Apply similarity thresholds and evaluate traceability predictions\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df: DataFrame with similarity scores and ground truth\n",
    "        model_thresholds: Dictionary mapping model names to thresholds (REQUIRED)\n",
    "        default_threshold: Not used - kept for compatibility but will raise error if model_thresholds is None\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Copy of input DataFrame with added prediction and evaluation columns\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If model_thresholds is None (no fallback to hard-coded values)\n",
    "    \"\"\"\n",
    "    # Create a copy of the combined DataFrame for evaluation\n",
    "    combined_traced_eval_df = combined_df.copy()\n",
    "    \n",
    "    try:\n",
    "        # STRICT: Require model thresholds to be provided (no hard-coded fallbacks)\n",
    "        if model_thresholds is None:\n",
    "            error_msg = (\n",
    "                \"model_thresholds parameter is required and cannot be None. \"\n",
    "                \"Please run Cell [5] - Model evaluation and threshold optimization first to calculate optimal thresholds. \"\n",
    "                \"This ensures we use F2-optimized thresholds rather than potentially outdated hard-coded values.\"\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        else:\n",
    "            logger.info(f\"Using provided model thresholds (calculated from threshold optimization)\")\n",
    "        \n",
    "        # Make sure there are no None values in similarity scores\n",
    "        if 'similarity_score' in combined_traced_eval_df.columns:\n",
    "            null_count = combined_traced_eval_df['similarity_score'].isna().sum()\n",
    "            if null_count > 0:\n",
    "                logger.warning(f\"Found {null_count} null values in similarity_score column before applying thresholds. Replacing with 0.\")\n",
    "                combined_traced_eval_df['similarity_score'] = combined_traced_eval_df['similarity_score'].fillna(0)\n",
    "            \n",
    "            # Ensure similarity_score is numeric\n",
    "            if combined_traced_eval_df['similarity_score'].dtype == object:\n",
    "                try:\n",
    "                    combined_traced_eval_df['similarity_score'] = pd.to_numeric(combined_traced_eval_df['similarity_score'])\n",
    "                    logger.info(\"Converted similarity_score to numeric type for threshold application\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error converting similarity_score to numeric: {str(e)}\")\n",
    "                    logger.error(\"Using zeros for similarity_score\")\n",
    "                    combined_traced_eval_df['similarity_score'] = 0\n",
    "        \n",
    "        # Apply thresholds to determine if a pair is traceable based on similarity score\n",
    "        def is_traceable(row):\n",
    "            # Check which column has the model information\n",
    "            if 'sentence_transformer_model' in row:\n",
    "                model = row['sentence_transformer_model']\n",
    "            elif 'model' in row:\n",
    "                model = row['model']\n",
    "            else:\n",
    "                error_msg = f\"No model column found for row and no default threshold available (strict mode)\"\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "            \n",
    "            # Check for None values in similarity_score\n",
    "            similarity = row['similarity_score']\n",
    "            if similarity is None:\n",
    "                logger.warning(f\"Found None value in similarity_score for {model}, using 0\")\n",
    "                similarity = 0\n",
    "            \n",
    "            # Get threshold for this model (STRICT: must be in model_thresholds)\n",
    "            if model in model_thresholds:\n",
    "                threshold = model_thresholds[model]\n",
    "            else:\n",
    "                error_msg = (\n",
    "                    f\"No threshold found for model '{model}' in provided model_thresholds. \"\n",
    "                    f\"Available models: {list(model_thresholds.keys())}. \"\n",
    "                    f\"Please ensure all models have calculated thresholds from Cell [5].\"\n",
    "                )\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "                \n",
    "            # Ensure threshold is not None\n",
    "            if threshold is None:\n",
    "                error_msg = f\"Threshold for model '{model}' is None. This should not happen with calculated thresholds.\"\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "                \n",
    "            return float(similarity) >= float(threshold)\n",
    "        \n",
    "        # Add predicted traceable column\n",
    "        combined_traced_eval_df['predicted_traceable'] = combined_traced_eval_df.apply(is_traceable, axis=1)\n",
    "        \n",
    "        # Add column for confusion matrix category (TP, FP, FN, TN)\n",
    "        def get_confusion_category(row):\n",
    "            if 'ground_truth_traceable' not in row or pd.isna(row['ground_truth_traceable']):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            if row['ground_truth_traceable'] and row['predicted_traceable']:\n",
    "                return 'TP'  # True Positive\n",
    "            elif not row['ground_truth_traceable'] and row['predicted_traceable']:\n",
    "                return 'FP'  # False Positive\n",
    "            elif row['ground_truth_traceable'] and not row['predicted_traceable']:\n",
    "                return 'FN'  # False Negative\n",
    "            else:  # not ground_truth and not predicted\n",
    "                return 'TN'  # True Negative\n",
    "        \n",
    "        combined_traced_eval_df['confusion_category'] = combined_traced_eval_df.apply(get_confusion_category, axis=1)\n",
    "        \n",
    "        return combined_traced_eval_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in traceability evaluation: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        print(f\"Error in traceability evaluation: {str(e)}\")\n",
    "        return combined_traced_eval_df\n",
    "\n",
    "def display_evaluation_summary(combined_traced_eval_df, optimization_metric='F2'):\n",
    "    \"\"\"\n",
    "    Display summary statistics for traceability evaluation\n",
    "    \n",
    "    Parameters:\n",
    "        combined_traced_eval_df: DataFrame with traceability predictions and evaluations\n",
    "        optimization_metric: Metric used for optimization (for display purposes)\n",
    "    \"\"\"\n",
    "    # Display summary of ground truth values\n",
    "    print(f\"\\nGround Truth Distribution ({optimization_metric} optimized):\")\n",
    "    print(\"-\" * 40)\n",
    "    gt_counts = combined_traced_eval_df['ground_truth_traceable'].value_counts()\n",
    "    print(gt_counts)\n",
    "    print(f\"Percentage traceable: {gt_counts.get(True, 0) / len(combined_traced_eval_df) * 100:.2f}%\")\n",
    "    \n",
    "    # Display summary of predictions\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    pred_counts = combined_traced_eval_df['predicted_traceable'].value_counts()\n",
    "    print(pred_counts)\n",
    "    print(f\"Percentage predicted traceable: {pred_counts.get(True, 0) / len(combined_traced_eval_df) * 100:.2f}%\")\n",
    "    \n",
    "    # Display confusion category distribution\n",
    "    print(\"\\nConfusion Matrix Categories:\")\n",
    "    print(\"-\" * 40)\n",
    "    confusion_counts = combined_traced_eval_df['confusion_category'].value_counts()\n",
    "    print(confusion_counts)\n",
    "    for category, count in confusion_counts.items():\n",
    "        print(f\"{category}: {count} ({count/len(combined_traced_eval_df)*100:.2f}%)\")\n",
    "\n",
    "def create_source_target_matrix_visualization(combined_traced_eval_df, model_thresholds=None, \n",
    "                                             project_name=None, show_visualization=True):\n",
    "    \"\"\"\n",
    "    Create source-target matrix visualizations for traceability analysis\n",
    "    \n",
    "    Parameters:\n",
    "        combined_traced_eval_df: DataFrame with traceability predictions and evaluations\n",
    "        model_thresholds: Dictionary mapping model names to thresholds\n",
    "        project_name: Name of the project for visualization titles\n",
    "        show_visualization: Whether to display visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing visualization data for each model\n",
    "    \"\"\"\n",
    "    if not show_visualization:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Use global project name if not provided\n",
    "        if project_name is None:\n",
    "            project_name = globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "            \n",
    "        # STRICT: Require model thresholds to be provided (no hard-coded fallbacks in visualization)\n",
    "        if model_thresholds is None and 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "            # Create a dictionary mapping model names to optimal thresholds\n",
    "            model_thresholds = dict(zip(\n",
    "                globals()['best_thresholds_df']['model_name'],\n",
    "                globals()['best_thresholds_df']['best_threshold']\n",
    "            ))\n",
    "        elif model_thresholds is None:\n",
    "            # STRICT: No hard-coded fallback thresholds for visualization\n",
    "            error_msg = (\n",
    "                \"No model thresholds available for visualization. \"\n",
    "                \"Please run Cell [5] - Model evaluation and threshold optimization first to calculate \"\n",
    "                \"optimal thresholds, or provide model_thresholds parameter. \"\n",
    "                \"This ensures visualizations use data-driven thresholds rather than hard-coded values.\"\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        print(\"\\nCreating source-target requirement matrix visualization...\")\n",
    "        \n",
    "        # Get list of all models\n",
    "        all_models = combined_traced_eval_df['model'].unique()\n",
    "        \n",
    "        # Define intuitive color palette for TP, FP, FN, TN\n",
    "        # Using color scheme as requested: TP (good), FP (ok), FN (not great), TN (neutral)\n",
    "        color_palette = {\n",
    "            'TP': '#1A85FF',  # Good - Blue\n",
    "            'FP': '#FFC61A',  # Okay - Yellow/Gold\n",
    "            'FN': '#D41159',  # Not Great - Magenta/Red\n",
    "            'TN': '#CCCCCC',  # Neutral - Light Gray\n",
    "            'Unknown': '#FFFFFF'  # White for unknown\n",
    "        }\n",
    "        \n",
    "        # Import needed matplotlib components\n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        from matplotlib.patches import Patch\n",
    "        \n",
    "        # Store visualization data for each model\n",
    "        viz_data = {}\n",
    "        \n",
    "        # For each model, create a matrix visualization\n",
    "        for model_name in all_models:\n",
    "            model_df = combined_traced_eval_df[combined_traced_eval_df['model'] == model_name].copy()\n",
    "            \n",
    "            # Get unique source and target requirements\n",
    "            source_reqs = sorted(model_df['source_id'].unique())\n",
    "            target_reqs = sorted(model_df['target_id'].unique())\n",
    "            \n",
    "            # Check if the matrix would be too large\n",
    "            is_large_matrix = len(source_reqs) > 50 or len(target_reqs) > 50\n",
    "            if is_large_matrix:\n",
    "                print(f\"Warning: Matrix for {model_name} would be too large ({len(source_reqs)}x{len(target_reqs)}). Reducing tick frequency...\")\n",
    "            \n",
    "            # Create a 2D matrix to hold our visualization data\n",
    "            # Initialize with zeros (we'll use different numbers for each category)\n",
    "            matrix = np.zeros((len(source_reqs), len(target_reqs)))\n",
    "            \n",
    "            # Define numeric values for each category\n",
    "            category_to_value = {\n",
    "                'TP': 0,\n",
    "                'FP': 1,\n",
    "                'FN': 2,\n",
    "                'TN': 3,\n",
    "                'Unknown': 4\n",
    "            }\n",
    "            \n",
    "            # Fill the matrix with the appropriate values\n",
    "            for _, row in model_df.iterrows():\n",
    "                source_idx = source_reqs.index(row['source_id'])\n",
    "                target_idx = target_reqs.index(row['target_id'])\n",
    "                category = row['confusion_category']\n",
    "                matrix[source_idx, target_idx] = category_to_value.get(category, 4)\n",
    "            \n",
    "            # Create explicit color map\n",
    "            colors = [\n",
    "                color_palette['TP'],     # 0 = TP (Blue)\n",
    "                color_palette['FP'],     # 1 = FP (Yellow)\n",
    "                color_palette['FN'],     # 2 = FN (Red)\n",
    "                color_palette['TN'],     # 3 = TN (Gray)\n",
    "                color_palette['Unknown'] # 4 = Unknown (White)\n",
    "            ]\n",
    "            \n",
    "            # Create a colormap with exactly 5 colors\n",
    "            cmap = LinearSegmentedColormap.from_list('confusion_cmap', colors, N=5)\n",
    "            \n",
    "            # Determine figure size based on matrix dimensions\n",
    "            fig_width = min(20, max(10, len(target_reqs) * 0.15))\n",
    "            fig_height = min(16, max(8, len(source_reqs) * 0.15))\n",
    "            \n",
    "            # Create the figure\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            # Plot the matrix\n",
    "            im = ax.imshow(matrix, cmap=cmap, vmin=0, vmax=4, aspect='auto')\n",
    "            \n",
    "            # Adjust tick labels based on matrix size\n",
    "            if is_large_matrix:\n",
    "                # Add tick marks at regular intervals\n",
    "                x_step = max(1, len(target_reqs) // 10)\n",
    "                y_step = max(1, len(source_reqs) // 10)\n",
    "                \n",
    "                x_ticks = np.arange(0, len(target_reqs), x_step)\n",
    "                y_ticks = np.arange(0, len(source_reqs), y_step)\n",
    "                \n",
    "                ax.set_xticks(x_ticks)\n",
    "                ax.set_yticks(y_ticks)\n",
    "                \n",
    "                ax.set_xticklabels([target_reqs[i] for i in x_ticks], rotation=90)\n",
    "                ax.set_yticklabels([source_reqs[i] for i in y_ticks])\n",
    "            else:\n",
    "                # Show all ticks for small matrices\n",
    "                ax.set_xticks(np.arange(len(target_reqs)))\n",
    "                ax.set_yticks(np.arange(len(source_reqs)))\n",
    "                \n",
    "                ax.set_xticklabels(target_reqs, rotation=90)\n",
    "                ax.set_yticklabels(source_reqs)\n",
    "            \n",
    "            # Add gridlines to make it easier to read\n",
    "            ax.set_xticks(np.arange(-.5, len(target_reqs), 1), minor=True)\n",
    "            ax.set_yticks(np.arange(-.5, len(source_reqs), 1), minor=True)\n",
    "            \n",
    "            # Hide minor ticks but show the grid\n",
    "            ax.tick_params(which='minor', length=0)\n",
    "            \n",
    "            # Turn off the frame\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            \n",
    "            # Create custom legend\n",
    "            legend_elements = [\n",
    "                Patch(facecolor=color_palette['TP'], label='True Positive'),\n",
    "                Patch(facecolor=color_palette['FP'], label='False Positive'),\n",
    "                Patch(facecolor=color_palette['FN'], label='False Negative'),\n",
    "                Patch(facecolor=color_palette['TN'], label='True Negative')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1), \n",
    "                     fontsize=12, frameon=True, edgecolor='black')\n",
    "            \n",
    "            # Set titles and labels\n",
    "            model_short_name = model_name.split('/')[-1] if '/' in model_name else model_name\n",
    "            # STRICT: model_name must exist in model_thresholds (no fallback)\n",
    "            if model_name not in model_thresholds:\n",
    "                error_msg = f\"Model '{model_name}' not found in model_thresholds for visualization title. Available: {list(model_thresholds.keys())}\"\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "            plt.title(f'Project: {project_name} - {model_short_name}\\nThreshold: {model_thresholds[model_name]:.3f}',\n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "            plt.xlabel('Target Requirements', fontsize=12, labelpad=10)\n",
    "            plt.ylabel('Source Requirements', fontsize=12, labelpad=10)\n",
    "            \n",
    "            # Count occurrences of each category\n",
    "            tp_mask = matrix == category_to_value['TP']\n",
    "            fp_mask = matrix == category_to_value['FP']\n",
    "            fn_mask = matrix == category_to_value['FN']\n",
    "            tn_mask = matrix == category_to_value['TN']\n",
    "            \n",
    "            tp_count = np.sum(tp_mask)\n",
    "            fp_count = np.sum(fp_mask)\n",
    "            fn_count = np.sum(fn_mask)\n",
    "            tn_count = np.sum(tn_mask)\n",
    "            \n",
    "            # Debug output to verify counts\n",
    "            print(f\"Category counts for {model_short_name}:\")\n",
    "            print(f\"TP: {int(tp_count)}, FP: {int(fp_count)}, FN: {int(fn_count)}, TN: {int(tn_count)}\")\n",
    "            \n",
    "            total = tp_count + fp_count + fn_count + tn_count\n",
    "            if total > 0:\n",
    "                accuracy = (tp_count + tn_count) / total\n",
    "                precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "                recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                f2 = 5 * precision * recall / (4 * precision + recall) if (4 * precision + recall) > 0 else 0\n",
    "                \n",
    "                stats_text = (f\"TP: {int(tp_count)} ({tp_count/total*100:.1f}%) | FP: {int(fp_count)} ({fp_count/total*100:.1f}%)\\n\"\n",
    "                             f\"FN: {int(fn_count)} ({fn_count/total*100:.1f}%) | TN: {int(tn_count)} ({tn_count/total*100:.1f}%)\\n\"\n",
    "                             f\"Acc: {accuracy:.3f} | Prec: {precision:.3f} | Rec: {recall:.3f}\\n\"\n",
    "                             f\"F1: {f1:.3f} | F2: {f2:.3f}\")\n",
    "                \n",
    "                # Add the text box at the bottom right of the figure\n",
    "                plt.figtext(0.95, 0.02, stats_text, horizontalalignment='right', \n",
    "                          bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'), \n",
    "                          fontsize=10)\n",
    "                \n",
    "                # Print summary statistics for this model\n",
    "                print(f\"\\nConfusion Matrix Statistics for {model_short_name} (Project: {project_name}):\")\n",
    "                print(f\"True Positives: {int(tp_count)} ({tp_count/total*100:.2f}%)\")\n",
    "                print(f\"False Positives: {int(fp_count)} ({fp_count/total*100:.2f}%)\")\n",
    "                print(f\"False Negatives: {int(fn_count)} ({fn_count/total*100:.2f}%)\")\n",
    "                print(f\"True Negatives: {int(tn_count)} ({tn_count/total*100:.2f}%)\")\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy:.3f}\")\n",
    "                print(f\"Precision: {precision:.3f}\")\n",
    "                print(f\"Recall: {recall:.3f}\")\n",
    "                print(f\"F1 Score: {f1:.3f}\")\n",
    "                print(f\"F2 Score: {f2:.3f}\")\n",
    "                \n",
    "                # Store visualization data\n",
    "                viz_data[model_name] = {\n",
    "                    'matrix': matrix,\n",
    "                    'source_reqs': source_reqs,\n",
    "                    'target_reqs': target_reqs,\n",
    "                    'metrics': {\n",
    "                        'tp': int(tp_count),\n",
    "                        'fp': int(fp_count),\n",
    "                        'fn': int(fn_count),\n",
    "                        'tn': int(tn_count),\n",
    "                        'accuracy': accuracy,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1': f1,\n",
    "                        'f2': f2\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            if show_visualization:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close(fig)\n",
    "                \n",
    "        return viz_data\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating visualizations: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        print(f\"Error creating visualizations: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Get model thresholds from best_thresholds_df if available\n",
    "if 'best_thresholds_df' in globals() and not best_thresholds_df.empty:\n",
    "    # Create a dictionary mapping model names to optimal thresholds\n",
    "    model_thresholds = dict(zip(\n",
    "        best_thresholds_df['model_name'],\n",
    "        best_thresholds_df['best_threshold']\n",
    "    ))\n",
    "    logger.info(f\"Using optimal thresholds from model evaluation (optimized for {OPTIMIZATION_METRIC})\")\n",
    "else:\n",
    "    # STRICT: No hard-coded fallback thresholds\n",
    "    error_msg = (\n",
    "        \"No model evaluation results available (best_thresholds_df is empty or not found). \"\n",
    "        \"Please run Cell [5] - Model evaluation and threshold optimization first to calculate \"\n",
    "        f\"{OPTIMIZATION_METRIC}-optimized thresholds for all models. This ensures we use data-driven \"\n",
    "        \"thresholds rather than potentially outdated hard-coded values.\"\n",
    "    )\n",
    "    logger.error(error_msg)\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Apply thresholds and evaluate traceability\n",
    "combined_traced_eval_df = apply_thresholds_and_evaluate(combined_df, model_thresholds)\n",
    "\n",
    "# Display evaluation summary\n",
    "display_evaluation_summary(combined_traced_eval_df, OPTIMIZATION_METRIC)\n",
    "\n",
    "# Create visualizations if enabled\n",
    "if SHOW_VISUALIZATION:\n",
    "    viz_data = create_source_target_matrix_visualization(\n",
    "        combined_traced_eval_df, \n",
    "        model_thresholds,\n",
    "        NEO4J_PROJECT_NAME,\n",
    "        SHOW_VISUALIZATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Analysis Summary and Conclusions\n",
    "# Purpose: Summarize findings and draw conclusions about traceability prediction\n",
    "# Dependencies: pandas\n",
    "# Breadcrumbs: Analysis -> Summary\n",
    "\n",
    "def create_analysis_summary(best_thresholds_df=None, optimization_metric=None, project_name=None):\n",
    "    \"\"\"\n",
    "    Create a summary of model evaluation and traceability analysis\n",
    "    \n",
    "    Parameters:\n",
    "        best_thresholds_df: DataFrame containing model evaluation results\n",
    "        optimization_metric: Metric used for optimization ('F1' or 'F2')\n",
    "        project_name: Name of the project being analyzed\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing summary information\n",
    "    \"\"\"\n",
    "    # Use global variables if parameters not provided\n",
    "    _best_thresholds_df = best_thresholds_df if best_thresholds_df is not None else globals().get('best_thresholds_df')\n",
    "    _optimization_metric = optimization_metric if optimization_metric is not None else globals().get('OPTIMIZATION_METRIC', 'F2')\n",
    "    _project_name = project_name if project_name is not None else globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "    \n",
    "    # Create empty result dictionary\n",
    "    summary = {\n",
    "        'project_name': _project_name,\n",
    "        'optimization_metric': _optimization_metric,\n",
    "        'has_model_evaluation': False,\n",
    "        'models_evaluated': 0,\n",
    "        'best_model': None,\n",
    "        'best_model_threshold': None,\n",
    "        'best_model_metrics': {},\n",
    "        'model_family_comparison': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check if we have model evaluation results available\n",
    "    if _best_thresholds_df is not None and not _best_thresholds_df.empty:\n",
    "        summary['has_model_evaluation'] = True\n",
    "        summary['models_evaluated'] = len(_best_thresholds_df)\n",
    "        \n",
    "        # Identify the best performing model\n",
    "        if _optimization_metric == 'F2':\n",
    "            best_model_idx = _best_thresholds_df['f2_score'].idxmax()\n",
    "            sort_metric = 'f2_score'\n",
    "        else:\n",
    "            best_model_idx = _best_thresholds_df['f1_score'].idxmax()\n",
    "            sort_metric = 'f1_score'\n",
    "            \n",
    "        best_model = _best_thresholds_df.loc[best_model_idx]\n",
    "        summary['best_model'] = best_model['model_name']\n",
    "        summary['best_model_threshold'] = float(best_model['best_threshold'])\n",
    "        summary['sort_metric'] = sort_metric\n",
    "        \n",
    "        # Store best model metrics\n",
    "        summary['best_model_metrics'] = {\n",
    "            'threshold': float(best_model['best_threshold']),\n",
    "            'precision': float(best_model['precision']),\n",
    "            'recall': float(best_model['recall']),\n",
    "            'f1_score': float(best_model['f1_score']),\n",
    "            'f2_score': float(best_model['f2_score']),\n",
    "            'matthews_corr': float(best_model['matthews_corr']),\n",
    "            'balanced_accuracy': float(best_model['balanced_accuracy']),\n",
    "            'confusion_matrix': {\n",
    "                'tp': int(best_model['true_positives']),\n",
    "                'fp': int(best_model['false_positives']),\n",
    "                'fn': int(best_model['false_negatives']),\n",
    "                'tn': int(best_model['true_negatives'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Model family analysis\n",
    "        mpnet_models = _best_thresholds_df[_best_thresholds_df['model_name'].str.contains('mpnet', case=False, na=False)]\n",
    "        minilm_models = _best_thresholds_df[_best_thresholds_df['model_name'].str.contains('MiniLM', na=False)]\n",
    "        bert_models = _best_thresholds_df[_best_thresholds_df['model_name'].str.contains('bert', case=False, na=False)]\n",
    "        \n",
    "        model_families = {}\n",
    "        if not mpnet_models.empty:\n",
    "            model_families['MPNet'] = float(mpnet_models[sort_metric].mean())\n",
    "        if not minilm_models.empty:\n",
    "            model_families['MiniLM'] = float(minilm_models[sort_metric].mean())\n",
    "        if not bert_models.empty:\n",
    "            model_families['BERT'] = float(bert_models[sort_metric].mean())\n",
    "            \n",
    "        summary['model_family_comparison'] = model_families\n",
    "        \n",
    "        if model_families:\n",
    "            best_family = max(model_families.items(), key=lambda x: x[1])\n",
    "            summary['best_model_family'] = best_family[0]\n",
    "            summary['best_model_family_score'] = best_family[1]\n",
    "        \n",
    "        # Threshold analysis\n",
    "        threshold_corr = _best_thresholds_df[['best_threshold', sort_metric]].corr().iloc[0, 1]\n",
    "        summary['threshold_correlation'] = float(threshold_corr)\n",
    "        \n",
    "        # Precision-recall tradeoff analysis\n",
    "        prec_recall_corr = _best_thresholds_df[['precision', 'recall']].corr().iloc[0, 1]\n",
    "        summary['precision_recall_correlation'] = float(prec_recall_corr)\n",
    "        \n",
    "        # Average model performance\n",
    "        summary['average_performance'] = {\n",
    "            'precision': float(_best_thresholds_df['precision'].mean()),\n",
    "            'precision_std': float(_best_thresholds_df['precision'].std()),\n",
    "            'recall': float(_best_thresholds_df['recall'].mean()),\n",
    "            'recall_std': float(_best_thresholds_df['recall'].std()),\n",
    "            'f1_score': float(_best_thresholds_df['f1_score'].mean()),\n",
    "            'f1_score_std': float(_best_thresholds_df['f1_score'].std()),\n",
    "            'f2_score': float(_best_thresholds_df['f2_score'].mean()),\n",
    "            'f2_score_std': float(_best_thresholds_df['f2_score'].std()),\n",
    "            'matthews_corr': float(_best_thresholds_df['matthews_corr'].mean()),\n",
    "            'matthews_corr_std': float(_best_thresholds_df['matthews_corr'].std())\n",
    "        }\n",
    "        \n",
    "        # Top 3 models\n",
    "        top3 = _best_thresholds_df.sort_values(sort_metric, ascending=False).head(3)\n",
    "        summary['top_models'] = [\n",
    "            {\n",
    "                'model_name': row['model_name'],\n",
    "                'score': float(row[sort_metric]),\n",
    "                'threshold': float(row['best_threshold'])\n",
    "            }\n",
    "            for _, row in top3.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Recommendations\n",
    "        recommendations = [\n",
    "            f\"Use {best_model['model_name']} for requirement traceability prediction\",\n",
    "            f\"Apply a similarity threshold of {best_model['best_threshold']:.3f}\",\n",
    "            f\"Expected performance: {best_model[sort_metric]:.3f} {sort_metric.replace('_', ' ').title()}\"\n",
    "        ]\n",
    "        \n",
    "        # Add precision-recall guidance if needed\n",
    "        if abs(best_model['precision'] - best_model['recall']) > 0.1:\n",
    "            if best_model['precision'] > best_model['recall']:\n",
    "                recommendations.append(f\"Note that precision ({best_model['precision']:.3f}) is higher than recall ({best_model['recall']:.3f}), meaning the model misses some true links but has fewer false positives\")\n",
    "            else:\n",
    "                recommendations.append(f\"Note that recall ({best_model['recall']:.3f}) is higher than precision ({best_model['precision']:.3f}), meaning the model captures most true links but generates more false positives\")\n",
    "        \n",
    "        summary['recommendations'] = recommendations\n",
    "        \n",
    "        # Limitations\n",
    "        summary['limitations'] = [\n",
    "            \"Results are specific to the current dataset and domain\",\n",
    "            \"Ground truth may contain imperfections that affect evaluation\",\n",
    "            \"Similarity-based approaches alone may miss some semantic connections\",\n",
    "            \"Threshold optimization is based on a single metric and may not be optimal for all use cases\"\n",
    "        ]\n",
    "        \n",
    "    return summary\n",
    "\n",
    "def display_analysis_summary(summary=None, show_visualization=False):\n",
    "    \"\"\"\n",
    "    Display summary of traceability analysis results\n",
    "    \n",
    "    Parameters:\n",
    "        summary: Dictionary with analysis summary (from create_analysis_summary)\n",
    "        show_visualization: Whether to display a visualization of top models\n",
    "    \"\"\"\n",
    "    if summary is None:\n",
    "        # Create summary if not provided\n",
    "        summary = create_analysis_summary()\n",
    "    \n",
    "    if not summary['has_model_evaluation']:\n",
    "        print(\"\\nInsufficient evaluation data to draw conclusions.\")\n",
    "        print(\"Please run model evaluation in Cell 6 to generate performance metrics.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTRACEABILITY ANALYSIS SUMMARY FOR PROJECT: {summary['project_name']}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Optimization metric: {summary['optimization_metric']}\")\n",
    "    print(f\"Total models evaluated: {summary['models_evaluated']}\")\n",
    "    print(f\"\\nBest performing model: {summary['best_model']}\")\n",
    "    print(f\"  - Threshold: {summary['best_model_metrics']['threshold']:.3f}\")\n",
    "    print(f\"  - {summary['sort_metric'].replace('_', ' ').title()}: {summary['best_model_metrics'][summary['sort_metric']]:.3f}\")\n",
    "    print(f\"  - Precision: {summary['best_model_metrics']['precision']:.3f}\")\n",
    "    print(f\"  - Recall: {summary['best_model_metrics']['recall']:.3f}\")\n",
    "    print(f\"  - F1 Score: {summary['best_model_metrics']['f1_score']:.3f}\")\n",
    "    print(f\"  - F2 Score: {summary['best_model_metrics']['f2_score']:.3f}\")\n",
    "    print(f\"  - Matthews Correlation: {summary['best_model_metrics']['matthews_corr']:.3f}\")\n",
    "    print(f\"  - Balanced Accuracy: {summary['best_model_metrics']['balanced_accuracy']:.3f}\")\n",
    "    print(f\"  - Confusion Matrix Stats:\")\n",
    "    print(f\"    * True Positives: {summary['best_model_metrics']['confusion_matrix']['tp']}\")\n",
    "    print(f\"    * False Positives: {summary['best_model_metrics']['confusion_matrix']['fp']}\")\n",
    "    print(f\"    * False Negatives: {summary['best_model_metrics']['confusion_matrix']['fn']}\")\n",
    "    print(f\"    * True Negatives: {summary['best_model_metrics']['confusion_matrix']['tn']}\")\n",
    "    \n",
    "    # Average performance across all models\n",
    "    print(\"\\nAverage model performance:\")\n",
    "    avg = summary['average_performance']\n",
    "    print(f\"  - Precision: {avg['precision']:.3f} ({avg['precision_std']:.3f})\")\n",
    "    print(f\"  - Recall: {avg['recall']:.3f} ({avg['recall_std']:.3f})\")\n",
    "    print(f\"  - F1 Score: {avg['f1_score']:.3f} ({avg['f1_score_std']:.3f})\")\n",
    "    print(f\"  - F2 Score: {avg['f2_score']:.3f} ({avg['f2_score_std']:.3f})\")\n",
    "    print(f\"  - Matthews Correlation: {avg['matthews_corr']:.3f} ({avg['matthews_corr_std']:.3f})\")\n",
    "    \n",
    "    # Performance difference between best and worst models\n",
    "    top_models = summary['top_models']\n",
    "    if len(top_models) > 0:\n",
    "        print(f\"\\nPerformance range ({summary['sort_metric']}):\")\n",
    "        print(f\"  - Best: {top_models[0]['score']:.3f} ({top_models[0]['model_name']})\")\n",
    "        print(f\"  - Worst: {top_models[-1]['score']:.3f} ({top_models[-1]['model_name']})\")\n",
    "        print(f\"  - Difference: {top_models[0]['score'] - top_models[-1]['score']:.3f}\")\n",
    "    \n",
    "    # Top 3 models\n",
    "    print(\"\\nTop 3 models:\")\n",
    "    for i, model in enumerate(top_models[:3], 1):\n",
    "        print(f\"  {i}. {model['model_name']}: {model['score']:.3f} {summary['sort_metric'].replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\nCONCLUSIONS FOR PROJECT {summary['project_name']}:\")\n",
    "    \n",
    "    # Draw conclusions about model types\n",
    "    for family, score in summary['model_family_comparison'].items():\n",
    "        print(f\"- {family} models average {summary['sort_metric']}: {score:.3f}\")\n",
    "        \n",
    "    # Identify if any model family consistently performs better\n",
    "    if 'best_model_family' in summary:\n",
    "        print(f\"- {summary['best_model_family']} models tend to perform best overall for this dataset\")\n",
    "    \n",
    "    # Identify optimal threshold range\n",
    "    print(f\"- Correlation between threshold and {summary['sort_metric']}: {summary['threshold_correlation']:.3f}\")\n",
    "    \n",
    "    if abs(summary['threshold_correlation']) > 0.3:\n",
    "        if summary['threshold_correlation'] > 0:\n",
    "            print(\"- Higher thresholds tend to produce better results\")\n",
    "        else:\n",
    "            print(\"- Lower thresholds tend to produce better results\")\n",
    "    else:\n",
    "        print(\"- No strong correlation between threshold values and performance\")\n",
    "    \n",
    "    # Analysis of precision vs recall tradeoff\n",
    "    print(f\"- Precision-Recall tradeoff correlation: {summary['precision_recall_correlation']:.3f}\")\n",
    "    \n",
    "    if summary['precision_recall_correlation'] < -0.5:\n",
    "        print(\"- Strong precision-recall tradeoff observed across models\")\n",
    "    elif summary['precision_recall_correlation'] < -0.3:\n",
    "        print(\"- Moderate precision-recall tradeoff observed across models\")\n",
    "    else:\n",
    "        print(\"- Minimal precision-recall tradeoff observed across models\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\nRECOMMENDATIONS FOR PROJECT {summary['project_name']}:\")\n",
    "    for i, rec in enumerate(summary['recommendations'], 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Limitations\n",
    "    print(f\"\\nLIMITATIONS FOR PROJECT {summary['project_name']}:\")\n",
    "    for i, lim in enumerate(summary['limitations'], 1):\n",
    "        print(f\"{i}. {lim}\")\n",
    "    \n",
    "    # Create visualization if enabled\n",
    "    if show_visualization and 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "        try:\n",
    "            # Create a bar chart comparing top models\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Select top 5 models (or fewer if not enough)\n",
    "            top_models_df = globals()['best_thresholds_df'].head(min(5, len(globals()['best_thresholds_df'])))\n",
    "            \n",
    "            # Prepare data for visualization\n",
    "            model_names = [m.split('/')[-1] if '/' in m else m for m in top_models_df['model_name']]\n",
    "            metrics = ['precision', 'recall', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "            metric_labels = ['Precision', 'Recall', 'F1 Score', 'F2 Score', 'Matthews Corr']\n",
    "            \n",
    "            # Define colors\n",
    "            colors = ['#1A85FF', '#FFC61A', '#00C9A7', '#845EC2', '#D41159']\n",
    "            \n",
    "            # Plot each metric as a group of bars\n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.15\n",
    "            \n",
    "            for i, metric in enumerate(metrics):\n",
    "                plt.bar(x + (i - 2) * width, top_models_df[metric], width, label=metric_labels[i], color=colors[i])\n",
    "            \n",
    "            # Customize plot\n",
    "            plt.xlabel('Model', fontsize=12)\n",
    "            plt.ylabel('Score', fontsize=12)\n",
    "            plt.title(f'Project: {summary[\"project_name\"]} - Top Models Comparison', fontsize=14)\n",
    "            plt.xticks(x, model_names, rotation=45, ha='right')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add optimization metric in subtitle\n",
    "            plt.figtext(0.5, 0.01, f\"Optimized for {summary['optimization_metric']} score\", \n",
    "                       ha='center', fontsize=10, \n",
    "                       bbox=dict(facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create summary visualization: {str(e)}\")\n",
    "            print(f\"Could not create summary visualization: {str(e)}\")\n",
    "\n",
    "# Check if we have model evaluation results available from cell 5\n",
    "if 'best_thresholds_df' in globals() and not best_thresholds_df.empty:\n",
    "    # Create and display analysis summary\n",
    "    summary = create_analysis_summary()\n",
    "    display_analysis_summary(summary, SHOW_VISUALIZATION)\n",
    "else:\n",
    "    print(\"\\nInsufficient evaluation data to draw conclusions.\")\n",
    "    print(\"Please run model evaluation in Cell 6 to generate performance metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Metrics Comparison Whisker Chart with Heatmap Stats\n",
    "# Purpose: Create box plots with statistical heatmap for metrics across models\n",
    "# Dependencies: pandas, matplotlib, seaborn\n",
    "# Breadcrumbs: Visualization -> Metrics Distribution\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_metrics_whisker_plot(model_metrics_df=None, project_name=None, show_visualization=True):\n",
    "    \"\"\"\n",
    "    Create whisker plots showing the distribution of metrics across models with a statistical heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        model_metrics_df: DataFrame containing model metrics (default: best_thresholds_df)\n",
    "        project_name: Name of the project for visualization titles\n",
    "        show_visualization: Whether to display visualizations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing plotting data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global variables if parameters not provided\n",
    "        _model_metrics_df = model_metrics_df if model_metrics_df is not None else globals().get('best_thresholds_df', pd.DataFrame())\n",
    "        _project_name = project_name if project_name is not None else globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "        _show_visualization = show_visualization if show_visualization is not None else globals().get('SHOW_VISUALIZATION', True)\n",
    "        \n",
    "        if _model_metrics_df.empty:\n",
    "            print(\"No model metrics data available. Please run model evaluation in Cell 5 first.\")\n",
    "            return {}\n",
    "            \n",
    "        # Select the metrics we want to visualize\n",
    "        metrics_to_plot = [\n",
    "            'accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "            'f1_score', 'f2_score', 'matthews_corr'\n",
    "        ]\n",
    "        \n",
    "        # Define human-readable names for the metrics\n",
    "        metric_names = {\n",
    "            'accuracy': 'Accuracy',\n",
    "            'balanced_accuracy': 'Balanced Accuracy',\n",
    "            'precision': 'Precision',\n",
    "            'recall': 'Recall',\n",
    "            'f1_score': 'F1 Score',\n",
    "            'f2_score': 'F2 Score',\n",
    "            'matthews_corr': 'Matthews Correlation'\n",
    "        }\n",
    "        \n",
    "        # Reshape data from wide to long format for easier plotting with seaborn\n",
    "        plot_data = pd.melt(\n",
    "            _model_metrics_df, \n",
    "            id_vars=['model_name'], \n",
    "            value_vars=metrics_to_plot,\n",
    "            var_name='metric', \n",
    "            value_name='score'\n",
    "        )\n",
    "        \n",
    "        # Map metric names to their human-readable versions\n",
    "        plot_data['metric'] = plot_data['metric'].map(metric_names)\n",
    "        \n",
    "        # Calculate metric statistics for the heatmap\n",
    "        stats_list = ['min', '25%', 'mean', '50%', '75%', 'max', 'std']\n",
    "        stats_names = ['Min', 'Q1', 'Mean', 'Median', 'Q3', 'Max', 'Std Dev']\n",
    "        \n",
    "        # Create stats DataFrames - one for numeric values, one for formatted strings\n",
    "        stats_values = {}\n",
    "        for metric in metrics_to_plot:\n",
    "            metric_display_name = metric_names[metric]\n",
    "            stats = _model_metrics_df[metric].describe()\n",
    "            stats_values[metric_display_name] = [stats[stat] for stat in stats_list]\n",
    "        \n",
    "        # Create numeric DataFrame for heatmap coloring\n",
    "        stats_df_numeric = pd.DataFrame(stats_values, index=stats_names)\n",
    "        \n",
    "        # Create a new DataFrame with object dtype for formatted strings\n",
    "        formatted_data = {}\n",
    "        for col in stats_df_numeric.columns:\n",
    "            formatted_data[col] = []\n",
    "            for idx in stats_names:\n",
    "                value = stats_df_numeric.loc[idx, col]\n",
    "                if idx == 'Std Dev':\n",
    "                    # Special handling for std dev to use scientific notation if very small\n",
    "                    if value < 0.001:\n",
    "                        formatted_data[col].append(f\"{value:.2e}\")\n",
    "                    else:\n",
    "                        formatted_data[col].append(f\"{value:.3f}\")\n",
    "                else:\n",
    "                    # Format other statistics\n",
    "                    formatted_data[col].append(f\"{value:.3f}\")\n",
    "        \n",
    "        # Create formatted DataFrame with object dtype\n",
    "        stats_df_formatted = pd.DataFrame(formatted_data, index=stats_names)\n",
    "        \n",
    "        # Create a figure with two subplots (box plot above, heatmap below)\n",
    "        fig = plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Create grid for the plots\n",
    "        gs = plt.GridSpec(2, 1, height_ratios=[2, 1], hspace=0.3)\n",
    "        \n",
    "        # Box plot subplot\n",
    "        ax_box = fig.add_subplot(gs[0])\n",
    "        \n",
    "        # Create box plot with consistent color\n",
    "        sns.boxplot(\n",
    "            x='metric', \n",
    "            y='score', \n",
    "            data=plot_data, \n",
    "            ax=ax_box,\n",
    "            color='khaki',  # Light yellow color for all boxes\n",
    "            width=0.5\n",
    "        )\n",
    "        \n",
    "        # Add individual data points\n",
    "        sns.stripplot(\n",
    "            x='metric', \n",
    "            y='score', \n",
    "            data=plot_data, \n",
    "            jitter=True, \n",
    "            color='black',\n",
    "            marker='o', \n",
    "            alpha=0.5,\n",
    "            size=4,\n",
    "            ax=ax_box\n",
    "        )\n",
    "        \n",
    "        # Customize the box plot\n",
    "        ax_box.set_title(f'Project: {_project_name} - Distribution of Performance Metrics', fontsize=14)\n",
    "        ax_box.set_xlabel('')  # Remove x-axis label as it's clear from the plot\n",
    "        ax_box.set_ylabel('Score', fontsize=12)\n",
    "        ax_box.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax_box.set_ylim(0, 1.0)  # Metrics are typically in range [0, 1]\n",
    "        \n",
    "        # Add a horizontal line at y=0.5 as a reference\n",
    "        ax_box.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add legend indicating data source\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='khaki', edgecolor='black', alpha=0.7, label='Sentence Transformer/TF-IDF')]\n",
    "        ax_box.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        # Heatmap subplot\n",
    "        ax_heatmap = fig.add_subplot(gs[1])\n",
    "        \n",
    "        # Create a mask for NaN values\n",
    "        mask = np.isnan(stats_df_numeric.values)\n",
    "        \n",
    "        # Create the heatmap\n",
    "        cmap = sns.light_palette(\"gold\", as_cmap=True)  # Light yellow color palette to match box plots\n",
    "        sns.heatmap(\n",
    "            stats_df_numeric,\n",
    "            annot=stats_df_formatted.values,  # Use formatted strings for annotations\n",
    "            fmt=\"\",  # No additional formatting needed as we pre-formatted the values\n",
    "            cmap=cmap,\n",
    "            linewidths=0.5,\n",
    "            linecolor='lightgray',\n",
    "            cbar=False,  # No color bar needed\n",
    "            ax=ax_heatmap,\n",
    "            mask=mask,\n",
    "            annot_kws={\"size\": 10, \"weight\": \"normal\"},\n",
    "            vmin=0,  # Set minimum value for color scaling\n",
    "            vmax=1.0  # Set maximum value for color scaling (most metrics are 0-1)\n",
    "        )\n",
    "        \n",
    "        # Customize heatmap appearance\n",
    "        ax_heatmap.set_title('Statistical Summary', fontsize=12)\n",
    "        ax_heatmap.set_xticklabels(ax_heatmap.get_xticklabels(), rotation=0, ha='center')\n",
    "        \n",
    "        # Custom color for std dev row - we need to identify the cells in the last row\n",
    "        # and make them a light gray because they're not on the same scale\n",
    "        cells = ax_heatmap.get_children()\n",
    "        std_dev_row_idx = len(stats_names) - 1  # Std Dev is the last row\n",
    "        \n",
    "        # Filter for rectangle patches (cells) that are in the std dev row\n",
    "        # This works because cells are ordered row by row from bottom to top\n",
    "        for i, cell in enumerate(cells):\n",
    "            if hasattr(cell, 'get_xy'):  # Check if it's a patch with coordinates\n",
    "                # Get row index from patch position\n",
    "                row = int(i / len(stats_df_numeric.columns))\n",
    "                if row == std_dev_row_idx:\n",
    "                    cell.set_facecolor('#f5f5f5')  # Light gray for std dev row\n",
    "        \n",
    "        # Adjust figure layout\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "        \n",
    "        if _show_visualization:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        return {\n",
    "            'plot_data': plot_data,\n",
    "            'stats_df_numeric': stats_df_numeric,\n",
    "            'stats_df_formatted': stats_df_formatted\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating metrics whisker plot: {str(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return {}\n",
    "\n",
    "# Create and display metrics whisker plot with statistical heatmap\n",
    "metrics_viz_data = create_metrics_whisker_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Store Whisker Chart Data in Neo4j\n",
    "# Purpose: Store the metrics statistics data from whisker chart in Neo4j linked to project\n",
    "# Dependencies: neo4j, pandas, logging\n",
    "# Breadcrumbs: Data Storage -> Neo4j Persistence\n",
    "\n",
    "import json\n",
    "from datetime import datetime  # Added global import for datetime\n",
    "\n",
    "def store_whisker_chart_data_in_neo4j(metrics_viz_data=None, driver=None, project_name=None):\n",
    "    \"\"\"\n",
    "    Store the metrics statistics data from the whisker chart in Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        metrics_viz_data: Dictionary containing whisker chart data (from create_metrics_whisker_plot)\n",
    "        driver: Neo4j driver connection\n",
    "        project_name: Project name to attach the metrics data to\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use global variables if parameters not provided\n",
    "        _metrics_viz_data = metrics_viz_data if metrics_viz_data is not None else globals().get('metrics_viz_data', {})\n",
    "        _driver = driver if driver is not None else globals().get('driver')\n",
    "        _project_name = project_name if project_name is not None else globals().get('NEO4J_PROJECT_NAME', 'Unknown Project')\n",
    "        \n",
    "        if not _metrics_viz_data or not _driver:\n",
    "            logger.error(\"Missing metrics data or Neo4j driver for storage\")\n",
    "            return False\n",
    "        \n",
    "        # Check if we have the stats data\n",
    "        if 'stats_df_numeric' not in _metrics_viz_data or _metrics_viz_data['stats_df_numeric'].empty:\n",
    "            logger.error(\"No statistical metrics data available for storage\")\n",
    "            return False\n",
    "        \n",
    "        # Get the stats DataFrames\n",
    "        stats_df = _metrics_viz_data['stats_df_numeric']\n",
    "        \n",
    "        # Prepare data for Neo4j storage - convert stats dataframe to dictionary\n",
    "        metrics_data = {}\n",
    "        for column in stats_df.columns:\n",
    "            # Each column is a metric\n",
    "            metric_stats = {}\n",
    "            for idx, value in stats_df[column].items():\n",
    "                # Each row is a statistic (min, max, etc.)\n",
    "                # Convert to standard lowercase keys without spaces\n",
    "                key = idx.lower().replace(' ', '_')\n",
    "                metric_stats[key] = float(value)\n",
    "            \n",
    "            # Add metric stats to overall data\n",
    "            metrics_data[column.lower().replace(' ', '_')] = metric_stats\n",
    "        \n",
    "        # Also add the number of models analyzed\n",
    "        if 'plot_data' in _metrics_viz_data and not _metrics_viz_data['plot_data'].empty:\n",
    "            model_count = _metrics_viz_data['plot_data']['model_name'].nunique()\n",
    "            metrics_data['model_count'] = model_count\n",
    "        \n",
    "        # Create model_data dictionary to store the individual data points that make up the whisker chart\n",
    "        model_data = {}\n",
    "        \n",
    "        # Create results_data dictionary to store TP, FP, FN, TN for each model\n",
    "        results_data = {}\n",
    "        \n",
    "        # If we have best_thresholds_df, extract the confusion matrix data (TP, FP, FN, TN)\n",
    "        if 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "            best_df = globals()['best_thresholds_df']\n",
    "            print(f\"\\nExtracting confusion matrix data from best_thresholds_df with shape {best_df.shape}\")\n",
    "            \n",
    "            # For each model, store its confusion matrix data\n",
    "            for _, row in best_df.iterrows():\n",
    "                model_name = row['model_name']\n",
    "                # Extract just the model name without path\n",
    "                if '/' in model_name:\n",
    "                    model_key = model_name.split('/')[-1]\n",
    "                else:\n",
    "                    model_key = model_name\n",
    "                \n",
    "                # Store confusion matrix data for this model\n",
    "                results_data[model_key] = {\n",
    "                    'true_positives': int(row['true_positives']) if not pd.isna(row['true_positives']) else 0,\n",
    "                    'false_positives': int(row['false_positives']) if not pd.isna(row['false_positives']) else 0,\n",
    "                    'false_negatives': int(row['false_negatives']) if not pd.isna(row['false_negatives']) else 0,\n",
    "                    'true_negatives': int(row['true_negatives']) if not pd.isna(row['true_negatives']) else 0,\n",
    "                    'threshold': float(row['best_threshold']) if not pd.isna(row['best_threshold']) else 0.0\n",
    "                }\n",
    "            \n",
    "            print(f\"  Added confusion matrix data for {len(results_data)} models\")\n",
    "            # Show a few examples\n",
    "            for model_name, data in list(results_data.items())[:3]:\n",
    "                print(f\"    {model_name}: TP={data['true_positives']}, FP={data['false_positives']}, \" +\n",
    "                      f\"FN={data['false_negatives']}, TN={data['true_negatives']}, threshold={data['threshold']}\")\n",
    "            if len(results_data) > 3:\n",
    "                print(f\"    ... and {len(results_data) - 3} more\")\n",
    "        \n",
    "        # If we have plot_data from the whisker chart in cell 8, extract the individual data points by metric\n",
    "        if 'plot_data' in _metrics_viz_data and not _metrics_viz_data['plot_data'].empty:\n",
    "            plot_df = _metrics_viz_data['plot_data']\n",
    "            print(f\"\\nExtracting whisker chart data points from plot_data with shape {plot_df.shape}\")\n",
    "            \n",
    "            # Group plot data by metric and extract all scores by model\n",
    "            for metric in plot_df['metric'].unique():\n",
    "                # Convert metric to standardized key format\n",
    "                metric_key = metric.lower().replace(' ', '_')\n",
    "                model_data[metric_key] = {}\n",
    "                \n",
    "                # Get data for just this metric\n",
    "                metric_data = plot_df[plot_df['metric'] == metric]\n",
    "                \n",
    "                # Store scores by model name\n",
    "                for _, row in metric_data.iterrows():\n",
    "                    model_name = row['model_name']\n",
    "                    # For model names with paths, just use the last part\n",
    "                    if '/' in model_name:\n",
    "                        model_key = model_name.split('/')[-1]\n",
    "                    else:\n",
    "                        model_key = model_name\n",
    "                    \n",
    "                    model_data[metric_key][model_key] = float(row['score'])\n",
    "                \n",
    "                print(f\"  Added {len(metric_data)} data points for {metric}\")\n",
    "        \n",
    "        # If model_data is empty and we have best_thresholds_df, try to get data from there as fallback\n",
    "        if not model_data and 'best_thresholds_df' in globals() and not globals()['best_thresholds_df'].empty:\n",
    "            best_df = globals()['best_thresholds_df']\n",
    "            print(f\"\\nExtracting data points from best_thresholds_df with shape {best_df.shape}\")\n",
    "            \n",
    "            # Metrics we want to capture\n",
    "            metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "                       'f1_score', 'f2_score', 'matthews_corr']\n",
    "            \n",
    "            # For each metric, create an entry with model scores\n",
    "            for metric in metrics:\n",
    "                if metric in best_df.columns:\n",
    "                    metric_key = metric.lower()\n",
    "                    model_data[metric_key] = {}\n",
    "                    \n",
    "                    # For each model, store its score for this metric\n",
    "                    for _, row in best_df.iterrows():\n",
    "                        model_name = row['model_name']\n",
    "                        # Extract just the model name without path\n",
    "                        if '/' in model_name:\n",
    "                            model_key = model_name.split('/')[-1]\n",
    "                        else:\n",
    "                            model_key = model_name\n",
    "                        \n",
    "                        model_data[metric_key][model_key] = float(row[metric])\n",
    "                    \n",
    "                    print(f\"  Added {len(best_df)} data points for {metric}\")\n",
    "        \n",
    "        # If we've collected data points, print a summary\n",
    "        if model_data:\n",
    "            print(\"\\nModel data summary:\")\n",
    "            for metric, values in model_data.items():\n",
    "                print(f\"  {metric}: {len(values)} data points\")\n",
    "                # Show a few examples\n",
    "                for model_name, score in list(values.items())[:3]:\n",
    "                    print(f\"    {model_name}: {score}\")\n",
    "                if len(values) > 3:\n",
    "                    print(f\"    ... and {len(values) - 3} more\")\n",
    "        else:\n",
    "            print(\"No individual data points were extracted for the model_data field\")\n",
    "        \n",
    "        # Serialize data to JSON for Neo4j storage\n",
    "        metrics_json = json.dumps(metrics_data)\n",
    "        model_data_json = json.dumps(model_data)\n",
    "        results_json = json.dumps(results_data)\n",
    "        \n",
    "        # Current timestamp for the analysis record\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Cypher query to create metrics data connected to project\n",
    "        # IMPORTANT: Fixed to use stable identifiers in the MERGE pattern\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})\n",
    "        MERGE (m:MetricsAnalysis {project_name: $project_name, model_type: $model_type, analysis_type: 'whisker_chart'})\n",
    "        MERGE (p)-[r:HAS_METRICS_ANALYSIS {model_type: $model_type}]->(m)\n",
    "        SET m.metrics_data = $metrics_data,\n",
    "            m.model_count = $model_count,\n",
    "            m.model_data = $model_data,\n",
    "            m.results = $results_data,\n",
    "            m.created_at = CASE WHEN m.created_at IS NULL THEN $timestamp ELSE m.created_at END,\n",
    "            m.last_updated = $timestamp,\n",
    "            r.timestamp = $timestamp,\n",
    "            r.last_updated = $timestamp\n",
    "        RETURN p.name as project_name, m.created_at as created_at\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute query to store metrics data\n",
    "        with _driver.session() as session:\n",
    "            result = session.run(\n",
    "                query,\n",
    "                project_name=_project_name,\n",
    "                model_type=\"sentence_transformer_tf_idf\",\n",
    "                timestamp=timestamp,\n",
    "                metrics_data=metrics_json,\n",
    "                model_data=model_data_json,\n",
    "                results_data=results_json,\n",
    "                model_count=model_count if 'model_count' in locals() else 0\n",
    "            ).single()\n",
    "            \n",
    "            if result:\n",
    "                logger.info(f\"Successfully stored whisker chart metrics data for project: {result['project_name']} at {result['created_at']}\")\n",
    "                print(f\"Stored metrics analysis data for {model_count if 'model_count' in locals() else 0} models in Neo4j at {timestamp}\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(f\"No result returned when storing metrics data for project: {_project_name}\")\n",
    "                print(\"No result returned when storing metrics data. Check logs for details.\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing whisker chart data in Neo4j: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        print(f\"Error storing whisker chart data in Neo4j: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Store the metrics data in Neo4j if available\n",
    "if 'metrics_viz_data' in globals() and metrics_viz_data:\n",
    "    success = store_whisker_chart_data_in_neo4j()\n",
    "    if success:\n",
    "        print(\"\\nMetrics whisker chart data successfully stored in Neo4j\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Project: {NEO4J_PROJECT_NAME}\")\n",
    "        print(f\"Model type: sentence_transformer_tf_idf\")\n",
    "        print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "    else:\n",
    "        print(\"\\nFailed to store metrics whisker chart data. Check logs for details.\")\n",
    "else:\n",
    "    print(\"\\nNo metrics whisker chart data available for storage\")\n",
    "    print(\"Please run cell 8 first to generate the data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

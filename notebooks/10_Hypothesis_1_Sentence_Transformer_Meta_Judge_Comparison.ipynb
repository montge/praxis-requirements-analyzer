{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1: Hallucination Reduction for NPD Accuracy Improvement\n",
    "**Techniques reducing the hallucinations generated by LLMs by 30% can improve the accuracy of outputs used in NPD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for sentence transformer and meta judge comparison\n",
    "# Dependencies: os, logging, numpy, pandas, dotenv, neo4j, matplotlib, seaborn, sklearn, scipy, statsmodels, json, datetime, pathlib\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score,\n",
    "    matthews_corrcoef, confusion_matrix, balanced_accuracy_score,\n",
    "    cohen_kappa_score, roc_auc_score, precision_recall_curve, auc,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import scipy.stats as stats\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.stats import wilcoxon, friedmanchisquare, ttest_rel\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configure logging and load environment variables\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Neo4j credentials from environment variables\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME'),\n",
    "        'OPTIMIZATION_METRIC': os.getenv('OPTIMIZATION_METRIC', 'F2').upper(),\n",
    "        'SHOW_VISUALIZATION': os.getenv('SHOW_VISUALIZATION', 'False').lower() == 'true',\n",
    "        'MATCH_DIRECTION': os.getenv('MATCH_DIRECTION', 'source_to_target')\n",
    "    }\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = Path('results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    config['RESULTS_DIR'] = results_dir\n",
    "    \n",
    "    logger.info(f\"Using {config['OPTIMIZATION_METRIC']} score for threshold optimization\")\n",
    "    logger.info(f\"Visualization display is set to: {config['SHOW_VISUALIZATION']}\")\n",
    "    logger.info(f\"Results will be saved to: {config['RESULTS_DIR']}\")\n",
    "    \n",
    "    return config, logger\n",
    "\n",
    "# Execute setup\n",
    "CONFIG, logger = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Connection Setup\n",
    "# Purpose: Create connection to Neo4j database\n",
    "# Dependencies: Neo4j GraphDatabase, config from Cell [0]\n",
    "# Breadcrumbs: Setup -> Database Connection -> Driver Creation\n",
    "\n",
    "def create_neo4j_driver(config):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance\n",
    "    \n",
    "    Parameters:\n",
    "        config (dict): Configuration dictionary with Neo4j credentials\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(\n",
    "            config['NEO4J_URI'], \n",
    "            auth=(config['NEO4J_USER'], config['NEO4J_PASSWORD'])\n",
    "        )\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = create_neo4j_driver(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Query SIMILAR_TO Links\n",
    "# Purpose: Retrieve sentence transformer similarity links from Neo4j\n",
    "# Dependencies: Neo4j driver from Cell [1], project configuration\n",
    "# Breadcrumbs: Database Connection -> Similarity Data Retrieval -> Sentence Transformer Links\n",
    "\n",
    "def query_similar_to_links(driver, project_name, match_direction='source_to_target'):\n",
    "    \"\"\"\n",
    "    Query SIMILAR_TO links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name: Name of the project\n",
    "        match_direction: Direction of matching ('source_to_target', 'target_to_source', 'both')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing similarity links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Querying SIMILAR_TO links for project: {project_name}\")\n",
    "        logger.info(f\"Match direction: {match_direction}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        # Query for source-to-target links\n",
    "        if match_direction in ['source_to_target', 'both']:\n",
    "            source_to_target_query = \"\"\"\n",
    "            MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source_req:Requirement)-[r:SIMILAR_TO]->(target_req:Requirement)\n",
    "            WHERE source_req.type = 'SOURCE' AND target_req.type = 'TARGET'\n",
    "            AND EXISTS { \n",
    "                MATCH (source_req)-[:GROUND_TRUTH]->() \n",
    "            } \n",
    "            AND EXISTS { \n",
    "                MATCH ()-[:GROUND_TRUTH]->(target_req) \n",
    "            }\n",
    "            RETURN \n",
    "                p.name as project_name,\n",
    "                source_req.id as source_id,\n",
    "                target_req.id as target_id,\n",
    "                r.model as sentence_transformer_model,\n",
    "                r.similarity as similarity_score,\n",
    "                r.timestamp as timestamp,\n",
    "                'source_to_target' as direction\n",
    "            \"\"\"\n",
    "            \n",
    "            with driver.session() as session:\n",
    "                results = session.run(source_to_target_query, project_name=project_name).data()\n",
    "                all_results.extend(results)\n",
    "                logger.info(f\"Retrieved {len(results)} source-to-target SIMILAR_TO links\")\n",
    "        \n",
    "        # Query for target-to-source links\n",
    "        if match_direction in ['target_to_source', 'both']:\n",
    "            target_to_source_query = \"\"\"\n",
    "            MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(target_req:Requirement)-[r:SIMILAR_TO]->(source_req:Requirement)\n",
    "            WHERE target_req.type = 'TARGET' AND source_req.type = 'SOURCE'\n",
    "            AND EXISTS { \n",
    "                MATCH (source_req)-[:GROUND_TRUTH]->() \n",
    "            } \n",
    "            AND EXISTS { \n",
    "                MATCH ()-[:GROUND_TRUTH]->(target_req) \n",
    "            }\n",
    "            RETURN \n",
    "                p.name as project_name,\n",
    "                source_req.id as source_id,\n",
    "                target_req.id as target_id,\n",
    "                r.model as sentence_transformer_model,\n",
    "                r.similarity as similarity_score,\n",
    "                r.timestamp as timestamp,\n",
    "                'target_to_source' as direction\n",
    "            \"\"\"\n",
    "            \n",
    "            with driver.session() as session:\n",
    "                results = session.run(target_to_source_query, project_name=project_name).data()\n",
    "                all_results.extend(results)\n",
    "                logger.info(f\"Retrieved {len(results)} target-to-source SIMILAR_TO links\")\n",
    "        \n",
    "        if all_results:\n",
    "            df = pd.DataFrame(all_results)\n",
    "            df['model'] = df['sentence_transformer_model']  # Add alias for compatibility\n",
    "            logger.info(f\"Total SIMILAR_TO links retrieved: {len(df)}\")\n",
    "            \n",
    "            # Count unique models\n",
    "            model_counts = df['sentence_transformer_model'].value_counts()\n",
    "            logger.info(\"Models found:\")\n",
    "            for model, count in model_counts.items():\n",
    "                logger.info(f\"  - {model}: {count} links\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No SIMILAR_TO links found\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying SIMILAR_TO links: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Query SIMILAR_TO links\n",
    "similar_to_df = query_similar_to_links(driver, CONFIG['NEO4J_PROJECT_NAME'], CONFIG['MATCH_DIRECTION'])\n",
    "\n",
    "# Display info\n",
    "if not similar_to_df.empty:\n",
    "    print(f\"\\nSIMILAR_TO Links Summary:\")\n",
    "    print(f\"Total links: {len(similar_to_df)}\")\n",
    "    print(f\"Unique models: {similar_to_df['model'].nunique()}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(similar_to_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Query Ground Truth Links\n",
    "# Purpose: Retrieve ground truth traceability links from Neo4j\n",
    "# Dependencies: Neo4j driver from Cell [1], project configuration\n",
    "# Breadcrumbs: Database Connection -> Ground Truth Data Retrieval -> Traceability Links\n",
    "\n",
    "def query_ground_truth_links(driver, project_name):\n",
    "    \"\"\"\n",
    "    Query ground truth traceability links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name: Name of the project\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ground truth links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ground_truth_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:GROUND_TRUTH]->(target:Requirement)\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            source.type as source_type,\n",
    "            target.id as target_id,\n",
    "            target.type as target_type,\n",
    "            1 as ground_truth\n",
    "        ORDER BY source.id, target.id DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            results = session.run(ground_truth_query, project_name=project_name).data()\n",
    "            \n",
    "            if results:\n",
    "                logger.info(f\"Retrieved {len(results)} ground truth links\")\n",
    "                return pd.DataFrame(results)\n",
    "            else:\n",
    "                logger.warning(\"No ground truth links found\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying ground truth links: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Query ground truth links\n",
    "df_ground_truth = query_ground_truth_links(driver, CONFIG['NEO4J_PROJECT_NAME'])\n",
    "\n",
    "# Display info\n",
    "if not df_ground_truth.empty:\n",
    "    print(f\"\\nGround Truth Links Summary:\")\n",
    "    print(f\"Total links: {len(df_ground_truth)}\")\n",
    "    print(f\"Unique source requirements: {df_ground_truth['source_id'].nunique()}\")\n",
    "    print(f\"Unique target requirements: {df_ground_truth['target_id'].nunique()}\")\n",
    "    print(f\"Link density: {len(df_ground_truth) / (df_ground_truth['source_id'].nunique() * df_ground_truth['target_id'].nunique()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Create Combined Dataset\n",
    "# Purpose: Merge similarity and ground truth data for analysis\n",
    "# Dependencies: similar_to_df from Cell [2], df_ground_truth from Cell [3]\n",
    "# Breadcrumbs: Data Retrieval -> Data Merge -> Combined Dataset Creation\n",
    "\n",
    "def create_combined_dataset(similar_to_df, df_ground_truth):\n",
    "    \"\"\"\n",
    "    Create a combined dataset with similarity scores and ground truth labels\n",
    "    \n",
    "    Parameters:\n",
    "        similar_to_df: DataFrame with similarity scores\n",
    "        df_ground_truth: DataFrame with ground truth links\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset\n",
    "    \"\"\"\n",
    "    if similar_to_df.empty:\n",
    "        logger.error(\"No SIMILAR_TO links available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with similarity data\n",
    "    combined_df = similar_to_df.copy()\n",
    "    \n",
    "    # Add ground truth information\n",
    "    if not df_ground_truth.empty:\n",
    "        # Create set of ground truth pairs\n",
    "        ground_truth_pairs = set(zip(df_ground_truth['source_id'], df_ground_truth['target_id']))\n",
    "        \n",
    "        # Add ground truth column\n",
    "        combined_df['ground_truth_traceable'] = combined_df.apply(\n",
    "            lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Ground truth distribution: {combined_df['ground_truth_traceable'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        logger.warning(\"No ground truth data available\")\n",
    "        combined_df['ground_truth_traceable'] = False\n",
    "    \n",
    "    # Clean similarity scores\n",
    "    combined_df['similarity_score'] = pd.to_numeric(combined_df['similarity_score'], errors='coerce').fillna(0)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Create combined dataset\n",
    "combined_df = create_combined_dataset(similar_to_df, df_ground_truth)\n",
    "\n",
    "# Display info\n",
    "if not combined_df.empty:\n",
    "    print(f\"\\nCombined Dataset Summary:\")\n",
    "    print(f\"Total records: {len(combined_df)}\")\n",
    "    print(f\"Ground truth positive: {combined_df['ground_truth_traceable'].sum()}\")\n",
    "    print(f\"Ground truth negative: {(~combined_df['ground_truth_traceable']).sum()}\")\n",
    "    print(f\"Similarity score range: [{combined_df['similarity_score'].min():.4f}, {combined_df['similarity_score'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Model Evaluation and Threshold Optimization\n",
    "# Purpose: Evaluate each model and find optimal thresholds\n",
    "# Dependencies: combined_df from Cell [4], sklearn metrics from Cell [0]\n",
    "# Breadcrumbs: Combined Dataset -> Model Evaluation -> Threshold Optimization\n",
    "\n",
    "def evaluate_model_thresholds(df, model_name, score_column='similarity_score', \n",
    "                             ground_truth_column='ground_truth_traceable', \n",
    "                             optimize_for='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance across different thresholds\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing model predictions and ground truth\n",
    "        model_name: Name of the model to evaluate\n",
    "        score_column: Column containing similarity scores\n",
    "        ground_truth_column: Column containing ground truth values\n",
    "        optimize_for: Metric to optimize for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this model\n",
    "        model_df = df[df['model'] == model_name].copy()\n",
    "        \n",
    "        if model_df.empty:\n",
    "            logger.warning(f\"No data available for model: {model_name}\")\n",
    "            return {}\n",
    "            \n",
    "        if ground_truth_column not in model_df.columns:\n",
    "            logger.warning(f\"Ground truth column '{ground_truth_column}' not found for model: {model_name}\")\n",
    "            return {}\n",
    "        \n",
    "        # Get ground truth and scores\n",
    "        y_true = model_df[ground_truth_column].astype(int).values\n",
    "        \n",
    "        # Check for and handle None/NaN values in similarity scores\n",
    "        if model_df[score_column].isna().any():\n",
    "            logger.warning(f\"Found NaN values in {score_column} for model {model_name}. Filling with 0.\")\n",
    "            model_df[score_column] = model_df[score_column].fillna(0)\n",
    "        \n",
    "        # Ensure similarity scores are numeric\n",
    "        if model_df[score_column].dtype == object:\n",
    "            try:\n",
    "                model_df[score_column] = pd.to_numeric(model_df[score_column])\n",
    "                logger.info(f\"Converted {score_column} to numeric for model {model_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error converting {score_column} to numeric: {str(e)}\")\n",
    "                model_df[score_column] = 0\n",
    "        \n",
    "        scores = model_df[score_column].values\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"  - Data points: {len(model_df)}\")\n",
    "        print(f\"  - Positive examples: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Negative examples: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.2f}%)\")\n",
    "        print(f\"  - Score range: {scores.min():.4f} to {scores.max():.4f}\")\n",
    "        \n",
    "        # If all ground truth values are the same, we can't calculate meaningful metrics\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            logger.warning(f\"Insufficient ground truth variety for model {model_name} - all values are {np.unique(y_true)[0]}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "                'error': 'Only one class present'\n",
    "            }\n",
    "        \n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "        \n",
    "        # Add a threshold of 1.0 to the end for completeness\n",
    "        thresholds = np.append(thresholds, 1.0)\n",
    "        \n",
    "        # Calculate metrics for each threshold\n",
    "        results = []\n",
    "        \n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            # Convert scores to binary predictions using this threshold\n",
    "            y_pred = (scores >= threshold).astype(int)\n",
    "            \n",
    "            # Skip if all predictions are the same (all 0 or all 1)\n",
    "            if len(np.unique(y_pred)) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Confusion matrix components\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "            prec = precision[min(i, len(precision)-1)]\n",
    "            rec = recall[min(i, len(recall)-1)]\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "            \n",
    "            # Additional metrics\n",
    "            tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity/True Negative Rate\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss Rate/False Negative Rate\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'accuracy': accuracy,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'tnr': tnr,  # specificity\n",
    "                'fnr': fnr,  # miss rate\n",
    "                'f1_score': f1,\n",
    "                'f2_score': f2,\n",
    "                'mcc': mcc  # Matthews Correlation Coefficient\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if results_df.empty:\n",
    "            logger.warning(f\"No valid threshold results for model {model_name}\")\n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'data_points': len(model_df),\n",
    "                'ground_truth_positive': int(y_true.sum()),\n",
    "                'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "                'error': 'No valid thresholds found'\n",
    "            }\n",
    "        \n",
    "        # Find best threshold based on optimization metric\n",
    "        if optimize_for == 'F1':\n",
    "            best_idx = results_df['f1_score'].idxmax()\n",
    "            best_metric = 'f1_score'\n",
    "        else:  # F2\n",
    "            best_idx = results_df['f2_score'].idxmax()\n",
    "            best_metric = 'f2_score'\n",
    "            \n",
    "        best_result = results_df.loc[best_idx]\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df),\n",
    "            'ground_truth_positive': int(y_true.sum()),\n",
    "            'ground_truth_negative': int(len(y_true) - y_true.sum()),\n",
    "            'best_threshold': best_result['threshold'],\n",
    "            'best_precision': best_result['precision'],\n",
    "            'best_recall': best_result['recall'],\n",
    "            'best_accuracy': best_result['accuracy'],\n",
    "            'best_balanced_accuracy': best_result['balanced_accuracy'],\n",
    "            'best_f1': best_result['f1_score'],\n",
    "            'best_f2': best_result['f2_score'],\n",
    "            'best_tnr': best_result['tnr'],\n",
    "            'best_fnr': best_result['fnr'],\n",
    "            'best_mcc': best_result['mcc'],\n",
    "            'best_tp': int(best_result['tp']),\n",
    "            'best_fp': int(best_result['fp']),\n",
    "            'best_fn': int(best_result['fn']),\n",
    "            'best_tn': int(best_result['tn']),\n",
    "            'optimization_metric': optimize_for,\n",
    "            'threshold_results': results_df\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error evaluating model {model_name}: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'data_points': len(model_df) if 'model_df' in locals() else 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def evaluate_all_models(combined_df, optimization_metric='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate all models in the combined dataset\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df: DataFrame containing model predictions and ground truth\n",
    "        optimization_metric: Metric to optimize thresholds for ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (evaluation_results, best_thresholds_df)\n",
    "            - evaluation_results: List of dictionaries with evaluation results\n",
    "            - best_thresholds_df: DataFrame with best thresholds for each model\n",
    "    \"\"\"\n",
    "    # Check if we have the necessary data\n",
    "    if 'ground_truth_traceable' not in combined_df.columns or 'model' not in combined_df.columns:\n",
    "        logger.error(\"Cannot evaluate models: missing ground truth or model data\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Get list of all models\n",
    "    all_models = combined_df['model'].unique()\n",
    "    \n",
    "    # Evaluate each model\n",
    "    evaluation_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(all_models)} sentence transformer models\")\n",
    "    print(f\"Optimizing for {optimization_metric} score\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model in all_models:\n",
    "        print(f\"\\nEvaluating model: {model}\")\n",
    "        result = evaluate_model_thresholds(combined_df, model, optimize_for=optimization_metric)\n",
    "        \n",
    "        if result:\n",
    "            evaluation_results.append(result)\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"  - Error: {result['error']}\")\n",
    "                \n",
    "            if 'best_threshold' in result:\n",
    "                print(f\"  - Best threshold: {result['best_threshold']:.3f}\")\n",
    "                print(f\"  - Confusion Matrix (TP, FP, FN, TN): {result['best_tp']}, {result['best_fp']}, {result['best_fn']}, {result['best_tn']}\")\n",
    "                print(f\"  - Accuracy: {result['best_accuracy']:.3f}\")\n",
    "                print(f\"  - Balanced Accuracy: {result['best_balanced_accuracy']:.3f}\")\n",
    "                print(f\"  - Precision: {result['best_precision']:.3f}\")\n",
    "                print(f\"  - Recall/TPR: {result['best_recall']:.3f}\")\n",
    "                print(f\"  - Specificity/TNR: {result['best_tnr']:.3f}\")\n",
    "                print(f\"  - Miss Rate/FNR: {result['best_fnr']:.3f}\")\n",
    "                print(f\"  - F1: {result['best_f1']:.3f}\")\n",
    "                print(f\"  - F2: {result['best_f2']:.3f}\")\n",
    "                print(f\"  - Matthews Correlation Coefficient: {result['best_mcc']:.3f}\")\n",
    "    \n",
    "    # Create DataFrame of best thresholds with all metrics\n",
    "    if evaluation_results:\n",
    "        best_thresholds_df = pd.DataFrame([\n",
    "            {\n",
    "                'model_name': r['model_name'],\n",
    "                'best_threshold': r['best_threshold'] if 'best_threshold' in r else np.nan,\n",
    "                'accuracy': r['best_accuracy'] if 'best_accuracy' in r else np.nan,\n",
    "                'balanced_accuracy': r['best_balanced_accuracy'] if 'best_balanced_accuracy' in r else np.nan,\n",
    "                'precision': r['best_precision'] if 'best_precision' in r else np.nan,\n",
    "                'recall': r['best_recall'] if 'best_recall' in r else np.nan,\n",
    "                'specificity': r['best_tnr'] if 'best_tnr' in r else np.nan,\n",
    "                'miss_rate': r['best_fnr'] if 'best_fnr' in r else np.nan,\n",
    "                'f1_score': r['best_f1'] if 'best_f1' in r else np.nan,\n",
    "                'f2_score': r['best_f2'] if 'best_f2' in r else np.nan,\n",
    "                'matthews_corr': r['best_mcc'] if 'best_mcc' in r else np.nan,\n",
    "                'true_positives': r['best_tp'] if 'best_tp' in r else np.nan,\n",
    "                'false_positives': r['best_fp'] if 'best_fp' in r else np.nan,\n",
    "                'false_negatives': r['best_fn'] if 'best_fn' in r else np.nan,\n",
    "                'true_negatives': r['best_tn'] if 'best_tn' in r else np.nan,\n",
    "                'data_points': r['data_points'],\n",
    "                'ground_truth_positive': r['ground_truth_positive'] if 'ground_truth_positive' in r else 0,\n",
    "                'ground_truth_negative': r['ground_truth_negative'] if 'ground_truth_negative' in r else 0,\n",
    "            }\n",
    "            for r in evaluation_results if 'best_threshold' in r\n",
    "        ])\n",
    "        \n",
    "        # Sort by the appropriate metric\n",
    "        sort_col = 'f1_score' if optimization_metric == 'F1' else 'f2_score'\n",
    "        best_thresholds_df = best_thresholds_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\nBest Thresholds by Model:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(best_thresholds_df.to_string())\n",
    "        \n",
    "        return evaluation_results, best_thresholds_df\n",
    "    else:\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# Run model evaluation\n",
    "if 'combined_df' in globals() and not combined_df.empty:\n",
    "    evaluation_results, best_thresholds_df = evaluate_all_models(combined_df, CONFIG['OPTIMIZATION_METRIC'])\n",
    "else:\n",
    "    print(\"\\nCombined dataset not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - Query LLM Meta Judge Links\n",
    "# Purpose: Retrieve LLM_RESULT_META_JUDGE links from Neo4j for comparison\n",
    "# Dependencies: Neo4j driver from Cell [1], environment variables\n",
    "# Breadcrumbs: Database Connection -> Meta Judge Data Retrieval -> LLM Results\n",
    "\n",
    "def get_analysis_model_ids():\n",
    "    \"\"\"\n",
    "    Get the list of model IDs to analyze from environment variables\n",
    "    \n",
    "    Returns:\n",
    "        list: List of model IDs to filter by\n",
    "    \"\"\"\n",
    "    # Get the comma-separated list of model ID variable names\n",
    "    model_id_vars = os.getenv('RESULTS_ANALYSIS_MODEL_IDS', '')\n",
    "    \n",
    "    if not model_id_vars:\n",
    "        logger.warning(\"RESULTS_ANALYSIS_MODEL_IDS not set, will retrieve all models\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the comma-separated list\n",
    "    model_id_var_names = [var.strip() for var in model_id_vars.split(',') if var.strip()]\n",
    "    \n",
    "    # Get the actual model IDs from the environment variables\n",
    "    model_ids = []\n",
    "    for var_name in model_id_var_names:\n",
    "        model_id = os.getenv(var_name)\n",
    "        if model_id:\n",
    "            model_ids.append(model_id)\n",
    "            logger.info(f\"Added model ID from {var_name}: {model_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"Environment variable {var_name} not found or empty\")\n",
    "    \n",
    "    return model_ids\n",
    "\n",
    "def query_meta_judge_links(driver, project_name, model_ids=None):\n",
    "    \"\"\"\n",
    "    Query LLM_RESULT_META_JUDGE links from Neo4j\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection\n",
    "        project_name: Name of the project\n",
    "        model_ids: List of model IDs to filter by (optional)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing meta judge links\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Querying LLM_RESULT_META_JUDGE links for project: {project_name}\")\n",
    "        \n",
    "        if model_ids:\n",
    "            logger.info(f\"Filtering for models: {model_ids}\")\n",
    "        \n",
    "        # Base query for meta-judge links\n",
    "        base_query = \"\"\"\n",
    "        MATCH (p:Project {name: $project_name})-[:CONTAINS]->(d:Document)-[:CONTAINS]->(source:Requirement)-[r:LLM_RESULT_META_JUDGE]->(target:Requirement)\n",
    "        WHERE source.type = 'SOURCE' and target.type = 'TARGET'\n",
    "        AND source.project = $project_name AND target.project = $project_name\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add model filter if model_ids provided\n",
    "        if model_ids:\n",
    "            model_filter = \" AND r.model IN $model_ids\"\n",
    "        else:\n",
    "            model_filter = \"\"\n",
    "        \n",
    "        # Add ground truth filter\n",
    "        ground_truth_filter = \"\"\"\n",
    "        AND EXISTS { (source)-[:GROUND_TRUTH]->() }\n",
    "        AND EXISTS { ()-[:GROUND_TRUTH]->(target) }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Return clause\n",
    "        return_clause = \"\"\"\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            source.id as source_id,\n",
    "            target.id as target_id,\n",
    "            r.is_traceable as is_traceable,\n",
    "            r.judge_score as judge_score,\n",
    "            r.semantic_alignment as semantic_alignment,\n",
    "            r.non_functional_coverage as non_functional_coverage,\n",
    "            r.final_score as final_score,\n",
    "            r.actor_score as actor_score,\n",
    "            r.functional_completeness as functional_completeness,\n",
    "            r.model as model\n",
    "        ORDER BY source.id, target.id\n",
    "        \"\"\"\n",
    "        \n",
    "        # Combine query parts\n",
    "        meta_judge_query = base_query + model_filter + ground_truth_filter + return_clause\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            # Execute query with appropriate parameters\n",
    "            if model_ids:\n",
    "                results = session.run(meta_judge_query, \n",
    "                                    project_name=project_name, \n",
    "                                    model_ids=model_ids).data()\n",
    "            else:\n",
    "                results = session.run(meta_judge_query, \n",
    "                                    project_name=project_name).data()\n",
    "            \n",
    "            if results:\n",
    "                logger.info(f\"Retrieved {len(results)} meta judge links\")\n",
    "                meta_judge_df = pd.DataFrame(results)\n",
    "                \n",
    "                # Convert boolean columns to boolean type if they exist as strings\n",
    "                if 'is_traceable' in meta_judge_df.columns:\n",
    "                    if meta_judge_df['is_traceable'].dtype == 'object':\n",
    "                        meta_judge_df['is_traceable'] = meta_judge_df['is_traceable'].map(\n",
    "                            lambda x: str(x).lower() == 'true' if pd.notna(x) else False\n",
    "                        )\n",
    "                \n",
    "                # Convert numeric columns to float\n",
    "                numeric_columns = [\n",
    "                    'judge_score', 'semantic_alignment', 'non_functional_coverage',\n",
    "                    'final_score', 'actor_score', 'functional_completeness'\n",
    "                ]\n",
    "                \n",
    "                for col in numeric_columns:\n",
    "                    if col in meta_judge_df.columns:\n",
    "                        meta_judge_df[col] = pd.to_numeric(meta_judge_df[col], errors='coerce')\n",
    "                \n",
    "                return meta_judge_df\n",
    "            else:\n",
    "                logger.warning(f\"No meta judge links found for project: {project_name}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying meta judge links: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Get model IDs from environment variables\n",
    "analysis_model_ids = get_analysis_model_ids()\n",
    "\n",
    "# Query meta judge links with optional model filtering\n",
    "meta_judge_df = query_meta_judge_links(driver, CONFIG['NEO4J_PROJECT_NAME'], analysis_model_ids)\n",
    "\n",
    "# Display information about the retrieved data\n",
    "if not meta_judge_df.empty:\n",
    "    print(\"\\nMeta Judge Links Summary:\")\n",
    "    print(f\"Total meta judge links: {len(meta_judge_df)}\")\n",
    "    print(f\"Unique models: {meta_judge_df['model'].nunique()}\")\n",
    "    print(f\"Models found: {', '.join(meta_judge_df['model'].unique())}\")\n",
    "    \n",
    "    if analysis_model_ids:\n",
    "        print(f\"\\nFiltered to models from RESULTS_ANALYSIS_MODEL_IDS:\")\n",
    "        for model_id in analysis_model_ids:\n",
    "            count = len(meta_judge_df[meta_judge_df['model'] == model_id])\n",
    "            print(f\"  - {model_id}: {count} links\")\n",
    "    \n",
    "    # Display score statistics\n",
    "    score_columns = ['judge_score', 'semantic_alignment', 'non_functional_coverage', \n",
    "                     'final_score', 'actor_score', 'functional_completeness']\n",
    "    \n",
    "    print(\"\\nScore Statistics:\")\n",
    "    for col in score_columns:\n",
    "        if col in meta_judge_df.columns:\n",
    "            stats = meta_judge_df[col].describe()\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Mean: {stats['mean']:.3f}, Std: {stats['std']:.3f}\")\n",
    "            print(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "else:\n",
    "    print(\"\\nNo meta judge links found.\")\n",
    "    if analysis_model_ids:\n",
    "        print(f\"Attempted to filter for models: {analysis_model_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Create Combined Meta Judge Dataset\n",
    "# Purpose: Combine meta judge data with ground truth for analysis\n",
    "# Dependencies: meta_judge_df from Cell [6], df_ground_truth from Cell [3]\n",
    "# Breadcrumbs: Meta Judge Data -> Data Merge -> Combined Meta Judge Dataset\n",
    "\n",
    "def create_meta_judge_combined_dataset(meta_judge_df, df_ground_truth):\n",
    "    \"\"\"\n",
    "    Create a combined dataset with meta judge scores and ground truth labels\n",
    "    \n",
    "    Parameters:\n",
    "        meta_judge_df: DataFrame with meta judge data\n",
    "        df_ground_truth: DataFrame with ground truth links\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset\n",
    "    \"\"\"\n",
    "    if meta_judge_df.empty:\n",
    "        logger.error(\"No meta judge data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with meta judge data\n",
    "    combined_meta_df = meta_judge_df.copy()\n",
    "    \n",
    "    # Filter to only include valid source and target requirements from ground truth\n",
    "    if not df_ground_truth.empty:\n",
    "        # Extract unique source and target IDs with ground truth links\n",
    "        valid_source_ids = df_ground_truth['source_id'].unique()\n",
    "        valid_target_ids = df_ground_truth['target_id'].unique()\n",
    "        \n",
    "        # Filter meta_judge_df to only include valid source and target requirements\n",
    "        combined_meta_df = combined_meta_df[\n",
    "            (combined_meta_df['source_id'].isin(valid_source_ids)) & \n",
    "            (combined_meta_df['target_id'].isin(valid_target_ids))\n",
    "        ].copy()\n",
    "        \n",
    "        logger.info(f\"Filtered meta judge data from {len(meta_judge_df)} to {len(combined_meta_df)} rows\")\n",
    "        \n",
    "        # Create set of ground truth pairs\n",
    "        ground_truth_pairs = set(zip(df_ground_truth['source_id'], df_ground_truth['target_id']))\n",
    "        \n",
    "        # Add ground truth column\n",
    "        combined_meta_df['ground_truth_traceable'] = combined_meta_df.apply(\n",
    "            lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Ground truth distribution: {combined_meta_df['ground_truth_traceable'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        logger.warning(\"No ground truth data available\")\n",
    "        combined_meta_df['ground_truth_traceable'] = False\n",
    "    \n",
    "    # Add derived total score columns for meta judge\n",
    "    if 'judge_score' in combined_meta_df.columns and 'actor_score' in combined_meta_df.columns:\n",
    "        # Original total_score: judge_score + actor_score\n",
    "        combined_meta_df['total_score'] = combined_meta_df['judge_score'] + combined_meta_df['actor_score']\n",
    "        \n",
    "        # Alternative total scores\n",
    "        if 'final_score' in combined_meta_df.columns:\n",
    "            # Alternative 1: actor_score + final_score\n",
    "            combined_meta_df['total_score_with_final'] = combined_meta_df['actor_score'] + combined_meta_df['final_score']\n",
    "            \n",
    "            # Alternative 2: actor_score + judge_score + final_score\n",
    "            combined_meta_df['total_score_all'] = (combined_meta_df['actor_score'] + \n",
    "                                                   combined_meta_df['judge_score'] + \n",
    "                                                   combined_meta_df['final_score'])\n",
    "    \n",
    "    # Add total combined score with all available metrics\n",
    "    available_metrics = []\n",
    "    for metric in ['judge_score', 'semantic_alignment', 'non_functional_coverage', \n",
    "                   'final_score', 'actor_score', 'functional_completeness']:\n",
    "        if metric in combined_meta_df.columns:\n",
    "            available_metrics.append(metric)\n",
    "    \n",
    "    if available_metrics:\n",
    "        combined_meta_df['total_combined_score'] = combined_meta_df[available_metrics].sum(axis=1)\n",
    "        logger.info(f\"Created total_combined_score from {len(available_metrics)} metrics\")\n",
    "    \n",
    "    # Convert is_traceable to numeric for threshold evaluation\n",
    "    if 'is_traceable' in combined_meta_df.columns:\n",
    "        combined_meta_df['is_traceable_numeric'] = combined_meta_df['is_traceable'].astype(int)\n",
    "    \n",
    "    return combined_meta_df\n",
    "\n",
    "# Create combined meta judge dataset\n",
    "combined_meta_df = create_meta_judge_combined_dataset(meta_judge_df, df_ground_truth)\n",
    "\n",
    "# Display info\n",
    "if not combined_meta_df.empty:\n",
    "    print(f\"\\nCombined Meta Judge Dataset Summary:\")\n",
    "    print(f\"Total records: {len(combined_meta_df)}\")\n",
    "    print(f\"Ground truth positive: {combined_meta_df['ground_truth_traceable'].sum()}\")\n",
    "    print(f\"Ground truth negative: {(~combined_meta_df['ground_truth_traceable']).sum()}\")\n",
    "    \n",
    "    # Show available score columns\n",
    "    score_cols = ['is_traceable', 'judge_score', 'actor_score', 'final_score', \n",
    "                  'total_score', 'total_score_with_final', 'total_score_all', 'total_combined_score']\n",
    "    available_scores = [col for col in score_cols if col in combined_meta_df.columns]\n",
    "    print(f\"Available score columns: {', '.join(available_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Evaluate Meta Judge Models with Multiple Score Columns\n",
    "# Purpose: Evaluate meta judge performance using different scoring methods\n",
    "# Dependencies: combined_meta_df from Cell [7], evaluation functions from Cell [5]\n",
    "# Breadcrumbs: Combined Meta Judge Dataset -> Model Evaluation -> Multiple Scoring Methods\n",
    "\n",
    "def evaluate_meta_judge_models(combined_meta_df, optimization_metric='F2'):\n",
    "    \"\"\"\n",
    "    Evaluate meta judge models using different scoring columns\n",
    "    \n",
    "    Parameters:\n",
    "        combined_meta_df: DataFrame with meta judge predictions and ground truth\n",
    "        optimization_metric: Metric to optimize ('F1' or 'F2')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (evaluation_results, best_thresholds_df)\n",
    "    \"\"\"\n",
    "    if combined_meta_df.empty or 'ground_truth_traceable' not in combined_meta_df.columns:\n",
    "        logger.error(\"Cannot evaluate: missing data or ground truth\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Get unique models\n",
    "    models = combined_meta_df['model'].unique()\n",
    "    \n",
    "    # Define score columns to evaluate\n",
    "    score_columns_to_evaluate = [\n",
    "        'is_traceable_numeric',  # Boolean indicator converted to numeric\n",
    "        'actor_score',          # Individual score\n",
    "        'judge_score',          # Individual score\n",
    "        'final_score',          # Individual score\n",
    "        'total_score',          # judge_score + actor_score\n",
    "        'total_score_with_final',  # actor_score + final_score\n",
    "        'total_score_all',      # actor_score + judge_score + final_score\n",
    "        'total_combined_score'  # sum of all available metrics\n",
    "    ]\n",
    "    \n",
    "    # Filter to only columns that exist\n",
    "    score_columns_to_evaluate = [\n",
    "        col for col in score_columns_to_evaluate \n",
    "        if col in combined_meta_df.columns and not combined_meta_df[col].isna().all()\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(models)} Meta Judge models\")\n",
    "    print(f\"Score columns to evaluate: {len(score_columns_to_evaluate)}\")\n",
    "    print(f\"Optimizing for {optimization_metric} score\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_evaluation_results = []\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nEvaluating model: {model}\")\n",
    "        model_df = combined_meta_df[combined_meta_df['model'] == model]\n",
    "        \n",
    "        for score_column in score_columns_to_evaluate:\n",
    "            # Use the same evaluation function from sentence transformers\n",
    "            result = evaluate_model_thresholds(\n",
    "                model_df, \n",
    "                model, \n",
    "                score_column=score_column,\n",
    "                ground_truth_column='ground_truth_traceable',\n",
    "                optimize_for=optimization_metric\n",
    "            )\n",
    "            \n",
    "            if result and 'best_threshold' in result:\n",
    "                # Add score column info\n",
    "                result['score_column'] = score_column\n",
    "                result['method'] = 'meta_judge'\n",
    "                all_evaluation_results.append(result)\n",
    "                \n",
    "                print(f\"\\n  Score column: {score_column}\")\n",
    "                print(f\"    Best threshold: {result['best_threshold']:.3f}\")\n",
    "                print(f\"    {optimization_metric}: {result[f'best_{optimization_metric.lower()}']:.3f}\")\n",
    "                print(f\"    Precision: {result['best_precision']:.3f}, Recall: {result['best_recall']:.3f}\")\n",
    "                print(f\"    TP={result['best_tp']}, FP={result['best_fp']}, FN={result['best_fn']}, TN={result['best_tn']}\")\n",
    "    \n",
    "    # Create summary DataFrame with ALL columns to match sentence transformer output\n",
    "    if all_evaluation_results:\n",
    "        meta_judge_results_df = pd.DataFrame([{\n",
    "            'model_name': r['model_name'],\n",
    "            'score_column': r['score_column'],\n",
    "            'method': 'meta_judge',\n",
    "            'best_threshold': r['best_threshold'],\n",
    "            'accuracy': r['best_accuracy'],\n",
    "            'balanced_accuracy': r['best_balanced_accuracy'],\n",
    "            'precision': r['best_precision'],\n",
    "            'recall': r['best_recall'],\n",
    "            'specificity': r['best_tnr'],\n",
    "            'miss_rate': r['best_fnr'],\n",
    "            'f1_score': r['best_f1'],\n",
    "            'f2_score': r['best_f2'],\n",
    "            'matthews_corr': r['best_mcc'],\n",
    "            'true_positives': r['best_tp'],\n",
    "            'false_positives': r['best_fp'],\n",
    "            'false_negatives': r['best_fn'],\n",
    "            'true_negatives': r['best_tn'],\n",
    "            'data_points': r['data_points'],\n",
    "            'ground_truth_positive': r['ground_truth_positive'],\n",
    "            'ground_truth_negative': r['ground_truth_negative']\n",
    "        } for r in all_evaluation_results])\n",
    "        \n",
    "        # Sort by optimization metric\n",
    "        sort_col = f'{optimization_metric.lower()}_score'\n",
    "        meta_judge_results_df = meta_judge_results_df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Display results with all columns to match sentence transformer format\n",
    "        print(f\"\\n\\nMeta Judge Best Results (sorted by {CONFIG['OPTIMIZATION_METRIC']}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display columns in same order as sentence transformer\n",
    "        display_cols = ['model_name', 'score_column', 'best_threshold', 'accuracy', \n",
    "                       'balanced_accuracy', 'precision', 'recall', 'specificity', \n",
    "                       'miss_rate', 'f1_score', 'f2_score', 'matthews_corr', \n",
    "                       'true_positives', 'false_positives', 'false_negatives', \n",
    "                       'true_negatives', 'data_points', 'ground_truth_positive', \n",
    "                       'ground_truth_negative']\n",
    "        \n",
    "        # Only display columns that exist\n",
    "        display_cols = [col for col in display_cols if col in meta_judge_results_df.columns]\n",
    "        \n",
    "        print(meta_judge_results_df[display_cols].to_string())\n",
    "        \n",
    "        return all_evaluation_results, meta_judge_results_df\n",
    "    \n",
    "    return [], pd.DataFrame()\n",
    "\n",
    "# Evaluate meta judge models\n",
    "meta_judge_evaluation_results, meta_judge_best_df = evaluate_meta_judge_models(\n",
    "    combined_meta_df, \n",
    "    CONFIG['OPTIMIZATION_METRIC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Compare Sentence Transformers vs Meta Judge with 30% Improvement Hypothesis Testing\n",
    "# Purpose: Test the core hypothesis that hallucination-reducing techniques achieve 30% improvement in NPD accuracy\n",
    "# Dependencies: best_thresholds_df from Cell [5], meta_judge_best_df from Cell [8]\n",
    "# Breadcrumbs: Model Evaluation -> Hypothesis Testing -> 30% Improvement Validation\n",
    "\n",
    "def calculate_improvement_metrics(baseline_metrics, enhanced_metrics):\n",
    "    \"\"\"\n",
    "    Calculate improvement metrics and test 30% improvement hypothesis\n",
    "    \n",
    "    Parameters:\n",
    "        baseline_metrics: Dictionary with baseline performance metrics\n",
    "        enhanced_metrics: Dictionary with enhanced method performance metrics\n",
    "    \n",
    "    Returns:\n",
    "        dict: Improvement analysis results\n",
    "    \"\"\"\n",
    "    improvements = {}\n",
    "    \n",
    "    # Calculate percentage improvements for each metric\n",
    "    metrics_to_test = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1_score', 'f2_score', 'matthews_corr']\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        if metric in baseline_metrics and metric in enhanced_metrics:\n",
    "            baseline_val = baseline_metrics[metric]\n",
    "            enhanced_val = enhanced_metrics[metric]\n",
    "            \n",
    "            if baseline_val > 0:\n",
    "                pct_improvement = ((enhanced_val - baseline_val) / baseline_val) * 100\n",
    "                improvements[f'{metric}_improvement_pct'] = pct_improvement\n",
    "                improvements[f'{metric}_baseline'] = baseline_val\n",
    "                improvements[f'{metric}_enhanced'] = enhanced_val\n",
    "                improvements[f'{metric}_absolute_diff'] = enhanced_val - baseline_val\n",
    "    \n",
    "    # Hallucination reduction metrics (FP and FN reduction)\n",
    "    if all(key in baseline_metrics for key in ['false_positives', 'false_negatives']) and \\\n",
    "       all(key in enhanced_metrics for key in ['false_positives', 'false_negatives']):\n",
    "        \n",
    "        # Calculate FP reduction (over-identification hallucination reduction)\n",
    "        fp_baseline = baseline_metrics['false_positives']\n",
    "        fp_enhanced = enhanced_metrics['false_positives']\n",
    "        fp_reduction = fp_baseline - fp_enhanced\n",
    "        fp_reduction_pct = (fp_reduction / fp_baseline * 100) if fp_baseline > 0 else 0\n",
    "        \n",
    "        # Calculate FN reduction (under-identification hallucination reduction)\n",
    "        fn_baseline = baseline_metrics['false_negatives']\n",
    "        fn_enhanced = enhanced_metrics['false_negatives']\n",
    "        fn_reduction = fn_baseline - fn_enhanced\n",
    "        fn_reduction_pct = (fn_reduction / fn_baseline * 100) if fn_baseline > 0 else 0\n",
    "        \n",
    "        improvements.update({\n",
    "            'fp_reduction': fp_reduction,\n",
    "            'fp_reduction_pct': fp_reduction_pct,\n",
    "            'fn_reduction': fn_reduction,\n",
    "            'fn_reduction_pct': fn_reduction_pct,\n",
    "            'total_error_reduction': fp_reduction + fn_reduction,\n",
    "            'hallucination_reduction_score': (fp_reduction_pct + fn_reduction_pct) / 2\n",
    "        })\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "def test_30_percent_hypothesis(improvements, primary_metric='f2_score'):\n",
    "    \"\"\"\n",
    "    Test the hypothesis that improvement is 30% for the primary metric\n",
    "    \n",
    "    Parameters:\n",
    "        improvements: Dictionary with improvement calculations\n",
    "        primary_metric: Primary metric to test (default: f2_score)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hypothesis test results\n",
    "    \"\"\"\n",
    "    improvement_key = f'{primary_metric}_improvement_pct'\n",
    "    \n",
    "    if improvement_key not in improvements:\n",
    "        return {'error': f'Primary metric {primary_metric} not available for testing'}\n",
    "    \n",
    "    observed_improvement = improvements[improvement_key]\n",
    "    \n",
    "    # H0: improvement  30%\n",
    "    # H1: improvement > 30% \n",
    "    null_hypothesis_threshold = 30.0\n",
    "    \n",
    "    # Simple threshold test\n",
    "    meets_threshold = observed_improvement >= null_hypothesis_threshold\n",
    "    \n",
    "    # Calculate effect size (Cohen's d equivalent for percentage improvement)\n",
    "    effect_size = (observed_improvement - null_hypothesis_threshold) / 10  # Normalize by 10% as standard unit\n",
    "    \n",
    "    return {\n",
    "        'observed_improvement': observed_improvement,\n",
    "        'null_threshold': null_hypothesis_threshold,\n",
    "        'meets_30_percent_threshold': meets_threshold,\n",
    "        'improvement_above_threshold': observed_improvement - null_hypothesis_threshold,\n",
    "        'effect_size': effect_size,\n",
    "        'practical_significance': 'Large' if abs(effect_size) > 0.8 else 'Medium' if abs(effect_size) > 0.5 else 'Small'\n",
    "    }\n",
    "\n",
    "def bootstrap_improvement_confidence_interval(baseline_df, enhanced_df, combined_df, combined_meta_df, \n",
    "                                            metric_func, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals for improvement percentages\n",
    "    \n",
    "    Parameters:\n",
    "        baseline_df: Best baseline configuration\n",
    "        enhanced_df: Best enhanced configuration  \n",
    "        combined_df: Combined sentence transformer data\n",
    "        combined_meta_df: Combined meta judge data\n",
    "        metric_func: Function to calculate metric\n",
    "        n_bootstraps: Number of bootstrap samples\n",
    "        alpha: Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap confidence interval results\n",
    "    \"\"\"\n",
    "    # Get prediction data for both methods\n",
    "    baseline_model = baseline_df['model_name']\n",
    "    baseline_threshold = baseline_df['best_threshold']\n",
    "    \n",
    "    enhanced_model = enhanced_df['model_name']\n",
    "    enhanced_score_col = enhanced_df['score_column']\n",
    "    enhanced_threshold = enhanced_df['best_threshold']\n",
    "    \n",
    "    # Get predictions\n",
    "    baseline_data = combined_df[combined_df['model'] == baseline_model].copy()\n",
    "    baseline_data['predicted'] = (baseline_data['similarity_score'] >= baseline_threshold).astype(int)\n",
    "    y_true_baseline = baseline_data['ground_truth_traceable'].astype(int).values\n",
    "    y_pred_baseline = baseline_data['predicted'].values\n",
    "    \n",
    "    enhanced_data = combined_meta_df[combined_meta_df['model'] == enhanced_model].copy()\n",
    "    enhanced_data['predicted'] = (enhanced_data[enhanced_score_col] >= enhanced_threshold).astype(int)\n",
    "    y_true_enhanced = enhanced_data['ground_truth_traceable'].astype(int).values\n",
    "    y_pred_enhanced = enhanced_data['predicted'].values\n",
    "    \n",
    "    # Ensure we have the same test instances by merging on source_id and target_id\n",
    "    merged = pd.merge(\n",
    "        baseline_data[['source_id', 'target_id', 'predicted', 'ground_truth_traceable']],\n",
    "        enhanced_data[['source_id', 'target_id', 'predicted']],\n",
    "        on=['source_id', 'target_id'],\n",
    "        suffixes=('_baseline', '_enhanced')\n",
    "    )\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        return {'error': 'No common test instances found between methods'}\n",
    "    \n",
    "    y_true = merged['ground_truth_traceable'].astype(int).values\n",
    "    y_pred_baseline = merged['predicted_baseline'].values\n",
    "    y_pred_enhanced = merged['predicted_enhanced'].values\n",
    "    \n",
    "    # Bootstrap sampling\n",
    "    baseline_scores = []\n",
    "    enhanced_scores = []\n",
    "    improvements = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample indices\n",
    "        indices = resample(np.arange(len(y_true)), n_samples=len(y_true))\n",
    "        \n",
    "        # Calculate metrics for resampled data\n",
    "        baseline_score = metric_func(y_true[indices], y_pred_baseline[indices], beta=2, zero_division=0)\n",
    "        enhanced_score = metric_func(y_true[indices], y_pred_enhanced[indices], beta=2, zero_division=0)\n",
    "        \n",
    "        baseline_scores.append(baseline_score)\n",
    "        enhanced_scores.append(enhanced_score)\n",
    "        \n",
    "        # Calculate percentage improvement\n",
    "        if baseline_score > 0:\n",
    "            improvement_pct = ((enhanced_score - baseline_score) / baseline_score) * 100\n",
    "            improvements.append(improvement_pct)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    improvement_ci = np.percentile(improvements, [lower_percentile, upper_percentile])\n",
    "    \n",
    "    # Test if 30% threshold is in confidence interval\n",
    "    threshold_in_ci = improvement_ci[0] <= 30.0 <= improvement_ci[1]\n",
    "    exceeds_30_percent = improvement_ci[0] > 30.0\n",
    "    \n",
    "    return {\n",
    "        'mean_improvement': np.mean(improvements),\n",
    "        'improvement_ci_lower': improvement_ci[0],\n",
    "        'improvement_ci_upper': improvement_ci[1],\n",
    "        'baseline_mean': np.mean(baseline_scores),\n",
    "        'enhanced_mean': np.mean(enhanced_scores),\n",
    "        'common_instances': len(merged),\n",
    "        'exceeds_30_percent_threshold': exceeds_30_percent,\n",
    "        'threshold_in_ci': threshold_in_ci,\n",
    "        'bootstrap_samples': len(improvements)\n",
    "    }\n",
    "\n",
    "def compare_approaches_with_hypothesis_testing(sentence_transformer_df, meta_judge_df, \n",
    "                                             combined_df, combined_meta_df, optimization_metric='F2'):\n",
    "    \"\"\"\n",
    "    Compare approaches with specific focus on 30% improvement hypothesis testing\n",
    "    \n",
    "    Parameters:\n",
    "        sentence_transformer_df: Results from sentence transformer evaluation\n",
    "        meta_judge_df: Results from meta judge evaluation\n",
    "        combined_df: Combined sentence transformer data\n",
    "        combined_meta_df: Combined meta judge data\n",
    "        optimization_metric: Metric used for optimization\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete comparison and hypothesis testing results\n",
    "    \"\"\"\n",
    "    # Add method column if not present\n",
    "    if 'method' not in sentence_transformer_df.columns:\n",
    "        sentence_transformer_df = sentence_transformer_df.copy()\n",
    "        sentence_transformer_df['method'] = 'sentence_transformer'\n",
    "        sentence_transformer_df['score_column'] = 'similarity_score'\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = pd.concat([sentence_transformer_df, meta_judge_df], ignore_index=True)\n",
    "    \n",
    "    # Sort by optimization metric\n",
    "    metric_col = f'{optimization_metric.lower()}_score'\n",
    "    all_results = all_results.sort_values(metric_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Get best from each method\n",
    "    best_baseline = all_results[all_results['method'] == 'sentence_transformer'].iloc[0]\n",
    "    best_enhanced = all_results[all_results['method'] == 'meta_judge'].iloc[0]\n",
    "    \n",
    "    # Calculate improvement metrics\n",
    "    improvements = calculate_improvement_metrics(best_baseline.to_dict(), best_enhanced.to_dict())\n",
    "    \n",
    "    # Test 30% hypothesis\n",
    "    primary_metric_key = f'{optimization_metric.lower()}_score'\n",
    "    hypothesis_results = test_30_percent_hypothesis(improvements, primary_metric_key)\n",
    "    \n",
    "    # Bootstrap confidence intervals\n",
    "    print(\"Calculating bootstrap confidence intervals...\")\n",
    "    bootstrap_results = bootstrap_improvement_confidence_interval(\n",
    "        best_baseline, best_enhanced, combined_df, combined_meta_df, fbeta_score\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'comparison_df': all_results,\n",
    "        'best_baseline': best_baseline,\n",
    "        'best_enhanced': best_enhanced,\n",
    "        'improvements': improvements,\n",
    "        'hypothesis_test': hypothesis_results,\n",
    "        'bootstrap_ci': bootstrap_results\n",
    "    }\n",
    "\n",
    "# Create comparison with hypothesis testing if both results are available\n",
    "if 'best_thresholds_df' in globals() and not best_thresholds_df.empty and not meta_judge_best_df.empty:\n",
    "    \n",
    "    print(\"HALLUCINATION REDUCTION VALIDATION: 30% IMPROVEMENT HYPOTHESIS TESTING\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"CORE HYPOTHESIS: Techniques reducing LLM hallucinations by 30% can improve NPD accuracy\")\n",
    "    print(\"OPERATIONAL HYPOTHESIS: Multi-stage LLM refinement achieves 30% improvement vs baseline\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Perform comprehensive comparison with hypothesis testing\n",
    "    analysis_results = compare_approaches_with_hypothesis_testing(\n",
    "        best_thresholds_df, meta_judge_best_df, combined_df, combined_meta_df, CONFIG['OPTIMIZATION_METRIC']\n",
    "    )\n",
    "    \n",
    "    comparison_df = analysis_results['comparison_df']\n",
    "    best_baseline = analysis_results['best_baseline']\n",
    "    best_enhanced = analysis_results['best_enhanced']\n",
    "    improvements = analysis_results['improvements']\n",
    "    hypothesis_test = analysis_results['hypothesis_test']\n",
    "    bootstrap_ci = analysis_results['bootstrap_ci']\n",
    "    \n",
    "    print(f\"\\n1. BASELINE vs ENHANCED APPROACH COMPARISON\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"BASELINE (Sentence Transformer): {best_baseline['model_name']}\")\n",
    "    print(f\"  Threshold: {best_baseline.get('best_threshold', 'N/A')}\")\n",
    "    \n",
    "    # Fix the syntax error by using a variable for the column name\n",
    "    optimization_metric_lower = CONFIG['OPTIMIZATION_METRIC'].lower()\n",
    "    score_column = f'{optimization_metric_lower}_score'\n",
    "    print(f\"  {CONFIG['OPTIMIZATION_METRIC']} Score: {best_baseline[score_column]:.4f}\")\n",
    "    print(f\"  Precision: {best_baseline['precision']:.4f}, Recall: {best_baseline['recall']:.4f}\")\n",
    "    print(f\"  False Positives: {best_baseline['false_positives']}, False Negatives: {best_baseline['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nENHANCED (Meta Judge): {best_enhanced['model_name']} ({best_enhanced['score_column']})\")\n",
    "    print(f\"  Threshold: {best_enhanced.get('best_threshold', 'N/A')}\")\n",
    "    print(f\"  {CONFIG['OPTIMIZATION_METRIC']} Score: {best_enhanced[score_column]:.4f}\")\n",
    "    print(f\"  Precision: {best_enhanced['precision']:.4f}, Recall: {best_enhanced['recall']:.4f}\")\n",
    "    print(f\"  False Positives: {best_enhanced['false_positives']}, False Negatives: {best_enhanced['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\n2. ACCURACY IMPROVEMENT ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Display improvements for all metrics\n",
    "    metrics_order = ['f2_score', 'f1_score', 'precision', 'recall', 'accuracy', 'balanced_accuracy', 'matthews_corr']\n",
    "    \n",
    "    for metric in metrics_order:\n",
    "        improvement_key = f'{metric}_improvement_pct'\n",
    "        if improvement_key in improvements:\n",
    "            baseline_val = improvements[f'{metric}_baseline']\n",
    "            enhanced_val = improvements[f'{metric}_enhanced']\n",
    "            improvement_pct = improvements[improvement_key]\n",
    "            \n",
    "            print(f\"{metric.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Baseline: {baseline_val:.4f}  Enhanced: {enhanced_val:.4f}\")\n",
    "            print(f\"  Improvement: {improvement_pct:+.1f}% {'' if improvement_pct >= 30 else ''}\")\n",
    "    \n",
    "    print(f\"\\n3. HALLUCINATION REDUCTION METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if 'fp_reduction' in improvements:\n",
    "        print(f\"FALSE POSITIVE REDUCTION (Over-identification Hallucinations):\")\n",
    "        print(f\"  Baseline FP: {best_baseline['false_positives']}  Enhanced FP: {best_enhanced['false_positives']}\")\n",
    "        print(f\"  Reduction: {improvements['fp_reduction']} ({improvements['fp_reduction_pct']:+.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nFALSE NEGATIVE REDUCTION (Under-identification Hallucinations):\")\n",
    "        print(f\"  Baseline FN: {best_baseline['false_negatives']}  Enhanced FN: {best_enhanced['false_negatives']}\")\n",
    "        print(f\"  Reduction: {improvements['fn_reduction']} ({improvements['fn_reduction_pct']:+.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTOTAL ERROR REDUCTION:\")\n",
    "        print(f\"  Total errors reduced: {improvements['total_error_reduction']}\")\n",
    "        print(f\"  Hallucination reduction score: {improvements['hallucination_reduction_score']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n4. 30% IMPROVEMENT HYPOTHESIS TEST\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if 'error' not in hypothesis_test:\n",
    "        primary_metric_display = f\"{CONFIG['OPTIMIZATION_METRIC']}_SCORE\"\n",
    "        observed_improvement = hypothesis_test['observed_improvement']\n",
    "        \n",
    "        print(f\"PRIMARY METRIC: {primary_metric_display}\")\n",
    "        print(f\"H: Improvement  30%\")\n",
    "        print(f\"H: Improvement > 30%\")\n",
    "        print(f\"\")\n",
    "        print(f\"OBSERVED IMPROVEMENT: {observed_improvement:.2f}%\")\n",
    "        print(f\"THRESHOLD: {hypothesis_test['null_threshold']:.1f}%\")\n",
    "        print(f\"EXCEEDS THRESHOLD: {'YES' if hypothesis_test['meets_30_percent_threshold'] else 'NO'}\")\n",
    "        print(f\"MARGIN: {hypothesis_test['improvement_above_threshold']:+.2f}%\")\n",
    "        print(f\"EFFECT SIZE: {hypothesis_test['effect_size']:.3f} ({hypothesis_test['practical_significance']})\")\n",
    "        \n",
    "        if hypothesis_test['meets_30_percent_threshold']:\n",
    "            print(f\"\")\n",
    "            print(f\" HYPOTHESIS VALIDATED: The enhancement achieves 30% improvement!\")\n",
    "            print(f\" NPD IMPACT: Significant accuracy improvement confirmed for requirements traceability\")\n",
    "        else:\n",
    "            print(f\"\")\n",
    "            print(f\" HYPOTHESIS NOT MET: Improvement below 30% threshold\")\n",
    "            print(f\" NPD IMPACT: Enhancement shows improvement but below significance threshold\")\n",
    "    \n",
    "    print(f\"\\n5. BOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if 'error' not in bootstrap_ci:\n",
    "        print(f\"BOOTSTRAP ANALYSIS:\")\n",
    "        print(f\"  Common test instances: {bootstrap_ci['common_instances']}\")\n",
    "        print(f\"  Bootstrap samples: {bootstrap_ci['bootstrap_samples']}\")\n",
    "        print(f\"\")\n",
    "        print(f\"IMPROVEMENT CONFIDENCE INTERVAL:\")\n",
    "        print(f\"  Mean improvement: {bootstrap_ci['mean_improvement']:.2f}%\")\n",
    "        print(f\"  95% CI: [{bootstrap_ci['improvement_ci_lower']:.2f}%, {bootstrap_ci['improvement_ci_upper']:.2f}%]\")\n",
    "        print(f\"\")\n",
    "        print(f\"STATISTICAL SIGNIFICANCE:\")\n",
    "        print(f\"  CI exceeds 30% threshold: {'YES' if bootstrap_ci['exceeds_30_percent_threshold'] else 'NO'}\")\n",
    "        print(f\"  30% within CI: {'YES' if bootstrap_ci['threshold_in_ci'] else 'NO'}\")\n",
    "        \n",
    "        if bootstrap_ci['exceeds_30_percent_threshold']:\n",
    "            print(f\"   STATISTICALLY SIGNIFICANT: Lower bound > 30%\")\n",
    "        elif not bootstrap_ci['threshold_in_ci']:\n",
    "            print(f\"   NOT SIGNIFICANT: 30% threshold outside confidence interval\")\n",
    "        else:\n",
    "            print(f\"    INCONCLUSIVE: 30% threshold within confidence interval\")\n",
    "    else:\n",
    "        print(f\"Bootstrap analysis error: {bootstrap_ci['error']}\")\n",
    "    \n",
    "    print(f\"\\n6. NPD REQUIREMENTS TRACEABILITY IMPACT ASSESSMENT\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calculate business impact metrics\n",
    "    total_requirements = best_baseline['data_points']\n",
    "    accuracy_gain = improvements.get('accuracy_improvement_pct', 0)\n",
    "    precision_gain = improvements.get('precision_improvement_pct', 0)\n",
    "    recall_gain = improvements.get('recall_improvement_pct', 0)\n",
    "    \n",
    "    print(f\"OPERATIONAL METRICS:\")\n",
    "    print(f\"  Total requirement pairs analyzed: {total_requirements}\")\n",
    "    print(f\"  Accuracy improvement: {accuracy_gain:+.1f}%\")\n",
    "    print(f\"  Precision improvement: {precision_gain:+.1f}% (reduced false connections)\")\n",
    "    print(f\"  Recall improvement: {recall_gain:+.1f}% (fewer missed connections)\")\n",
    "    \n",
    "    if 'fp_reduction' in improvements and 'fn_reduction' in improvements:\n",
    "        print(f\"\")\n",
    "        print(f\"QUALITY ASSURANCE IMPACT:\")\n",
    "        print(f\"  Reduced over-identification errors: {improvements['fp_reduction']} ({improvements['fp_reduction_pct']:+.1f}%)\")\n",
    "        print(f\"  Reduced under-identification errors: {improvements['fn_reduction']} ({improvements['fn_reduction_pct']:+.1f}%)\")\n",
    "        print(f\"  Total error reduction: {improvements['total_error_reduction']} cases\")\n",
    "    \n",
    "    print(f\"\\n7. VALIDATION SUMMARY\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Final validation summary\n",
    "    meets_hypothesis = hypothesis_test.get('meets_30_percent_threshold', False)\n",
    "    is_statistically_significant = bootstrap_ci.get('exceeds_30_percent_threshold', False)\n",
    "    \n",
    "    print(f\"CORE HYPOTHESIS VALIDATION:\")\n",
    "    print(f\"   Multi-stage LLM refinement implemented: YES\")\n",
    "    print(f\"   Hallucination reduction technique applied: YES\") \n",
    "    print(f\"   NPD requirements traceability tested: YES\")\n",
    "    print(f\"   30% improvement achieved: {'YES' if meets_hypothesis else 'NO'}\")\n",
    "    print(f\"   Statistical significance confirmed: {'YES' if is_statistically_significant else 'NO'}\")\n",
    "    \n",
    "    if meets_hypothesis and is_statistically_significant:\n",
    "        print(f\"\")\n",
    "        print(f\" RESEARCH HYPOTHESIS VALIDATED\")\n",
    "        print(f\"   Multi-stage LLM refinement successfully reduces hallucinations\")\n",
    "        print(f\"   and achieves statistically significant 30% improvement in\")\n",
    "        print(f\"   NPD requirements traceability accuracy.\")\n",
    "    elif meets_hypothesis:\n",
    "        print(f\"\")\n",
    "        print(f\"  HYPOTHESIS PARTIALLY VALIDATED\")\n",
    "        print(f\"   30% improvement achieved but statistical significance unclear.\")\n",
    "        print(f\"   Consider larger sample size for definitive validation.\")\n",
    "    else:\n",
    "        print(f\"\")\n",
    "        print(f\" HYPOTHESIS NOT VALIDATED\") \n",
    "        print(f\"   Enhancement shows improvement but below 30% threshold.\")\n",
    "        print(f\"   Further refinement of hallucination reduction techniques needed.\")\n",
    "    \n",
    "    # Save comparison results for next cells\n",
    "    globals()['comparison_df'] = comparison_df\n",
    "    globals()['hypothesis_validation_results'] = {\n",
    "        'improvements': improvements,\n",
    "        'hypothesis_test': hypothesis_test,\n",
    "        'bootstrap_ci': bootstrap_ci,\n",
    "        'validated': meets_hypothesis and is_statistically_significant\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\"\\nCannot create comparison - missing results from one or both approaches\")\n",
    "    print(\"Please ensure you have run:\")\n",
    "    print(\"  - Cell 5: Sentence transformer evaluation (creates best_thresholds_df)\")\n",
    "    print(\"  - Cell 8: Meta judge evaluation (creates meta_judge_best_df)\")\n",
    "    print(\"  - Cell 4: Combined sentence transformer dataset (creates combined_df)\")\n",
    "    print(\"  - Cell 7: Combined meta judge dataset (creates combined_meta_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Visualize Comparison Results\n",
    "# Purpose: Create visualizations comparing the two approaches\n",
    "# Dependencies: comparison_df from Cell [9], matplotlib and seaborn from Cell [0]\n",
    "# Breadcrumbs: Approach Comparison -> Data Visualization -> Performance Charts\n",
    "\n",
    "if CONFIG['SHOW_VISUALIZATION'] and 'comparison_df' in globals() and not comparison_df.empty:\n",
    "    # Create visualization comparing approaches\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Comparison: Sentence Transformers vs Meta Judge - Project: {CONFIG[\"NEO4J_PROJECT_NAME\"]}', \n",
    "                 fontsize=16)\n",
    "    \n",
    "    # 1. F2 Score comparison\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # Get best result for each method\n",
    "    best_by_method = comparison_df.groupby('method')[f'{CONFIG[\"OPTIMIZATION_METRIC\"].lower()}_score'].agg(['max', 'mean']).reset_index()\n",
    "    \n",
    "    x = np.arange(len(best_by_method))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, best_by_method['max'], width, label='Best', color='#1A85FF')\n",
    "    ax.bar(x + width/2, best_by_method['mean'], width, label='Average', color='#FFC61A')\n",
    "    \n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel(f'{CONFIG[\"OPTIMIZATION_METRIC\"]} Score')\n",
    "    ax.set_title(f'{CONFIG[\"OPTIMIZATION_METRIC\"]} Score Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(best_by_method['method'])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Precision-Recall Scatter\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    for method in comparison_df['method'].unique():\n",
    "        method_data = comparison_df[comparison_df['method'] == method]\n",
    "        ax.scatter(method_data['recall'], method_data['precision'], \n",
    "                  label=method, s=100, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision vs Recall Trade-off')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Top models bar chart\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    top_n = 10\n",
    "    top_models = comparison_df.head(top_n)\n",
    "    \n",
    "    y_pos = np.arange(len(top_models))\n",
    "    colors = ['#1A85FF' if m == 'sentence_transformer' else '#D41159' \n",
    "              for m in top_models['method']]\n",
    "    \n",
    "    bars = ax.barh(y_pos, top_models[f'{CONFIG[\"OPTIMIZATION_METRIC\"].lower()}_score'], color=colors)\n",
    "    \n",
    "    # Create labels\n",
    "    labels = []\n",
    "    for _, row in top_models.iterrows():\n",
    "        if row['method'] == 'sentence_transformer':\n",
    "            label = row['model_name'].split('/')[-1] if '/' in row['model_name'] else row['model_name']\n",
    "        else:\n",
    "            label = f\"{row['model_name']} ({row['score_column']})\"\n",
    "        labels.append(label)\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(f'{CONFIG[\"OPTIMIZATION_METRIC\"]} Score')\n",
    "    ax.set_title(f'Top {top_n} Configurations')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#1A85FF', label='Sentence Transformer'),\n",
    "        Patch(facecolor='#D41159', label='Meta Judge')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    # 4. Confusion Matrix Comparison for best of each\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Get best from each method\n",
    "    best_st = comparison_df[comparison_df['method'] == 'sentence_transformer'].iloc[0] if len(comparison_df[comparison_df['method'] == 'sentence_transformer']) > 0 else None\n",
    "    best_mj = comparison_df[comparison_df['method'] == 'meta_judge'].iloc[0] if len(comparison_df[comparison_df['method'] == 'meta_judge']) > 0 else None\n",
    "    \n",
    "    if best_st is not None and best_mj is not None:\n",
    "        # Create grouped bar chart for confusion matrix values\n",
    "        metrics = ['TP', 'FP', 'FN', 'TN']\n",
    "        st_values = [best_st['true_positives'], best_st['false_positives'], \n",
    "                     best_st['false_negatives'], best_st['true_negatives']]\n",
    "        mj_values = [best_mj['true_positives'], best_mj['false_positives'], \n",
    "                     best_mj['false_negatives'], best_mj['true_negatives']]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, st_values, width, label='Sentence Transformer', color='#1A85FF')\n",
    "        ax.bar(x + width/2, mj_values, width, label='Meta Judge', color='#D41159')\n",
    "        \n",
    "        ax.set_xlabel('Metric')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Confusion Matrix Comparison (Best Models)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison of best models\n",
    "    print(\"\\n\\nDETAILED COMPARISON OF BEST MODELS:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for method in ['sentence_transformer', 'meta_judge']:\n",
    "        method_best = comparison_df[comparison_df['method'] == method]\n",
    "        if not method_best.empty:\n",
    "            best = method_best.iloc[0]\n",
    "            print(f\"\\nBest {method.upper()}:\")\n",
    "            print(f\"  Model: {best['model_name']}\")\n",
    "            if method == 'meta_judge':\n",
    "                print(f\"  Score Column: {best['score_column']}\")\n",
    "            print(f\"  Threshold: {best.get('best_threshold', 'N/A')}\")\n",
    "            print(f\"  {CONFIG['OPTIMIZATION_METRIC']} Score: {best[f'{CONFIG[\"OPTIMIZATION_METRIC\"].lower()}_score']:.3f}\")\n",
    "            print(f\"  Precision: {best['precision']:.3f}\")\n",
    "            print(f\"  Recall: {best['recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {best['f1_score']:.3f}\")\n",
    "            print(f\"  Matthews Correlation: {best['matthews_corr']:.3f}\")\n",
    "            print(f\"  Confusion Matrix:\")\n",
    "            print(f\"    True Positives:  {best['true_positives']:4d}\")\n",
    "            print(f\"    False Positives: {best['false_positives']:4d}\")\n",
    "            print(f\"    False Negatives: {best['false_negatives']:4d}\")\n",
    "            print(f\"    True Negatives:  {best['true_negatives']:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Statistical Significance Testing for 30% Improvement Hypothesis\n",
    "# Purpose: Perform rigorous statistical tests to validate the 30% improvement hypothesis for hallucination reduction\n",
    "# Dependencies: comparison_df from Cell [9], scipy.stats, sklearn.utils, statsmodels from Cell [0]\n",
    "# Breadcrumbs: Performance Analysis -> Hypothesis Testing -> 30% Improvement Statistical Validation\n",
    "\n",
    "def get_predictions_for_best_models(combined_df, combined_meta_df, best_st_config, best_mj_config):\n",
    "    \"\"\"\n",
    "    Get the actual predictions for the best configurations of each method\n",
    "    \n",
    "    Parameters:\n",
    "        combined_df: DataFrame with sentence transformer data\n",
    "        combined_meta_df: DataFrame with meta judge data\n",
    "        best_st_config: Best sentence transformer configuration\n",
    "        best_mj_config: Best meta judge configuration\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged predictions from both methods on common instances\n",
    "    \"\"\"\n",
    "    # Get sentence transformer predictions\n",
    "    st_model = best_st_config['model_name']\n",
    "    st_threshold = best_st_config['best_threshold']\n",
    "    \n",
    "    st_data = combined_df[combined_df['model'] == st_model].copy()\n",
    "    st_data['predicted'] = (st_data['similarity_score'] >= st_threshold).astype(int)\n",
    "    st_data['actual'] = st_data['ground_truth_traceable'].astype(int)\n",
    "    \n",
    "    # Get meta judge predictions\n",
    "    mj_model = best_mj_config['model_name']\n",
    "    mj_score_col = best_mj_config['score_column']\n",
    "    mj_threshold = best_mj_config['best_threshold']\n",
    "    \n",
    "    mj_data = combined_meta_df[combined_meta_df['model'] == mj_model].copy()\n",
    "    mj_data['predicted'] = (mj_data[mj_score_col] >= mj_threshold).astype(int)\n",
    "    mj_data['actual'] = mj_data['ground_truth_traceable'].astype(int)\n",
    "    \n",
    "    # Merge on source_id and target_id to ensure we're comparing the same instances\n",
    "    merged = pd.merge(\n",
    "        st_data[['source_id', 'target_id', 'predicted', 'actual']],\n",
    "        mj_data[['source_id', 'target_id', 'predicted']],\n",
    "        on=['source_id', 'target_id'],\n",
    "        suffixes=('_st', '_mj')\n",
    "    )\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def mcnemars_test_with_effect_size(predictions_df):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test with effect size calculation\n",
    "    \n",
    "    Parameters:\n",
    "        predictions_df: DataFrame with predictions from both methods\n",
    "    \n",
    "    Returns:\n",
    "        dict: Test results with effect size\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    st_correct = (predictions_df['predicted_st'] == predictions_df['actual']).astype(int)\n",
    "    mj_correct = (predictions_df['predicted_mj'] == predictions_df['actual']).astype(int)\n",
    "    \n",
    "    # Count the disagreements\n",
    "    b = ((st_correct == 1) & (mj_correct == 0)).sum()  # ST correct, MJ wrong\n",
    "    c = ((st_correct == 0) & (mj_correct == 1)).sum()  # ST wrong, MJ correct\n",
    "    \n",
    "    # Create 2x2 contingency table\n",
    "    n00 = ((st_correct == 1) & (mj_correct == 1)).sum()  # both correct\n",
    "    n01 = b  # ST correct, MJ wrong\n",
    "    n10 = c  # ST wrong, MJ correct\n",
    "    n11 = ((st_correct == 0) & (mj_correct == 0)).sum()  # both wrong\n",
    "    \n",
    "    contingency_table = np.array([[n00, n01], [n10, n11]])\n",
    "    \n",
    "    # Perform McNemar's test\n",
    "    if b + c > 0:\n",
    "        result = mcnemar(contingency_table, exact=True if (b + c) < 25 else False)\n",
    "        \n",
    "        # Calculate effect size (Cohen's g for McNemar's test)\n",
    "        total_discordant = b + c\n",
    "        effect_size = abs(b - c) / np.sqrt(b + c) if total_discordant > 0 else 0\n",
    "        \n",
    "        # Calculate improvement direction\n",
    "        mj_better = c > b  # More cases where MJ correct and ST wrong\n",
    "        \n",
    "        return {\n",
    "            'statistic': result.statistic,\n",
    "            'pvalue': result.pvalue,\n",
    "            'b': b,  # ST correct, MJ wrong\n",
    "            'c': c,  # ST wrong, MJ correct  \n",
    "            'contingency_table': contingency_table,\n",
    "            'effect_size': effect_size,\n",
    "            'mj_better': mj_better,\n",
    "            'improvement_cases': c - b\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'error': 'No disagreements between methods',\n",
    "            'contingency_table': contingency_table\n",
    "        }\n",
    "\n",
    "def one_sided_improvement_test(baseline_score, enhanced_score, n_samples, improvement_threshold=30.0):\n",
    "    \"\"\"\n",
    "    Perform one-sided test for improvement above threshold\n",
    "    \n",
    "    Parameters:\n",
    "        baseline_score: Baseline method score\n",
    "        enhanced_score: Enhanced method score\n",
    "        n_samples: Number of test samples\n",
    "        improvement_threshold: Minimum improvement threshold (default: 30%)\n",
    "    \n",
    "    Returns:\n",
    "        dict: One-sided test results\n",
    "    \"\"\"\n",
    "    # Calculate observed improvement percentage\n",
    "    observed_improvement = ((enhanced_score - baseline_score) / baseline_score) * 100\n",
    "    \n",
    "    # For large samples, use normal approximation\n",
    "    # Standard error estimation for proportion differences\n",
    "    p1 = baseline_score  # Baseline success rate\n",
    "    p2 = enhanced_score  # Enhanced success rate\n",
    "    \n",
    "    # Pooled standard error for difference in proportions\n",
    "    se_diff = np.sqrt((p1 * (1 - p1) + p2 * (1 - p2)) / n_samples)\n",
    "    \n",
    "    # Convert improvement threshold to absolute difference\n",
    "    threshold_absolute = (improvement_threshold / 100) * baseline_score\n",
    "    \n",
    "    # Z-statistic for one-sided test\n",
    "    # H0: p2 - p1 <= threshold_absolute\n",
    "    # H1: p2 - p1 > threshold_absolute\n",
    "    observed_diff = enhanced_score - baseline_score\n",
    "    z_statistic = (observed_diff - threshold_absolute) / se_diff if se_diff > 0 else 0\n",
    "    \n",
    "    # One-sided p-value (upper tail) - using scipy_stats to avoid naming conflicts\n",
    "    p_value = 1 - scipy_stats.norm.cdf(z_statistic)\n",
    "    \n",
    "    return {\n",
    "        'observed_improvement_pct': observed_improvement,\n",
    "        'improvement_threshold': improvement_threshold,\n",
    "        'observed_difference': observed_diff,\n",
    "        'threshold_difference': threshold_absolute,\n",
    "        'z_statistic': z_statistic,\n",
    "        'p_value': p_value,\n",
    "        'meets_threshold': observed_improvement >= improvement_threshold,\n",
    "        'statistically_significant': p_value < 0.05,\n",
    "        'standard_error': se_diff\n",
    "    }\n",
    "\n",
    "def bootstrap_hypothesis_test(y_true, y_pred_baseline, y_pred_enhanced, metric_func, \n",
    "                            improvement_threshold=30.0, n_bootstraps=10000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Bootstrap test for 30% improvement hypothesis with p-value calculation\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True labels\n",
    "        y_pred_baseline: Baseline predictions\n",
    "        y_pred_enhanced: Enhanced predictions\n",
    "        metric_func: Function to calculate metric\n",
    "        improvement_threshold: Minimum improvement threshold (default: 30%)\n",
    "        n_bootstraps: Number of bootstrap samples\n",
    "        alpha: Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap hypothesis test results\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    improvements = []\n",
    "    baseline_scores = []\n",
    "    enhanced_scores = []\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample indices\n",
    "        indices = resample(np.arange(n_samples), n_samples=n_samples)\n",
    "        \n",
    "        # Calculate metrics for resampled data\n",
    "        baseline_score = metric_func(y_true[indices], y_pred_baseline[indices], beta=2, zero_division=0)\n",
    "        enhanced_score = metric_func(y_true[indices], y_pred_enhanced[indices], beta=2, zero_division=0)\n",
    "        \n",
    "        baseline_scores.append(baseline_score)\n",
    "        enhanced_scores.append(enhanced_score)\n",
    "        \n",
    "        # Calculate percentage improvement\n",
    "        if baseline_score > 0:\n",
    "            improvement_pct = ((enhanced_score - baseline_score) / baseline_score) * 100\n",
    "            improvements.append(improvement_pct)\n",
    "        else:\n",
    "            improvements.append(0)\n",
    "    \n",
    "    improvements = np.array(improvements)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    improvement_ci = np.percentile(improvements, [lower_percentile, upper_percentile])\n",
    "    \n",
    "    # Hypothesis test: H0: improvement <= threshold, H1: improvement > threshold\n",
    "    # P-value = proportion of bootstrap samples with improvement <= threshold\n",
    "    p_value_bootstrap = np.mean(improvements <= improvement_threshold)\n",
    "    \n",
    "    # One-sided confidence interval (lower bound only for improvement)\n",
    "    improvement_ci_lower = np.percentile(improvements, alpha * 100)\n",
    "    \n",
    "    return {\n",
    "        'mean_improvement': np.mean(improvements),\n",
    "        'std_improvement': np.std(improvements),\n",
    "        'improvement_ci': improvement_ci,\n",
    "        'improvement_ci_lower_one_sided': improvement_ci_lower,\n",
    "        'baseline_mean': np.mean(baseline_scores),\n",
    "        'enhanced_mean': np.mean(enhanced_scores),\n",
    "        'p_value_bootstrap': p_value_bootstrap,\n",
    "        'meets_threshold_ci': improvement_ci[0] > improvement_threshold,\n",
    "        'meets_threshold_one_sided': improvement_ci_lower > improvement_threshold,\n",
    "        'effect_size': np.mean(improvements) / np.std(improvements) if np.std(improvements) > 0 else 0,\n",
    "        'bootstrap_samples': len(improvements),\n",
    "        'power': 1 - p_value_bootstrap if p_value_bootstrap < 0.5 else p_value_bootstrap\n",
    "    }\n",
    "\n",
    "def permutation_test_improvement(y_true, y_pred_baseline, y_pred_enhanced, metric_func,\n",
    "                               improvement_threshold=30.0, n_permutations=10000):\n",
    "    \"\"\"\n",
    "    Permutation test for improvement significance\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True labels\n",
    "        y_pred_baseline: Baseline predictions\n",
    "        y_pred_enhanced: Enhanced predictions\n",
    "        metric_func: Function to calculate metric\n",
    "        improvement_threshold: Minimum improvement threshold\n",
    "        n_permutations: Number of permutation samples\n",
    "    \n",
    "    Returns:\n",
    "        dict: Permutation test results\n",
    "    \"\"\"\n",
    "    # Calculate observed improvement\n",
    "    baseline_score = metric_func(y_true, y_pred_baseline, beta=2, zero_division=0)\n",
    "    enhanced_score = metric_func(y_true, y_pred_enhanced, beta=2, zero_division=0)\n",
    "    observed_improvement = ((enhanced_score - baseline_score) / baseline_score) * 100 if baseline_score > 0 else 0\n",
    "    \n",
    "    # Combine predictions for permutation\n",
    "    combined_predictions = np.column_stack([y_pred_baseline, y_pred_enhanced])\n",
    "    \n",
    "    improvements_null = []\n",
    "    \n",
    "    for _ in range(n_permutations):\n",
    "        # Randomly permute the method labels\n",
    "        permuted_indices = np.random.permutation(2)\n",
    "        perm_pred1 = combined_predictions[:, permuted_indices[0]]\n",
    "        perm_pred2 = combined_predictions[:, permuted_indices[1]]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score1 = metric_func(y_true, perm_pred1, beta=2, zero_division=0)\n",
    "        score2 = metric_func(y_true, perm_pred2, beta=2, zero_division=0)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if score1 > 0:\n",
    "            improvement = ((score2 - score1) / score1) * 100\n",
    "            improvements_null.append(improvement)\n",
    "    \n",
    "    # Calculate p-value: proportion of null improvements >= observed\n",
    "    p_value_permutation = np.mean(np.array(improvements_null) >= observed_improvement)\n",
    "    \n",
    "    return {\n",
    "        'observed_improvement': observed_improvement,\n",
    "        'p_value_permutation': p_value_permutation,\n",
    "        'null_distribution_mean': np.mean(improvements_null),\n",
    "        'null_distribution_std': np.std(improvements_null),\n",
    "        'permutation_samples': len(improvements_null)\n",
    "    }\n",
    "\n",
    "def comprehensive_hypothesis_validation(baseline_config, enhanced_config, predictions_df, \n",
    "                                      y_true, y_pred_baseline, y_pred_enhanced,\n",
    "                                      improvement_threshold=30.0, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive statistical validation of the 30% improvement hypothesis\n",
    "    \n",
    "    Parameters:\n",
    "        baseline_config: Baseline method configuration\n",
    "        enhanced_config: Enhanced method configuration\n",
    "        predictions_df: DataFrame with predictions from both methods\n",
    "        y_true: True labels\n",
    "        y_pred_baseline: Baseline predictions\n",
    "        y_pred_enhanced: Enhanced predictions\n",
    "        improvement_threshold: Minimum improvement threshold (default: 30%)\n",
    "        alpha: Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive validation results\n",
    "    \"\"\"\n",
    "    # Calculate observed metrics\n",
    "    baseline_f2 = fbeta_score(y_true, y_pred_baseline, beta=2, zero_division=0)\n",
    "    enhanced_f2 = fbeta_score(y_true, y_pred_enhanced, beta=2, zero_division=0)\n",
    "    observed_improvement = ((enhanced_f2 - baseline_f2) / baseline_f2) * 100 if baseline_f2 > 0 else 0\n",
    "    \n",
    "    # 1. McNemar's test with effect size\n",
    "    mcnemar_results = mcnemars_test_with_effect_size(predictions_df)\n",
    "    \n",
    "    # 2. One-sided improvement test\n",
    "    one_sided_results = one_sided_improvement_test(baseline_f2, enhanced_f2, len(y_true), improvement_threshold)\n",
    "    \n",
    "    # 3. Bootstrap hypothesis test\n",
    "    bootstrap_results = bootstrap_hypothesis_test(\n",
    "        y_true, y_pred_baseline, y_pred_enhanced, fbeta_score, \n",
    "        improvement_threshold, n_bootstraps=10000, alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # 4. Permutation test\n",
    "    permutation_results = permutation_test_improvement(\n",
    "        y_true, y_pred_baseline, y_pred_enhanced, fbeta_score,\n",
    "        improvement_threshold, n_permutations=5000\n",
    "    )\n",
    "    \n",
    "    # 5. Power analysis\n",
    "    effect_size_cohen = (enhanced_f2 - baseline_f2) / np.sqrt((baseline_f2 * (1 - baseline_f2) + enhanced_f2 * (1 - enhanced_f2)) / 2)\n",
    "    \n",
    "    return {\n",
    "        'observed_improvement': observed_improvement,\n",
    "        'improvement_threshold': improvement_threshold,\n",
    "        'sample_size': len(y_true),\n",
    "        'mcnemar_test': mcnemar_results,\n",
    "        'one_sided_test': one_sided_results,\n",
    "        'bootstrap_test': bootstrap_results,\n",
    "        'permutation_test': permutation_results,\n",
    "        'effect_size_cohen': effect_size_cohen,\n",
    "        'baseline_f2': baseline_f2,\n",
    "        'enhanced_f2': enhanced_f2\n",
    "    }\n",
    "\n",
    "# Perform comprehensive statistical validation if we have comparison results\n",
    "if 'comparison_df' in globals() and not comparison_df.empty and 'hypothesis_validation_results' in globals():\n",
    "    \n",
    "    print(\"COMPREHENSIVE STATISTICAL VALIDATION: 30% IMPROVEMENT HYPOTHESIS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"STATISTICAL FRAMEWORK: Multiple testing approaches to validate hallucination reduction claims\")\n",
    "    print(\"PRIMARY HYPOTHESIS: H: Improvement  30%, H: Improvement > 30%\")\n",
    "    print(\"SIGNIFICANCE LEVEL:  = 0.05\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Get best configurations for each method\n",
    "    best_st = comparison_df[comparison_df['method'] == 'sentence_transformer'].iloc[0]\n",
    "    best_mj = comparison_df[comparison_df['method'] == 'meta_judge'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nMETHODS UNDER COMPARISON:\")\n",
    "    print(f\"  BASELINE: {best_st['model_name']} (Sentence Transformer)\")\n",
    "    print(f\"  ENHANCED: {best_mj['model_name']} - {best_mj['score_column']} (Meta Judge)\")\n",
    "    print(f\"  F2 SCORES: Baseline = {best_st['f2_score']:.4f}, Enhanced = {best_mj['f2_score']:.4f}\")\n",
    "    print(f\"  OBSERVED IMPROVEMENT: {((best_mj['f2_score'] - best_st['f2_score']) / best_st['f2_score'] * 100):.2f}%\")\n",
    "    \n",
    "    # Get predictions for statistical tests\n",
    "    predictions = get_predictions_for_best_models(combined_df, combined_meta_df, best_st, best_mj)\n",
    "    \n",
    "    if not predictions.empty:\n",
    "        y_true = predictions['actual'].values\n",
    "        y_pred_st = predictions['predicted_st'].values\n",
    "        y_pred_mj = predictions['predicted_mj'].values\n",
    "        \n",
    "        print(f\"\\nTEST DATASET:\")\n",
    "        print(f\"  Common instances: {len(predictions)}\")\n",
    "        print(f\"  Positive cases: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.1f}%)\")\n",
    "        print(f\"  Negative cases: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.1f}%)\")\n",
    "        \n",
    "        # Perform comprehensive validation\n",
    "        validation_results = comprehensive_hypothesis_validation(\n",
    "            best_st, best_mj, predictions, y_true, y_pred_st, y_pred_mj,\n",
    "            improvement_threshold=30.0, alpha=0.05\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n1. McNEMAR'S TEST FOR PAIRED CLASSIFIER COMPARISON\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        mcnemar_test = validation_results['mcnemar_test']\n",
    "        if 'error' not in mcnemar_test:\n",
    "            print(f\"Contingency Table (Method Performance):\")\n",
    "            print(f\"                    MJ Correct    MJ Wrong\")\n",
    "            print(f\"ST Correct:         {mcnemar_test['contingency_table'][0,0]:>10} {mcnemar_test['contingency_table'][0,1]:>10}\")\n",
    "            print(f\"ST Wrong:           {mcnemar_test['contingency_table'][1,0]:>10} {mcnemar_test['contingency_table'][1,1]:>10}\")\n",
    "            print(f\"\")\n",
    "            print(f\"Disagreement Analysis:\")\n",
    "            print(f\"  ST correct, MJ wrong: {mcnemar_test['b']} cases\")\n",
    "            print(f\"  ST wrong, MJ correct: {mcnemar_test['c']} cases\")\n",
    "            print(f\"  Net improvement: {mcnemar_test['improvement_cases']} cases\")\n",
    "            print(f\"\")\n",
    "            print(f\"Statistical Results:\")\n",
    "            print(f\"  McNemar's : {mcnemar_test['statistic']:.4f}\")\n",
    "            print(f\"  p-value: {mcnemar_test['pvalue']:.4f}\")\n",
    "            print(f\"  Effect size: {mcnemar_test['effect_size']:.4f}\")\n",
    "            print(f\"  MJ superior: {'YES' if mcnemar_test['mj_better'] else 'NO'}\")\n",
    "            \n",
    "            if mcnemar_test['pvalue'] < 0.05:\n",
    "                print(f\"   SIGNIFICANT: Methods perform differently (p < 0.05)\")\n",
    "            else:\n",
    "                print(f\"   NOT SIGNIFICANT: No significant difference in method performance\")\n",
    "        \n",
    "        print(f\"\\n2. ONE-SIDED IMPROVEMENT TEST (30% THRESHOLD)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        one_sided = validation_results['one_sided_test']\n",
    "        print(f\"Hypothesis Testing:\")\n",
    "        print(f\"  H: Improvement  30%\")\n",
    "        print(f\"  H: Improvement > 30%\")\n",
    "        print(f\"\")\n",
    "        print(f\"Test Results:\")\n",
    "        print(f\"  Observed improvement: {one_sided['observed_improvement_pct']:.2f}%\")\n",
    "        print(f\"  Improvement threshold: {one_sided['improvement_threshold']:.1f}%\")\n",
    "        print(f\"  Z-statistic: {one_sided['z_statistic']:.4f}\")\n",
    "        print(f\"  p-value (one-sided): {one_sided['p_value']:.4f}\")\n",
    "        print(f\"  Standard error: {one_sided['standard_error']:.4f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"Hypothesis Decision:\")\n",
    "        print(f\"  Meets 30% threshold: {'YES' if one_sided['meets_threshold'] else 'NO'}\")\n",
    "        print(f\"  Statistically significant: {'YES' if one_sided['statistically_significant'] else 'NO'}\")\n",
    "        \n",
    "        if one_sided['meets_threshold'] and one_sided['statistically_significant']:\n",
    "            print(f\"   HYPOTHESIS VALIDATED: 30% improvement with statistical significance\")\n",
    "        elif one_sided['meets_threshold']:\n",
    "            print(f\"    THRESHOLD MET: But not statistically significant\")\n",
    "        else:\n",
    "            print(f\"   HYPOTHESIS REJECTED: Does not meet 30% threshold\")\n",
    "        \n",
    "        print(f\"\\n3. BOOTSTRAP HYPOTHESIS TEST (10,000 SAMPLES)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        bootstrap = validation_results['bootstrap_test']\n",
    "        print(f\"Bootstrap Distribution:\")\n",
    "        print(f\"  Mean improvement: {bootstrap['mean_improvement']:.2f}%\")\n",
    "        print(f\"  Standard deviation: {bootstrap['std_improvement']:.2f}%\")\n",
    "        print(f\"  95% CI: [{bootstrap['improvement_ci'][0]:.2f}%, {bootstrap['improvement_ci'][1]:.2f}%]\")\n",
    "        print(f\"  95% Lower bound: {bootstrap['improvement_ci_lower_one_sided']:.2f}%\")\n",
    "        print(f\"\")\n",
    "        print(f\"Hypothesis Test Results:\")\n",
    "        print(f\"  Bootstrap p-value: {bootstrap['p_value_bootstrap']:.4f}\")\n",
    "        print(f\"  Effect size (standardized): {bootstrap['effect_size']:.3f}\")\n",
    "        print(f\"  Statistical power: {bootstrap['power']:.3f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"Confidence Interval Tests:\")\n",
    "        print(f\"  Two-sided CI exceeds 30%: {'YES' if bootstrap['meets_threshold_ci'] else 'NO'}\")\n",
    "        print(f\"  One-sided CI exceeds 30%: {'YES' if bootstrap['meets_threshold_one_sided'] else 'NO'}\")\n",
    "        \n",
    "        if bootstrap['meets_threshold_one_sided']:\n",
    "            print(f\"   BOOTSTRAP VALIDATION: 95% confident improvement > 30%\")\n",
    "        elif bootstrap['p_value_bootstrap'] < 0.05:\n",
    "            print(f\"    SIGNIFICANT IMPROVEMENT: But uncertain if > 30%\")\n",
    "        else:\n",
    "            print(f\"   BOOTSTRAP REJECTION: Cannot confirm > 30% improvement\")\n",
    "        \n",
    "        print(f\"\\n4. PERMUTATION TEST (5,000 SAMPLES)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        permutation = validation_results['permutation_test']\n",
    "        print(f\"Permutation Analysis:\")\n",
    "        print(f\"  Observed improvement: {permutation['observed_improvement']:.2f}%\")\n",
    "        print(f\"  Null distribution mean: {permutation['null_distribution_mean']:.2f}%\")\n",
    "        print(f\"  Null distribution std: {permutation['null_distribution_std']:.2f}%\")\n",
    "        print(f\"  Permutation p-value: {permutation['p_value_permutation']:.4f}\")\n",
    "        print(f\"\")\n",
    "        print(f\"Null Hypothesis Rejection:\")\n",
    "        if permutation['p_value_permutation'] < 0.05:\n",
    "            print(f\"   SIGNIFICANT: Observed improvement unlikely due to chance (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"   NOT SIGNIFICANT: Improvement could be due to random chance\")\n",
    "        \n",
    "        print(f\"\\n5. EFFECT SIZE AND POWER ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        cohen_d = validation_results['effect_size_cohen']\n",
    "        print(f\"Effect Size Analysis:\")\n",
    "        print(f\"  Cohen's d: {cohen_d:.4f}\")\n",
    "        \n",
    "        if abs(cohen_d) >= 0.8:\n",
    "            effect_interpretation = \"Large\"\n",
    "        elif abs(cohen_d) >= 0.5:\n",
    "            effect_interpretation = \"Medium\"\n",
    "        elif abs(cohen_d) >= 0.2:\n",
    "            effect_interpretation = \"Small\"\n",
    "        else:\n",
    "            effect_interpretation = \"Negligible\"\n",
    "        \n",
    "        print(f\"  Effect interpretation: {effect_interpretation}\")\n",
    "        print(f\"  Practical significance: {'YES' if abs(cohen_d) >= 0.5 else 'NO'}\")\n",
    "        \n",
    "        # Sample size adequacy\n",
    "        n = len(y_true)\n",
    "        min_n_for_power = 8 * (1.96 + 0.84)**2 / (cohen_d**2) if cohen_d > 0 else float('inf')\n",
    "        print(f\"\")\n",
    "        print(f\"Power Analysis:\")\n",
    "        print(f\"  Current sample size: {n}\")\n",
    "        print(f\"  Minimum n for 80% power: {min_n_for_power:.0f}\")\n",
    "        print(f\"  Sample size adequate: {'YES' if n >= min_n_for_power else 'NO'}\")\n",
    "        \n",
    "        print(f\"\\n6. MULTIPLE TESTING CORRECTION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Collect all p-values for correction\n",
    "        p_values = []\n",
    "        test_names = []\n",
    "        \n",
    "        if 'pvalue' in mcnemar_test:\n",
    "            p_values.append(mcnemar_test['pvalue'])\n",
    "            test_names.append(\"McNemar's Test\")\n",
    "        \n",
    "        p_values.append(one_sided['p_value'])\n",
    "        test_names.append(\"One-sided Improvement\")\n",
    "        \n",
    "        p_values.append(bootstrap['p_value_bootstrap'])\n",
    "        test_names.append(\"Bootstrap Test\")\n",
    "        \n",
    "        p_values.append(permutation['p_value_permutation'])\n",
    "        test_names.append(\"Permutation Test\")\n",
    "        \n",
    "        # Apply Bonferroni correction\n",
    "        if p_values:\n",
    "            reject, p_adjusted, _, _ = multipletests(p_values, method='bonferroni', alpha=0.05)\n",
    "            \n",
    "            print(f\"Bonferroni Correction ( = 0.05):\")\n",
    "            for i, (test, p_raw, p_adj, rejected) in enumerate(zip(test_names, p_values, p_adjusted, reject)):\n",
    "                print(f\"  {test}: p = {p_raw:.4f}  p_adj = {p_adj:.4f} {'' if rejected else ''}\")\n",
    "        \n",
    "        print(f\"\\n7. FINAL HYPOTHESIS VALIDATION DECISION\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Decision criteria\n",
    "        meets_30_percent = validation_results['observed_improvement'] >= 30.0\n",
    "        one_sided_significant = one_sided['statistically_significant']\n",
    "        bootstrap_significant = bootstrap['meets_threshold_one_sided']\n",
    "        effect_size_adequate = abs(cohen_d) >= 0.5\n",
    "        \n",
    "        # Count supporting evidence\n",
    "        evidence_count = sum([\n",
    "            meets_30_percent,\n",
    "            one_sided_significant,\n",
    "            bootstrap_significant,\n",
    "            effect_size_adequate\n",
    "        ])\n",
    "        \n",
    "        print(f\"EVIDENCE SUMMARY:\")\n",
    "        print(f\"   Observed improvement  30%: {'YES' if meets_30_percent else 'NO'}\")\n",
    "        print(f\"   One-sided test significant: {'YES' if one_sided_significant else 'NO'}\")\n",
    "        print(f\"   Bootstrap CI supports 30%: {'YES' if bootstrap_significant else 'NO'}\")\n",
    "        print(f\"   Effect size adequate: {'YES' if effect_size_adequate else 'NO'}\")\n",
    "        print(f\"\")\n",
    "        print(f\"SUPPORTING EVIDENCE: {evidence_count}/4 criteria met\")\n",
    "        \n",
    "        # Final decision\n",
    "        if evidence_count >= 3:\n",
    "            print(f\"\")\n",
    "            print(f\" STRONG STATISTICAL VALIDATION\")\n",
    "            print(f\"   The 30% improvement hypothesis is STRONGLY SUPPORTED\")\n",
    "            print(f\"   by multiple independent statistical tests.\")\n",
    "            validation_strength = \"STRONG\"\n",
    "        elif evidence_count >= 2:\n",
    "            print(f\"\")\n",
    "            print(f\"  MODERATE STATISTICAL VALIDATION\")\n",
    "            print(f\"   The 30% improvement hypothesis is MODERATELY SUPPORTED.\")\n",
    "            print(f\"   Consider additional validation or larger sample size.\")\n",
    "            validation_strength = \"MODERATE\"\n",
    "        else:\n",
    "            print(f\"\")\n",
    "            print(f\" INSUFFICIENT STATISTICAL VALIDATION\")\n",
    "            print(f\"   The 30% improvement hypothesis is NOT SUPPORTED\")\n",
    "            print(f\"   by the statistical evidence.\")\n",
    "            validation_strength = \"INSUFFICIENT\"\n",
    "        \n",
    "        print(f\"\\n8. NPD BUSINESS IMPACT VALIDATION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"HALLUCINATION REDUCTION EFFECTIVENESS:\")\n",
    "        fp_reduction = best_st['false_positives'] - best_mj['false_positives']\n",
    "        fn_reduction = best_st['false_negatives'] - best_mj['false_negatives']\n",
    "        \n",
    "        print(f\"  False Positive Reduction: {fp_reduction} cases\")\n",
    "        print(f\"  False Negative Reduction: {fn_reduction} cases\")\n",
    "        print(f\"  Total Error Reduction: {fp_reduction + fn_reduction} cases\")\n",
    "        print(f\"\")\n",
    "        print(f\"NPD REQUIREMENTS TRACEABILITY IMPACT:\")\n",
    "        print(f\"  Accuracy improvement: {((best_mj['accuracy'] - best_st['accuracy']) / best_st['accuracy'] * 100):+.1f}%\")\n",
    "        print(f\"  Precision improvement: {((best_mj['precision'] - best_st['precision']) / best_st['precision'] * 100):+.1f}%\")\n",
    "        print(f\"  Recall improvement: {((best_mj['recall'] - best_st['recall']) / best_st['recall'] * 100):+.1f}%\")\n",
    "        \n",
    "        # Save validation results\n",
    "        globals()['statistical_validation_results'] = {\n",
    "            'validation_strength': validation_strength,\n",
    "            'evidence_count': evidence_count,\n",
    "            'meets_30_percent': meets_30_percent,\n",
    "            'comprehensive_results': validation_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 100)\n",
    "        print(f\"STATISTICAL VALIDATION COMPLETE\")\n",
    "        print(f\"Validation Strength: {validation_strength}\")\n",
    "        print(f\"Evidence Score: {evidence_count}/4\")\n",
    "        print(f\"=\" * 100)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nError: Could not match predictions between methods for statistical testing.\")\n",
    "        print(\"Ensure both methods were evaluated on the same requirement pairs.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nStatistical validation not available.\")\n",
    "    print(\"Please ensure you have run:\")\n",
    "    print(\"  - Cell 9: Hypothesis testing comparison (creates comparison_df and hypothesis_validation_results)\")\n",
    "    print(\"  - All previous cells to load the required data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [12] - Hypothesis Testing: Meta Judge Scoring Approaches for 30% Hallucination Reduction\n",
    "# Purpose: Test whether enhanced meta judge scoring methods achieve 30% improvement over baseline actor scores\n",
    "# Dependencies: meta_judge_df from Cell [6], df_ground_truth from Cell [3], statistical libraries from Cell [0]\n",
    "# Breadcrumbs: Meta Judge Data -> Hypothesis Testing -> 30% Improvement Validation for Scoring Methods\n",
    "\n",
    "def test_30_percent_improvement_hypothesis(baseline_score, enhanced_score, approach_name, baseline_name=\"actor_score\"):\n",
    "    \"\"\"\n",
    "    Test the 30% improvement hypothesis for a specific scoring approach\n",
    "    \n",
    "    Parameters:\n",
    "        baseline_score: Baseline method F2 score\n",
    "        enhanced_score: Enhanced method F2 score\n",
    "        approach_name: Name of the enhanced approach\n",
    "        baseline_name: Name of the baseline approach\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hypothesis test results\n",
    "    \"\"\"\n",
    "    if baseline_score <= 0:\n",
    "        return {'error': 'Invalid baseline score', 'approach': approach_name}\n",
    "    \n",
    "    # Calculate percentage improvement\n",
    "    improvement_pct = ((enhanced_score - baseline_score) / baseline_score) * 100\n",
    "    \n",
    "    # Test hypothesis: H0: improvement  30%, H1: improvement > 30%\n",
    "    meets_threshold = improvement_pct >= 30.0\n",
    "    improvement_above_threshold = improvement_pct - 30.0\n",
    "    \n",
    "    # Calculate effect size (Cohen's d equivalent)\n",
    "    effect_size = improvement_above_threshold / 10  # Normalize by 10% as standard unit\n",
    "    \n",
    "    if abs(effect_size) >= 0.8:\n",
    "        practical_significance = \"Large\"\n",
    "    elif abs(effect_size) >= 0.5:\n",
    "        practical_significance = \"Medium\"\n",
    "    elif abs(effect_size) >= 0.2:\n",
    "        practical_significance = \"Small\"\n",
    "    else:\n",
    "        practical_significance = \"Negligible\"\n",
    "    \n",
    "    return {\n",
    "        'approach': approach_name,\n",
    "        'baseline_name': baseline_name,\n",
    "        'baseline_score': baseline_score,\n",
    "        'enhanced_score': enhanced_score,\n",
    "        'improvement_pct': improvement_pct,\n",
    "        'improvement_absolute': enhanced_score - baseline_score,\n",
    "        'meets_30_percent_threshold': meets_threshold,\n",
    "        'improvement_above_threshold': improvement_above_threshold,\n",
    "        'effect_size': effect_size,\n",
    "        'practical_significance': practical_significance\n",
    "    }\n",
    "\n",
    "def bootstrap_improvement_test_meta_judge(y_true, y_pred_baseline, y_pred_enhanced, \n",
    "                                        approach_name, n_bootstraps=5000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Bootstrap test for 30% improvement hypothesis in meta judge scoring\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True labels\n",
    "        y_pred_baseline: Baseline predictions (actor_score)\n",
    "        y_pred_enhanced: Enhanced predictions (meta judge approach)\n",
    "        approach_name: Name of the enhanced approach\n",
    "        n_bootstraps: Number of bootstrap samples\n",
    "        alpha: Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bootstrap test results\n",
    "    \"\"\"\n",
    "    improvements = []\n",
    "    baseline_scores = []\n",
    "    enhanced_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample indices\n",
    "        indices = resample(np.arange(len(y_true)), n_samples=len(y_true))\n",
    "        \n",
    "        # Calculate F2 scores for resampled data\n",
    "        baseline_f2 = fbeta_score(y_true[indices], y_pred_baseline[indices], beta=2, zero_division=0)\n",
    "        enhanced_f2 = fbeta_score(y_true[indices], y_pred_enhanced[indices], beta=2, zero_division=0)\n",
    "        \n",
    "        baseline_scores.append(baseline_f2)\n",
    "        enhanced_scores.append(enhanced_f2)\n",
    "        \n",
    "        # Calculate percentage improvement\n",
    "        if baseline_f2 > 0:\n",
    "            improvement_pct = ((enhanced_f2 - baseline_f2) / baseline_f2) * 100\n",
    "            improvements.append(improvement_pct)\n",
    "        else:\n",
    "            improvements.append(0)\n",
    "    \n",
    "    improvements = np.array(improvements)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    improvement_ci = np.percentile(improvements, [lower_percentile, upper_percentile])\n",
    "    \n",
    "    # One-sided confidence interval (lower bound for improvement)\n",
    "    improvement_ci_lower = np.percentile(improvements, alpha * 100)\n",
    "    \n",
    "    # Test if improvement exceeds 30%\n",
    "    exceeds_30_percent_ci = improvement_ci[0] > 30.0\n",
    "    exceeds_30_percent_one_sided = improvement_ci_lower > 30.0\n",
    "    \n",
    "    return {\n",
    "        'approach': approach_name,\n",
    "        'mean_improvement': np.mean(improvements),\n",
    "        'std_improvement': np.std(improvements),\n",
    "        'improvement_ci': improvement_ci,\n",
    "        'improvement_ci_lower_one_sided': improvement_ci_lower,\n",
    "        'exceeds_30_percent_ci': exceeds_30_percent_ci,\n",
    "        'exceeds_30_percent_one_sided': exceeds_30_percent_one_sided,\n",
    "        'bootstrap_samples': len(improvements)\n",
    "    }\n",
    "\n",
    "print(\"HYPOTHESIS TESTING: META JUDGE SCORING APPROACHES\")\n",
    "print(\"=\" * 100)\n",
    "print(\"CORE HYPOTHESIS: Techniques reducing LLM hallucinations by 30% can improve NPD accuracy\")\n",
    "print(\"OPERATIONAL HYPOTHESIS: Enhanced meta judge scoring achieves 30% improvement over baseline actor_score\")\n",
    "print(\"BASELINE METHOD: actor_score (original actor predictions)\")\n",
    "print(\"ENHANCED METHODS: final_score, combined scores (hallucination reduction techniques)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check what data is available\n",
    "available_data = []\n",
    "if 'meta_judge_df' in globals():\n",
    "    available_data.append('meta_judge_df')\n",
    "if 'combined_meta_df' in globals():\n",
    "    available_data.append('combined_meta_df')\n",
    "if 'df_ground_truth' in globals():\n",
    "    available_data.append('df_ground_truth')\n",
    "\n",
    "print(f\"\\nAvailable data structures: {available_data}\")\n",
    "\n",
    "# Use combined_meta_df if available, otherwise meta_judge_df\n",
    "if 'combined_meta_df' in globals() and not combined_meta_df.empty:\n",
    "    working_df = combined_meta_df.copy()\n",
    "    ground_truth_col = 'ground_truth_traceable'\n",
    "    print(f\"\\nUsing combined_meta_df with {len(working_df)} records\")\n",
    "elif 'meta_judge_df' in globals() and not meta_judge_df.empty and 'df_ground_truth' in globals() and not df_ground_truth.empty:\n",
    "    working_df = meta_judge_df.copy()\n",
    "    \n",
    "    # Add ground truth if not present\n",
    "    if 'ground_truth' not in working_df.columns:\n",
    "        ground_truth_pairs = set(zip(df_ground_truth['source_id'], df_ground_truth['target_id']))\n",
    "        working_df['ground_truth'] = working_df.apply(\n",
    "            lambda row: (row['source_id'], row['target_id']) in ground_truth_pairs, \n",
    "            axis=1\n",
    "        )\n",
    "    ground_truth_col = 'ground_truth'\n",
    "    print(f\"\\nUsing meta_judge_df with {len(working_df)} records\")\n",
    "else:\n",
    "    print(\"\\nERROR: Required data not available. Please run previous cells to load meta judge and ground truth data.\")\n",
    "    working_df = pd.DataFrame()\n",
    "\n",
    "if not working_df.empty and ground_truth_col in working_df.columns:\n",
    "    \n",
    "    print(f\"\\n1. BASELINE vs ENHANCED APPROACHES IDENTIFICATION\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Define baseline and enhanced approaches for hypothesis testing\n",
    "    baseline_approach = 'actor_score'\n",
    "    \n",
    "    # Enhanced approaches that represent hallucination reduction techniques\n",
    "    enhanced_approaches = {}\n",
    "    \n",
    "    # Individual enhanced scores\n",
    "    if 'final_score' in working_df.columns:\n",
    "        enhanced_approaches['final_score'] = 'Meta-judge refined score'\n",
    "    \n",
    "    # Combined scores representing multi-stage refinement\n",
    "    available_combinations = []\n",
    "    if 'actor_score' in working_df.columns and 'final_score' in working_df.columns:\n",
    "        working_df['total_actor_final'] = working_df['actor_score'].fillna(0) + working_df['final_score'].fillna(0)\n",
    "        enhanced_approaches['total_actor_final'] = 'Actor + Final (two-stage refinement)'\n",
    "        available_combinations.append('total_actor_final')\n",
    "    \n",
    "    if all(col in working_df.columns for col in ['actor_score', 'judge_score', 'final_score']):\n",
    "        working_df['total_all_three'] = (working_df['actor_score'].fillna(0) + \n",
    "                                       working_df['judge_score'].fillna(0) + \n",
    "                                       working_df['final_score'].fillna(0))\n",
    "        enhanced_approaches['total_all_three'] = 'Actor + Judge + Final (three-stage refinement)'\n",
    "        available_combinations.append('total_all_three')\n",
    "    \n",
    "    # All available metrics combination\n",
    "    individual_scores = ['actor_score', 'judge_score', 'final_score', \n",
    "                       'semantic_alignment', 'non_functional_coverage', \n",
    "                       'functional_completeness']\n",
    "    available_scores = [score for score in individual_scores if score in working_df.columns]\n",
    "    \n",
    "    if len(available_scores) >= 3:\n",
    "        working_df['total_combined_all'] = sum(working_df[score].fillna(0) for score in available_scores)\n",
    "        enhanced_approaches['total_combined_all'] = f'Combined all {len(available_scores)} metrics'\n",
    "        available_combinations.append('total_combined_all')\n",
    "    \n",
    "    print(f\"BASELINE APPROACH: {baseline_approach} (original actor predictions)\")\n",
    "    print(f\"ENHANCED APPROACHES ({len(enhanced_approaches)} total):\")\n",
    "    for approach, description in enhanced_approaches.items():\n",
    "        print(f\"  - {approach}: {description}\")\n",
    "    \n",
    "    # Get ground truth labels\n",
    "    y_true = working_df[ground_truth_col].astype(int).values\n",
    "    print(f\"\\nGROUND TRUTH DISTRIBUTION:\")\n",
    "    print(f\"  Total instances: {len(y_true)}\")\n",
    "    print(f\"  Positive cases: {y_true.sum()} ({y_true.sum()/len(y_true)*100:.1f}%)\")\n",
    "    print(f\"  Negative cases: {len(y_true) - y_true.sum()} ({(len(y_true) - y_true.sum())/len(y_true)*100:.1f}%)\")\n",
    "    \n",
    "    # Check if baseline approach is available\n",
    "    if baseline_approach not in working_df.columns:\n",
    "        print(f\"\\nERROR: Baseline approach '{baseline_approach}' not found in data\")\n",
    "        print(f\"Available columns: {list(working_df.columns)}\")\n",
    "    else:\n",
    "        print(f\"\\n2. OPTIMAL THRESHOLD IDENTIFICATION FOR EACH APPROACH\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Store results for each approach\n",
    "        approach_results = {}\n",
    "        \n",
    "        # Evaluate baseline approach\n",
    "        baseline_scores = working_df[baseline_approach].fillna(0).values\n",
    "        thresholds = np.percentile(baseline_scores[baseline_scores > 0], np.linspace(0, 100, 50)) if np.any(baseline_scores > 0) else [0]\n",
    "        \n",
    "        best_baseline_f2 = 0\n",
    "        best_baseline_threshold = 0\n",
    "        best_baseline_predictions = None\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (baseline_scores >= threshold).astype(int)\n",
    "            if len(np.unique(y_pred)) >= 2:\n",
    "                f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "                if f2 > best_baseline_f2:\n",
    "                    best_baseline_f2 = f2\n",
    "                    best_baseline_threshold = threshold\n",
    "                    best_baseline_predictions = y_pred\n",
    "        \n",
    "        approach_results[baseline_approach] = {\n",
    "            'threshold': best_baseline_threshold,\n",
    "            'f2_score': best_baseline_f2,\n",
    "            'predictions': best_baseline_predictions,\n",
    "            'is_baseline': True\n",
    "        }\n",
    "        \n",
    "        print(f\"BASELINE - {baseline_approach}:\")\n",
    "        print(f\"  Optimal threshold: {best_baseline_threshold:.3f}\")\n",
    "        print(f\"  F2 Score: {best_baseline_f2:.3f}\")\n",
    "        \n",
    "        # Evaluate enhanced approaches\n",
    "        for approach_name in enhanced_approaches.keys():\n",
    "            if approach_name in working_df.columns:\n",
    "                approach_scores = working_df[approach_name].fillna(0).values\n",
    "                thresholds = np.percentile(approach_scores[approach_scores > 0], np.linspace(0, 100, 50)) if np.any(approach_scores > 0) else [0]\n",
    "                \n",
    "                best_f2 = 0\n",
    "                best_threshold = 0\n",
    "                best_predictions = None\n",
    "                best_metrics = {}\n",
    "                \n",
    "                for threshold in thresholds:\n",
    "                    y_pred = (approach_scores >= threshold).astype(int)\n",
    "                    if len(np.unique(y_pred)) >= 2:\n",
    "                        f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
    "                        if f2 > best_f2:\n",
    "                            best_f2 = f2\n",
    "                            best_threshold = threshold\n",
    "                            best_predictions = y_pred\n",
    "                            best_metrics = {\n",
    "                                'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "                                'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "                                'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "                                'accuracy': accuracy_score(y_true, y_pred)\n",
    "                            }\n",
    "                \n",
    "                if best_predictions is not None:\n",
    "                    approach_results[approach_name] = {\n",
    "                        'threshold': best_threshold,\n",
    "                        'f2_score': best_f2,\n",
    "                        'predictions': best_predictions,\n",
    "                        'metrics': best_metrics,\n",
    "                        'is_baseline': False\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"\\nENHANCED - {approach_name} ({enhanced_approaches[approach_name]}):\")\n",
    "                    print(f\"  Optimal threshold: {best_threshold:.3f}\")\n",
    "                    print(f\"  F2 Score: {best_f2:.3f}\")\n",
    "                    print(f\"  Precision: {best_metrics['precision']:.3f}, Recall: {best_metrics['recall']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n3. 30% IMPROVEMENT HYPOTHESIS TESTING\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Test 30% improvement hypothesis for each enhanced approach\n",
    "        hypothesis_results = []\n",
    "        \n",
    "        for approach_name, results in approach_results.items():\n",
    "            if not results['is_baseline']:\n",
    "                hypothesis_test = test_30_percent_improvement_hypothesis(\n",
    "                    best_baseline_f2, \n",
    "                    results['f2_score'], \n",
    "                    approach_name, \n",
    "                    baseline_approach\n",
    "                )\n",
    "                hypothesis_results.append(hypothesis_test)\n",
    "                \n",
    "                print(f\"\\nHYPOTHESIS TEST: {baseline_approach} vs {approach_name}\")\n",
    "                print(f\"  Baseline F2: {hypothesis_test['baseline_score']:.4f}\")\n",
    "                print(f\"  Enhanced F2: {hypothesis_test['enhanced_score']:.4f}\")\n",
    "                print(f\"  Improvement: {hypothesis_test['improvement_pct']:+.2f}%\")\n",
    "                print(f\"  Meets 30% threshold: {' YES' if hypothesis_test['meets_30_percent_threshold'] else ' NO'}\")\n",
    "                print(f\"  Margin above/below 30%: {hypothesis_test['improvement_above_threshold']:+.2f}%\")\n",
    "                print(f\"  Effect size: {hypothesis_test['effect_size']:.3f} ({hypothesis_test['practical_significance']})\")\n",
    "        \n",
    "        print(f\"\\n4. BOOTSTRAP CONFIDENCE INTERVALS AND STATISTICAL VALIDATION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Bootstrap testing for approaches that have sufficient improvement\n",
    "        bootstrap_results = []\n",
    "        \n",
    "        for approach_name, results in approach_results.items():\n",
    "            if not results['is_baseline'] and results['predictions'] is not None:\n",
    "                bootstrap_test = bootstrap_improvement_test_meta_judge(\n",
    "                    y_true, \n",
    "                    best_baseline_predictions, \n",
    "                    results['predictions'], \n",
    "                    approach_name\n",
    "                )\n",
    "                bootstrap_results.append(bootstrap_test)\n",
    "                \n",
    "                print(f\"\\nBOOTSTRAP ANALYSIS: {approach_name}\")\n",
    "                print(f\"  Mean improvement: {bootstrap_test['mean_improvement']:.2f}%\")\n",
    "                print(f\"  95% CI: [{bootstrap_test['improvement_ci'][0]:.2f}%, {bootstrap_test['improvement_ci'][1]:.2f}%]\")\n",
    "                print(f\"  95% Lower bound: {bootstrap_test['improvement_ci_lower_one_sided']:.2f}%\")\n",
    "                print(f\"  CI exceeds 30% (two-sided): {' YES' if bootstrap_test['exceeds_30_percent_ci'] else ' NO'}\")\n",
    "                print(f\"  CI exceeds 30% (one-sided): {' YES' if bootstrap_test['exceeds_30_percent_one_sided'] else ' NO'}\")\n",
    "                \n",
    "                if bootstrap_test['exceeds_30_percent_one_sided']:\n",
    "                    print(f\"   STATISTICALLY VALIDATED: 95% confident improvement > 30%\")\n",
    "                elif bootstrap_test['exceeds_30_percent_ci']:\n",
    "                    print(f\"    LIKELY VALIDATED: Strong evidence for > 30% improvement\")\n",
    "                else:\n",
    "                    print(f\"   NOT VALIDATED: Cannot confirm > 30% improvement with confidence\")\n",
    "        \n",
    "        print(f\"\\n5. COMPREHENSIVE HYPOTHESIS VALIDATION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Identify best performing enhanced approach\n",
    "        valid_enhanced_approaches = [r for r in hypothesis_results if 'error' not in r]\n",
    "        \n",
    "        if valid_enhanced_approaches:\n",
    "            best_enhanced = max(valid_enhanced_approaches, key=lambda x: x['enhanced_score'])\n",
    "            \n",
    "            print(f\"BEST ENHANCED APPROACH: {best_enhanced['approach']}\")\n",
    "            print(f\"  Description: {enhanced_approaches[best_enhanced['approach']]}\")\n",
    "            print(f\"  Improvement: {best_enhanced['improvement_pct']:+.2f}%\")\n",
    "            print(f\"  F2 Score: {best_enhanced['baseline_score']:.4f}  {best_enhanced['enhanced_score']:.4f}\")\n",
    "            \n",
    "            # Count approaches that meet the 30% threshold\n",
    "            approaches_meeting_threshold = [r for r in valid_enhanced_approaches if r['meets_30_percent_threshold']]\n",
    "            \n",
    "            print(f\"\\nHYPOTHESIS VALIDATION RESULTS:\")\n",
    "            print(f\"  Enhanced approaches tested: {len(valid_enhanced_approaches)}\")\n",
    "            print(f\"  Approaches meeting 30% improvement: {len(approaches_meeting_threshold)}\")\n",
    "            \n",
    "            if approaches_meeting_threshold:\n",
    "                print(f\"\\n   APPROACHES MEETING 30% THRESHOLD:\")\n",
    "                for approach in approaches_meeting_threshold:\n",
    "                    print(f\"    - {approach['approach']}: {approach['improvement_pct']:+.2f}% improvement\")\n",
    "                \n",
    "                # Check statistical validation\n",
    "                statistically_validated = []\n",
    "                for bootstrap_result in bootstrap_results:\n",
    "                    if bootstrap_result['exceeds_30_percent_one_sided']:\n",
    "                        statistically_validated.append(bootstrap_result['approach'])\n",
    "                \n",
    "                if statistically_validated:\n",
    "                    print(f\"\\n   STATISTICALLY VALIDATED APPROACHES:\")\n",
    "                    for approach in statistically_validated:\n",
    "                        print(f\"    - {approach}: Bootstrap CI confirms > 30% improvement\")\n",
    "                else:\n",
    "                    print(f\"\\n    NO STATISTICALLY VALIDATED APPROACHES\")\n",
    "                    print(f\"     Approaches meet 30% threshold but confidence intervals are inconclusive\")\n",
    "            else:\n",
    "                print(f\"\\n   NO APPROACHES MEET 30% THRESHOLD\")\n",
    "                print(f\"     Enhanced meta judge scoring does not achieve required improvement\")\n",
    "            \n",
    "            print(f\"\\n6. NPD BUSINESS IMPACT ASSESSMENT\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            print(f\"HALLUCINATION REDUCTION EFFECTIVENESS:\")\n",
    "            if best_enhanced['meets_30_percent_threshold']:\n",
    "                print(f\"   Meta judge refinement successfully reduces hallucinations\")\n",
    "                print(f\"   Achieves {best_enhanced['improvement_pct']:.1f}% improvement in requirements traceability\")\n",
    "                print(f\"   Practical significance: {best_enhanced['practical_significance']}\")\n",
    "            else:\n",
    "                print(f\"   Meta judge refinement shows improvement but below 30% threshold\")\n",
    "                print(f\"   Observed improvement: {best_enhanced['improvement_pct']:.1f}%\")\n",
    "                print(f\"   Additional refinement needed to reach 30% target\")\n",
    "            \n",
    "            print(f\"\\nOPERATIONAL RECOMMENDATIONS:\")\n",
    "            if approaches_meeting_threshold:\n",
    "                print(f\"  1. Deploy enhanced meta judge scoring in NPD requirements analysis\")\n",
    "                print(f\"  2. Prioritize {best_enhanced['approach']} approach for maximum accuracy gain\")\n",
    "                print(f\"  3. Monitor hallucination reduction in production environment\")\n",
    "            else:\n",
    "                print(f\"  1. Continue refinement of meta judge scoring approaches\")\n",
    "                print(f\"  2. Investigate additional hallucination reduction techniques\")\n",
    "                print(f\"  3. Consider ensemble methods or alternative architectures\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"ERROR: No valid enhanced approaches found for comparison\")\n",
    "        \n",
    "        # Visualization if enabled\n",
    "        if valid_enhanced_approaches and CONFIG.get('SHOW_VISUALIZATION', False):\n",
    "            print(f\"\\n7. VISUALIZATION\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Prepare data for visualization\n",
    "            viz_data = []\n",
    "            \n",
    "            # Add baseline\n",
    "            viz_data.append({\n",
    "                'Approach': baseline_approach,\n",
    "                'F2 Score': best_baseline_f2,\n",
    "                'Type': 'Baseline',\n",
    "                'Improvement %': 0.0\n",
    "            })\n",
    "            \n",
    "            # Add enhanced approaches\n",
    "            for result in valid_enhanced_approaches:\n",
    "                viz_data.append({\n",
    "                    'Approach': result['approach'],\n",
    "                    'F2 Score': result['enhanced_score'],\n",
    "                    'Type': 'Enhanced',\n",
    "                    'Improvement %': result['improvement_pct']\n",
    "                })\n",
    "            \n",
    "            viz_df = pd.DataFrame(viz_data)\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # F2 Score comparison\n",
    "            colors = ['lightblue' if t == 'Baseline' else 'lightcoral' for t in viz_df['Type']]\n",
    "            bars1 = ax1.bar(range(len(viz_df)), viz_df['F2 Score'], color=colors)\n",
    "            ax1.set_xticks(range(len(viz_df)))\n",
    "            ax1.set_xticklabels(viz_df['Approach'], rotation=45, ha='right')\n",
    "            ax1.set_ylabel('F2 Score')\n",
    "            ax1.set_title('F2 Score Comparison: Baseline vs Enhanced Approaches')\n",
    "            ax1.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add 30% improvement threshold line\n",
    "            if best_baseline_f2 > 0:\n",
    "                threshold_line = best_baseline_f2 * 1.3\n",
    "                ax1.axhline(y=threshold_line, color='red', linestyle='--', alpha=0.7, \n",
    "                           label=f'30% Improvement Threshold ({threshold_line:.3f})')\n",
    "                ax1.legend()\n",
    "            \n",
    "            # Improvement percentage comparison\n",
    "            enhanced_only = viz_df[viz_df['Type'] == 'Enhanced']\n",
    "            colors2 = ['green' if imp >= 30 else 'orange' for imp in enhanced_only['Improvement %']]\n",
    "            bars2 = ax2.bar(range(len(enhanced_only)), enhanced_only['Improvement %'], color=colors2)\n",
    "            ax2.set_xticks(range(len(enhanced_only)))\n",
    "            ax2.set_xticklabels(enhanced_only['Approach'], rotation=45, ha='right')\n",
    "            ax2.set_ylabel('Improvement %')\n",
    "            ax2.set_title('Improvement Percentage vs 30% Threshold')\n",
    "            ax2.axhline(y=30, color='red', linestyle='--', alpha=0.7, label='30% Threshold')\n",
    "            ax2.grid(axis='y', alpha=0.3)\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 100)\n",
    "        print(f\"HYPOTHESIS TESTING COMPLETE\")\n",
    "        print(f\"Meta judge scoring approaches evaluated against 30% improvement threshold\")\n",
    "        print(f\"=\" * 100)\n",
    "\n",
    "else:\n",
    "    print(\"\\nERROR: Cannot proceed with hypothesis testing\")\n",
    "    print(\"Required data not available or ground truth column missing\")\n",
    "    print(\"\\nPlease ensure you have:\")\n",
    "    print(\"  - Loaded meta judge data (meta_judge_df or combined_meta_df)\")\n",
    "    print(\"  - Loaded ground truth data (df_ground_truth)\")\n",
    "    print(\"  - Run previous cells to establish proper data relationships\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

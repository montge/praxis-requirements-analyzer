{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements Traceability Analysis and Model Comparison\n",
    "**Analysis and visualization of requirements traceability model performance with cross-project comparison, LLM vs baseline evaluation, confusion matrix analysis, and role-based performance assessment using Neo4j data integration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings\n",
    "# Dependencies: pandas, numpy, neo4j, matplotlib, seaborn, dotenv, re\n",
    "# Breadcrumbs: Setup -> Imports\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configure logging and load environment variables for traceability analysis\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (config_dict, logger_instance) containing configuration parameters and logger\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Neo4j credentials from environment variables\n",
    "    config = {\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'),\n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD')\n",
    "    }\n",
    "    \n",
    "    # Set figure style for consistent visualization\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_context(\"talk\")\n",
    "    \n",
    "    logger.info(\"Environment setup completed for traceability analysis\")\n",
    "    return config, logger\n",
    "\n",
    "# Execute setup\n",
    "CONFIG, logger = setup_environment()\n",
    "NEO4J_URI = CONFIG['NEO4J_URI']\n",
    "NEO4J_USER = CONFIG['NEO4J_USER']\n",
    "NEO4J_PASSWORD = CONFIG['NEO4J_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Neo4j Connection Setup\n",
    "# Purpose: Create and test connection to Neo4j database containing traceability data\n",
    "# Dependencies: neo4j, logging\n",
    "# Breadcrumbs: Setup -> Database Connection\n",
    "\n",
    "def create_neo4j_driver(uri=None, user=None, password=None):\n",
    "    \"\"\"\n",
    "    Create and return a Neo4j driver instance for database connectivity\n",
    "    \n",
    "    Parameters:\n",
    "        uri (str, optional): Neo4j URI. Defaults to NEO4J_URI from environment.\n",
    "        user (str, optional): Neo4j username. Defaults to NEO4J_USER from environment.\n",
    "        password (str, optional): Neo4j password. Defaults to NEO4J_PASSWORD from environment.\n",
    "    \n",
    "    Returns:\n",
    "        GraphDatabase.driver: Connected Neo4j driver instance\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If connection to Neo4j fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use parameters if provided, otherwise use globals from setup_environment\n",
    "        _uri = uri if uri is not None else NEO4J_URI\n",
    "        _user = user if user is not None else NEO4J_USER\n",
    "        _password = password if password is not None else NEO4J_PASSWORD\n",
    "        \n",
    "        driver = GraphDatabase.driver(_uri, auth=(_user, _password))\n",
    "        logger.info(\"Successfully connected to Neo4j database\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Neo4j: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def test_neo4j_connection(driver):\n",
    "    \"\"\"\n",
    "    Test the Neo4j database connection\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver instance to test\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if connection successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 1 as test\").single()\n",
    "            if result and result[\"test\"] == 1:\n",
    "                print(\"✓ Successfully connected to Neo4j database\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"✗ Failed to connect to Neo4j database\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database connection test failed: {str(e)}\")\n",
    "        print(\"✗ Failed to connect to Neo4j database\")\n",
    "        return False\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = create_neo4j_driver()\n",
    "\n",
    "# Test connection\n",
    "test_neo4j_connection(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Query Model Data from Neo4j\n",
    "# Purpose: Retrieve individual model data points for visualization and analysis\n",
    "# Dependencies: neo4j, pandas, json, logging\n",
    "# Breadcrumbs: Data Acquisition -> Model Data Retrieval\n",
    "\n",
    "def query_model_data(driver):\n",
    "    \"\"\"\n",
    "    Query model data from Neo4j for all model types including metrics analysis\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver instance for database connection\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing model data with parsed JSON metrics,\n",
    "                     display names, and metadata. Empty DataFrame if no data found.\n",
    "                     \n",
    "    Raises:\n",
    "        Exception: If database query fails or data parsing encounters errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query for metrics analysis data including model_data\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Project)-[r:HAS_METRICS_ANALYSIS]->(m:MetricsAnalysis)\n",
    "        WHERE m.analysis_type = 'whisker_chart'\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            m.model_type as model_type,\n",
    "            m.model_data as model_data,\n",
    "            m.model_count as model_count,\n",
    "            m.created_at as created_at\n",
    "        ORDER BY p.name, m.model_type\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            results = session.run(query).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(\"No model data found in Neo4j\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            # Convert results to DataFrame\n",
    "            model_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Parse JSON data\n",
    "            model_df['model_data_parsed'] = model_df['model_data'].apply(\n",
    "                lambda x: json.loads(x) if isinstance(x, str) else x\n",
    "            )\n",
    "            \n",
    "            # Make sentence transformer/tf-idf human readable\n",
    "            model_df['display_model_type'] = model_df['model_type'].apply(\n",
    "                lambda x: \"Sentence Transformer/TF-IDF\" if x == \"sentence_transformer_tf_idf\" else x\n",
    "            )\n",
    "                \n",
    "            logger.info(f\"Retrieved model data for {model_df['model_type'].nunique()} model types across {model_df['project_name'].nunique()} projects\")\n",
    "            \n",
    "            return model_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying model data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Query model data for both sentence transformers and LLMs\n",
    "model_df = query_model_data(driver)\n",
    "\n",
    "# Display summary of retrieved data\n",
    "if not model_df.empty:\n",
    "    print(f\"\\nRetrieved model data for {model_df['project_name'].nunique()} projects:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for project in model_df['project_name'].unique():\n",
    "        project_data = model_df[model_df['project_name'] == project]\n",
    "        print(f\"\\nProject: {project}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for _, row in project_data.iterrows():\n",
    "            print(f\"  - {row['display_model_type']}: {row['model_count']} models analyzed on {row['created_at']}\")\n",
    "            \n",
    "    # Display example of model data structure\n",
    "    print(\"\\nExample model data structure:\")\n",
    "    print(\"-\" * 40)\n",
    "    first_model_data = model_df.iloc[0]['model_data_parsed']\n",
    "    if first_model_data:\n",
    "        for metric, models in list(first_model_data.items())[:2]:  # Show first 2 metrics only\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            for model_name, score in list(models.items())[:3]:  # Show first 3 models only\n",
    "                print(f\"    - {model_name}: {score}\")\n",
    "else:\n",
    "    print(\"No model data found. Please run analysis notebooks first to generate data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Process and Prepare Model Data for Visualization\n",
    "# Purpose: Extract and organize individual model data points for comprehensive visualization\n",
    "# Dependencies: pandas, logging\n",
    "# Breadcrumbs: Data Processing -> Model Data Organization\n",
    "\n",
    "def extract_raw_model_data(model_df):\n",
    "    \"\"\"\n",
    "    Extract raw model data points from Neo4j data and organize by project, metric, and model\n",
    "    \n",
    "    Parameters:\n",
    "        model_df (pd.DataFrame): DataFrame containing model data from Neo4j with parsed JSON\n",
    "        \n",
    "    Returns:\n",
    "        dict: Nested dictionary structure organized as:\n",
    "              {project: {metric: {model_type: {model_name: [scores]}}}}\n",
    "              Returns empty dict if input DataFrame is empty or invalid.\n",
    "              \n",
    "    Notes:\n",
    "        - Converts raw metric names to human-readable display names\n",
    "        - Handles multiple score entries per model for statistical analysis\n",
    "        - Supports metrics: accuracy, balanced_accuracy, precision, recall, f1_score, f2_score, matthews_corr\n",
    "    \"\"\"\n",
    "    if model_df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize data structure\n",
    "    # Format: {project: {metric: {model_type: {model_name: [scores]}}}}\n",
    "    organized_data = {}\n",
    "    \n",
    "    # Define metrics to extract\n",
    "    metrics_to_extract = [\n",
    "        'accuracy', 'balanced_accuracy', 'precision', 'recall', \n",
    "        'f1_score', 'f2_score', 'matthews_corr'\n",
    "    ]\n",
    "    \n",
    "    # Human readable metric names mapping\n",
    "    metric_display_names = {\n",
    "        'accuracy': 'Accuracy',\n",
    "        'balanced_accuracy': 'Balanced Accuracy',\n",
    "        'precision': 'Precision',\n",
    "        'recall': 'Recall',\n",
    "        'f1_score': 'F1 Score',\n",
    "        'f2_score': 'F2 Score',\n",
    "        'matthews_corr': 'Matthews Correlation'\n",
    "    }\n",
    "    \n",
    "    for _, row in model_df.iterrows():\n",
    "        project = row['project_name']\n",
    "        model_type = row['display_model_type']\n",
    "        model_data = row['model_data_parsed']\n",
    "        \n",
    "        if not model_data or not isinstance(model_data, dict):\n",
    "            continue\n",
    "            \n",
    "        if project not in organized_data:\n",
    "            organized_data[project] = {}\n",
    "            \n",
    "        # Process each metric in model_data\n",
    "        for metric_key, models in model_data.items():\n",
    "            # Get display name for metric\n",
    "            metric_display = None\n",
    "            for raw_name, display_name in metric_display_names.items():\n",
    "                if raw_name.lower() == metric_key.lower() or raw_name.replace('_', '') == metric_key.replace('_', ''):\n",
    "                    metric_display = display_name\n",
    "                    break\n",
    "                    \n",
    "            if not metric_display:\n",
    "                # If no match found, use capitalized version of the key\n",
    "                metric_display = metric_key.replace('_', ' ').title()\n",
    "                \n",
    "            if metric_display not in organized_data[project]:\n",
    "                organized_data[project][metric_display] = {}\n",
    "                \n",
    "            if model_type not in organized_data[project][metric_display]:\n",
    "                organized_data[project][metric_display][model_type] = {}\n",
    "                \n",
    "            # Add model scores\n",
    "            for model_name, score in models.items():\n",
    "                if isinstance(score, (int, float)):\n",
    "                    # Add to existing list or create new one\n",
    "                    if model_name in organized_data[project][metric_display][model_type]:\n",
    "                        organized_data[project][metric_display][model_type][model_name].append(score)\n",
    "                    else:\n",
    "                        organized_data[project][metric_display][model_type][model_name] = [score]\n",
    "    \n",
    "    return organized_data\n",
    "\n",
    "# Extract raw model data\n",
    "raw_model_data = extract_raw_model_data(model_df)\n",
    "\n",
    "# Display a summary of the organized raw data\n",
    "if raw_model_data:\n",
    "    print(\"\\nRaw model data summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for project, metrics in raw_model_data.items():\n",
    "        print(f\"\\nProject: {project}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for metric, model_types in metrics.items():\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            for model_type, models in model_types.items():\n",
    "                total_points = sum(len(scores) for scores in models.values())\n",
    "                print(f\"    - {model_type}: {len(models)} models, {total_points} total data points\")\n",
    "                # Show example data\n",
    "                if models:\n",
    "                    first_model = next(iter(models.items()))\n",
    "                    print(f\"      Example: {first_model[0]}: {first_model[1][:5]}...\")\n",
    "else:\n",
    "    print(\"No raw model data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Create Metric-Specific Visualizations with Individual Data Points\n",
    "# Purpose: Generate comprehensive boxplots with individual data points for model type comparison\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy\n",
    "# Breadcrumbs: Visualization -> Metric Comparison Charts\n",
    "\n",
    "def create_metric_visualizations_with_points(raw_model_data):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for each metric in each project comparing model performance\n",
    "    \n",
    "    Parameters:\n",
    "        raw_model_data (dict): Nested dictionary containing organized model data from extract_raw_model_data()\n",
    "                              Structure: {project: {metric: {model_type: {model_name: [scores]}}}}\n",
    "                              \n",
    "    Returns:\n",
    "        None: Displays visualizations directly using matplotlib/seaborn\n",
    "        \n",
    "    Notes:\n",
    "        - Creates separate boxplots for each metric in each project\n",
    "        - Includes individual data points with jitter for detailed analysis\n",
    "        - Generates statistical heatmaps with min, max, mean, median, quartiles\n",
    "        - Uses consistent color mapping across visualizations\n",
    "        - Saves high-resolution PNG files for each visualization\n",
    "        - Prioritizes Sentence Transformer/TF-IDF models in display order\n",
    "    \"\"\"\n",
    "    # Import numpy for jitter function\n",
    "    import numpy as np\n",
    "    \n",
    "    if not raw_model_data:\n",
    "        print(\"No data available for visualization.\")\n",
    "        return\n",
    "    \n",
    "    # Create dynamic color map based on actual model types in the data\n",
    "    all_model_types = set()\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        for metric, model_types in metrics.items():\n",
    "            all_model_types.update(model_types.keys())\n",
    "    \n",
    "    # Define color palette\n",
    "    color_palette = ['gold', 'skyblue', 'lightgreen', 'lightpink', 'mediumpurple', 'lightcoral', 'lightblue', 'lightsalmon']\n",
    "    \n",
    "    # Create color map dynamically\n",
    "    color_map = {}\n",
    "    for i, model_type in enumerate(all_model_types):\n",
    "        color_map[model_type] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # Process each project\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        print(f\"\\nGenerating visualizations for project: {project}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Create a figure for each metric\n",
    "        for metric_name, model_types in metrics.items():\n",
    "            # Skip if no model types\n",
    "            if not model_types:\n",
    "                print(f\"  Skipping metric {metric_name}: No model types\")\n",
    "                continue\n",
    "                \n",
    "            # Print status update\n",
    "            print(f\"  Processing metric: {metric_name}\")\n",
    "            \n",
    "            # Create combined dataset for all model types\n",
    "            all_models_data = []\n",
    "            for model_type, models in model_types.items():\n",
    "                for model_name, scores in models.items():\n",
    "                    # Add each score as a separate row\n",
    "                    for score in scores:\n",
    "                        all_models_data.append({\n",
    "                            'model_type': model_type,\n",
    "                            'model_name': model_name,\n",
    "                            'score': score\n",
    "                        })\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(all_models_data)\n",
    "            \n",
    "            # Skip if no data\n",
    "            if df.empty:\n",
    "                print(f\"  Skipping metric {metric_name}: No data\")\n",
    "                continue\n",
    "            \n",
    "            # Get unique model types and ensure specific order with Sentence Transformer first\n",
    "            model_types_list = list(df['model_type'].unique())\n",
    "            \n",
    "            # Custom sort to put 'Sentence Transformer/TF-IDF' first\n",
    "            if 'Sentence Transformer/TF-IDF' in model_types_list:\n",
    "                model_types_list.remove('Sentence Transformer/TF-IDF')\n",
    "                # Sort other model types alphanumerically\n",
    "                model_types_list.sort()\n",
    "                # Put Sentence Transformer first\n",
    "                model_types_list = ['Sentence Transformer/TF-IDF'] + model_types_list\n",
    "            \n",
    "            # Calculate statistics for each model type in the desired order\n",
    "            stats_data = []\n",
    "            for model_type in model_types_list:\n",
    "                model_scores = df[df['model_type'] == model_type]['score']\n",
    "                stats_data.append({\n",
    "                    'model_type': model_type,\n",
    "                    'min': model_scores.min(),\n",
    "                    'q1': model_scores.quantile(0.25),\n",
    "                    'median': model_scores.median(),\n",
    "                    'mean': model_scores.mean(),\n",
    "                    'q3': model_scores.quantile(0.75),\n",
    "                    'max': model_scores.max(),\n",
    "                    'std': model_scores.std()\n",
    "                })\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            stats_df = pd.DataFrame(stats_data)\n",
    "            \n",
    "            # Create figure with subplots explicitly\n",
    "            fig = plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Create subplots with specific heights and spacing\n",
    "            # Use larger gridspec to have more control over spacing\n",
    "            gs = plt.GridSpec(20, 1, figure=fig)\n",
    "            \n",
    "            # Boxplot occupies the top 14 rows\n",
    "            ax_box = fig.add_subplot(gs[0:14, 0])\n",
    "            \n",
    "            # Stats table occupies the bottom 6 rows\n",
    "            ax_heatmap = fig.add_subplot(gs[14:20, 0])\n",
    "            \n",
    "            # Create boxplot with custom colors for each model type\n",
    "            for i, model_type in enumerate(model_types_list):\n",
    "                model_data = df[df['model_type'] == model_type]\n",
    "                \n",
    "                # Calculate position\n",
    "                positions = [i]\n",
    "                \n",
    "                # Create boxplot for this model type\n",
    "                boxplot = ax_box.boxplot(\n",
    "                    model_data['score'],\n",
    "                    positions=positions,\n",
    "                    patch_artist=True,\n",
    "                    widths=0.6,\n",
    "                    showcaps=True,\n",
    "                    showfliers=False,\n",
    "                    # Make box outlines thinner\n",
    "                    boxprops={'linewidth': 0.8},\n",
    "                    # Make whiskers thinner\n",
    "                    whiskerprops={'linewidth': 0.8},\n",
    "                    # Make caps thinner\n",
    "                    capprops={'linewidth': 0.8},\n",
    "                    # Make median line thinner\n",
    "                    medianprops={'linewidth': 0.8}\n",
    "                )\n",
    "                \n",
    "                # Set boxplot colors\n",
    "                for box in boxplot['boxes']:\n",
    "                    box.set(facecolor=color_map.get(model_type, 'gray'))\n",
    "                \n",
    "                # Number of points for this model type\n",
    "                n_points = len(model_data)\n",
    "                \n",
    "                # Create jittered x positions - similar to how it's done in the reference notebook\n",
    "                # Set jitter width relative to the boxplot width\n",
    "                jitter_width = 0.3\n",
    "                \n",
    "                # Generate random offsets for each point\n",
    "                jittered_x = np.full(n_points, i) + np.random.uniform(-jitter_width, jitter_width, size=n_points)\n",
    "                \n",
    "                # Add individual data points with jitter\n",
    "                ax_box.scatter(\n",
    "                    jittered_x,\n",
    "                    model_data['score'],\n",
    "                    color='black',\n",
    "                    alpha=0.5,\n",
    "                    s=20,\n",
    "                    zorder=3\n",
    "                )\n",
    "            \n",
    "            # Set x-ticks to model types but don't show labels (they'll be in the heatmap)\n",
    "            ax_box.set_xticks(range(len(model_types_list)))\n",
    "            ax_box.set_xticklabels([''] * len(model_types_list))\n",
    "            \n",
    "            # Make tick marks thinner\n",
    "            ax_box.tick_params(axis='both', width=0.8, length=4, pad=4)\n",
    "            \n",
    "            # Remove bottom spacing by hiding x-axis\n",
    "            ax_box.spines['bottom'].set_visible(False)\n",
    "            ax_box.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "            \n",
    "            # Make all spines thinner\n",
    "            for spine in ax_box.spines.values():\n",
    "                spine.set_linewidth(0.8)\n",
    "            \n",
    "            # Customize the boxplot\n",
    "            ax_box.set_title(f'Project: {project} - {metric_name}', fontsize=14)\n",
    "            ax_box.set_xlabel('')  # Remove x-axis label\n",
    "            ax_box.set_ylabel('Score', fontsize=12)\n",
    "            ax_box.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.6)\n",
    "            ax_box.set_ylim(0, 1.0)  # Metrics are typically in range [0, 1]\n",
    "            \n",
    "            # Add a horizontal line at y=0.5 as a reference\n",
    "            ax_box.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=0.8)\n",
    "            \n",
    "            # Prepare data for heatmap\n",
    "            heatmap_data = pd.DataFrame(index=['Min', 'Q1', 'Median', 'Mean', 'Q3', 'Max', 'Std Dev'])\n",
    "            \n",
    "            # Add model data in the custom order\n",
    "            for model_type in model_types_list:\n",
    "                model_row = stats_df[stats_df['model_type'] == model_type].iloc[0]\n",
    "                heatmap_data[model_type] = [\n",
    "                    model_row['min'], model_row['q1'], model_row['median'], \n",
    "                    model_row['mean'], model_row['q3'], model_row['max'], model_row['std']\n",
    "                ]\n",
    "            \n",
    "            # Create formatted version for display\n",
    "            formatted_data = {}\n",
    "            for col in heatmap_data.columns:\n",
    "                formatted_data[col] = []\n",
    "                for i, val in enumerate(heatmap_data[col]):\n",
    "                    if i == 6:  # Std Dev row\n",
    "                        formatted_data[col].append(f\"{val:.3f}\")\n",
    "                    else:\n",
    "                        formatted_data[col].append(f\"{val:.3f}\")\n",
    "            \n",
    "            formatted_df = pd.DataFrame(formatted_data, index=heatmap_data.index)\n",
    "            \n",
    "            # Create the heatmap with custom colormap\n",
    "            sns.heatmap(\n",
    "                heatmap_data,\n",
    "                annot=formatted_df,\n",
    "                fmt=\"\",\n",
    "                cmap=\"YlGnBu\",\n",
    "                linewidths=0.5,\n",
    "                linecolor='lightgray',\n",
    "                cbar=False,\n",
    "                ax=ax_heatmap,\n",
    "                vmin=0,\n",
    "                vmax=1.0  # Set maximum value for color scaling (most metrics are 0-1)\n",
    "            )\n",
    "            \n",
    "            # Make heatmap tick marks thinner\n",
    "            ax_heatmap.tick_params(axis='both', width=0.8, length=4)\n",
    "            \n",
    "            # Customize heatmap appearance\n",
    "            ax_heatmap.set_title('')  # Remove redundant title\n",
    "            ax_heatmap.set_xticklabels(ax_heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Custom color for std dev row\n",
    "            cells = ax_heatmap.get_children()\n",
    "            std_dev_row_idx = 6  # Std Dev is the last row\n",
    "            \n",
    "            # Filter for rectangle patches (cells) that are in the std dev row\n",
    "            for i, cell in enumerate(cells):\n",
    "                if hasattr(cell, 'get_xy'):  # Check if it's a patch with coordinates\n",
    "                    y = cell.get_xy()[1]\n",
    "                    # If this cell is in the std dev row\n",
    "                    if abs(y - std_dev_row_idx) < 0.1:\n",
    "                        cell.set_facecolor('#f5f5f5')  # Light gray for std dev row\n",
    "            \n",
    "            # Adjust layout to reduce space between plots\n",
    "            plt.subplots_adjust(hspace=1.0)  # Minimal space between subplots\n",
    "            \n",
    "            # Save figure with bbox_inches to ensure all elements are included\n",
    "            plt.savefig(f\"{project}_{metric_name.replace(' ', '_')}_with_points.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"  Generated visualization for metric: {metric_name}\")\n",
    "        \n",
    "        print(f\"Completed visualizations for project: {project}\")\n",
    "\n",
    "# Import required for patches\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Generate visualizations with individual data points\n",
    "create_metric_visualizations_with_points(raw_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Combined Multi-Project Performance Comparison\n",
    "# Purpose: Create comprehensive cross-project analysis comparing model types and performance\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy\n",
    "# Breadcrumbs: Visualization -> Cross-Project Analysis\n",
    "\n",
    "def create_multi_project_comparison(raw_model_data):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison of model types across all projects with statistical analysis\n",
    "    \n",
    "    Parameters:\n",
    "        raw_model_data (dict): Nested dictionary containing organized model data from extract_raw_model_data()\n",
    "                              Structure: {project: {metric: {model_type: {model_name: [scores]}}}}\n",
    "                              \n",
    "    Returns:\n",
    "        None: Displays multiple visualizations and saves PNG files\n",
    "        \n",
    "    Notes:\n",
    "        - Focuses on key metrics: F1 Score, F2 Score, Matthews Correlation\n",
    "        - Creates grouped boxplots comparing model types across projects\n",
    "        - Generates detailed statistical tables for each metric\n",
    "        - Uses consistent color mapping with Sentence Transformer/TF-IDF prioritized\n",
    "        - Includes jittered individual data points for transparency\n",
    "        - Saves separate high-resolution images for each metric comparison\n",
    "    \"\"\"\n",
    "    # Import numpy for jitter function\n",
    "    import numpy as np\n",
    "    \n",
    "    if not raw_model_data:\n",
    "        print(\"No data available for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Select key metrics for comparison\n",
    "    key_metrics = ['F1 Score', 'F2 Score', 'Matthews Correlation']\n",
    "    \n",
    "    # Create dynamic color map based on actual model types in the data\n",
    "    all_model_types = set()\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        for metric, model_types in metrics.items():\n",
    "            all_model_types.update(model_types.keys())\n",
    "    \n",
    "    # Define color palette\n",
    "    color_palette = ['gold', 'skyblue', 'lightgreen', 'lightpink', 'mediumpurple', 'lightcoral', 'lightblue', 'lightsalmon']\n",
    "    \n",
    "    # Create color map dynamically\n",
    "    color_map = {}\n",
    "    for i, model_type in enumerate(all_model_types):\n",
    "        color_map[model_type] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # Count projects with data\n",
    "    projects_with_data = []\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        has_key_metric = any(metric in metrics for metric in key_metrics)\n",
    "        if has_key_metric:\n",
    "            projects_with_data.append(project)\n",
    "    \n",
    "    if not projects_with_data:\n",
    "        print(\"No projects have data for key metrics.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nCreating multi-project comparison for {len(projects_with_data)} projects\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Prepare data for each key metric\n",
    "    for metric_name in key_metrics:\n",
    "        print(f\"Processing metric: {metric_name}\")\n",
    "        \n",
    "        # Collect data from all projects\n",
    "        comparison_data = []\n",
    "        \n",
    "        for project in projects_with_data:\n",
    "            if metric_name not in raw_model_data[project]:\n",
    "                continue\n",
    "                \n",
    "            model_types = raw_model_data[project][metric_name]\n",
    "            \n",
    "            for model_type, models in model_types.items():\n",
    "                for model_name, scores in models.items():\n",
    "                    # Add each score as a separate row\n",
    "                    for score in scores:\n",
    "                        comparison_data.append({\n",
    "                            'project': project,\n",
    "                            'model_type': model_type,\n",
    "                            'model_name': model_name,\n",
    "                            'score': score\n",
    "                        })\n",
    "        \n",
    "        # Skip if not enough data\n",
    "        if len(comparison_data) < 5:\n",
    "            print(f\"  Skipping {metric_name}: Not enough data\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Create figure for this metric\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Order projects alphabetically\n",
    "        projects_ordered = sorted(df['project'].unique())\n",
    "        \n",
    "        # Get unique model types and ensure specific order with Sentence Transformer first\n",
    "        model_types_list = list(df['model_type'].unique())\n",
    "        if 'Sentence Transformer/TF-IDF' in model_types_list:\n",
    "            model_types_list.remove('Sentence Transformer/TF-IDF')\n",
    "            model_types_list.sort()\n",
    "            model_types_list = ['Sentence Transformer/TF-IDF'] + model_types_list\n",
    "        \n",
    "        # Create custom grouped boxplot\n",
    "        for p_idx, project in enumerate(projects_ordered):\n",
    "            project_data = df[df['project'] == project]\n",
    "            \n",
    "            # Base position for this project\n",
    "            base_pos = p_idx\n",
    "            \n",
    "            # Number of model types for spacing\n",
    "            n_models = len(model_types_list)\n",
    "            \n",
    "            # Width of a model group within a project\n",
    "            model_width = 0.8 / n_models\n",
    "            \n",
    "            # Plot each model type as a separate boxplot\n",
    "            for m_idx, model_type in enumerate(model_types_list):\n",
    "                model_project_data = project_data[project_data['model_type'] == model_type]\n",
    "                \n",
    "                if not model_project_data.empty:\n",
    "                    # Calculate position for this model within the project\n",
    "                    pos = base_pos - 0.4 + model_width * (m_idx + 0.5)\n",
    "                    \n",
    "                    # Create boxplot\n",
    "                    boxplot = plt.boxplot(\n",
    "                        model_project_data['score'],\n",
    "                        positions=[pos],\n",
    "                        patch_artist=True,\n",
    "                        widths=model_width * 0.8,\n",
    "                        showcaps=True,\n",
    "                        showfliers=False,\n",
    "                        boxprops={'linewidth': 0.8},\n",
    "                        whiskerprops={'linewidth': 0.8},\n",
    "                        capprops={'linewidth': 0.8},\n",
    "                        medianprops={'linewidth': 0.8, 'color': 'black'}\n",
    "                    )\n",
    "                    \n",
    "                    # Set color\n",
    "                    for box in boxplot['boxes']:\n",
    "                        box.set(facecolor=color_map.get(model_type, 'gray'))\n",
    "                    \n",
    "                    # Add jittered points\n",
    "                    n_points = len(model_project_data)\n",
    "                    jitter_width = model_width * 0.3\n",
    "                    jittered_x = np.full(n_points, pos) + np.random.uniform(-jitter_width, jitter_width, size=n_points)\n",
    "                    \n",
    "                    plt.scatter(\n",
    "                        jittered_x, \n",
    "                        model_project_data['score'], \n",
    "                        color='black',\n",
    "                        alpha=0.5,\n",
    "                        s=20,\n",
    "                        zorder=3\n",
    "                    )\n",
    "        \n",
    "        # Create custom legend\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=color_map.get(model_type, 'gray'), label=model_type)\n",
    "            for model_type in model_types_list\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, title='Model Type')\n",
    "        \n",
    "        # Set x-ticks at the middle of each project group\n",
    "        plt.xticks(range(len(projects_ordered)), projects_ordered)\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f'Cross-Project Comparison: {metric_name}', fontsize=16)\n",
    "        plt.xlabel('Project', fontsize=14)\n",
    "        plt.ylabel('Score', fontsize=14)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.6)\n",
    "        plt.ylim(0, 1.05)\n",
    "        \n",
    "        # Add a reference line at y=0.5\n",
    "        plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=0.8)\n",
    "        \n",
    "        # Make tick marks and spines thinner\n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(axis='both', width=0.8, length=4)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(0.8)\n",
    "        \n",
    "        # Calculate statistics table\n",
    "        stats_data = []\n",
    "        for project in projects_ordered:\n",
    "            for model_type in model_types_list:\n",
    "                subset = df[(df['project'] == project) & (df['model_type'] == model_type)]\n",
    "                if not subset.empty:\n",
    "                    stats_data.append({\n",
    "                        'project': project,\n",
    "                        'model_type': model_type,\n",
    "                        'mean': subset['score'].mean(),\n",
    "                        'median': subset['score'].median(),\n",
    "                        'min': subset['score'].min(),\n",
    "                        'max': subset['score'].max(),\n",
    "                        'count': len(subset)\n",
    "                    })\n",
    "        \n",
    "        # Create statistics table\n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        \n",
    "        # Sort stats by project and model_type\n",
    "        stats_df = stats_df.sort_values(['project', 'model_type'])\n",
    "        \n",
    "        # Create stats table as a separate figure\n",
    "        fig_stats, ax_stats = plt.subplots(figsize=(12, len(stats_df) * 0.4 + 1))\n",
    "        ax_stats.axis('tight')\n",
    "        ax_stats.axis('off')\n",
    "        \n",
    "        table = ax_stats.table(\n",
    "            cellText=stats_df[['project', 'model_type', 'mean', 'median', 'min', 'max', 'count']].round(3).values,\n",
    "            colLabels=['Project', 'Model Type', 'Mean', 'Median', 'Min', 'Max', 'Count'],\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.2)\n",
    "        \n",
    "        # Set title for stats table\n",
    "        plt.suptitle(f'Statistics for {metric_name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figures\n",
    "        plt.savefig(f\"multi_project_{metric_name.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "        fig_stats.savefig(f\"stats_{metric_name.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close(fig_stats)\n",
    "        \n",
    "        print(f\"  Generated comparison for {metric_name}\")\n",
    "\n",
    "# Import required for patches\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Create multi-project comparison\n",
    "create_multi_project_comparison(raw_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [6] - LLM Improvement Analysis\n",
    "# Purpose: Comprehensive analysis of LLM performance improvements over baseline techniques\n",
    "# Dependencies: pandas, matplotlib, numpy\n",
    "# Breadcrumbs: Analysis -> LLM Performance Evaluation\n",
    "\n",
    "def analyze_llm_improvements(raw_model_data):\n",
    "    \"\"\"\n",
    "    Analyze and quantify how LLMs improve over Sentence Transformer/TF-IDF baseline approaches\n",
    "    \n",
    "    Parameters:\n",
    "        raw_model_data (dict): Nested dictionary containing organized model data from extract_raw_model_data()\n",
    "                              Structure: {project: {metric: {model_type: {model_name: [scores]}}}}\n",
    "                              \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing improvement analysis results with columns for:\n",
    "                     - project, metric, llm_type, st_score, llm_score\n",
    "                     - abs_improvement, rel_improvement, sample_sizes\n",
    "                     \n",
    "    Notes:\n",
    "        - Compares LLM performance against Sentence Transformer/TF-IDF baseline\n",
    "        - Calculates both absolute and relative improvements\n",
    "        - Focuses on key metrics: Accuracy, Balanced Accuracy, Precision, Recall, F1, F2, Matthews Correlation\n",
    "        - Groups analysis by metric, LLM type, and project\n",
    "        - Generates comprehensive visualizations showing improvement patterns\n",
    "        - Creates heatmaps for cross-project comparison\n",
    "        - Maintains metric order consistency with other analysis notebooks\n",
    "    \"\"\"\n",
    "    if not raw_model_data:\n",
    "        print(\"No data available for analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Set of key metrics in the same order as in Meta Judge Analysis Notebook\n",
    "    key_metrics = ['Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', \n",
    "                  'F1 Score', 'F2 Score', 'Matthews Correlation']\n",
    "    \n",
    "    # Collect all data for comparison\n",
    "    improvement_data = []\n",
    "    \n",
    "    # Get all model types from data\n",
    "    all_model_types = set()\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        for metric_name, model_types in metrics.items():\n",
    "            all_model_types.update(model_types.keys())\n",
    "    \n",
    "    # Separate LLM model types from Sentence Transformer/TF-IDF\n",
    "    llm_model_types = [model for model in all_model_types if model != 'Sentence Transformer/TF-IDF']\n",
    "    \n",
    "    # Check if we have both model types for comparison\n",
    "    if 'Sentence Transformer/TF-IDF' not in all_model_types or not llm_model_types:\n",
    "        print(\"Both Sentence Transformer/TF-IDF and LLM models are required for comparison.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nANALYZING LLM IMPROVEMENTS OVER SENTENCE TRANSFORMER/TF-IDF\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Process data for each project and metric\n",
    "    for project, metrics in raw_model_data.items():\n",
    "        for metric_name, model_types in metrics.items():\n",
    "            if metric_name not in key_metrics:\n",
    "                continue\n",
    "                \n",
    "            # Check if we have both model types for this metric\n",
    "            if 'Sentence Transformer/TF-IDF' not in model_types:\n",
    "                continue\n",
    "                \n",
    "            # Get Sentence Transformer/TF-IDF scores\n",
    "            st_scores = []\n",
    "            for model_name, scores in model_types['Sentence Transformer/TF-IDF'].items():\n",
    "                st_scores.extend(scores)\n",
    "            \n",
    "            if not st_scores:\n",
    "                continue\n",
    "                \n",
    "            # Calculate average Sentence Transformer score\n",
    "            avg_st_score = sum(st_scores) / len(st_scores)\n",
    "            \n",
    "            # Compare with each LLM model type\n",
    "            for model_type in llm_model_types:\n",
    "                if model_type not in model_types:\n",
    "                    continue\n",
    "                    \n",
    "                llm_scores = []\n",
    "                for model_name, scores in model_types[model_type].items():\n",
    "                    llm_scores.extend(scores)\n",
    "                \n",
    "                if not llm_scores:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate average LLM score\n",
    "                avg_llm_score = sum(llm_scores) / len(llm_scores)\n",
    "                \n",
    "                # Calculate improvement\n",
    "                abs_improvement = avg_llm_score - avg_st_score\n",
    "                rel_improvement = (abs_improvement / avg_st_score) * 100 if avg_st_score > 0 else float('inf')\n",
    "                \n",
    "                # Store the comparison\n",
    "                improvement_data.append({\n",
    "                    'project': project,\n",
    "                    'metric': metric_name,\n",
    "                    'llm_type': model_type,\n",
    "                    'st_score': avg_st_score,\n",
    "                    'llm_score': avg_llm_score,\n",
    "                    'abs_improvement': abs_improvement,\n",
    "                    'rel_improvement': rel_improvement,\n",
    "                    'st_sample_size': len(st_scores),\n",
    "                    'llm_sample_size': len(llm_scores)\n",
    "                })\n",
    "    \n",
    "    if not improvement_data:\n",
    "        print(\"No comparable data between Sentence Transformer/TF-IDF and LLM models.\")\n",
    "        return\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    \n",
    "    # Sort by absolute improvement (descending)\n",
    "    improvement_df = improvement_df.sort_values('abs_improvement', ascending=False)\n",
    "    \n",
    "    # Display overall summary\n",
    "    print(\"\\n1. OVERALL IMPROVEMENT SUMMARY\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    avg_overall_improvement = improvement_df['abs_improvement'].mean()\n",
    "    avg_rel_improvement = improvement_df['rel_improvement'].mean()\n",
    "    \n",
    "    print(f\"Average absolute improvement across all metrics: {avg_overall_improvement:.3f}\")\n",
    "    print(f\"Average relative improvement: {avg_rel_improvement:.1f}%\")\n",
    "    \n",
    "    # Count cases where LLMs outperform ST/TF-IDF\n",
    "    better_count = (improvement_df['abs_improvement'] > 0).sum()\n",
    "    worse_count = (improvement_df['abs_improvement'] <= 0).sum()\n",
    "    \n",
    "    print(f\"LLMs outperform ST/TF-IDF in {better_count} of {len(improvement_df)} cases ({better_count/len(improvement_df)*100:.1f}%)\")\n",
    "    print(f\"LLMs underperform ST/TF-IDF in {worse_count} of {len(improvement_df)} cases ({worse_count/len(improvement_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze by metric, maintaining the same order as Meta Judge notebook\n",
    "    print(\"\\n2. IMPROVEMENT BY METRIC\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Create a categorical type with our preferred order\n",
    "    metric_cat_type = pd.CategoricalDtype(categories=key_metrics, ordered=True)\n",
    "    improvement_df['metric_ordered'] = improvement_df['metric'].astype(metric_cat_type)\n",
    "    \n",
    "    # Group by the ordered metric and calculate means\n",
    "    metric_improvement = improvement_df.groupby('metric_ordered')['abs_improvement'].agg(['mean', 'count'])\n",
    "    \n",
    "    for metric, row in metric_improvement.iterrows():\n",
    "        metric_data = improvement_df[improvement_df['metric'] == metric]\n",
    "        better_rate = (metric_data['abs_improvement'] > 0).mean() * 100\n",
    "        print(f\"  {metric}: {row['mean']:.3f} avg improvement ({better_rate:.1f}% of cases show improvement)\")\n",
    "    \n",
    "    # Analyze by LLM type\n",
    "    print(\"\\n3. IMPROVEMENT BY LARGE LANGUAGE MODEL NAME\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    llm_improvement = improvement_df.groupby('llm_type')['abs_improvement'].agg(['mean', 'count'])\n",
    "    llm_improvement = llm_improvement.sort_values('mean', ascending=False)\n",
    "    \n",
    "    for llm_type, row in llm_improvement.iterrows():\n",
    "        llm_data = improvement_df[improvement_df['llm_type'] == llm_type]\n",
    "        better_rate = (llm_data['abs_improvement'] > 0).mean() * 100\n",
    "        print(f\"  {llm_type}: {row['mean']:.3f} avg improvement ({better_rate:.1f}% of cases show improvement)\")\n",
    "    \n",
    "    # Analyze by project\n",
    "    print(\"\\n4. IMPROVEMENT BY PROJECT\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    project_improvement = improvement_df.groupby('project')['abs_improvement'].agg(['mean', 'count'])\n",
    "    project_improvement = project_improvement.sort_values('mean', ascending=False)\n",
    "    \n",
    "    for project, row in project_improvement.iterrows():\n",
    "        project_data = improvement_df[improvement_df['project'] == project]\n",
    "        better_rate = (project_data['abs_improvement'] > 0).mean() * 100\n",
    "        print(f\"  {project}: {row['mean']:.3f} avg improvement ({better_rate:.1f}% of cases show improvement)\")\n",
    "    \n",
    "    # Create visualization of improvements for all projects combined\n",
    "    if len(improvement_df['project'].unique()) > 1:\n",
    "        # Create combined visualization\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        \n",
    "        # Create a pivot table for ordered metrics\n",
    "        pivot_data = improvement_df.pivot_table(\n",
    "            index='metric_ordered', \n",
    "            columns='llm_type', \n",
    "            values='abs_improvement', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Plot the data\n",
    "        pivot_data.plot(\n",
    "            kind='bar',\n",
    "            ax=ax,\n",
    "            width=0.8,\n",
    "            alpha=0.7,\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.6)\n",
    "        \n",
    "        # Set y-axis limit to 0.50\n",
    "        ax.set_ylim(top=0.50)\n",
    "        \n",
    "        # Add title with extra padding to make room for legend\n",
    "        ax.set_title('Combined Projects: LLM Improvement over Sentence Transformer/TF-IDF by Metric', \n",
    "                     fontsize=16, pad=40)  # Add extra padding below title\n",
    "        \n",
    "        # Split y-axis label into two lines\n",
    "        ax.set_ylabel('Absolute Improvement\\n(LLM - ST/TF-IDF)', fontsize=14)\n",
    "        ax.set_xlabel('Metric', fontsize=14)\n",
    "        \n",
    "        # Add a zero line for reference\n",
    "        ax.axhline(y=0, color='red', linestyle='-', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Make tick marks thinner\n",
    "        ax.tick_params(axis='both', width=0.8, length=4)\n",
    "        \n",
    "        # Make all spines thinner\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(0.8)\n",
    "        \n",
    "        # Position the legend below the title but above the chart\n",
    "        legend = ax.legend(\n",
    "            title='Large Language Model Name',\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, 1.01),  # Position just below the title\n",
    "            ncol=3,\n",
    "            fontsize=12,\n",
    "            frameon=True  # Add a frame around the legend\n",
    "        )\n",
    "        \n",
    "        # Make sure legend title is visible\n",
    "        legend.get_title().set_fontsize(12)\n",
    "        \n",
    "        # Increase top margin to ensure nothing gets cut off\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(\"combined_llm_improvement_by_metric.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create project-specific visualizations with ordered metrics\n",
    "    for project_name in improvement_df['project'].unique():\n",
    "        project_df = improvement_df[improvement_df['project'] == project_name]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        \n",
    "        # Create a grouped bar chart of improvements by metric and LLM type\n",
    "        pivot_data = project_df.pivot_table(\n",
    "            index='metric_ordered', \n",
    "            columns='llm_type', \n",
    "            values='abs_improvement', \n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Plot the data\n",
    "        pivot_data.plot(\n",
    "            kind='bar',\n",
    "            ax=ax,\n",
    "            width=0.8,\n",
    "            alpha=0.7,\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.6)\n",
    "        \n",
    "        # Set y-axis limit to 0.50\n",
    "        ax.set_ylim(top=0.50)\n",
    "        \n",
    "        # Add title with extra padding to make room for legend\n",
    "        ax.set_title(f'Project: {project_name} - LLM Improvement over Sentence Transformer/TF-IDF by Metric', \n",
    "                     fontsize=16, pad=40)  # Add extra padding below title\n",
    "        \n",
    "        # Split y-axis label into two lines\n",
    "        ax.set_ylabel('Absolute Improvement\\n(LLM - ST/TF-IDF)', fontsize=14)\n",
    "        ax.set_xlabel('Metric', fontsize=14)\n",
    "        \n",
    "        # Add a zero line for reference\n",
    "        ax.axhline(y=0, color='red', linestyle='-', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Make tick marks thinner\n",
    "        ax.tick_params(axis='both', width=0.8, length=4)\n",
    "        \n",
    "        # Make all spines thinner\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(0.8)\n",
    "        \n",
    "        # Position the legend below the title but above the chart\n",
    "        legend = ax.legend(\n",
    "            title='Large Language Model Name',\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, 1.01),  # Position just below the title\n",
    "            ncol=3,\n",
    "            fontsize=12,\n",
    "            frameon=True  # Add a frame around the legend\n",
    "        )\n",
    "        \n",
    "        # Make sure legend title is visible\n",
    "        legend.get_title().set_fontsize(12)\n",
    "        \n",
    "        # Increase top margin to ensure nothing gets cut off\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        \n",
    "        # Save the figure with project name in filename\n",
    "        plt.savefig(f\"{project_name}_llm_improvement_by_metric.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a heatmap of improvements by project and metric with ordered metrics\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Pivot the data to create a project × metric table of improvements\n",
    "    # Using the ordered metrics\n",
    "    heatmap_data = improvement_df.pivot_table(\n",
    "        index='project', \n",
    "        columns='metric_ordered', \n",
    "        values='abs_improvement', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap='RdYlGn',  # Red for negative, green for positive\n",
    "        center=0,       # Center the colormap at 0\n",
    "        annot=True,     # Show the values\n",
    "        fmt='.3f',      # Format as 3 decimal places\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Improvement (LLM - ST/TF-IDF)'}\n",
    "    )\n",
    "    \n",
    "    # Add title\n",
    "    plt.title('LLM Improvement over Sentence Transformer/TF-IDF by Project and Metric', fontsize=16)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(\"improvement_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return improvement_df\n",
    "\n",
    "# Analyze LLM improvements over Sentence Transformer/TF-IDF\n",
    "improvement_results = analyze_llm_improvements(raw_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [7] - Retrieve Confusion Matrix Data for Model Comparison\n",
    "# Purpose: Query Neo4j database for comprehensive confusion matrix data across model types\n",
    "# Dependencies: neo4j, pandas, json, logging\n",
    "# Breadcrumbs: Data Acquisition -> Confusion Matrix Retrieval\n",
    "\n",
    "def retrieve_confusion_matrix_data(driver):\n",
    "    \"\"\"\n",
    "    Retrieve comprehensive confusion matrix data (TP, FP, FN, TN) for sentence transformer and LLM models\n",
    "    \n",
    "    Parameters:\n",
    "        driver: Neo4j driver connection for database access\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with confusion matrix metrics across all models containing:\n",
    "                     - project_name, model_type, model_name, metric (TP/FP/FN/TN)\n",
    "                     - value, threshold, created_at\n",
    "                     Returns empty DataFrame if no data found or on error.\n",
    "                     \n",
    "    Notes:\n",
    "        - Retrieves data from MetricsAnalysis nodes with results field\n",
    "        - Parses JSON results to extract confusion matrix components\n",
    "        - Creates long-format DataFrame for easy analysis and visualization\n",
    "        - Handles multiple model types and projects in single query\n",
    "        - Includes threshold information for reproducibility\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query to retrieve metrics analysis data for both model types\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Project)-[r:HAS_METRICS_ANALYSIS]->(m:MetricsAnalysis)\n",
    "        WHERE m.results IS NOT NULL\n",
    "        RETURN \n",
    "            p.name as project_name,\n",
    "            m.model_type as model_type,\n",
    "            m.results as results_json,\n",
    "            m.created_at as created_at\n",
    "        ORDER BY p.name, m.model_type\n",
    "        \"\"\"\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            results = session.run(query).data()\n",
    "            \n",
    "            if not results:\n",
    "                logger.warning(\"No confusion matrix data found in Neo4j\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            print(f\"Retrieved metrics data for {len(results)} analysis records\")\n",
    "            \n",
    "            # Process results to extract confusion matrix data\n",
    "            combined_data = []\n",
    "            \n",
    "            for record in results:\n",
    "                project_name = record['project_name']\n",
    "                model_type = record['model_type']\n",
    "                created_at = record['created_at']\n",
    "                \n",
    "                # Parse the results JSON\n",
    "                if record['results_json'] and isinstance(record['results_json'], str):\n",
    "                    try:\n",
    "                        results_data = json.loads(record['results_json'])\n",
    "                        \n",
    "                        # Extract confusion matrix data for each model\n",
    "                        for model_name, metrics in results_data.items():\n",
    "                            # Create a row for each confusion matrix component\n",
    "                            for metric in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']:\n",
    "                                if metric in metrics:\n",
    "                                    combined_data.append({\n",
    "                                        'project_name': project_name,\n",
    "                                        'model_type': model_type,\n",
    "                                        'model_name': model_name,\n",
    "                                        'metric': metric,\n",
    "                                        'value': metrics[metric],\n",
    "                                        'threshold': metrics.get('threshold', None),\n",
    "                                        'created_at': created_at\n",
    "                                    })\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON for {project_name}, {model_type}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            confusion_df = pd.DataFrame(combined_data)\n",
    "            \n",
    "            if confusion_df.empty:\n",
    "                print(\"No confusion matrix data could be extracted\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Display summary of retrieved data\n",
    "            project_count = confusion_df['project_name'].nunique()\n",
    "            model_type_count = confusion_df['model_type'].nunique()\n",
    "            model_count = confusion_df['model_name'].nunique()\n",
    "            \n",
    "            print(f\"Extracted confusion matrix data for {project_count} projects, {model_type_count} model types, and {model_count} models\")\n",
    "            \n",
    "            # Count metrics by type\n",
    "            metric_counts = confusion_df['metric'].value_counts()\n",
    "            print(\"\\nMetric counts:\")\n",
    "            for metric, count in metric_counts.items():\n",
    "                print(f\"  {metric}: {count}\")\n",
    "            \n",
    "            # Count by model type\n",
    "            model_type_counts = confusion_df['model_type'].value_counts()\n",
    "            print(\"\\nModel type counts:\")\n",
    "            for model_type, count in model_type_counts.items():\n",
    "                print(f\"  {model_type}: {count}\")\n",
    "            \n",
    "            return confusion_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving confusion matrix data: {str(e)}\")\n",
    "        logger.error(\"Exception details:\", exc_info=True)\n",
    "        print(f\"Error retrieving confusion matrix data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Retrieve confusion matrix data from Neo4j\n",
    "confusion_matrix_df = retrieve_confusion_matrix_data(driver)\n",
    "\n",
    "# Display sample of the data\n",
    "if not confusion_matrix_df.empty:\n",
    "    display(confusion_matrix_df.head(10))\n",
    "    \n",
    "    # Show distribution of metrics by project and model type\n",
    "    print(\"\\nProjects and model types in the dataset:\")\n",
    "    for project in confusion_matrix_df['project_name'].unique():\n",
    "        project_data = confusion_matrix_df[confusion_matrix_df['project_name'] == project]\n",
    "        print(f\"\\n  Project: {project}\")\n",
    "        for model_type in project_data['model_type'].unique():\n",
    "            model_type_data = project_data[project_data['model_type'] == model_type]\n",
    "            models_count = model_type_data['model_name'].nunique()\n",
    "            print(f\"    {model_type}: {models_count} models\")\n",
    "else:\n",
    "    print(\"No confusion matrix data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [8] - Create Combined Whisker Charts for Confusion Matrix Metrics\n",
    "# Purpose: Generate comprehensive whisker chart visualizations for confusion matrix analysis\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy\n",
    "# Breadcrumbs: Visualization -> Confusion Matrix Analysis\n",
    "\n",
    "def create_confusion_matrix_whisker_charts(confusion_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive whisker charts comparing confusion matrix metrics across model types\n",
    "    \n",
    "    Parameters:\n",
    "        confusion_df (pd.DataFrame): DataFrame containing confusion matrix metrics with columns:\n",
    "                                    - project_name, model_type, model_name, metric, value\n",
    "                                    \n",
    "    Returns:\n",
    "        None: Displays visualizations directly using matplotlib/seaborn\n",
    "        \n",
    "    Notes:\n",
    "        - Creates separate subplot grids for each project (2x2 layout for TP, FP, FN, TN)\n",
    "        - Uses human-readable labels for confusion matrix components\n",
    "        - Includes individual data points with jitter for detailed analysis\n",
    "        - Generates combined view with all metrics side-by-side for comparison\n",
    "        - Handles large datasets with appropriate scaling and positioning\n",
    "        - Prints statistical summaries for each model type and metric\n",
    "    \"\"\"\n",
    "    if confusion_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Define human-readable names for metrics\n",
    "    metric_labels = {\n",
    "        'true_positives': 'True Positives (TP)',\n",
    "        'false_positives': 'False Positives (FP)', \n",
    "        'false_negatives': 'False Negatives (FN)',\n",
    "        'true_negatives': 'True Negatives (TN)'\n",
    "    }\n",
    "    \n",
    "    # Get list of all projects\n",
    "    projects = confusion_df['project_name'].unique()\n",
    "    \n",
    "    # Process each project separately\n",
    "    for project in projects:\n",
    "        project_data = confusion_df[confusion_df['project_name'] == project]\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if project_data.empty:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing visualization for project: {project}\")\n",
    "        \n",
    "        # Get model types for this project\n",
    "        model_types = project_data['model_type'].unique()\n",
    "        print(f\"  Model types: {', '.join(model_types)}\")\n",
    "        \n",
    "        # Create figure for this project - one subplot for each metric\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Process each confusion matrix metric\n",
    "        for i, metric in enumerate(['true_positives', 'false_positives', 'false_negatives', 'true_negatives']):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Filter data for this metric\n",
    "            metric_data = project_data[project_data['metric'] == metric]\n",
    "            \n",
    "            if metric_data.empty:\n",
    "                ax.text(0.5, 0.5, f\"No data for {metric_labels[metric]}\", \n",
    "                       horizontalalignment='center', fontsize=14)\n",
    "                print(f\"  No data available for {metric_labels[metric]}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate positions for boxplots\n",
    "            model_count = len(model_types)\n",
    "            positions = np.arange(model_count)\n",
    "            \n",
    "            # Create a boxplot for each model type\n",
    "            boxplot_data = []\n",
    "            boxplot_positions = []\n",
    "            model_type_names = []\n",
    "            \n",
    "            # Print statistics for this metric\n",
    "            print(f\"\\n  Statistics for {metric_labels[metric]}:\")\n",
    "            \n",
    "            for pos, model_type in enumerate(model_types):\n",
    "                # Filter data for this model type\n",
    "                model_type_data = metric_data[metric_data['model_type'] == model_type]\n",
    "                \n",
    "                if not model_type_data.empty:\n",
    "                    boxplot_data.append(model_type_data['value'])\n",
    "                    boxplot_positions.append(pos)\n",
    "                    \n",
    "                    # Format model type name for display\n",
    "                    if model_type == 'sentence_transformer_tf_idf':\n",
    "                        display_name = 'Sentence Transformer/TF-IDF'\n",
    "                    else:\n",
    "                        display_name = model_type.upper() if model_type == 'llm' else model_type\n",
    "                    \n",
    "                    model_type_names.append(display_name)\n",
    "                    \n",
    "                    # Calculate and print statistics\n",
    "                    mean_val = model_type_data['value'].mean()\n",
    "                    median_val = model_type_data['value'].median()\n",
    "                    min_val = model_type_data['value'].min()\n",
    "                    max_val = model_type_data['value'].max()\n",
    "                    count = len(model_type_data)\n",
    "                    print(f\"    {display_name}: Mean={mean_val:.1f}, Median={median_val:.1f}, Min={min_val}, Max={max_val}, Count={count}\")\n",
    "            \n",
    "            # Create boxplot with uniform color\n",
    "            bplot = ax.boxplot(\n",
    "                boxplot_data, \n",
    "                positions=boxplot_positions, \n",
    "                patch_artist=True, \n",
    "                widths=0.6,\n",
    "                showfliers=False\n",
    "            )\n",
    "            \n",
    "            # Set uniform color for all boxes\n",
    "            for patch in bplot['boxes']:\n",
    "                patch.set_facecolor('lightgray')\n",
    "            \n",
    "            # Add individual data points with jitter\n",
    "            for j, (model_type, pos) in enumerate(zip(model_types, positions)):\n",
    "                model_type_data = metric_data[metric_data['model_type'] == model_type]\n",
    "                \n",
    "                if not model_type_data.empty:\n",
    "                    # Create jittered x positions\n",
    "                    x = np.random.normal(pos, 0.1, size=len(model_type_data))\n",
    "                    \n",
    "                    # Plot points\n",
    "                    ax.scatter(\n",
    "                        x, \n",
    "                        model_type_data['value'], \n",
    "                        color='black', \n",
    "                        alpha=0.5, \n",
    "                        s=30,\n",
    "                        zorder=10\n",
    "                    )\n",
    "            \n",
    "            # Set axis labels and title\n",
    "            ax.set_title(metric_labels[metric], fontsize=16)\n",
    "            ax.set_ylabel('Count', fontsize=14)\n",
    "            ax.set_xticks(positions)\n",
    "            ax.set_xticklabels(model_type_names, rotation=45, ha='right', fontsize=12)\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "        # Add overall title\n",
    "        plt.suptitle(f'Confusion Matrix Metrics Comparison for Project: {project}', fontsize=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92)  # Make room for title\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a single boxplot with all metrics side by side (alternative view)\n",
    "        print(f\"\\nCreating combined view for project: {project}\")\n",
    "        \n",
    "        # Reshape data for combined visualization\n",
    "        plot_data = []\n",
    "        for metric in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']:\n",
    "            metric_data = project_data[project_data['metric'] == metric]\n",
    "            \n",
    "            for _, row in metric_data.iterrows():\n",
    "                model_name = 'Sentence Transformer/TF-IDF' if row['model_type'] == 'sentence_transformer_tf_idf' else row['model_type'].upper() if row['model_type'] == 'llm' else row['model_type']\n",
    "                plot_data.append({\n",
    "                    'metric': metric_labels[metric],\n",
    "                    'model_type': model_name,\n",
    "                    'value': row['value'],\n",
    "                    'model_name': row['model_name']\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        if not plot_df.empty:\n",
    "            print(\"  Model types in combined view:\")\n",
    "            for model_type in sorted(plot_df['model_type'].unique()):\n",
    "                print(f\"    - {model_type}\")\n",
    "                \n",
    "            # Create a figure for manual plotting (avoiding seaborn's hue parameter)\n",
    "            fig, ax = plt.subplots(figsize=(16, 10))\n",
    "            \n",
    "            # Get unique metrics and model types\n",
    "            unique_metrics = [metric_labels[m] for m in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']]\n",
    "            unique_model_types = sorted(plot_df['model_type'].unique())\n",
    "            \n",
    "            # Set up positions\n",
    "            n_metrics = len(unique_metrics)\n",
    "            n_models = len(unique_model_types)\n",
    "            \n",
    "            # Calculate bar width and positions\n",
    "            metric_width = 0.8\n",
    "            group_width = metric_width / n_models\n",
    "            \n",
    "            # For each metric, create grouped boxplots\n",
    "            positions = []\n",
    "            for i, metric in enumerate(unique_metrics):\n",
    "                metric_data = plot_df[plot_df['metric'] == metric]\n",
    "                \n",
    "                # Base position for this metric\n",
    "                base_pos = i\n",
    "                \n",
    "                for j, model_type in enumerate(unique_model_types):\n",
    "                    model_data = metric_data[metric_data['model_type'] == model_type]\n",
    "                    \n",
    "                    if not model_data.empty:\n",
    "                        # Calculate position for this model within the metric group\n",
    "                        pos = base_pos - metric_width/2 + (j + 0.5) * group_width\n",
    "                        positions.append(pos)\n",
    "                        \n",
    "                        # Create boxplot\n",
    "                        bp = ax.boxplot(\n",
    "                            model_data['value'], \n",
    "                            positions=[pos],\n",
    "                            widths=group_width * 0.8,\n",
    "                            patch_artist=True,\n",
    "                            showfliers=False\n",
    "                        )\n",
    "                        \n",
    "                        # Set color to gray\n",
    "                        for element in ['boxes', 'whiskers', 'caps', 'medians']:\n",
    "                            plt.setp(bp[element], color='black')\n",
    "                        for box in bp['boxes']:\n",
    "                            box.set(facecolor='lightgray')\n",
    "                            \n",
    "                        # Add individual points with jitter\n",
    "                        x = np.random.normal(pos, group_width * 0.1, size=len(model_data))\n",
    "                        ax.scatter(x, model_data['value'], color='black', alpha=0.5, s=20)\n",
    "            \n",
    "            # Set x-ticks at the middle of each metric group\n",
    "            ax.set_xticks(range(len(unique_metrics)))\n",
    "            ax.set_xticklabels(unique_metrics, fontsize=12)\n",
    "            \n",
    "            plt.title(f'Combined Confusion Matrix Metrics for Project: {project}', fontsize=16)\n",
    "            plt.ylabel('Count', fontsize=14)\n",
    "            plt.xlabel('Metric', fontsize=14)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Create whisker charts for confusion matrix data\n",
    "create_confusion_matrix_whisker_charts(confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [9] - Analysis of Confusion Matrix Metrics and F2 Performance\n",
    "# Purpose: Comprehensive analysis of model improvements focusing on F2 optimization and tradeoffs\n",
    "# Dependencies: pandas, numpy, logging\n",
    "# Breadcrumbs: Analysis -> Confusion Matrix Performance Evaluation\n",
    "\n",
    "def analyze_confusion_matrix_improvements(confusion_df):\n",
    "    \"\"\"\n",
    "    Analyze improvements in confusion matrix metrics between model types with F2 score focus\n",
    "    \n",
    "    Parameters:\n",
    "        confusion_df (pd.DataFrame): DataFrame containing confusion matrix metrics with columns:\n",
    "                                    - project_name, model_type, model_name, metric, value\n",
    "                                    \n",
    "    Returns:\n",
    "        None: Prints comprehensive analysis directly to console\n",
    "        \n",
    "    Notes:\n",
    "        - Focuses on F2 score optimization as primary objective\n",
    "        - Calculates precision, recall, F1, F2, and accuracy from confusion matrix\n",
    "        - Analyzes model performance by project with baseline comparisons\n",
    "        - Identifies best performing models for each metric\n",
    "        - Evaluates TP/FP tradeoffs critical for F2 optimization\n",
    "        - Provides actionable recommendations based on performance analysis\n",
    "        - Suggests areas for further model improvement\n",
    "    \"\"\"\n",
    "    if confusion_df.empty:\n",
    "        print(\"No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS OF MODEL IMPROVEMENTS BASED ON CONFUSION MATRIX METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Focus: F2 score optimization, maximizing TP and TN, minimizing FN, allowing for reasonable FP\")\n",
    "    \n",
    "    # Get list of all projects\n",
    "    projects = confusion_df['project_name'].unique()\n",
    "    \n",
    "    # Process each project separately\n",
    "    for project in projects:\n",
    "        print(f\"\\n\\nPROJECT: {project}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        project_data = confusion_df[confusion_df['project_name'] == project]\n",
    "        if project_data.empty:\n",
    "            print(\"  No data available for this project\")\n",
    "            continue\n",
    "        \n",
    "        # Get unique model types for this project\n",
    "        model_types = project_data['model_type'].unique()\n",
    "        \n",
    "        # First, convert the long-format data to wide format for easier analysis\n",
    "        # We need to pivot the data to have one row per model and columns for TP, FP, FN, TN\n",
    "        \n",
    "        # Create a dictionary to store metrics for each model\n",
    "        model_metrics = {}\n",
    "        \n",
    "        # Process each model type\n",
    "        for model_type in model_types:\n",
    "            model_type_data = project_data[project_data['model_type'] == model_type]\n",
    "            \n",
    "            # Get all models for this type\n",
    "            models = model_type_data['model_name'].unique()\n",
    "            \n",
    "            for model in models:\n",
    "                model_data = model_type_data[model_type_data['model_name'] == model]\n",
    "                \n",
    "                # Initialize metrics dictionary for this model\n",
    "                if model not in model_metrics:\n",
    "                    model_metrics[model] = {\n",
    "                        'model_type': model_type,\n",
    "                        'display_name': 'Sentence Transformer/TF-IDF' if model_type == 'sentence_transformer_tf_idf' else model_type.upper() if model_type == 'llm' else model_type\n",
    "                    }\n",
    "                \n",
    "                # Extract metrics for this model\n",
    "                for _, row in model_data.iterrows():\n",
    "                    metric = row['metric']\n",
    "                    value = row['value']\n",
    "                    model_metrics[model][metric] = value\n",
    "        \n",
    "        # Convert dictionary to DataFrame\n",
    "        metrics_df = pd.DataFrame.from_dict(model_metrics, orient='index')\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        for metric in ['true_positives', 'false_positives', 'false_negatives', 'true_negatives']:\n",
    "            if metric not in metrics_df.columns:\n",
    "                metrics_df[metric] = 0\n",
    "            else:\n",
    "                metrics_df[metric] = metrics_df[metric].fillna(0)\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        metrics_df['total_predictions'] = metrics_df['true_positives'] + metrics_df['false_positives'] + \\\n",
    "                                         metrics_df['false_negatives'] + metrics_df['true_negatives']\n",
    "        \n",
    "        # Calculate precision, recall, and F2 score\n",
    "        metrics_df['precision'] = metrics_df['true_positives'] / (metrics_df['true_positives'] + metrics_df['false_positives']).replace(0, np.nan)\n",
    "        metrics_df['recall'] = metrics_df['true_positives'] / (metrics_df['true_positives'] + metrics_df['false_negatives']).replace(0, np.nan)\n",
    "        \n",
    "        # F2 score formula: 5*precision*recall / (4*precision + recall)\n",
    "        metrics_df['f2_score'] = (5 * metrics_df['precision'] * metrics_df['recall']) / \\\n",
    "                                 (4 * metrics_df['precision'] + metrics_df['recall']).replace(0, np.nan)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        metrics_df['accuracy'] = (metrics_df['true_positives'] + metrics_df['true_negatives']) / \\\n",
    "                                 metrics_df['total_predictions'].replace(0, np.nan)\n",
    "        \n",
    "        # Group by model type and calculate average metrics\n",
    "        grouped_metrics = metrics_df.groupby('model_type').agg({\n",
    "            'true_positives': 'mean',\n",
    "            'false_positives': 'mean',\n",
    "            'false_negatives': 'mean',\n",
    "            'true_negatives': 'mean',\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'f2_score': 'mean',\n",
    "            'accuracy': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add display names\n",
    "        grouped_metrics['display_name'] = grouped_metrics['model_type'].apply(\n",
    "            lambda x: 'Sentence Transformer/TF-IDF' if x == 'sentence_transformer_tf_idf' \n",
    "                     else x.upper() if x == 'llm' else x\n",
    "        )\n",
    "        \n",
    "        # Print summary by model type\n",
    "        print(\"\\nAVERAGE METRICS BY MODEL TYPE:\")\n",
    "        for _, row in grouped_metrics.iterrows():\n",
    "            print(f\"\\n  {row['display_name']}:\")\n",
    "            print(f\"    TP: {row['true_positives']:.1f}\")\n",
    "            print(f\"    FP: {row['false_positives']:.1f}\")\n",
    "            print(f\"    FN: {row['false_negatives']:.1f}\")\n",
    "            print(f\"    TN: {row['true_negatives']:.1f}\")\n",
    "            print(f\"    Precision: {row['precision']:.3f}\")\n",
    "            print(f\"    Recall: {row['recall']:.3f}\")\n",
    "            print(f\"    F2 Score: {row['f2_score']:.3f}\")\n",
    "            print(f\"    Accuracy: {row['accuracy']:.3f}\")\n",
    "        \n",
    "        # Find reference model type (sentence transformer)\n",
    "        st_metrics = grouped_metrics[grouped_metrics['model_type'] == 'sentence_transformer_tf_idf']\n",
    "        \n",
    "        if not st_metrics.empty:\n",
    "            st_row = st_metrics.iloc[0]\n",
    "            \n",
    "            # Calculate improvements relative to sentence transformer\n",
    "            print(\"\\nIMPROVEMENTS RELATIVE TO SENTENCE TRANSFORMER/TF-IDF:\")\n",
    "            \n",
    "            for _, row in grouped_metrics[grouped_metrics['model_type'] != 'sentence_transformer_tf_idf'].iterrows():\n",
    "                print(f\"\\n  {row['display_name']} vs. Sentence Transformer/TF-IDF:\")\n",
    "                \n",
    "                # Calculate percentage improvements\n",
    "                tp_change = (row['true_positives'] - st_row['true_positives']) / st_row['true_positives'] * 100 if st_row['true_positives'] > 0 else float('inf')\n",
    "                fp_change = (row['false_positives'] - st_row['false_positives']) / st_row['false_positives'] * 100 if st_row['false_positives'] > 0 else float('inf')\n",
    "                fn_change = (row['false_negatives'] - st_row['false_negatives']) / st_row['false_negatives'] * 100 if st_row['false_negatives'] > 0 else float('inf')\n",
    "                tn_change = (row['true_negatives'] - st_row['true_negatives']) / st_row['true_negatives'] * 100 if st_row['true_negatives'] > 0 else float('inf')\n",
    "                \n",
    "                prec_change = (row['precision'] - st_row['precision']) / st_row['precision'] * 100 if st_row['precision'] > 0 else float('inf')\n",
    "                recall_change = (row['recall'] - st_row['recall']) / st_row['recall'] * 100 if st_row['recall'] > 0 else float('inf')\n",
    "                f2_change = (row['f2_score'] - st_row['f2_score']) / st_row['f2_score'] * 100 if st_row['f2_score'] > 0 else float('inf')\n",
    "                acc_change = (row['accuracy'] - st_row['accuracy']) / st_row['accuracy'] * 100 if st_row['accuracy'] > 0 else float('inf')\n",
    "                \n",
    "                # Print changes\n",
    "                print(f\"    TP: {row['true_positives']:.1f} ({tp_change:+.1f}%)\")\n",
    "                print(f\"    FP: {row['false_positives']:.1f} ({fp_change:+.1f}%)\")\n",
    "                print(f\"    FN: {row['false_negatives']:.1f} ({fn_change:+.1f}%)\")\n",
    "                print(f\"    TN: {row['true_negatives']:.1f} ({tn_change:+.1f}%)\")\n",
    "                print(f\"    Precision: {row['precision']:.3f} ({prec_change:+.1f}%)\")\n",
    "                print(f\"    Recall: {row['recall']:.3f} ({recall_change:+.1f}%)\")\n",
    "                print(f\"    F2 Score: {row['f2_score']:.3f} ({f2_change:+.1f}%)\")\n",
    "                print(f\"    Accuracy: {row['accuracy']:.3f} ({acc_change:+.1f}%)\")\n",
    "            \n",
    "            # Find best model for F2 score\n",
    "            best_f2_idx = metrics_df['f2_score'].idxmax()\n",
    "            if pd.notna(best_f2_idx):\n",
    "                best_model = metrics_df.loc[best_f2_idx]\n",
    "                \n",
    "                print(\"\\nBEST MODEL FOR F2 SCORE:\")\n",
    "                print(f\"  Model: {best_f2_idx}\")\n",
    "                print(f\"  Type: {best_model['display_name']}\")\n",
    "                print(f\"  F2 Score: {best_model['f2_score']:.3f}\")\n",
    "                print(f\"  TP: {best_model['true_positives']:.1f}\")\n",
    "                print(f\"  FP: {best_model['false_positives']:.1f}\")\n",
    "                print(f\"  FN: {best_model['false_negatives']:.1f}\")\n",
    "                print(f\"  TN: {best_model['true_negatives']:.1f}\")\n",
    "            \n",
    "            # Analysis of tradeoffs\n",
    "            print(\"\\nTRADEOFF ANALYSIS:\")\n",
    "            \n",
    "            # 1. Find model with highest TP\n",
    "            best_tp_idx = metrics_df['true_positives'].idxmax()\n",
    "            if pd.notna(best_tp_idx):\n",
    "                model_tp = metrics_df.loc[best_tp_idx]\n",
    "                print(f\"  Best TP: {model_tp['display_name']} ({best_tp_idx}) with {model_tp['true_positives']:.1f} true positives\")\n",
    "            \n",
    "            # 2. Find model with lowest FN (critical for F2)\n",
    "            best_fn_idx = metrics_df['false_negatives'].idxmin()\n",
    "            if pd.notna(best_fn_idx):\n",
    "                model_fn = metrics_df.loc[best_fn_idx]\n",
    "                print(f\"  Lowest FN: {model_fn['display_name']} ({best_fn_idx}) with {model_fn['false_negatives']:.1f} false negatives\")\n",
    "            \n",
    "            # Check for TP/FP tradeoff (a key concern for F2)\n",
    "            print(\"\\n  TP/FP TRADEOFF ANALYSIS:\")\n",
    "            tp_fp_ratio = metrics_df['true_positives'] / metrics_df['false_positives'].replace(0, np.nan)\n",
    "            best_ratio_idx = tp_fp_ratio.idxmax()\n",
    "            if pd.notna(best_ratio_idx):\n",
    "                model_ratio = metrics_df.loc[best_ratio_idx]\n",
    "                print(f\"  Best TP/FP ratio: {model_ratio['display_name']} ({best_ratio_idx}) with ratio {tp_fp_ratio[best_ratio_idx]:.2f}\")\n",
    "                print(f\"    TP: {model_ratio['true_positives']:.1f}, FP: {model_ratio['false_positives']:.1f}\")\n",
    "            \n",
    "            # Final recommendations\n",
    "            print(\"\\nRECOMMENDATIONS BASED ON F2 OPTIMIZATION PRIORITY:\")\n",
    "            \n",
    "            llm_models = metrics_df[metrics_df['model_type'] != 'sentence_transformer_tf_idf']\n",
    "            if not llm_models.empty:\n",
    "                # Check if LLMs outperform sentence transformers on key metrics\n",
    "                st_avg_f2 = st_metrics['f2_score'].values[0]\n",
    "                llm_avg_f2 = llm_models['f2_score'].mean()\n",
    "                \n",
    "                if llm_avg_f2 > st_avg_f2:\n",
    "                    print(\"  1. LLM models generally outperform Sentence Transformer/TF-IDF on F2 score\")\n",
    "                    \n",
    "                    # Find best LLM model for F2\n",
    "                    best_llm_f2_idx = llm_models['f2_score'].idxmax()\n",
    "                    if pd.notna(best_llm_f2_idx):\n",
    "                        best_llm = llm_models.loc[best_llm_f2_idx]\n",
    "                        print(f\"  2. Recommended model: {best_llm_f2_idx} (F2: {best_llm['f2_score']:.3f})\")\n",
    "                        \n",
    "                        # Analyze why this model performs well\n",
    "                        print(f\"  3. This model performs well because:\", end=\" \")\n",
    "                        \n",
    "                        reasons = []\n",
    "                        if best_llm['true_positives'] > st_metrics['true_positives'].values[0]:\n",
    "                            reasons.append(f\"higher TP ({best_llm['true_positives']:.1f} vs {st_metrics['true_positives'].values[0]:.1f})\")\n",
    "                        \n",
    "                        if best_llm['false_negatives'] < st_metrics['false_negatives'].values[0]:\n",
    "                            reasons.append(f\"lower FN ({best_llm['false_negatives']:.1f} vs {st_metrics['false_negatives'].values[0]:.1f})\")\n",
    "                        \n",
    "                        if reasons:\n",
    "                            print(\", \".join(reasons))\n",
    "                        else:\n",
    "                            print(\"balanced performance across metrics\")\n",
    "                else:\n",
    "                    print(\"  1. Sentence Transformer/TF-IDF models still perform better on F2 score compared to LLMs\")\n",
    "                    print(f\"  2. Recommended model: Best sentence transformer (F2: {st_avg_f2:.3f})\")\n",
    "            \n",
    "            # Suggestions for improvement\n",
    "            print(\"\\n  SUGGESTIONS FOR IMPROVEMENT:\")\n",
    "            # Look at where the best model is weak\n",
    "            if pd.notna(best_f2_idx):\n",
    "                best_model = metrics_df.loc[best_f2_idx]\n",
    "                \n",
    "                weaknesses = []\n",
    "                if best_model['false_negatives'] > metrics_df['false_negatives'].min():\n",
    "                    weakness_diff = best_model['false_negatives'] - metrics_df['false_negatives'].min()\n",
    "                    weaknesses.append(f\"reduce false negatives (current: {best_model['false_negatives']:.1f}, potential improvement: {weakness_diff:.1f})\")\n",
    "                \n",
    "                if best_model['true_positives'] < metrics_df['true_positives'].max():\n",
    "                    weakness_diff = metrics_df['true_positives'].max() - best_model['true_positives']\n",
    "                    weaknesses.append(f\"increase true positives (current: {best_model['true_positives']:.1f}, potential improvement: {weakness_diff:.1f})\")\n",
    "                \n",
    "                if weaknesses:\n",
    "                    print(\"    Areas for further optimization: \" + \", \".join(weaknesses))\n",
    "                else:\n",
    "                    print(\"    The best model already optimizes the key metrics for F2 score\")\n",
    "        else:\n",
    "            print(\"\\nNo Sentence Transformer/TF-IDF baseline for comparison\")\n",
    "\n",
    "# Run the analysis on confusion matrix data\n",
    "analyze_confusion_matrix_improvements(confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [10] - Project-Based LLM Type Improvement Whisker Charts\n",
    "# Purpose: Generate detailed LLM improvement analysis with whisker charts by project and model type\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy\n",
    "# Breadcrumbs: Visualization -> LLM Improvement Analysis\n",
    "\n",
    "def create_project_llm_improvement_whiskers(confusion_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive charts showing LLM type improvements over Sentence Transformer baselines\n",
    "    \n",
    "    Parameters:\n",
    "        confusion_df (pd.DataFrame): DataFrame containing confusion matrix metrics with columns:\n",
    "                                    - project_name, model_type, model_name, metric, value\n",
    "                                    \n",
    "    Returns:\n",
    "        None: Displays visualizations and summary tables directly\n",
    "        \n",
    "    Notes:\n",
    "        - Creates one chart per project with grouped whiskers for each LLM type\n",
    "        - Shows percentage improvements for TP, FP, FN (inverted), TN relative to baseline\n",
    "        - Uses Sentence Transformer/TF-IDF as baseline for comparison calculations\n",
    "        - Generates detailed summary statistics tables by LLM type and metric\n",
    "        - Provides model instance breakdown showing individual performance\n",
    "        - Uses consistent color scheme across all visualizations\n",
    "        - Includes mean value annotations on boxplots for easy interpretation\n",
    "    \"\"\"\n",
    "    if confusion_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Metric labels for display\n",
    "    metric_labels = {\n",
    "        'true_positives': 'TP',\n",
    "        'false_positives': 'FP',\n",
    "        'false_negatives': 'FN (inv)',\n",
    "        'true_negatives': 'TN'\n",
    "    }\n",
    "    \n",
    "    # Get list of all projects\n",
    "    projects = confusion_df['project_name'].unique()\n",
    "    \n",
    "    # Process each project separately\n",
    "    for project in projects:\n",
    "        print(f\"\\nAnalyzing project: {project}\")\n",
    "        \n",
    "        # Get data for this project\n",
    "        project_data = confusion_df[confusion_df['project_name'] == project]\n",
    "        \n",
    "        # Extract sentence transformer data for baseline\n",
    "        st_data = project_data[project_data['model_type'] == 'sentence_transformer_tf_idf']\n",
    "        \n",
    "        if st_data.empty:\n",
    "            print(f\"  No Sentence Transformer baseline found for project {project}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate baseline values for each metric\n",
    "        st_baseline = {}\n",
    "        for metric in metric_labels.keys():\n",
    "            metric_data = st_data[st_data['metric'] == metric]\n",
    "            if not metric_data.empty:\n",
    "                st_baseline[metric] = metric_data['value'].mean()\n",
    "                print(f\"  {metric} baseline: {st_baseline[metric]:.1f}\")\n",
    "            else:\n",
    "                st_baseline[metric] = None\n",
    "                print(f\"  {metric} baseline: None\")\n",
    "        \n",
    "        # Get all LLM data for this project\n",
    "        llm_data = project_data[project_data['model_type'] != 'sentence_transformer_tf_idf']\n",
    "        \n",
    "        if llm_data.empty:\n",
    "            print(f\"  No LLM models found for project {project}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # We need to correctly identify the LLM model types\n",
    "        # First, check what we have\n",
    "        print(\"\\nLLM model information available:\")\n",
    "        display(llm_data[['model_type', 'model_name']].drop_duplicates().head(10))\n",
    "        \n",
    "        # The actual LLM types (like \"claude-3-7-sonnet\") are in model_type\n",
    "        # Let's use that as our grouping variable instead of model_name\n",
    "        llm_types = llm_data['model_type'].unique()\n",
    "        print(f\"  Found {len(llm_types)} LLM types: {', '.join(llm_types)}\")\n",
    "        \n",
    "        # Create a DataFrame to hold improvement data\n",
    "        improvement_data = []\n",
    "        \n",
    "        # Calculate improvements for each LLM type and metric\n",
    "        for llm_type in llm_types:\n",
    "            type_data = llm_data[llm_data['model_type'] == llm_type]\n",
    "            \n",
    "            for metric in metric_labels.keys():\n",
    "                metric_data = type_data[type_data['metric'] == metric]\n",
    "                \n",
    "                if not metric_data.empty and st_baseline[metric] is not None and st_baseline[metric] > 0:\n",
    "                    for _, row in metric_data.iterrows():\n",
    "                        # Calculate percentage improvement\n",
    "                        improvement = ((row['value'] - st_baseline[metric]) / st_baseline[metric]) * 100\n",
    "                        \n",
    "                        # For FN, invert so negative means fewer FNs (which is good)\n",
    "                        if metric == 'false_negatives':\n",
    "                            improvement = -improvement\n",
    "                            \n",
    "                        improvement_data.append({\n",
    "                            'LLM Type': llm_type,\n",
    "                            'Model Name': row['model_name'],\n",
    "                            'Metric': metric,\n",
    "                            'Metric Display': metric_labels[metric],\n",
    "                            'Improvement (%)': improvement,\n",
    "                            'Original Value': row['value'],\n",
    "                            'Baseline Value': st_baseline[metric]\n",
    "                        })\n",
    "        \n",
    "        # Skip if no improvement data\n",
    "        if not improvement_data:\n",
    "            print(f\"  No valid improvement data for project {project}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        improvements_df = pd.DataFrame(improvement_data)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(16, 7))\n",
    "        \n",
    "        # Create grouped box plot\n",
    "        # X-axis grouped by LLM type, with each group having 4 boxes (TP, FP, FN, TN)\n",
    "        ax = sns.boxplot(\n",
    "            x='LLM Type',\n",
    "            y='Improvement (%)',\n",
    "            hue='Metric Display',\n",
    "            data=improvements_df,\n",
    "            palette='Set3',\n",
    "            width=0.8\n",
    "        )\n",
    "        \n",
    "        # Add reference line at 0%\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add mean value labels to each column\n",
    "        # First, calculate means for each combination\n",
    "        means = improvements_df.groupby(['LLM Type', 'Metric Display'])['Improvement (%)'].mean().reset_index()\n",
    "        \n",
    "        # Get the x-tick positions\n",
    "        x_ticks = list(range(len(llm_types)))\n",
    "        \n",
    "        # Iterate through means and add text annotations\n",
    "        for llm_idx, llm_type in enumerate(llm_types):\n",
    "            metrics_for_llm = means[means['LLM Type'] == llm_type]\n",
    "            \n",
    "            # Calculate width of each box\n",
    "            box_width = 0.8 / len(metric_labels)\n",
    "            \n",
    "            for metric_idx, (_, row) in enumerate(metrics_for_llm.iterrows()):\n",
    "                metric = row['Metric Display']\n",
    "                mean_val = row['Improvement (%)']\n",
    "                \n",
    "                # Calculate position (adjust as needed based on box positions)\n",
    "                x_pos = llm_idx + (metric_idx - 1.5) * box_width\n",
    "                \n",
    "                # Add text annotation\n",
    "                ax.text(\n",
    "                    x_pos, \n",
    "                    mean_val + (5 if mean_val > 0 else -15),  # Adjust y position\n",
    "                    f\"{mean_val:.1f}%\",\n",
    "                    ha='center',\n",
    "                    va='bottom' if mean_val > 0 else 'top',\n",
    "                    fontsize=9,\n",
    "                    fontweight='bold',\n",
    "                    color='blue'\n",
    "                )\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f\"LLM Type Improvements Over Sentence Transformer - Project: {project}\", fontsize=16)\n",
    "        plt.ylabel(\"Percentage Improvement (%)\", fontsize=14)\n",
    "        plt.xlabel(\"LLM Type\", fontsize=14)\n",
    "        \n",
    "        # Adjust legend\n",
    "        plt.legend(title='Metric', fontsize=12)\n",
    "        \n",
    "        # Add grid\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate model names for readability\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary table\n",
    "        print(\"\\nSummary Statistics by LLM Type and Metric:\")\n",
    "        \n",
    "        # Pivot the data for a better summary\n",
    "        summary = pd.pivot_table(\n",
    "            improvements_df, \n",
    "            values='Improvement (%)',\n",
    "            index=['LLM Type'], \n",
    "            columns=['Metric Display'],\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Add average column\n",
    "        summary['Average'] = summary.mean(axis=1)\n",
    "        \n",
    "        # Sort by average improvement\n",
    "        summary = summary.sort_values('Average', ascending=False)\n",
    "        \n",
    "        # Display the summary\n",
    "        display(summary)\n",
    "        \n",
    "        # Show detailed model instance breakdown\n",
    "        print(\"\\nDetailed breakdown by model instance:\")\n",
    "        model_breakdown = pd.pivot_table(\n",
    "            improvements_df,\n",
    "            values='Improvement (%)',\n",
    "            index=['LLM Type', 'Model Name'],\n",
    "            columns=['Metric Display'],\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Add average column\n",
    "        model_breakdown['Average'] = model_breakdown.mean(axis=1)\n",
    "        \n",
    "        # Sort by LLM Type and then by Average\n",
    "        model_breakdown = model_breakdown.sort_values(['LLM Type', 'Average'], ascending=[True, False])\n",
    "        \n",
    "        display(model_breakdown)\n",
    "\n",
    "# Run the visualization function\n",
    "create_project_llm_improvement_whiskers(confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [11] - Role-Based MCC Comparison by LLM Type and Project\n",
    "# Purpose: Comprehensive analysis of Matthews Correlation Coefficient across roles and model types\n",
    "# Dependencies: pandas, matplotlib, seaborn, numpy, re\n",
    "# Breadcrumbs: Visualization -> Role-Based Performance Analysis\n",
    "\n",
    "def create_role_mcc_comparison(model_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing MCC scores across different roles by LLM type per project\n",
    "    \n",
    "    Parameters:\n",
    "        model_df (pd.DataFrame): DataFrame containing model performance data from Neo4j with columns:\n",
    "                                - project_name, model_type, model_data_parsed, display_model_type\n",
    "                                \n",
    "    Returns:\n",
    "        None: Displays visualizations and summary tables directly\n",
    "        \n",
    "    Notes:\n",
    "        - Extracts role information from model names (actor, judge, meta_judge)\n",
    "        - Focuses on Matthews Correlation Coefficient as primary metric\n",
    "        - Creates grouped boxplots comparing roles within each model type\n",
    "        - Maintains specific role order: actor, judge, meta_judge\n",
    "        - Handles final_score as meta_judge role to avoid duplicates\n",
    "        - Generates separate visualizations for model types and individual LLMs\n",
    "        - Includes statistical summary tables with mean and count aggregations\n",
    "        - Uses stripplot overlay for individual data point visibility\n",
    "    \"\"\"\n",
    "    if model_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Extract role information from model names\n",
    "    role_data = []\n",
    "    \n",
    "    # Process model data to extract roles and MCC scores\n",
    "    for _, row in model_df.iterrows():\n",
    "        project_name = row['project_name']\n",
    "        model_type = row['display_model_type']\n",
    "        model_data = row['model_data_parsed']\n",
    "        \n",
    "        if not model_data or not isinstance(model_data, dict):\n",
    "            continue\n",
    "            \n",
    "        # Find Matthews correlation data\n",
    "        mcc_data = None\n",
    "        for metric_key, models in model_data.items():\n",
    "            if 'matthews' in metric_key.lower() or 'matthews_corr' in metric_key.lower():\n",
    "                mcc_data = models\n",
    "                break\n",
    "                \n",
    "        if not mcc_data:\n",
    "            continue\n",
    "        \n",
    "        # Find the final_score (meta_judge) if it exists\n",
    "        final_score = None\n",
    "        for model_name, score in mcc_data.items():\n",
    "            if 'final_score' in model_name.lower():\n",
    "                final_score = score\n",
    "                break\n",
    "            \n",
    "        # Extract role from model names and organize data\n",
    "        for model_name, score in mcc_data.items():\n",
    "            # More precise role extraction\n",
    "            if 'actor' in model_name.lower() and 'meta' not in model_name.lower():\n",
    "                role = 'actor'\n",
    "            elif 'judge' in model_name.lower() and 'meta' not in model_name.lower():\n",
    "                role = 'judge'\n",
    "            elif 'meta_judge' in model_name.lower() or 'meta judge' in model_name.lower() or 'final_score' in model_name.lower():\n",
    "                # Skip all meta_judge entries except final_score to avoid duplicates\n",
    "                if 'final_score' not in model_name.lower():\n",
    "                    continue\n",
    "                role = 'meta_judge'\n",
    "            else:\n",
    "                # Skip unknown roles\n",
    "                continue\n",
    "                \n",
    "            # Extract LLM name (if available)\n",
    "            llm_match = re.search(r'(claude|gpt|gemini|llama|mistral)', model_name.lower())\n",
    "            llm_name = llm_match.group(1) if llm_match else \"unknown\"\n",
    "            \n",
    "            # Add to dataset\n",
    "            role_data.append({\n",
    "                'project': project_name,\n",
    "                'model_type': model_type,\n",
    "                'model_name': model_name,\n",
    "                'role': role,\n",
    "                'llm_name': llm_name,\n",
    "                'mcc_score': score\n",
    "            })\n",
    "            \n",
    "        # If we found a final_score but it wasn't added through the loop above\n",
    "        # (because it wasn't named properly), add it explicitly\n",
    "        if final_score is not None:\n",
    "            meta_judge_exists = any(item['role'] == 'meta_judge' and item['model_type'] == model_type \n",
    "                                   for item in role_data if item['project'] == project_name)\n",
    "            \n",
    "            if not meta_judge_exists:\n",
    "                # Add final_score as meta_judge\n",
    "                role_data.append({\n",
    "                    'project': project_name,\n",
    "                    'model_type': model_type,\n",
    "                    'model_name': 'final_score',\n",
    "                    'role': 'meta_judge',\n",
    "                    'llm_name': \"unknown\",\n",
    "                    'mcc_score': final_score\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    role_df = pd.DataFrame(role_data)\n",
    "    \n",
    "    if role_df.empty:\n",
    "        print(\"No role-based MCC data found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    # Filter out Sentence Transformer/TF-IDF data points\n",
    "    role_df = role_df[role_df['model_type'] != 'Sentence Transformer/TF-IDF']\n",
    "        \n",
    "    # Display summary of extracted data\n",
    "    print(f\"Extracted MCC data for {role_df['project'].nunique()} projects, \"\n",
    "          f\"{role_df['model_type'].nunique()} model types, and \"\n",
    "          f\"{role_df['role'].nunique()} roles\")\n",
    "    \n",
    "    # Show unique roles found\n",
    "    print(\"\\nRoles found in the data:\")\n",
    "    for role in sorted(role_df['role'].unique()):\n",
    "        print(f\"  - {role}\")\n",
    "    \n",
    "    # Set the order of roles (actor, judge, meta_judge)\n",
    "    role_order = ['actor', 'judge', 'meta_judge']\n",
    "    \n",
    "    # Create visualizations for each project\n",
    "    for project in role_df['project'].unique():\n",
    "        project_data = role_df[role_df['project'] == project]\n",
    "        \n",
    "        print(f\"\\nCreating visualization for project: {project}\")\n",
    "        print(f\"  Model types: {', '.join(project_data['model_type'].unique())}\")\n",
    "        print(f\"  Roles: {', '.join(project_data['role'].unique())}\")\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create grouped boxplot with specific role order\n",
    "        ax = sns.boxplot(\n",
    "            x='model_type',\n",
    "            y='mcc_score',\n",
    "            hue='role',\n",
    "            data=project_data,\n",
    "            palette='Set2',\n",
    "            width=0.8,\n",
    "            showfliers=False,\n",
    "            hue_order=role_order  # Specify the order of roles\n",
    "        )\n",
    "        \n",
    "        # Add individual data points with jitter\n",
    "        sns.stripplot(\n",
    "            x='model_type',\n",
    "            y='mcc_score',\n",
    "            hue='role',\n",
    "            data=project_data,\n",
    "            palette='Set2',\n",
    "            size=6,\n",
    "            alpha=0.7,\n",
    "            jitter=True,\n",
    "            dodge=True,\n",
    "            marker='o',\n",
    "            linewidth=1,\n",
    "            edgecolor='black',\n",
    "            legend=False,\n",
    "            hue_order=role_order  # Specify the order of roles\n",
    "        )\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f\"Matthews Correlation Coefficient by Role and Model Type - {project}\", fontsize=16)\n",
    "        plt.xlabel(\"Model Type\", fontsize=14)\n",
    "        plt.ylabel(\"Matthews Correlation Coefficient\", fontsize=14)\n",
    "        plt.ylim(0, 1.0)  # MCC is typically in range [-1, 1] but most values are positive\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Update the legend with the correct order\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles=handles[:3], labels=labels[:3], title=\"Role\", fontsize=12)\n",
    "        \n",
    "        # Set x-axis tick labels at 45 degrees with right alignment\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add mean value annotations\n",
    "        means = project_data.groupby(['model_type', 'role'])['mcc_score'].mean().reset_index()\n",
    "        \n",
    "        for _, row in means.iterrows():\n",
    "            # Calculate position based on the ordered roles\n",
    "            model_types = sorted(project_data['model_type'].unique())\n",
    "            model_idx = model_types.index(row['model_type'])\n",
    "            \n",
    "            # Get the role index based on our ordered list\n",
    "            if row['role'] in role_order:\n",
    "                role_idx = role_order.index(row['role'])\n",
    "                \n",
    "                # Calculate offset for each role\n",
    "                offset = (role_idx - (len(role_order) - 1) / 2) * (0.8 / len(role_order))\n",
    "                \n",
    "                # Add annotation\n",
    "                plt.text(\n",
    "                    model_idx + offset, \n",
    "                    row['mcc_score'] + 0.03,\n",
    "                    f\"{row['mcc_score']:.3f}\",\n",
    "                    ha='center',\n",
    "                    va='bottom',\n",
    "                    fontsize=9,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{project}_role_mcc_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a summary table with roles in the correct order\n",
    "        print(\"\\nSummary Statistics by Model Type and Role:\")\n",
    "        \n",
    "        # Ensure roles appear in the desired order in the pivot table\n",
    "        summary = pd.pivot_table(\n",
    "            project_data,\n",
    "            values='mcc_score',\n",
    "            index=['model_type'],\n",
    "            columns=['role'],\n",
    "            aggfunc=['mean', 'count']\n",
    "        )\n",
    "        \n",
    "        # Flatten hierarchical index for better display\n",
    "        summary.columns = [f\"{agg}_{role}\" for agg, role in summary.columns]\n",
    "        display(summary)\n",
    "        \n",
    "        # Optional: LLM-specific analysis if LLM info available\n",
    "        if 'llm_name' in project_data.columns and project_data['llm_name'].nunique() > 1:\n",
    "            print(\"\\nLLM-specific MCC Comparison:\")\n",
    "            \n",
    "            # Filter out unknown LLMs\n",
    "            llm_data = project_data[project_data['llm_name'] != 'unknown']\n",
    "            \n",
    "            if not llm_data.empty:\n",
    "                # Create figure for LLM comparison\n",
    "                plt.figure(figsize=(16, 8))\n",
    "                \n",
    "                # Create grouped boxplot by LLM with specified role order\n",
    "                sns.boxplot(\n",
    "                    x='llm_name',\n",
    "                    y='mcc_score',\n",
    "                    hue='role',\n",
    "                    data=llm_data,\n",
    "                    palette='Set3',\n",
    "                    width=0.8,\n",
    "                    showfliers=False,\n",
    "                    hue_order=role_order  # Specify role order\n",
    "                )\n",
    "                \n",
    "                # Add jittered points\n",
    "                sns.stripplot(\n",
    "                    x='llm_name',\n",
    "                    y='mcc_score',\n",
    "                    hue='role',\n",
    "                    data=llm_data,\n",
    "                    palette='Set3',\n",
    "                    size=6,\n",
    "                    alpha=0.7,\n",
    "                    jitter=True,\n",
    "                    dodge=True,\n",
    "                    marker='o',\n",
    "                    linewidth=1,\n",
    "                    edgecolor='black',\n",
    "                    legend=False,\n",
    "                    hue_order=role_order  # Specify role order\n",
    "                )\n",
    "                \n",
    "                # Customize\n",
    "                plt.title(f\"Matthews Correlation by Role and LLM - {project}\", fontsize=16)\n",
    "                plt.xlabel(\"LLM Name\", fontsize=14)\n",
    "                plt.ylabel(\"Matthews Correlation Coefficient\", fontsize=14)\n",
    "                plt.ylim(0, 1.0)\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Update the legend with the correct order\n",
    "                handles, labels = plt.gca().get_legend_handles_labels()\n",
    "                plt.legend(handles=handles[:3], labels=labels[:3], title=\"Role\", fontsize=12)\n",
    "                \n",
    "                # Set x-axis tick labels at 45 degrees with right alignment\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{project}_llm_role_mcc_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                # Show LLM summary table\n",
    "                llm_summary = pd.pivot_table(\n",
    "                    llm_data,\n",
    "                    values='mcc_score',\n",
    "                    index=['llm_name'],\n",
    "                    columns=['role'],\n",
    "                    aggfunc=['mean', 'count']\n",
    "                )\n",
    "                \n",
    "                llm_summary.columns = [f\"{agg}_{role}\" for agg, role in llm_summary.columns]\n",
    "                display(llm_summary)\n",
    "\n",
    "# Run the role-based MCC comparison\n",
    "create_role_mcc_comparison(model_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-LLM Requirements Analysis Testing\n",
    "**Testing of multiple LLM models for requirements analysis workflows using Neo4j data and the LLM Manager framework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [0] - Setup and Imports\n",
    "# Purpose: Import all required libraries and configure environment settings for Multi-LLM testing\n",
    "# Dependencies: asyncio, sys, os, logging, pathlib, dotenv, transformers, src modules\n",
    "# Breadcrumbs: Setup -> Imports -> Environment Configuration\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configure Python path, logging, and load environment variables\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters including MODEL_TEST flag and project paths\n",
    "    \"\"\"\n",
    "    # Get the absolute path to the project root directory (parent of notebooks)\n",
    "    project_root = Path.cwd().parent\n",
    "    src_path = project_root / 'src'\n",
    "    \n",
    "    # Add the project root to Python path if not already there\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    \n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Configuration from environment variables\n",
    "    config = {\n",
    "        'PROJECT_ROOT': project_root,\n",
    "        'SRC_PATH': src_path,\n",
    "        'MODEL_TEST': os.getenv('MODEL_TEST', 'False').lower() in ('true', '1', 't'),\n",
    "        'NEO4J_URI': os.getenv('NEO4J_URI'),\n",
    "        'NEO4J_USER': os.getenv('NEO4J_USER'), \n",
    "        'NEO4J_PASSWORD': os.getenv('NEO4J_PASSWORD'),\n",
    "        'NEO4J_DATABASE': os.getenv('NEO4J_DATABASE'),\n",
    "        'NEO4J_PROJECT_NAME': os.getenv('NEO4J_PROJECT_NAME')\n",
    "    }\n",
    "    \n",
    "    print(f\"Project root added to path: {project_root}\")\n",
    "    print(f\"MODEL_TEST environment variable: {config['MODEL_TEST']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Execute setup when imported\n",
    "CONFIG = setup_environment()\n",
    "\n",
    "# Import project modules after path setup\n",
    "from praxis_requirements_analyzer.utils.logger import setup_logger\n",
    "from praxis_requirements_analyzer.llm.manager.llm_manager import LLMManager\n",
    "from praxis_requirements_analyzer.neo4j import Neo4jClient, RequirementsClient, SchemaExtractor\n",
    "from praxis_requirements_analyzer.requirements_analyzer.requirements_workflow import RequirementsWorkflow\n",
    "from praxis_requirements_analyzer.requirements_analyzer.requirements_prompt_manager import RequirementsPromptManager\n",
    "\n",
    "# Set up debug logging for requirements analysis\n",
    "requirements_logger = setup_logger(\"praxis_requirements_analyzer.requirements_analyzer.requirements_workflow\", logging.DEBUG)\n",
    "prompt_logger = setup_logger(\"praxis_requirements_analyzer.requirements_analyzer.requirements_prompt_manager\", logging.DEBUG)\n",
    "\n",
    "# Initialize LLM Manager\n",
    "llm_manager = LLMManager()\n",
    "\n",
    "# Optional: Import transformers for token estimation if MODEL_TEST is enabled\n",
    "if CONFIG['MODEL_TEST']:\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        print(\"Transformers library imported for token estimation\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: transformers library not available for token estimation\")\n",
    "        \n",
    "print(\"Setup completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [1] - Initialize LLM Models and Run Basic Tests\n",
    "# Purpose: Initialize the LLM Manager and run basic functionality tests on all models\n",
    "# Dependencies: LLMManager, transformers (if MODEL_TEST=True)\n",
    "# Breadcrumbs: Setup -> LLM Initialization -> Model Testing\n",
    "\n",
    "# Initialize LLM models\n",
    "await llm_manager.initialize_models()\n",
    "\n",
    "if CONFIG['MODEL_TEST']:\n",
    "    def estimate_tokens(text: str, model_name: str = \"HuggingFaceH4/zephyr-7b-beta\") -> int:\n",
    "        \"\"\"Estimate the number of tokens in a text string\"\"\"\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            return len(tokenizer.encode(text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error estimating tokens: {e}\")\n",
    "            return len(text.split()) * 2  # Rough fallback estimate\n",
    "\n",
    "    # Test prompt and system message\n",
    "    test_prompt = \"What is 2 + 2? Please provide only the direct answer.\"\n",
    "    system_message = \"You are a calculator. Provide only the numerical answer.\"\n",
    "\n",
    "    # Calculate total tokens needed\n",
    "    total_tokens = estimate_tokens(test_prompt + system_message)\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    max_tokens = min(512, total_tokens * 2)  # Set limit to 2x input tokens, max 512\n",
    "    print(f\"Max tokens: {max_tokens}\")\n",
    "\n",
    "    # Function to run tests\n",
    "    async def test_all_models():\n",
    "        tasks = []\n",
    "        for model_name in llm_manager.models.keys():\n",
    "            task = llm_manager.test_model(\n",
    "                model_name=model_name,\n",
    "                prompt=test_prompt,\n",
    "                system_message=system_message\n",
    "            )\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "    # Run the tests\n",
    "    results = await test_all_models()\n",
    "\n",
    "    # Display results\n",
    "    for result in results:\n",
    "        print(f\"\\nModel: {result['model']}\")\n",
    "        print(\"-\" * 50)\n",
    "        if \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"Response: {result['response']}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"MODEL_TEST is set to False - skipping LLM testing\")\n",
    "    print(\"Set MODEL_TEST=True in .env file to enable model testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [2] - Fetch Requirements from Neo4j Database\n",
    "# Purpose: Connect to Neo4j database and retrieve source and target requirements\n",
    "# Dependencies: Neo4jClient, RequirementsClient\n",
    "# Breadcrumbs: Setup -> Database Connection -> Requirements Retrieval\n",
    "\n",
    "async def fetch_requirements():\n",
    "    \"\"\"\n",
    "    Fetch requirements from Neo4j database using configured connection parameters\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'source' and 'target' requirement lists\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Neo4j client using configuration from setup\n",
    "        neo4j_client = Neo4jClient(\n",
    "            uri=CONFIG[\"NEO4J_URI\"],\n",
    "            user=CONFIG[\"NEO4J_USER\"],\n",
    "            password=CONFIG[\"NEO4J_PASSWORD\"],\n",
    "            database=CONFIG[\"NEO4J_DATABASE\"]\n",
    "        )\n",
    "\n",
    "        # Connect to Neo4j\n",
    "        neo4j_client.connect()\n",
    "        print(f\"Connected to Neo4j database: {CONFIG['NEO4J_DATABASE']}\")\n",
    "\n",
    "        # Initialize requirements client\n",
    "        requirements_client = RequirementsClient(\n",
    "            neo4j_client=neo4j_client,\n",
    "            project_name=CONFIG[\"NEO4J_PROJECT_NAME\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Fetch requirements\n",
    "            requirements = await requirements_client.get_requirements()\n",
    "\n",
    "            # Display results summary\n",
    "            print(f\"\\nRequirements Summary for Project: {CONFIG['NEO4J_PROJECT_NAME']}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Source requirements: {len(requirements['source'])}\")\n",
    "            print(f\"Target requirements: {len(requirements['target'])}\")\n",
    "\n",
    "            # Show sample source requirements\n",
    "            if requirements['source']:\n",
    "                print(f\"\\nSample Source Requirements (showing first 3):\")\n",
    "                print(\"-\" * 50)\n",
    "                for i, req in enumerate(requirements['source'][:3]):\n",
    "                    print(f\"\\n{i+1}. ID: {req.id}\")\n",
    "                    print(f\"   Type: {req.type}\")\n",
    "                    print(f\"   Content: {req.content[:150]}...\")\n",
    "\n",
    "            # Show sample target requirements\n",
    "            if requirements['target']:\n",
    "                print(f\"\\nSample Target Requirements (showing first 3):\")\n",
    "                print(\"-\" * 50)\n",
    "                for i, req in enumerate(requirements['target'][:3]):\n",
    "                    print(f\"\\n{i+1}. ID: {req.id}\")\n",
    "                    print(f\"   Type: {req.type}\")\n",
    "                    print(f\"   Content: {req.content[:150]}...\")\n",
    "\n",
    "            return requirements\n",
    "\n",
    "        finally:\n",
    "            # Close Neo4j connection when done\n",
    "            neo4j_client.close()\n",
    "            print(\"\\nNeo4j connection closed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching requirements: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute the fetch operation\n",
    "requirements = await fetch_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Filter and Display Source Requirements\n",
    "# Purpose: Filter requirements to get only SOURCE type requirements\n",
    "# Dependencies: requirements data from Cell 2\n",
    "# Breadcrumbs: Requirements Retrieval -> Source Filtering -> Display\n",
    "\n",
    "# Filter to get only SOURCE type requirements\n",
    "source_requirements = {\n",
    "    'source': list(filter(lambda req: req.type == 'SOURCE', requirements['source']))\n",
    "}\n",
    "\n",
    "print(f\"Filtered Source Requirements:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total SOURCE requirements: {len(source_requirements['source'])}\")\n",
    "\n",
    "# Display first few source requirements for verification\n",
    "if source_requirements['source']:\n",
    "    print(f\"\\nFirst 3 Source Requirements:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, req in enumerate(source_requirements['source'][:3]):\n",
    "        print(f\"\\n{i+1}. ID: {req.id}\")\n",
    "        print(f\"   Type: {req.type}\")\n",
    "        print(f\"   Content: {req.content[:100]}...\")\n",
    "\n",
    "source_requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [4] - Filter and Display Target Requirements\n",
    "# Purpose: Filter requirements to get only TARGET type requirements\n",
    "# Dependencies: requirements data from Cell 2\n",
    "# Breadcrumbs: Requirements Retrieval -> Target Filtering -> Display\n",
    "\n",
    "# Filter to get only TARGET type requirements\n",
    "target_requirements = {\n",
    "    'target': list(filter(lambda req: req.type == 'TARGET', requirements['target']))\n",
    "}\n",
    "\n",
    "print(f\"Filtered Target Requirements:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total TARGET requirements: {len(target_requirements['target'])}\")\n",
    "\n",
    "# Display first few target requirements for verification\n",
    "if target_requirements['target']:\n",
    "    print(f\"\\nFirst 3 Target Requirements:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, req in enumerate(target_requirements['target'][:3]):\n",
    "        print(f\"\\n{i+1}. ID: {req.id}\")\n",
    "        print(f\"   Type: {req.type}\")\n",
    "        print(f\"   Content: {req.content[:100]}...\")\n",
    "\n",
    "target_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [5] - Requirements Analysis Workflow Testing\n",
    "# Purpose: Test the requirements analysis workflow using multiple LLM models\n",
    "# Dependencies: RequirementsWorkflow, RequirementsPromptManager, filtered requirements\n",
    "# Breadcrumbs: Requirements Filtering -> Workflow Initialization -> Multi-Model Analysis\n",
    "\n",
    "# Initialize prompt manager with default settings\n",
    "prompt_manager = RequirementsPromptManager()\n",
    "\n",
    "# Create workflow instance using our existing llm_manager\n",
    "workflow = RequirementsWorkflow(llm_manager, prompt_manager)\n",
    "\n",
    "# Get first 3 requirements of each type for testing\n",
    "source_reqs = source_requirements['source'][:3]\n",
    "target_reqs = target_requirements['target'][:3]\n",
    "\n",
    "print(f\"Requirements Analysis Configuration:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source requirements to test: {len(source_reqs)}\")\n",
    "print(f\"Target requirements to test: {len(target_reqs)}\")\n",
    "print(f\"Available LLM models: {list(llm_manager.models.keys())}\")\n",
    "\n",
    "async def process_requirements(reqs, req_type=\"Source\"):\n",
    "    \"\"\"\n",
    "    Process requirements using the analysis workflow with multiple LLM models\n",
    "    \n",
    "    Parameters:\n",
    "        reqs: List of requirements to process\n",
    "        req_type: Type identifier for display purposes (\"Source\" or \"Target\")\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting {req_type} Requirements:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, req in enumerate(reqs, 1):\n",
    "        print(f\"\\nProcessing {req_type} Requirement {i}/{len(reqs)} - ID: {req.id}\")\n",
    "        print(f\"Content Preview: {req.content[:200]}...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Run analysis workflow on the requirement\n",
    "            results = await workflow.test_models(req)\n",
    "            \n",
    "            # Display results for each model\n",
    "            for model_name, result in results.items():\n",
    "                print(f\"\\nModel: {model_name}\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "                if \"error\" in result:\n",
    "                    print(f\"ERROR: {result['error']}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each analysis type\n",
    "                analysis_types = [\n",
    "                    \"verification\",\n",
    "                    \"validation\", \n",
    "                    \"quality\",\n",
    "                    \"structure\",\n",
    "                    \"matching\"\n",
    "                ]\n",
    "                \n",
    "                analysis_completed = 0\n",
    "                for analysis_type in analysis_types:\n",
    "                    analysis_result = result.get(analysis_type)\n",
    "                    if analysis_result:\n",
    "                        analysis_completed += 1\n",
    "                        print(f\"\\n{analysis_type.title()} Analysis:\")\n",
    "                        print(\"-\" * 30)\n",
    "                        print(f\"Model Used: {analysis_result['model']}\")\n",
    "                        \n",
    "                        # Format response for better readability\n",
    "                        response = analysis_result['response']\n",
    "                        if len(response) > 150:\n",
    "                            print(f\"Response: {response[:150]}... [truncated]\")\n",
    "                        else:\n",
    "                            print(f\"Response: {response}\")\n",
    "                \n",
    "                print(f\"\\nAnalysis Summary: {analysis_completed}/{len(analysis_types)} completed\")\n",
    "                print(\"=\" * 40)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing requirement {req.id}: {str(e)}\")\n",
    "\n",
    "print(\"\\nStarting Requirements Analysis Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process both source and target requirements\n",
    "if source_reqs:\n",
    "    await process_requirements(source_reqs, \"Source\")\n",
    "else:\n",
    "    print(\"WARNING: No source requirements available for testing\")\n",
    "\n",
    "if target_reqs:\n",
    "    await process_requirements(target_reqs, \"Target\")\n",
    "else:\n",
    "    print(\"WARNING: No target requirements available for testing\")\n",
    "\n",
    "print(\"\\nRequirements Analysis Test Complete\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
